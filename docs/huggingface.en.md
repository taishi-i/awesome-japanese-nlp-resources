# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
    [![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
    [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
    [![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
    [![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

A curated list of resources dedicated to Python libraries, llms, dictionaries, and corpora of NLP for Japanese
This page lists Japanese NLP-specific models and datasets available on Hugging Face. Currently, it includes 207 models and 43 datasets.

_Updated on Dec 15, 2025_

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Ranking](#Ranking)
   * [Models](#models-ranking)
   * [Datasets](#datasets-ranking)
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [sentence-similarity](#sentence-similarity)
   * [feature-extraction](#feature-extraction)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [translation](#translation)
   * [text-classification](#text-classification)
   * [text-ranking](#text-ranking)
   * [image-to-text](#image-to-text)
   * [token-classification](#token-classification)
   * [text-to-speech](#text-to-speech)
   * [question-answering](#question-answering)
   * [image-text-to-text](#image-text-to-text)
   * [audio-to-audio](#audio-to-audio)
   * [others](#others)
 * [Datasets](#Datasets)

## Ranking

### [Models](#models-ranking)

| # | Model | Downloads | Likes | Category |
|---|-------|-----------|-------|----------|
| 1 | [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) | ğŸ“¥ 6M | â­ 44 | automatic-speech-recognition |
| 2 | [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) | ğŸ“¥ 767k | â­ 13 | feature-extraction |
| 3 | [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) | ğŸ“¥ 540k | â­ 25 | token-classification |
| 4 | [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) | ğŸ“¥ 354k | â­ 11 | sentence-similarity |
| 5 | [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) | ğŸ“¥ 245k | â­ 70 | fill-mask |
| 6 | [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) | ğŸ“¥ 217k | â­ 15 | text-generation |
| 7 | [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | ğŸ“¥ 184k | â­ 158 | image-to-text |
| 8 | [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) | ğŸ“¥ 159k | â­ 56 | sentence-similarity |
| 9 | [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) | ğŸ“¥ 131k | â­ 6 | fill-mask |
| 10 | [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) | ğŸ“¥ 114k | â­ 1 | others |
| 11 | [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) | ğŸ“¥ 113k | â­ 66 | translation |
| 12 | [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) | ğŸ“¥ 106k | â­ 2 | others |
| 13 | [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) | ğŸ“¥ 102k | â­ 10 | others |
| 14 | [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) | ğŸ“¥ 101k | â­ 8 | fill-mask |
| 15 | [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) | ğŸ“¥ 88k | â­ 8 | translation |
| 16 | [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) | ğŸ“¥ 76k | â­ 38 | fill-mask |
| 17 | [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) | ğŸ“¥ 73k | â­ 33 | sentence-similarity |
| 18 | [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) | ğŸ“¥ 71k | â­ 13 | others |
| 19 | [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) | ğŸ“¥ 59k | â­ 8 | text-generation |
| 20 | [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) | ğŸ“¥ 55k | â­ 50 | feature-extraction |

### [Datasets](#datasets-ranking)

| # | Dataset | Downloads | Likes |
|---|---------|-----------|-------|
| 1 | [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) | ğŸ“¥ 56k | â­ 17 |
| 2 | [ASMR-Archive-Processed](https://huggingface.co/datasets/OmniAICreator/ASMR-Archive-Processed) | ğŸ“¥ 31k | â­ 37 |
| 3 | [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) | ğŸ“¥ 10k | â­ 8 |
| 4 | [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) | ğŸ“¥ 7k | â­ 8 |
| 5 | [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) | ğŸ“¥ 5k | â­ 22 |
| 6 | [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) | ğŸ“¥ 5k | â­ 5 |
| 7 | [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) | ğŸ“¥ 4k | â­ 18 |
| 8 | [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) | ğŸ“¥ 4k | â­ 24 |
| 9 | [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) | ğŸ“¥ 2k | â­ 12 |
| 10 | [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) | ğŸ“¥ 1k | â­ 5 |
| 11 | [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) | ğŸ“¥ 1k | â­ 134 |
| 12 | [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) | ğŸ“¥ 1k | â­ 20 |
| 13 | [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) | ğŸ“¥ 1k | â­ 37 |
| 14 | [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) | ğŸ“¥ 1k | â­ 15 |
| 15 | [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) | ğŸ“¥ 1k | â­ 23 |
| 16 | [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) | ğŸ“¥ 1k | â­ 4 |
| 17 | [japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) | ğŸ“¥ 1k | â­ 3 |
| 18 | [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) | ğŸ“¥ 992 | â­ 8 |
| 19 | [japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos) | ğŸ“¥ 955 | â­ 31 |
| 20 | [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) | ğŸ“¥ 895 | â­ 2 |

## Models
### text-generation
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - ğŸ“¥ 217k / â­ 15 / A lightweight 12â€‘layer, 768â€‘hidden Japanese GPTâ€‘NeoX model trained on CCâ€‘100, C4, and Wikipedia, deployable via Hugging Face and paired with a toy prefixâ€‘tuning weight that forces sentences to end with a smilingâ€‘face emoji.
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - ğŸ“¥ 59k / â­ 8 / LLMâ€‘jpâ€‘3.1â€‘1.8b is a 1.8â€¯b Japanese language model released by the National Institute of Informatics, preâ€‘trained with HuggingÂ Face Transformers, using a unigram tokenizer and distributed as a HuggingÂ Faceâ€‘compatible checkpoint.
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - ğŸ“¥ 48k / â­ 137 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced large language model built on Metaâ€¯Llamaâ€¯3â€¯8Bâ€¯Instruct, trained by ELYZA,â€¯Inc with additional preâ€‘training and instruction tuning, released under the Metaâ€¯Llamaâ€¯3 Community License.
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - ğŸ“¥ 40k / â­ 35 / TinySwallowâ€‘1.5B is a 1.5â€‘Bâ€‘parameter Japanese compact autoregressive language model distilled from Qwen2.5â€‘32Bâ€‘Instruct via TAID, preâ€‘trained on Japanese text, released under Apacheâ€¯2.0 for research with no commercial endorsement.
 * [Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3) - ğŸ“¥ 21k / â­ 14 / Japaneseâ€‘enhanced Llamaâ€¯3.1 models (8B and 70B) built by continual preâ€‘training on Metaâ€™s Llamaâ€¯3.1, featuring instructionâ€‘tuned variants such as 70Bâ€‘Instructâ€¯v0.3 that deliver detailed, conversationâ€‘aware responses and improved MTâ€‘Bench scores, released through Decemberâ€¯2024.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - ğŸ“¥ 17k / â­ 14 / Llamaâ€¯3.1â€¯Swallow offers Japaneseâ€‘enhanced 8B and 70B variants of Metaâ€™s Llamaâ€¯3.1, built via continual preâ€‘training and instructionâ€‘fineâ€‘tuning on synthetic Japanese data, with recent releases sharpening conversational abilities to emulate Gemmaâ€‘3â€‘27bâ€‘it.
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - ğŸ“¥ 17k / â­ 15 / Largeâ€‘language models (1.8â€‘13â€¯B parameters, transformerâ€‘based) from the National Institute of Informatics, released as Huggingâ€¯Face checkpoints (instructional and vanilla variants), pretrained on Japanese and multilingual corpora, and usable via standard torchâ€‘transformers with Japanese tokenizer.
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - ğŸ“¥ 16k / â­ 12 / LLMâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4 is a 1.8â€¯billionâ€‘parameter Japanese instructionâ€‘preâ€‘trained language model from NIIâ€™s LLMâ€‘jpâ€‘3.1 series, provided in Hugging Face Transformers format with specified dependencies and detailed architecture and tokenizer information.
 * [Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2) - ğŸ“¥ 13k / â­ 15 / Llamaâ€¯3.1â€¯Swallow delivers 8â€¯B and 70â€¯B Japaneseâ€‘enhanced versions of Metaâ€™s Llamaâ€¯3.1, trained through continuous preâ€‘training and instructionâ€‘tuned with synthetic Japanese data, with multiple released variants (v0.1â€“v0.3) and full details on the Swallow teamâ€™s website.
 * [llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3) - ğŸ“¥ 11k / â­ 4 / A 7.2â€¯Bâ€‘parameter Japanese LLMâ€‘jpâ€‘3â€‘7.2bâ€‘instruct3 from NIIâ€™s LLMâ€‘jpâ€‘3 series, packaged for Huggingâ€¯Face Transformers (torchâ€¯â‰¥â€¯2.3.0, transformersâ€¯â‰¥â€¯4.40.1) and pretrained on Japanese Wikipedia and Commonâ€¯Crawl.
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - ğŸ“¥ 11k / â­ 82 / 24â€‘layer, 1024â€‘hiddenâ€‘size Japanese GPTâ€‘2 medium trained on CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released Aprilâ€¯7â€¯2021 (updated Augâ€¯25â€¯2021) by rinna under MIT (see Huggingâ€¯Face and arXiv).
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - ğŸ“¥ 11k / â­ 25 / A 12â€‘layer, 768â€‘hidden transformerâ€‘based Japanese GPTâ€‘2 Small model trained on CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released Augâ€¯25â€¯2021 under the MIT license.
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - ğŸ“¥ 9k / â­ 4 / Experimental Japanese Llamaâ€‘3â€¯8B model created by extracting and upâ€‘sampling differences via a chatâ€‘vector approach from Metaâ€‘Llamaâ€‘3â€¯8Bâ€‘Instruct and applying them to Metaâ€‘Llamaâ€‘3â€¯70Bâ€‘Instruct, accompanied by its Huggingâ€¯Face model card.
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - ğŸ“¥ 8k / â­ 4 / Japaneseâ€‘enhanced 8B Llamaâ€‘3 from ELYZA, fineâ€‘tuned from Meta Llamaâ€¯3 and available in GGUF and AWQ quantizations for vLLM and OpenAIâ€‘style inference.
 * [llm-jp-3-13b](https://huggingface.co/llm-jp/llm-jp-3-13b) - ğŸ“¥ 7k / â­ 13 / Japanese-language Transformer models from NIIâ€™s Large Language Model R&D Centerâ€”including llmâ€‘jpâ€‘3â€‘1.8b, 3â€‘3.7b, 3â€‘13b and their instruct variantsâ€”are released as Hugging Face checkpoints requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, and accelerateâ€¯â‰¥â€¯0.29, trained on a mixed Japaneseâ€‘English corpus totaling roughly 1.3â€¯trillion tokens.
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ğŸ“¥ 6k / â­ 74 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€¯Bâ€‘parameter, Japaneseâ€‘enhanced variant of Metaâ€™s Llamaâ€‘2â€¯7â€¯B, preâ€‘trained on additional Japanese corpora and released in instruct and fast versions under the Llamaâ€‘2 Community License.
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - ğŸ“¥ 5k / â­ 32 / Japaneseâ€‘language, autoregressive LLMâ€¯â€œSarashina2.2â€‘3Bâ€‘instructâ€‘v0.1â€ from SBâ€¯Intuitions, evaluated on Japanese and English MT benchmarks, surpasses its own smaller siblings and rivals large models such as Qwen2.5â€‘3Bâ€‘instruct and Gemmaâ€‘2â€‘2bâ€‘jpnâ€‘it, though it has limited safety tuning.
 * [Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4) - ğŸ“¥ 5k / â­ 12 / Llamaâ€¯3.3â€¯Swallow is a 70â€‘B LLM that extends Metaâ€™s Llamaâ€¯3.3 with enhanced Japanese support and instructionâ€‘tuned variants, released in 8â€¯B and 70â€¯B versions from Decemberâ€¯2024 to Marchâ€¯2025 and available on HuggingFace.
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - ğŸ“¥ 5k / â­ 58 / A 2.7â€‘Bâ€‘parameter GPTâ€‘NeoX model trained by ABEJA on Japanese CCâ€‘100, Wikipedia, and OSCAR data, accessible with Huggingâ€¯Face Transformers for text generation, and released under the MIT license.
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - ğŸ“¥ 5k / â­ 56 / TinySwallowâ€‘1.5Bâ€‘Instruct is a Japaneseâ€‘instructionâ€‘tuned, 1.5â€‘B autoregressive language model distilled via TAID from Qwen2.5â€‘32Bâ€‘Instruct, intended only for research use.
 * [open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - ğŸ“¥ 4k / â­ 17 / OpenCALM is a Japanese decoderâ€‘only transformer suite by CyberAgent, ranging from 160â€¯M to 6.8â€¯B parameters, pretrained on Wikipedia and Commonâ€¯Crawl, released under CCâ€¯BYâ€‘SAâ€¯4.0, and usable via Huggingâ€¯Face Transformers.
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - ğŸ“¥ 4k / â­ 22 / Llamaâ€¯3.1â€¯Swallow is a set of 8B and 70B instructionâ€‘tuned Llamaâ€¯3.1 models that have been continually preâ€‘trained and SFTâ€‘fineâ€‘tuned on Japanese data, delivering stateâ€‘ofâ€‘theâ€‘art performance on Japanese MTâ€‘Bench.
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - ğŸ“¥ 4k / â­ 43 / TokyoTechâ€‘LLMâ€™s Swallowâ€¯Llamaâ€‘2 family delivers Japaneseâ€‘centric 7B, 13B, and 70B modelsâ€”including instructionâ€‘tuned and NVE variantsâ€”released from Decemberâ€¯2023 to Aprilâ€¯2024 with detailed version notes and an index of downloadable weights.
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - ğŸ“¥ 3k / â­ 15 / LLMâ€‘jpâ€‘3.1â€‘13bâ€‘instruct4 is a 13â€‘billionâ€‘parameter Japanese instructionâ€‘tuned language model from NIIâ€™s R&D Center, distributed in Huggingâ€¯Face Transformers format with specified dependencies and a unigramâ€‘byteâ€‘fallback tokenizer.
 * [Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1) - ğŸ“¥ 3k / â­ 1 / The Swallow series enhances Gemmaâ€‘2 with stronger Japanese while keeping English performance, offering 2B,â€¯9B andâ€¯27B pretrained and instructionâ€‘tuned models (released on HuggingFace) trained on synthetic Japanese data and evaluated on benchmarks like JCom,â€¯JSQuAD andâ€¯JMMLU.
 * [ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0) - ğŸ“¥ 3k / â­ 5 / ABEJAâ€‘Qwen2.5â€‘32bâ€‘Japaneseâ€‘v1.0 is a Japaneseâ€‘centric language model derived from Qwen2.5â€‘32Bâ€‘Instruct, refined through continued preâ€‘training and further SFT plus DPO fineâ€‘tuning by ABEJAâ€™s developers.
 * [llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m) - ğŸ“¥ 3k / â­ 1 / LLMâ€‘jpâ€‘3â€‘150m is a 150â€¯Mâ€‘parameter Japanese transformer released by NIIâ€™s LLM R&D Center in Huggingâ€‘Face format, requiring torchâ€¯â‰¥â€¯2.3.0 and transformersâ€¯â‰¥â€¯4.40.1, and trained on Japanese Wikipedia, Common Crawl, WARP, and Kaken datasets.
 * [Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1) - ğŸ“¥ 2k / â­ 2 / Gemmaâ€‘2â€¯Swallow is a suite of 2B, 9B, and 27B language models pretrained and instructionâ€‘tuned on Japanese data while preserving English capabilities, released Mayâ€¯19â€¯2025 and made available on Huggingâ€¯Face.
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - ğŸ“¥ 2k / â­ 19 / OpenCALM is a CyberAgentâ€‘developed suite of transformerâ€‘based decoderâ€‘only Japanese language models ranging from 160â€¯M to 6.8â€¯B parameters, trained on Japanese Wikipedia and Common Crawl, released under CCâ€‘BYâ€‘SAâ€¯4.0 and accessed via the GPTâ€‘NeoX library.
 * [Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2) - ğŸ“¥ 2k / â­ 4 / Llamaâ€¯3.1â€¯Swallow provides 8â€¯B and 70â€¯B Japaneseâ€‘enhanced versions of Metaâ€™s Llamaâ€¯3.1, created via continual preâ€‘training and instructionâ€‘tuned with synthetic Japanese data, and released in successive versions (v0.1â€“v0.3) by the Swallow team for both Japanese and English use.
 * [llm-jp-3.1-13b](https://huggingface.co/llm-jp/llm-jp-3.1-13b) - ğŸ“¥ 2k / â­ 2 / Provides 13â€¯Bâ€‘parameter LLMâ€‘jpâ€‘3.1 Japanese language models from NII, including preâ€‘trained and fineâ€‘tuned checkpoints, a Unigramâ€‘byteâ€‘fallback tokenizer, and HuggingÂ Face/torch dependency specifications.
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - ğŸ“¥ 2k / â­ 10 / GGUFâ€‘formatted, Massed Computeâ€‘quantised model files for Stability AIâ€™s Japanese StableLM Instruct Gammaâ€¯7B, supported by a16z funding and distributed via TheBlokeâ€™s Discord and Patreon channels.
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - ğŸ“¥ 2k / â­ 21 / rinna/youriâ€‘7b is a 32â€‘layer, 4096â€‘hiddenâ€‘size transformer that continually preâ€‘trains LLaMAâ€‘2â€‘7B on ~40â€¯B Japanese tokens using EleutherAI/gptâ€‘neox code, released under the LLaMAâ€‘2 license.
 * [ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1) - ğŸ“¥ 2k / â­ 10 / ABEJAâ€‘Qwen2.5â€‘7bâ€‘Japaneseâ€‘v0.1 is a Japaneseâ€‘trained Qwen2.5â€‘7Bâ€‘Instruct model distilled from ABEJAâ€‘Qwen2.5â€‘32bâ€‘Japaneseâ€‘v0.1, with instructionâ€‘following improved by ChatVector and no postâ€‘training.
 * [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - ğŸ“¥ 2k / â­ 17 / Swallow Llamaâ€¯2 is a TokyoTechâ€‘LLM collection of 7B,â€¯13B andâ€¯70B LLaMAâ€‘2 based models fineâ€‘tuned with supervised learning for Japanese and English, featuring NVE and â€œplusâ€ variants and regularly updated releases.
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ğŸ“¥ 2k / â­ 96 / Japaneseâ€‘augmented Llamaâ€‘2â€‘7B (6.27â€‘6.37â€¯Bâ€¯parameters) modelsâ€”including instruct and fast variantsâ€”are fineâ€‘tuned on additional pretraining for Japanese, released by ELYZA under the Llamaâ€¯2 Community License with sample usage code and developer credits.
 * [sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1) - ğŸ“¥ 2k / â­ 13 / Japanese autoregressive language models (sarashina2.2â€¯â€“â€¯0.5Bâ€‘instruct, 1Bâ€‘instruct, 3Bâ€‘instruct) trained by SBâ€¯Intuitions, benchmarked on Japanese/English tasks, with torchâ€‘transformers usage guides and a note on limited safety training.
 * [Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 2k / â­ 17 / Llamaâ€¯3.1â€¯Swallow delivers 8â€‘B and 70â€‘B Japaneseâ€‘enhanced language models built by continual preâ€‘training of Metaâ€™s Llamaâ€¯3.1 and supervised fineâ€‘tuning on synthetic Japanese data, with instructionâ€‘tuned variants released in Octoberâ€¯2024.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ğŸ“¥ 2k / â­ 81 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7B is a Japaneseâ€‘optimized Llamaâ€‘2â€‘7B model that adds extra preâ€‘training, with instruct and fast variants, and is released under the Llamaâ€¯2 Community License.
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - ğŸ“¥ 2k / â­ 205 / OpenCALM is a Japaneseâ€‘focused decoderâ€‘only transformer language model suite from CyberAgent, Inc., offering models from 160â€¯M to 6.8â€¯Bâ€¯parameters, trained on Japanese Wikipedia and CommonCrawl, and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - ğŸ“¥ 2k / â­ 53 / Japanese Stable LMâ€¯Instructâ€¯Gammaâ€¯7B is a 7â€‘billionâ€‘parameter, decoderâ€‘only, instructionâ€‘tuned Japanese language model from Stabilityâ€¯AI built atop the Baseâ€¯Gammaâ€¯7B, requiring Transformersâ€¯â‰¥â€¯4.34.0 and released under the Apacheâ€¯2.0 license.
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - ğŸ“¥ 2k / â­ 41 / Japaneseâ€‘centric instruction and generative LLMs from LLMâ€‘jp (13â€¯B and 1.3â€¯B parameter models trained on mixed Japanese/English/sourceâ€‘code data with Megatronâ€‘DeepSpeed) are released as Huggingâ€¯Face Transformers checkpoints requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, and accelerateâ€¯=â€¯0.23.
 * [japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - ğŸ“¥ 1k / â­ 104 / A 1.3â€‘B parameter Japanese GPT model, trained on Japanese C4, CCâ€‘100, and Wikipedia with a 24â€‘layer transformer and SentencePiece tokenization, released on Januaryâ€¯26â€¯2022 by rinna under the MIT license.
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - ğŸ“¥ 1k / â­ 15 / LLMâ€‘jp releases Japaneseâ€‘centric transformer models (1.3B and 13B) in Huggingâ€¯Face format, including instructionâ€‘fineâ€‘tuned, LoRA, and preâ€‘trained variants, trained on mixed Japaneseâ€‘Englishâ€‘code data and usable with PyTorchâ€¯â‰¥â€¯2.0,â€¯Transformersâ€¯â‰¥â€¯4.34, andâ€¯Accelerateâ€¯0.23.
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - ğŸ“¥ 1k / â­ 25 / Japanese Stableâ€¯LMâ€¯Baseâ€¯Gammaâ€¯7B is a 7â€‘billionâ€‘parameter decoderâ€‘only language model based on Mistralâ€‘7B, optimized for Japanese tasks and trained on Japanese Wikipedia, mc4, CCâ€‘100 and OSCAR corpora, released under Apacheâ€¯2.0 and requiring Transformersâ€¯â‰¥â€¯4.34.0.
 * [Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B) - ğŸ“¥ 1k / â­ 6 / An 8â€‘billionâ€‘parameter Llamaâ€¯3.1â€‘based model fineâ€‘tuned on a vast Japanese code and naturalâ€‘language corpus, equipped with instructionâ€‘following, causal and Fillâ€‘inâ€‘theâ€‘Middle inference, outperforms the base Llamaâ€¯3.1 and Qwen on Japanese and English codeâ€‘completion and generation tasks.
 * [Sakura-13B-Galgame-GGUF](https://huggingface.co/QuantFactory/Sakura-13B-Galgame-GGUF) - ğŸ“¥ 1k / â­ 2 / A llama.cppâ€‘quantized, nonâ€‘commercially licensed Sakuraâ€‘13Bâ€‘Galgame translation model that delivers offline, GPTâ€‘3.5â€‘level Japaneseâ€‘toâ€‘Japanese/English Galgame text with API support, continued preâ€‘training on lightâ€‘novel data, and ongoing experimental updates.
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - ğŸ“¥ 1k / â­ 37 / TokyoTechâ€‘LLM releases the Swallowâ€‘Llama seriesâ€”Japaneseâ€‘enhanced LLaMAâ€‘2 models of 7B, 13B and 70B sizesâ€”updated with supervised fineâ€‘tuning, NVE variants, and new versions from Decemberâ€¯2023 through Aprilâ€¯2024.
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - ğŸ“¥ 1k / â­ 17 / Japaneseâ€‘StableLMâ€‘Baseâ€‘Betaâ€‘70B is a 70â€‘billionâ€‘parameter, Llamaâ€‘2â€‘based decoderâ€‘only language model fineâ€‘tuned on diverse Japanese data for strong downstream Japanese performance, with smaller 7B variants and an instructionâ€‘following counterpart.
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ğŸ“¥ 1k / â­ 23 / Japaneseâ€‘enhanced Llamaâ€‘2â€‘7B models (ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b and variants) built via additional preâ€‘training, distributed under the Llamaâ€¯2 license with usage examples and details.
 * [Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1) - ğŸ“¥ 1k / â­ 3 / Japaneseâ€‘enhanced Gemmaâ€‘2 language models (2b,â€¯9b,â€¯27b) are provided, built through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning on synthetic Japanese data, and released on Hugging Face and the Swallow team website.
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - ğŸ“¥ 1k / â­ 26 / A 70â€‘billionâ€‘parameter Japanese decoderâ€‘only Llamaâ€‘2 model fineâ€‘tuned on Databricks Dollyâ€‘15k, Anthropic HH, and other public data (with a 7â€‘billion variant), accompanied by example usage for text generation.
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - ğŸ“¥ 1k / â­ 13 / A quantized Japaneseâ€‘centric multilingual 10â€‘B GPTâ€‘NeoX model (weblabâ€‘10bâ€‘instructionâ€‘sftâ€‘GPTQ) thatâ€™s smaller and faster than the 21.42â€¯GB original, requires a GPU with autoGPTQ, includes a 6â€¯GB GGUF variant usable on CPU with llama.cpp, and provides runâ€‘instructions for textâ€‘generationâ€‘webui locally or via Colab.
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - ğŸ“¥ 1k / â­ 15 / LLMâ€‘jpâ€™s 13B and 1.3B instructionâ€‘tuned language models, built on Megatronâ€‘DeepSpeed and Huggingâ€¯Faceâ€‘compatible, provide Japanese, English, and code preâ€‘training data and require torchâ€¯â‰¥â€¯2.0.0, transformersâ€¯â‰¥â€¯4.34.0, and accelerateâ€¯0.23.0.
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 8 / LLMâ€‘jp delivers 13B and 1.3B Japanese/English instructionâ€‘style language models in Huggingâ€‘Faceâ€‘Transformers format, pretrained on mixed Japaneseâ€‘Englishâ€‘code data, and requires torchâ‰¥2.0, transformersâ‰¥4.34, accelerate, and DeepSpeed for use.
 * [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ğŸ“¥ 1k / â­ 42 / A Japaneseâ€‘enhanced version of Metaâ€™s Llamaâ€¯2 (13â€¯B parameters) â€“ with instruct and fast variants â€“ produced by the ELYZA team, ready for use via Huggingâ€‘Face Transformers and distributed under the Llamaâ€¯2 Community License.
 * [Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1) - ğŸ“¥ 1k / â­ 3 / Gemmaâ€‘2â€‘Llamaâ€‘Swallow extends Gemmaâ€‘2 with Japaneseâ€‘focused continual preâ€‘training and instructionâ€‘fineâ€‘tuning, delivering 2B/9B/27B preâ€‘trained and instructionâ€‘tuned models released on HuggingFace and the Swallowâ€¯LLM website.
 * [japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - ğŸ“¥ 1k / â­ 15 / A 6â€‘layer, 512â€‘hidden transformer model trained on Japanese CCâ€‘100 and Wikipedia with SentencePiece tokenization, released on Augustâ€¯25â€¯2021 under the MIT license (seeâ€¯https://huggingface.co/rinna/japaneseâ€‘gpt2â€‘xsmall and arXivâ€¯2404.01657).
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - ğŸ“¥ 1k / â­ 11 / OpenCALM is a suite of Japanese decoderâ€‘only transformer language models, from 160â€¯M to 6.8â€¯B parameters, trained on Japanese Wikipedia and Common Crawl and released by CyberAgent under the CCâ€¯BYâ€‘SAâ€¯4.0 license.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - ğŸ“¥ 1k / â­ 3 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7bâ€‘fastâ€‘instruct is a 4.11â€¯GB GPUâ€‘only, autoGPTQâ€‘required, Japaneseâ€‘instructionâ€‘tuned quantization of Metaâ€™s Llamaâ€¯2 that reduces memory and speed at the cost of lower instructionâ€‘following performance, with a newer AWQ variant offering better compliance and gguf versions for CPU use.
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - ğŸ“¥ 1k / â­ 12 / Japanese autoregressive language models by SBâ€¯Intuitions (Sarashina2â€¯1Bâ€“3B) are provided with benchmark results on Japaneseâ€¯/â€¯English tasks, example PyTorch usage scripts, and a note that safety training is limited.
 * [llm-jp-3-1.8b-instruct](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct) - ğŸ“¥ 1k / â­ 25 / Japanese largeâ€‘language models (llmâ€‘jpâ€‘3â€‘*) from the National Institute of Informatics are released as Huggingâ€¯Face checkpoints with 1.8â€¯b, 3.7â€¯b, & 13â€¯b variants (including â€œâ€‘instructâ€) that require torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerate, flashâ€‘attn, and were pretrained on Japanese Wikipedia, Commonâ€¯Crawl, and several mixed corpora.
 * [shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b) - ğŸ“¥ 1k / â­ 2 / Bilingual Japaneseâ€‘English chat models from Shisaâ€¯AIâ€”Qwen2.5, Llamaâ€¯3.1/3.3, Mistralâ€¯Nemo, and Unphi4â€”share identical training data, excel at Japanese tasks while preserving solid English skills, and are released under Apacheâ€¯2.0 or MIT licenses.
 * [shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - ğŸ“¥ 1k / â­ 16 / Shisaâ€‘baseâ€‘7Bâ€‘v1 is a Mistralâ€‘7B model expanded with 8â€¯B Japaneseâ€‘token preâ€‘training from MADLADâ€‘400, trained for 2,400 A100â€‘40 GPUâ€‘hours, and it achieves classâ€‘leading performance on Japanese benchmarks among 7â€‘Bâ€‘parameter models.
 * [ALMA-7B-Ja-V2](https://huggingface.co/webbigdata/ALMA-7B-Ja-V2) - ğŸ“¥ 1k / â­ 20 / C3TRâ€‘Adapter is a 4â€‘bit QLoRA adapter for Googleâ€‘gemmaâ€‘7b that powers the ALMAâ€‘7Bâ€‘Jaâ€‘V2 modelâ€”Japaneseâ€‘English translation with optional German, Chinese, Icelandic, and Czech supportâ€”requiring 8.1â€¯GB GPU (but runnable on free Colab) and achieving improved benchmark scores.
 * [ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - ğŸ“¥ 1k / â­ 24 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘13bâ€‘fastâ€‘instruct is a 13.14â€¯Bâ€‘parameter, 44,581â€‘token Japaneseâ€‘language model built on Llamaâ€¯2 with additional preâ€‘training, released under the LLAMAâ€¯2 Community License.
 * [ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - ğŸ“¥ 1k / â­ 22 / ELYZAâ€‘Japaneseâ€‘Llamaâ€‘2â€‘13b is a 13.02â€¯Bâ€‘parameter Llamaâ€¯2 model finetuned for Japanese, with a 13.14â€¯Bâ€‘parameter fast variant, created by Akiraâ€¯Sasaki, Masatoâ€¯Hirakawa, Shintaroâ€¯Horie, Tomoakiâ€¯Nakamura, Samâ€¯Passaglia, and Daisukeâ€¯Oba under Metaâ€™s LLAMAâ€¯2 Community License.
 * [stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - ğŸ“¥ 1k / â­ 39 / Stockmark Inc.â€™s 13â€¯Bâ€‘parameter Japanese language model (stockmarkâ€‘13b and its instructionâ€‘tuned variant stockmarkâ€‘13bâ€‘instruct) was trained on ~220â€¯B tokens using AWS Trainium and neuronxâ€‘neometegron, and is released under the MIT license.
 * [ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - ğŸ“¥ 1k / â­ 7 / ELYZAâ€‘Japaneseâ€‘Llamaâ€‘2â€‘13bâ€‘fast is a â‰ˆ13â€¯Bâ€‘parameter Japaneseâ€‘enhanced Llamaâ€‘2 model with base, instruct, fast, and fastâ€‘instruct variants, released under the Llamaâ€¯2 Community License by Akiraâ€¯Sasaki, Masatoâ€¯Hirakawa, Shintaroâ€¯Horie, Tomoakiâ€¯Nakamura, Samâ€¯Passaglia, andâ€¯Daisukeâ€¯Oba.
 * [Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) - ğŸ“¥ 1k / â­ 3 / Llamaâ€¯3.1â€¯Swallow releases 8â€‘B and 70â€‘B Japaneseâ€‘enhanced variants of Metaâ€™s Llamaâ€¯3.1, built by continual preâ€‘training and Japanese SFT and available as openâ€‘source Megatronâ€‘LM models with benchmark results on major Japanese tasks.
 * [llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct) - ğŸ“¥ 1k / â­ 31 / Repository releases Japaneseâ€‘centric LLMs (1.8Bâ€‘3.1.8B, 3.3.7B, 13B, 172Bâ€¯Î²1 variants) from NIIâ€™s Large Language Model R&D Center, offering HuggingÂ Faceâ€‘compatible checkpoints, a Byteâ€‘fallback tokenizer, and preâ€‘training on Japanese Wikipedia, CommonÂ Crawl, WARP, Kaken, English Wikipedia, and Dolma corpora.

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - ğŸ“¥ 245k / â­ 70 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads, 32k vocabulary) pretrained on Septemberâ€¯2019 Japanese Wikipedia using MeCab IPA dictionary wordâ€‘level tokenization with WordPiece subwords and wholeâ€‘word masking for MLM, released under CCâ€¯BYâ€‘SAâ€¯3.0 (code atâ€¯clâ€‘tohoku/bertâ€‘japanese).
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - ğŸ“¥ 131k / â­ 6 / Japaneseâ€¯BERTâ€‘base pretrained on â‰ˆ30â€¯M Wikipedia sentences using Unidicâ€¯2.1.2 tokenization, characterâ€‘level subwords, wholeâ€‘word masking, 12 layers, 768â€‘dim hidden states, 12 heads, and a 6144â€‘token vocabulary.
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - ğŸ“¥ 101k / â­ 8 / BERTâ€‘base Japanese pretrained on 17â€¯M Wikipedia sentences (â‰ˆ2.6â€¯GB) using IPAâ€‘based word tokenization followed by characterâ€‘level splits, a 4â€¯000â€‘token vocab, 12 layers/768â€‘dim hidden states, 12 heads, and released under Creativeâ€¯Commonsâ€¯BYâ€‘SAâ€¯3.0 with code at clâ€‘tohoku/bertâ€‘japanese.
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - ğŸ“¥ 76k / â­ 38 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) pretrained on 2.6â€¯GB of Japanese Wikipedia (â‰ˆ17â€¯M sentences) using IPAâ€‘dictionary word tokenization combined with WordPiece, 32k vocabulary, trained for 1â€¯M steps on Cloud TPUs, whose code is at clâ€‘tohoku/bertâ€‘japanese and released under CCâ€¯BYâ€‘SAâ€¯3.0.
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - ğŸ“¥ 48k / â­ 48 / LINE DistilBERTâ€¯Japanese is a 66â€‘Mâ€‘parameter, 6â€‘layer DistilBERT trained on 131â€¯GB of Japanese web text with an inâ€‘house LINE BERTâ€‘base teacher, achieving strong JGLUE scores, tokenized via MeCab/Unidic and SentencePiece, and released under the Apacheâ€¯2.0 license.
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - ğŸ“¥ 37k / â­ 3 / Japanese RoBERTa base model preâ€‘trained on ~10â€¯M JST medical abstracts and 1.4â€¯M bodyâ€‘text sentences, using a 30,000â€‘token SentencePiece (Unigram) vocabulary, released under CCâ€¯4.0 and ready for fineâ€‘tuning or transformerâ€‘pipeline use.
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - ğŸ“¥ 27k / â­ 30 / A Japanese DeBERTaâ€¯V2 base model, preâ€‘trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with Juman++â€‘segmented text and sentenceâ€‘piece subwords, trained for three weeks on 8 NVIDIA A100 GPUs via the Huggingâ€¯Face Transformers library and ready for downstream fineâ€‘tuning.
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - ğŸ“¥ 17k / â­ 8 / Japanese DeBERTaâ€¯V2 large trained on ~171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with characterâ€‘level tokenization, wholeâ€‘word masking, and 26â€¯days of training on 16 A100 GPUs.
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - ğŸ“¥ 11k / â­ 27 / A 12â€‘layer, 768â€‘dimensional BERTâ€‘base model pretrained on 30â€¯M Japanese Wikipedia sentences, using Unidicâ€‘lite wordâ€‘level tokenization followed by WordPiece subwords and wholeâ€‘word masking, with a 32â€¯768â€‘token vocabulary and training code hosted atâ€¯cl-tohoku/bert-japanese.
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - ğŸ“¥ 10k / â­ 18 / ModernBERTâ€‘Jaâ€‘310M is a Japaneseâ€‘English BERT variant from SB Intuitions that blends localâ€¯+â€¯global attention with RoPE, trained on 4.09â€¯trillion tokens, 102,400â€‘word vocab, 8,192â€‘token sequences, and supports Flashâ€¯Attentionâ€¯2 for efficient inference.
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - ğŸ“¥ 10k / â­ 39 / Japaneseâ€¯RoBERTaâ€‘base pretrained by rinna, with detailed instructions to load the model, perform maskedâ€‘token prediction reliably by prependingâ€¯[CLS], using proper tokenization and explicit position_ids, and highlighting differences from the HuggingFace inference API.
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - ğŸ“¥ 7k / â­ 2 / Repository hosts a Japanese DeBERTaâ€¯V2 tiny model pretrained on 171â€¯GB of Wikipedia, CCâ€‘100, and OSCAR data, requiring Juman++ segmentation, SentencePiece tokenization, trained for 33â€¯h on 8â€¯A100 GPUs, and ready for downstream fineâ€‘tuning.
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - ğŸ“¥ 7k / â­ 5 / ModernBERTâ€‘Jaâ€‘30M is a Japanese BERT variant from SB Intuitions that blends local and global attention with RoPE, trained on 4.39â€¯trillion tokens with a 102,400â€‘token vocabulary and 8,192â€‘token limit, and provides multiple sizes (30â€¯M,â€¯70â€¯M,â€¯130â€¯M) and Flashâ€‘Attentionâ€¯2 compatibility.
 * [splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - ğŸ“¥ 6k / â­ 10 / Evaluation metrics (nDCG, recall, MRR) for Japanese retrieval modelsâ€”including spladeâ€‘japaneseâ€‘v3, JaColBERTv2, BGEâ€‘m3, various mâ€‘e5 sizes, and othersâ€”are reported on MIRACL and hotchpotch/JQaRA benchmarks, with spladeâ€‘japaneseâ€‘v2â€‘doc noted as queryâ€‘encoderâ€‘free.
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - ğŸ“¥ 3k / â­ 45 / Japaneseâ€¯ModernBERTâ€‘Jaâ€‘130M is a 130â€‘millionâ€‘parameter BERT variant that blends localâ€¯+â€¯global attention with RoPE, trained on a 4.39â€¯Tâ€‘token Japanese/English corpus, offers a 102,400â€‘vocab, 8,192â€‘token length, and is optimized for GPU Flashâ€¯Attentionâ€¯2.
 * [llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base) - ğŸ“¥ 2k / â­ 10 / llmâ€‘jpâ€‘modernbertâ€‘base is a modernBERTâ€‘base model trained on 3.4â€¯TB of Japanese data to a max sequence length of 8,192, built in two stages (1024â†’8192), and achieves 0.92 on JSTS, 0.912 on JNLI, and 0.88 on JCoLA, with all code and evaluation scripts on GitHub.
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - ğŸ“¥ 2k / â­ 8 / A JavaScriptâ€‘based Japanese RoBERTa base model trained on Wikipedia and CCâ€‘100, tokenized with Juman++ viaâ€¯BertJapaneseTokenizer, and fineâ€‘tuned over 700k steps on eight A100 GPUs.
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - ğŸ“¥ 2k / â­ 4 / DeBERTaV2â€¯baseâ€¯Japanese, pretrained on CCâ€‘100, mC4, OSCAR2301, Wikipedia and Wikinews for 1â€¯M steps with Adam/FP16, achieves top JGLUE NLU benchmarks close to Wasedaâ€¯RoBERTa.
 * [deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - ğŸ“¥ 2k / â­ 9 / Japanese DeBERTaâ€¯V2â€¯Large pretrained on 171â€¯GB of Japanese text (Wikipedia, CCâ€‘100, OSCAR) with Juman++/sentencepiece tokenization, trained over 36â€¯days on eight A100 GPUs.
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - ğŸ“¥ 2k / â­ 9 / A 24â€‘layer, 1024â€‘dimensional BERTâ€‘Large model pretrained on 4â€¯GB of Japanese Wikipedia (â‰ˆ30â€¯M sentences) using Unidicâ€¯2.1.2 word tokenization followed by WordPiece subwords, wholeâ€‘word masking, a 32,768â€‘token vocabulary, and trained with 512â€‘token instances, 256 batches, and 1â€¯M steps.
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - ğŸ“¥ 1k / â­ 6 / ModernBERTâ€‘Jaâ€‘70M is a 70â€‘Mâ€‘parameter Japanese BERT model trained on 4.39â€¯T tokens, using combined local/global attention and RoPE to efficiently model 8,192â€‘token sequences, and it supports Flashâ€¯Attentionâ€¯2 for optimal GPU performance.
 * [bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - ğŸ“¥ 1k / â­ 4 / BERT base model trained on Japanese Wikipedia using IPAâ€‘character tokenization with wholeâ€‘word masking, featuring 12 layers, 768 hidden units, 12 heads, a 4,000â€‘token vocabulary, and released under Creative Commonsâ€¯BYâ€‘SAâ€¯3.0.
 * [roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - ğŸ“¥ 1k / â­ 32 / Japanese RoBERTaâ€‘base pretrained on Japanese Wikipedia and CCâ€‘100, requiring Juman++ segmentation, fineâ€‘tunably deployed, and trained for 700â€¯k steps on 8 A100 GPUs with Adam (lrâ€¯=â€¯1â€¯Ã—â€¯10â»â´).
 * [roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - ğŸ“¥ 1k / â­ 3 / Japanese RoBERTaâ€‘Large, pretrained on Japanese Wikipedia and CCâ€‘100 with 512â€‘token max sequence, trained for 670â€¯k steps on eight NVIDIA A100 GPUs, and ready for fineâ€‘tuning with automatic Juman++/SentencePiece tokenization.

### sentence-similarity
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - ğŸ“¥ 354k / â­ 11 / Ruri offers new v3 Japanese textâ€‘embedding models (30Mâ€“310M parameters, max lengthâ€¯8192, JMTEB scoresâ€¯74.51â€“77.24) for easy use with Sentenceâ€¯Transformers, and benchmarks them against other Japanese and multilingual models on the JMTEB suite.
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - ğŸ“¥ 159k / â­ 56 / Ruriâ€¯v3 is a highâ€‘performance Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja that supports up to 8â€¯192â€‘token sequences, a 100â€¯kâ€‘token vocabulary, FlashAttention, and comes in several sizes usable via sentenceâ€‘transformers.
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - ğŸ“¥ 73k / â­ 33 / GLuCoSEâ€‘baseâ€‘ja is a Japanese sentenceâ€‘embedding model built on LUKE, trained on mixed web, NLI, and search data to produce 768â€‘dimensional meanâ€‘pooled vectors (512â€‘token max) ideal for cosineâ€‘based similarity and semantic search tasks, and it scores higher on Spearman/ Pearson metrics than competing baselines.
 * [sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - ğŸ“¥ 49k / â­ 13 / Japanese Sentenceâ€‘BERT model built onâ€¯colorfulscoop/bertâ€‘baseâ€‘ja, fineâ€‘tuned on 523,005 SNLI sentences with a 3â€‘label softmax classifier, achieving 0.8529 test accuracy and generating 512â€‘dim embeddings.
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - ğŸ“¥ 44k / â­ 3 / Ruriâ€¯v3 delivers stateâ€‘ofâ€‘theâ€‘art Japanese text embeddings built on ModernBERTâ€‘Ja, supporting up to 8,192 tokens, a 100â€¯kâ€‘token vocabulary, FlashAttention acceleration, and multiple model sizes ranging from 30â€¯M to 310â€¯M parameters.
 * [simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - ğŸ“¥ 26k / â­ 15 / A Japanese SimCSE model built on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2, fineâ€‘tuned for JSNLI over 20 epochs (best Spearmanâ€‘correlated checkpoint), usable with sentenceâ€‘transformers (needs fugashi/unidicâ€‘lite), and licensed under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - ğŸ“¥ 26k / â­ 2 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, offering a 100â€¯kâ€‘token vocabulary, 8â€¯192â€‘token sequence support, FlashAttention acceleration, and several size variants compatible with sentenceâ€‘transformers.
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - ğŸ“¥ 9k / â­ 43 / PLaMoâ€‘Embeddingâ€‘1B is a Japanese textâ€‘embedding model from Preferred Networks that generates highâ€‘quality vectors for retrieval, classification, and clustering, excels on the JMTEB benchmark, and is freely available under an Apacheâ€¯v2.0 license for commercial use.
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - ğŸ“¥ 8k / â­ 35 / A Japanese sentenceâ€‘transformer based on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite, trained for one epoch on JSNLI, maps sentences or paragraphs into 768â€‘dimensional dense vectors for clustering and semantic search.
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - ğŸ“¥ 8k / â­ 21 / GLuCoSEâ€¯v2 is a CPUâ€‘friendly Japanese text embedding model that excels at retrieval and semanticâ€‘similarity tasks, uses â€œquery:â€ / â€œpassage:â€ prefixes, and outperforms similarly sized models on MIRACL after distillationâ€‘based fineâ€‘tuning.
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - ğŸ“¥ 7k / â­ 1 / Ruriâ€¯v3 is a ModernBERTâ€‘Ja based Japanese textâ€‘embedding model with a 100â€¯kâ€‘token vocabulary, 8,192â€‘token support, FlashAttention integration, and four size variants (30â€¯Mâ€“310â€¯M parameters) that achieve stateâ€‘ofâ€‘theâ€‘art JMTEB benchmark scores.
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - ğŸ“¥ 7k / â­ 16 / A Japaneseâ€‘only document retrieval model trained via knowledge distillation on MMarco for 250k steps, JaColBERTv2 already outperforms multilingualâ€‘e5â€‘large, BGEâ€‘M3, and JaColBERT in early tests and offers strong outâ€‘ofâ€‘domain generalisation.
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - ğŸ“¥ 3k / â­ 44 / Ruri releases Japanese generalâ€‘text embedding v3 models (30â€¯Mâ€“310â€¯M parameters) that can be loaded withâ€¯sentence_transformers (using â€œã‚¯ã‚¨ãƒª:â€/â€œæ–‡ç« :â€ prefixes) and outperforms comparable models on JMTEB, as shown in its benchmark tables.
 * [ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m) - ğŸ“¥ 3k / â­ 1 / Ruri is a Japanese sentenceâ€‘embedding series based on ModernBERTâ€‘Ja, providing fineâ€‘tuned models from 30â€¯M to 310â€¯M parameters that integrate with sentenceâ€‘transformers and FlashAttentionâ€¯2 and reach JMTEB scores up to 77.2, all released under the Apacheâ€¯2.0 license.
 * [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) - ğŸ“¥ 3k / â­ 22 / Final JaColBERTv2.5 weights, trained on 40â€¯% of JaColBERTv2 data with an overhauled recipe, now outperform all prior modelsâ€”including JaColBERTV2 multilingual variants such as BGEâ€‘M3â€”across every dataset.
 * [sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b) - ğŸ“¥ 2k / â­ 38 / Sarashinaâ€‘Embeddingâ€‘v1â€‘1B is a 1.2â€‘Bâ€‘parameter Japanese sentence transformer that maps text to 1,792â€‘dimensional vectors, trained with multiâ€‘stage contrastive learning to achieve stateâ€‘ofâ€‘theâ€‘art performance on the JMTEB benchmark, and supports semantic similarity, search, paraphrase mining, classification and clustering under a nonâ€‘commercial license.
 * [RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja) - ğŸ“¥ 1k / â­ 8 / RoSEtta is a Japanese sentenceâ€‘embedding model based on RoFormer that supports up to 1024 tokens, excels at retrieval and semanticâ€‘similarity tasks, and is accessed via Sentence Transformers or Hugging Face Transformers with a required query/passage prefix.

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - ğŸ“¥ 767k / â­ 13 / Japanese CLOOB visionâ€‘language model (vitâ€‘baseâ€‘patch16â€‘224) trained on CC12M captions translated to Japanese, released by rinna Co. on 12â€¯Mayâ€¯2022 under Apacheâ€¯2.0 and hosted on Hugging Face.
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - ğŸ“¥ 55k / â­ 50 / Japanese sentenceâ€‘BERT v2, fineâ€‘tuned with MultipleNegativesRankingLoss on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘wholeâ€‘wordâ€‘masking, outperforms v1 by roughly 1.5â€‘2 points on a private dataset.
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - ğŸ“¥ 29k / â­ 22 / Japanese CLIP (ViTâ€‘Bâ€‘16) model trained on CC12M caption translations, released Mayâ€¯2022 by rinna under an ApacheÂ 2.0 license.
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - ğŸ“¥ 26k / â­ 10 / Japanese Sentenceâ€‘BERT v1 model, with a newer v2 offering a ~1.5â€‘point accuracy boost, and a Huggingâ€‘Faceâ€‘based SentenceBertJapanese example shown in the linked Qiita article.
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - ğŸ“¥ 22k / â­ 29 / A Japanese CLIP model by LY Corporation trained on ~1â€¯B web imageâ€‘text pairs, using an EVAâ€‘02â€‘B image encoder and a 12â€‘layer BERT text encoder, delivers strong zeroâ€‘shot imageâ€‘classification and retrieval results on STAIRâ€¯Captions, Recruit, and Japanese ImageNetâ€‘1K.
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - ğŸ“¥ 11k / â­ 2 / Repoâ€¯ja_ginza_electra bundles a spaCyâ€¯v3 Japanese NLP model that fineâ€‘tunes an ELECTRA discriminator on mC4 Japanese (built atop megagonlabs/transformersâ€‘udâ€‘japaneseâ€‘electraâ€‘baseâ€‘discrimininator), includes GiNZAâ€¯v5 bunsetuâ€‘phrase components, and is MITâ€‘licensed as a joint NINJALâ€‘Megagon Labs project.
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - ğŸ“¥ 4k / â­ 53 / A 300â€‘millionâ€‘parameter Japanese T5 model pretrained on ~100â€¯GB of Wikipedia (2020) and OSCAR CCâ€‘100 data, 25â€¯% smaller than Googleâ€™s mT5â€‘small yet 6â€¯pts higher on the livedoor newsâ€‘classification benchmark, requires fineâ€‘tuning and carries biasâ€‘risk warnings.
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - ğŸ“¥ 4k / â­ 17 / Sarashina2.2â€‘1B is a 1,792â€‘dimensional Japanese Sentence Transformer trained with multiâ€‘stage contrastive learning that scores stateâ€‘ofâ€‘theâ€‘art across 28 JMTEB datasets and enables semantic similarity, search, paraphrase mining, classification, and clustering.
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - ğŸ“¥ 3k / â­ 14 / A Japanese Sentenceâ€‘LUKE model trained on the same dataset as Sentenceâ€‘BERT, yielding 0.5â€¯pt higher quantitative and better qualitative accuracy, built upon studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite and exposed via a Transformerâ€‘style SentenceLukeJapanese class for easy inference.
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - ğŸ“¥ 2k / â­ 14 / A fineâ€‘tuned Japanese large BERT (clâ€‘tohoku/bertâ€‘largeâ€‘japaneseâ€‘v2) Supervisedâ€‘SimCSE model (CLS+MLP pooling, 1024â€‘dim, 5eâ€‘5â€¯lr, batchâ€¯512, BFloat16, maxâ€‘seqâ€¯64, 1â€¯M training examples) usable via sentenceâ€‘transformers or raw HuggingFace Transformers.
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - ğŸ“¥ 1k / â­ 2 / A Japanese BERTâ€‘base fineâ€‘tuned with supervised SimCSE on JSNLI (CLS pooling + trainingâ€‘only MLP) and released as supâ€‘simcseâ€‘jaâ€‘base, offering 768â€‘dim sentence embeddings, 1â€¯M training examples, lrâ€¯5eâ€‘5, batchâ€¯512, tempâ€¯0.05, usable via sentenceâ€‘transformers or HuggingFace Transformers.

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - ğŸ“¥ 6M / â­ 44 / Japanese fineâ€‘tuned wav2vec2â€‘large XLSRâ€‘53 speechâ€‘recognition model trained on Common Voice, CSS10, and JSUT, ready for 16â€¯kHz audio and usable via HuggingFace.
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - ğŸ“¥ 55k / â­ 4 / Fineâ€‘tuned Japaneseâ€‘wav2vec2â€‘base ASR model (predicting only Hiragana) trained on Common Voiceâ€¯11.0 (20â€¯epochs, 1eâ€‘4 lr, batchâ€¯16) achieving 0.1418 WER after 7â€¯k steps.
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - ğŸ“¥ 54k / â­ 78 / Kotobaâ€‘Whisperâ€¯v2.0 is a Japanese ASR model distilled from OpenAIâ€™s Whisperâ€‘largeâ€‘v3, trained on 7.2â€¯M ReazonSpeech clips, running 6.3â€¯Ã— faster yet achieving lower Japanese CER/WER than the teacher, with all training code and evaluation data publicly released.
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - ğŸ“¥ 16k / â­ 82 / Kotobaâ€‘Whisperâ€‘v2.2 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with a Transformersâ€‘based pipeline that adds speaker diarization and punctuation using pyannote tools.
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - ğŸ“¥ 11k / â­ 112 / Anime Whisper is a lightweight Japanese ASR model fineâ€‘tuned on ~5,300â€¯h of animeâ€‘style speech from the Galgame_Speech_ASR_16kHz dataset, built on kotobaâ€‘whisperâ€‘v2.0, and delivers highly accurate, punctuationâ€‘aligned transcriptions with minimal hallucinations, faithful nonâ€‘verbal sounds, and robust performance even on NSFW audioâ€”avoiding initial prompts to prevent quality degradation.
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - ğŸ“¥ 7k / â­ 35 / reazonspeechâ€‘nemoâ€‘v2 is a 619â€¯Mâ€‘parameter, subwordâ€‘based RNNâ€‘T Conformer model trained on the ReazonSpeechâ€¯v2.0 Japanese corpus that can transcribe multiâ€‘hour audio using Fastâ€‘Conformer with Linearly Scalable Attention and Longformer encoder, and is available via the reazonspeech library under the ApacheÂ 2.0 license.
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - ğŸ“¥ 6k / â­ 19 / Kotobaâ€‘Whisperâ€‘v2.1 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with a postâ€‘processing pipeline that adds punctuation to transcriptions while achieving error rates comparable to its predecessors.
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - ğŸ“¥ 3k / â­ 37 / Parakeetâ€¯TDTâ€‘CTCâ€¯0.6B (ja) is a 0.6â€‘billionâ€‘parameter Hybrid FastConformerâ€‘TDTâ€‘CTC ASR model from NVIDIA NeMo that transcribes 16â€¯kHz mono Japanese speech with punctuation and can be run or fineâ€‘tuned through the NeMo framework.
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - ğŸ“¥ 2k / â­ 17 / Kotobaâ€‘Whisperâ€‘Bilingualâ€¯v1.0 is a 6.3Ã— faster distilled Whisper model that performs Japaneseâ€¯â†”â€¯English ASR and bidirectional speechâ€‘toâ€‘text translation, trained by pseudoâ€‘labeling from Whisper largeâ€‘v3 with external LLMâ€‘generated transcripts, and evaluated against OpenAI and cascaded baselines.
 * [whisper-large-v2-translate-zh-v0.2-st](https://huggingface.co/chickenrice0721/whisper-large-v2-translate-zh-v0.2-st) - ğŸ“¥ 1k / â­ 1 / Fineâ€‘tuned openai/whisperâ€‘largeâ€‘v2 for Japaneseâ€‘toâ€‘Chinese translation, trained on 5,000â€¯h of Japanese audio with Chinese subtitles and delivering TERâ€¯60.48, BLEUâ€¯29.37, and CERâ€¯0.66.

### translation
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - ğŸ“¥ 113k / â­ 66 / Opusâ€‘mtâ€‘jaâ€‘en offers a Japaneseâ€‘toâ€‘English transformerâ€‘align model trained on the opus dataset with normalization and SentencePiece preprocessing, achieving 41.7â€¯BLEU (0.589â€¯chrF) on the Tatoebaâ€¯jaâ€‘en test set.
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - ğŸ“¥ 88k / â­ 8 / Fineâ€‘tuned LLaMAâ€¯3 (Youkoâ€¯QLoRA) on a new VNTL dataset to deliver more accurate yet literal Japaneseâ€‘toâ€‘English visualâ€‘novel translations, using the default prompt, multiâ€‘line support, and recommended neutral sampling (temperatureâ€¯0).
 * [fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en) - ğŸ“¥ 11k / â­ 32 / FuguMT is a Japaneseâ€‘toâ€‘English neural translation model built with Marianâ€‘NMT, Transformers, and SentencePiece, scoring 39.1 BLEU on the Tatoeba benchmark.
 * [fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja) - ğŸ“¥ 11k / â­ 54 / FuguMT is an Englishâ€‘toâ€‘Japanese Marianâ€‘NMT translation model that leverages transformers, SentencePiece and optional pySBD for segmentation, and scored a BLEU of 32.7 on the Tatoeba dataset.
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - ğŸ“¥ 9k / â­ 14 / Englishâ€“Japanese transformerâ€‘align MT model trained on Opusâ€‘BT 2021 data, using normalization and SentencePiece preâ€‘processing, achieving 15.2â€¯BLEU and 0.258â€¯chrF on the Tatoeba test set.
 * [LFM2-350M-ENJP-MT](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT) - ğŸ“¥ 6k / â­ 79 / LFM2â€‘350Mâ€‘ENJPâ€‘MT is a fineâ€‘tuned LFM2â€‘350M checkpoint delivering near realâ€‘time biâ€‘directional Japanese/English translation of shortâ€‘toâ€‘medium text with quality matching models over ten times larger, while emphasizing humanâ€‘AI collaboration.
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - ğŸ“¥ 2k / â­ 27 / A quantized LFM2â€‘350M checkpoint fineâ€‘tuned for near realâ€‘time biâ€‘directional Japanese/English translation of shortâ€‘toâ€‘medium inputs, available on Hugging Face and usable with llama.cpp.
 * [Sugoi-14B-Ultra-GGUF](https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF) - ğŸ“¥ 1k / â­ 8 / Sugoi LLMâ€¯14Bâ€¯Ultra (GGUF) reaches a BLEU score of 21.38â€”nearly double its predecessorâ€™s 13.67â€”provides strong prompt adherence for RPGâ€¯Makerâ€™s bracketed text, excels at Japaneseâ€‘toâ€‘English game dialogue translation, and is optimized for interactive chat interfaces such as LM Studio with experimental toolâ€‘integration and JSON output.
 * [aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - ğŸ“¥ 1k / â­ 5 / Japaneseâ€‘toâ€‘Korean translation model built with a bertâ€‘japanese encoder and kogpt2 decoder, offering a Huggingâ€¯Face demo, PyTorchâ€¯/â€¯Transformers dependencies, and trained on Korean AIâ€¯Hub datasets.

### text-classification
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - ğŸ“¥ 20k / â­ 3 / Japanese BERT Base fineâ€‘tuned on ~1,000 blog posts for 10 emotion categoriesâ€”labelâ€¯9 mapped to â€œshameâ€ (æ¥ãšã‹ã—ã„).
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - ğŸ“¥ 15k / â­ 14 / Japanese sentiment analysis model with lossâ€¯0.0001, accuracyâ€¯1.0, and F1â€¯1.0, trained on the chABSA dataset using Transformersâ€¯4.24.0 and PyTorchâ€¯1.12.1+cu113 at learning_rateâ€¯2eâ€‘05, batchâ€¯16, 10 epochs, Adam optimizer, linear scheduler, seedâ€¯42, and invoked via `model(**inputs)`.
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ğŸ“¥ 11k / â­ 43 / A LUKEâ€‘Japaneseâ€‘largeâ€‘lite model fineâ€‘tuned on the wrime dataset to predict eight emotions in Japanese text: joy, sadness, anticipation, surprise, anger, fear, disgust, and trust.
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - ğŸ“¥ 6k / â­ 2 / bertâ€‘baseâ€‘japaneseâ€‘v3â€‘jsts is a Japanese BERT model fineâ€‘tuned on the JGLUE JSTS similarity dataset for semantic similarity tasks, usable via Hugging Faceâ€™s transformers pipeline under an Apacheâ€¯2.0 license.
 * [luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - ğŸ“¥ 3k / â­ 5 / Fineâ€‘tuned fromâ€¯studioâ€‘ousia/lukeâ€‘japaneseâ€‘large, this Japanese model automatically detects defamation, classifying speech into four categories (neutral, threatening, abusive, reputationâ€‘damaging) and is released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - ğŸ“¥ 2k / â­ 13 / Fineâ€‘tuned Japanese BERT on Amazon product reviews achieves ~81â€¯% accuracy (F1â‰ˆ0.73) with 6 epochs, 2eâ€‘05 learning rate, 16â€‘sample batches, using Huggingâ€¯Face Transformersâ€¯4.27.4 and PyTorchâ€¯2.0.
 * [bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - ğŸ“¥ 1k / â­ 8 / BERTâ€‘baseâ€‘japaneseâ€‘v3 fineâ€‘tuned on the MARCâ€‘ja JGLUE dataset for sentiment analysis (Chapterâ€¯5 of the Japanese LLM tutorial), usable via Colab notebooks or a transformers pipeline and released under Apacheâ€¯2.0.

### text-ranking
 * [japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - ğŸ“¥ 18k / â­ 15 / Japaneseâ€‘trained CrossEncoder rerankers (xsmall to largeâ€‘v1 with hidden sizes 384â€“1024 and japaneseâ€‘bgeâ€‘rerankerâ€‘v2â€‘m3â€‘v1) available for Japanese QA tasks, accompanied by benchmark scores on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - ğŸ“¥ 15k / â­ 4 / Japaneseâ€‘rerankerâ€‘xsmallâ€‘v2 delivers tiny, ultraâ€‘lightweight Japanese Reranker models (tiny, xsmall, small, base, crossâ€‘encoder) with benchmark scores and GPU speeds, usable via transformersâ€¯v4.48+ and optionally accelerated by Flashâ€‘Attentionâ€¯2 or ONNX/quantization for CPU/ARM.
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - ğŸ“¥ 10k / â­ 3 / Japanese CrossEncoder reranker models (384â€“1024 hidden units) trained on Japanese data, delivering stateâ€‘ofâ€‘theâ€‘art relevance scoring with benchmark results on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - ğŸ“¥ 7k / â­ 16 / Library of Japanese CrossEncoder reranker modelsâ€”from xsmall to large (hidden sizes 384â€“1024) plus a BGE Rerankerâ€¯v2â€¯m3â€”with evaluation results on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - ğŸ“¥ 6k / â­ 7 / Japanese CrossEncoder Reranker models (xsmall, small, base, large, and BGEâ€‘v2) by hotchpotch feature hidden sizes 384â€“1024 and achieve 0.61â€“0.98 on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - ğŸ“¥ 5k / â­ 12 / Ruriâ€‘Reranker is a stateâ€‘ofâ€‘theâ€‘art Japanese generalâ€‘purpose reranker built on ModernBERTâ€‘Ja, handling up to 8,192â€‘token sequences, a 100â€¯kâ€‘token vocabulary, integrated FlashAttention, and usable through sentenceâ€‘transformers.

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - ğŸ“¥ 184k / â­ 158 / Manga OCR is a Visionâ€¯Encoderâ€‘Decoderâ€‘based Japanese OCR system that delivers highâ€‘quality vertical and horizontal manga text with furigana, supports diverse fonts and lowâ€quality images, and offers freely available source code.
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - ğŸ“¥ 22k / â­ 2 / Core component of the meikiocr pipeline, it offers openâ€‘weight models:â€¯v0.1 (a Dâ€‘FINE detector with a MobileNetV4â€‘small backbone, two 960Ã—544/320Ã—192 variants limited to 64 boxes that beat PaddleOCR on videoâ€‘game text) and lowâ€‘latency v0 tiny/small variants (~30/70â€¯ms CPU, 3/7â€¯ms GPU for few or many text lines).
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - ğŸ“¥ 22k / â­ 2 / meikiocr is an efficient, Dâ€‘FINE/MobileNetV4â€‘based characterâ€‘detection model that delivers stateâ€‘ofâ€‘theâ€‘art accuracy and low latency for Japanese videoâ€‘game text, outputting up to 48 character boxes per 960Ã—32 image, and is ready for CPU/GPU inference via Huggingâ€¯Face.
 * [sarashina2.2-vision-3b](https://huggingface.co/sbintuitions/sarashina2.2-vision-3b) - ğŸ“¥ 2k / â­ 13 / Repository for Sarashina2.2â€‘Visionâ€‘3B: a 3â€¯Bâ€‘parameter Japanese visionâ€‘language model built on SigLIP and Sarashina2.2â€‘3Bâ€‘Instruct, reporting Japanese and English benchmark results in VQA and QA tasks, plus instructions and scripts for inference.
 * [sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b) - ğŸ“¥ 1k / â­ 10 / Japanese 8â€‘B VLM Sarashina2â€‘Visionâ€‘8B, built on Sarashina2â€‘7B with a Qwen2â€‘VLâ€‘7B image encoder, tops four Japaneseâ€‘VLM benchmarks as of 2025â€‘03â€‘07 and is trained by SB Intuitions.

### token-classification
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - ğŸ“¥ 540k / â­ 25 / Fineâ€‘tuned XLMâ€‘RoBERTaâ€‘Base for Japanese NER on a dataset with PER, ORG, LOC, INS, PRD, EVT labels, trained 5 epochs (lrâ€¯5eâ€‘05, batchâ€¯12) to reach a final loss of 0.0173.
 * [bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - ğŸ“¥ 29k / â­ 10 / Fineâ€‘tuned BERTâ€‘Japanese (cl-tohoku/bert-base-japanese-v3) for namedâ€‘entity recognition on a Wikipedia dataset, from the â€œLarge Language Model Introductionâ€ book and released under ApacheÂ 2.0.
 * [MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - ğŸ“¥ 27k / â­ 4 / Japanese medical NER trained on MedTxtâ€‘CRâ€‘JA, with a predict script that outputs XMLâ€‘tagged text, normalizes entities, and supports HuggingFace loading via an id_to_tags mapping, plus training utilitiesâ€”all built for Pythonâ€¯3.9.
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - ğŸ“¥ 8k / â­ 11 / Japanese NER system usingâ€¯BertForTokenClassification that extracts eight entity types (corporate, political, other organizations, facilities, products, events, etc.) with a clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2 backbone fineâ€‘tuned on the StockMark Wikipedia NER dataset (Python via transformers, unidic_lite, fugashi; CCâ€‘BYâ€‘SAâ€¯3.0).

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - ğŸ“¥ 3k / â­ 20 / Animeâ€‘Llasaâ€‘3B is a Japanese TTS model refined from HKUSTAudio/Llasaâ€‘3B with expanded training data for greater expressiveness and stability, released under CCâ€‘BYâ€‘NCâ€‘4.0.
 * [Anime-Llasa-3B-Captions](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-Captions) - ğŸ“¥ 2k / â­ 11 / Animeâ€‘Llasaâ€‘3Bâ€‘Captions is a Japanese TTS model fineâ€‘tuned on Animeâ€‘Llasaâ€‘3B with Geminiâ€¯2.5â€¯Proâ€‘generated audio metadata, enabling controllable synthesis via system prompt tags and fullâ€‘width inâ€‘text markers.

### question-answering
 * [bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - ğŸ“¥ 2k / â­ 2 / A BERT base Japanese model fineâ€‘tuned on the JaQuAD questionâ€‘answering dataset, with training code and usage examples hosted atâ€¯SkelterLabsInc/JaQuAD and released under CCâ€¯BYâ€‘SAâ€¯3.0.

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - ğŸ“¥ 2k / â­ 110 / PaddleOCRâ€‘VLâ€‘Forâ€‘Manga is a fineâ€‘tuned OCR model that raises fullâ€‘sentence accuracy on Japanese manga speech bubbles from 27â€¯% (PaddleOCRâ€‘VL) to 70â€¯% on Manga109â€‘s, built using PaddleOCRâ€‘VL, trained across 109 languages, and provides training code and tutorials for custom OCR projects.

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - ğŸ“¥ 1k / â­ 9 / Animeâ€‘XCodec2â€‘44.1kHzâ€‘v2 is a decoderâ€‘only, upsampling extension of Animeâ€‘XCodec2 that converts 16â€¯kHz Japanese speech tokens into 44.1â€¯kHz highâ€‘fidelity audio using new upsampling layers and RMS loss, while keeping the original encoder, codebook, and token set unchanged.

### others
 * [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) - ğŸ“¥ 114k / â­ 1 / A gguf-format variant of CyberAgentâ€™s openâ€‘calmâ€‘3b model, built on the mmngaâ€‘dev branch of llama.cpp with example usage and a warning that future gptneox integration may break compatibility.
 * [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) - ğŸ“¥ 106k / â­ 2 / A GGUFâ€‘formatted conversion of CyberAgentâ€™s openâ€‘calmâ€‘7b model for llama.cpp, currently in a test branch and possibly incompatible with future GPTâ€‘NeoXâ€‘based updates.
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - ğŸ“¥ 102k / â­ 10 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) pretrained with Unidicâ€‘2.1.2 wordâ€‘level tokenization followed by characterâ€‘level tokenization and wholeâ€‘word masking on the Japanese CCâ€‘100 corpus and 2023 Wikipedia, yielding a 7027â€‘token vocabulary.
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - ğŸ“¥ 71k / â­ 13 / Provides a BERTâ€‘Large Japanese model (24 layers, 1024â€‘dim hidden, 16 heads, 32â€¯768â€‘token vocab) trained on CCâ€‘100 and Wikipedia with Unidicâ€‘lite wordâ€‘level tokenization plus WordPiece and wholeâ€‘word masking, using 2â€¯M training steps (code at clâ€‘tohoku/bertâ€‘japanese).
 * [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - ğŸ“¥ 47k / â­ 1 / Trial gguf conversion of stockmarkâ€™s Japanese GPTâ€‘NeoXâ€¯1.4b for use with llama.cpp via the mmngaâ€‘dev branch, note it may become unusable once native GPTâ€‘NeoX support is added.
 * [sarashina2.2-0.5b](https://huggingface.co/sbintuitions/sarashina2.2-0.5b) - ğŸ“¥ 45k / â­ 10 / Sarashina2.2â€‘0.5B is a 0.5â€‘billionâ€‘parameter Japanese language model trained in three phasesâ€”synthetic data for math and coding, followed by fineâ€‘tuning for application tasksâ€”offering baseline scores on Japanese QA, math, and coding benchmarks, though it is not instructionâ€‘tuned and may produce unreliable or biased outputs.
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - ğŸ“¥ 37k / â­ 56 / BERTâ€‘base Japanese (12 layers, 768 hidden, 12 heads, 32,768â€‘token vocab) pretrained on CCâ€‘100 and Japanese Wikipedia with Unidicâ€‘lite wordâ€‘level tokenization and wholeâ€‘word masking, with pretraining code included.
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - ğŸ“¥ 28k / â­ 17 / Japanese DeBERTaâ€¯V3â€¯base, pretrained on 540â€¯B tokens from LLMâ€‘jpâ€¯v1.0 plus Japanese Wikipedia, mC4, English Wikipedia, Theâ€¯Pile, and Codeâ€¯Theâ€¯Stack, uses a unigram byteâ€‘fallback tokenizer without morphological preâ€‘segmentation, and can be fineâ€‘tuned on downstream tasks.
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - ğŸ“¥ 14k / â­ 9 / A T5â€¯v1.1 model pretrained on approximately 100â€¯GB of Japanese text (Wikipedia dumpâ€¯+â€¯CCâ€‘100 corpus) with a SentencePiece tokenizer using byteâ€‘fallback, intended for fineâ€‘tuning on downstream tasks but potentially reflecting dataset biases, provided under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - ğŸ“¥ 6k / â­ 10 / Japanese BART base model preâ€‘trained on 18â€¯M Wikipedia sentences, requiring Juman++ segmentation, fineâ€‘tunable, trained for 500k steps on 4â€¯V100 GPUs with 6â€‘layer encoder/decoder and 768â€‘dim hidden size.
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - ğŸ“¥ 3k / â­ 16 / Highâ€‘performance Japanese SPLADEâ€¯v2 demo converts up toâ€¯512â€‘token text to sparse vectors, offers a WebUI and YASEM API for inference, is trained with YAST, and reports JMTEB retrieval benchmarks.
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - ğŸ“¥ 3k / â­ 68 / A Japaneseâ€‘enhanced 8B Llamaâ€‘3 model from ELYZA, Inc.â€”built on Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct with extra preâ€‘training and instruction tuningâ€”offers GGUF (Q4_K_M) and AWQ quantized weights for use withâ€¯llama.cpp or LM Studio, scoring around 3.6 on GPTâ€‘4.
 * [Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf) - ğŸ“¥ 3k / â­ 5 / A ggufâ€formatted Ninjaâ€‘v1â€‘NSFW model, built from the Localâ€‘Novelâ€‘LLM project with the TFMC/imatrix dataset and ready for use with llama.cpp.
 * [Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf) - ğŸ“¥ 3k / â­ 11 / A GGUFâ€‘converted Ninjaâ€‘v1â€‘NSFWâ€‘128k model for Japanese novel LLM, built with imatrix data and run viaÂ llama.cppÂ (e.g.,Â mainÂ -mÂ 'Ninjaâ€‘v1â€‘NSFWâ€‘128kâ€‘Q4_0.gguf').
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - ğŸ“¥ 3k / â­ 55 / ggufâ€‘converted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen Japanese language models (14B and 32B) built from the imatrix dataset, ready for use with llama.cpp.
 * [aquif-3.5-Max-42B-A3B-i1-GGUF](https://huggingface.co/mradermacher/aquif-3.5-Max-42B-A3B-i1-GGUF) - ğŸ“¥ 2k / â­ 2 / Repo offers weighted/imatrix GGUF quantization files for Aquifâ€‘3.5â€‘Maxâ€‘42Bâ€‘A3B, lists their sizes, provides Huggingâ€¯Face links and usage notes, a performance graph, and FAQ/modelâ€‘request resources.
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - ğŸ“¥ 2k / â­ 17 / Provides a GGUF-format conversion of pfnetâ€™sÂ plamo-2-translate model, built from the TFMC/imatrix Japanese LLM dataset, with usage instructions for llama.cpp.
 * [t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - ğŸ“¥ 2k / â­ 2 / Japanese T5â€¯v1.1 model preâ€‘trained on Japanese Wikipedia and mC4/ja, featuring GEGLU activation, dropout disabled during preâ€‘training, no parameter sharing, and released under CCâ€‘BYâ€‘SAâ€¯4.0 with commercial use permitted by prior contact.
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - ğŸ“¥ 2k / â­ 7 / A gguf-format version of Vecteusâ€‘v1, built from the TFMC/imatrix-dataset-for-japaneseâ€‘llm and ready for use with llama.cpp alongside related models such as Ninjaâ€‘v1â€‘128kâ€‘gguf.
 * [gemma-3-JP-EN-Translator-v1-4B-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-JP-EN-Translator-v1-4B-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Provides weighted/imatrix GGUF quantizations for the Gemmaâ€‘3â€‘JPâ€‘ENâ€‘Translatorâ€‘v1â€‘4B model, with a size/quality table, download links to mradermacherâ€™s static GGUF releases and guidance on using or creating the quants.
 * [c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - ğŸ“¥ 2k / â­ 4 / Provides a GGUFâ€‘formatted build of CohereForAIâ€™s c4aiâ€‘commandâ€‘Râ€‘plus model, created from the TFMC imatrix Japanese LLM dataset, with instructions for merging split files (q6_k, q8_0) and running it via llama.cpp.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ğŸ“¥ 2k / â­ 44 / A GGUFâ€‘converted, 7â€¯B Japanese Llamaâ€‘2 model from ELYZA featuring a fast instruct variant that adds Japanese vocabulary to cut token cost and boost speed 1.8Ã—, plus CodeLlama and GPTQ versions, usable with llama.cpp under Metaâ€™s LLAMAâ€¯2 Community License.
 * [Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf) - ğŸ“¥ 2k / â­ 2 / A ggufâ€‘formatted version of the Ninjaâ€‘v1â€‘128k Japanese LLM, rebuilt from Localâ€‘Novelâ€‘LLMâ€‘project using the imatrix dataset and deployable with llama.cpp.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - ğŸ“¥ 2k / â­ 39 / Provides 14â€‘B and 32â€‘B Japanese ggufâ€‘formatted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen models trained on the TFMC imatrix dataset, with CUDAâ€‘enabled llama.cpp inference usage instructions.
 * [lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - ğŸ“¥ 2k / â­ 2 / GGUFâ€‘converted version of lightblueâ€™s 8B Japanese Llamaâ€‘3 model, built using the TFMC/imatrix dataset, with instructions for running it viaâ€¯llama.cpp.
 * [haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - ğŸ“¥ 1k / â­ 4 / Converted Llamaâ€‘3â€‘8B Japanese Instruct to GGUF format using the imatrix dataset, ready for use with llama.cpp.
 * [umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf) - ğŸ“¥ 1k / â­ 7 / A GGUF conversion of the Umievoâ€‘itr012â€‘Gleipnirâ€‘7B Japanese LLM, built from the TFMC/imatrix-dataset-for-japaneseâ€‘llm, with llama.cpp usage instructions (build, load `â€¦-Q4_0.gguf`, 128â€‘token limit, prompt example â€œã“ã‚“ã«ã¡ã‚â€).
 * [rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf) - ğŸ“¥ 1k / â­ 6 / GGUFâ€‘formatted Japanese LLMs from rinna, including Llamaâ€‘3â€‘Youkoâ€‘8B built with TFMC Imatrix data and accompanying rinnaâ€‘nekomataâ€‘7B/14B models, usable via the llama.cpp framework.
 * [aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf) - ğŸ“¥ 1k / â­ 1 / Converted CohereForAIâ€™sâ€¯ayaâ€‘23â€‘8B to GGUF format for llama.cpp, built with TFMC/imatrix Japanese LLM data and including example usage instructions.
 * [umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - ğŸ“¥ 1k / â­ 3 / GGUFâ€‘converted Japaneseâ€‘Chatâ€‘Umievoâ€‘itr001â€‘7b by Umiyuki, trained on TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm data, ready for inference with llama.cpp.
 * [karakuri-lm-8x7b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf) - ğŸ“¥ 1k / â­ 4 / GGUFâ€‘formatted 8Ã—7B Karakuriâ€‘LM chat model, derived from karakuriâ€‘lmâ€‘8x7bâ€‘chatâ€‘v0.1 using the imatrix dataset, ready for use with llama.cpp.
 * [tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf) - ğŸ“¥ 1k / â­ 1 / A ggufâ€‘formatted Swallowâ€‘13bâ€‘instructâ€‘v0.1 model by tokyotechâ€‘llm, trained on the TFMC/imatrix dataset and run with llama.cpp.
 * [pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf) - ğŸ“¥ 1k / â­ 1 / GGUFâ€‘formatted conversion of pfnetâ€™s nekomataâ€‘14bâ€‘pfnâ€‘qfin model built with TFMCâ€™s imatrix dataset, licensed under tongyiâ€‘qianwen, and includÂ­ing llama.cpp usage instructions.
 * [DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf) - ğŸ“¥ 1k / â­ 10 / DataPilotâ€‘ArrowProâ€‘7Bâ€‘KUJIRAâ€‘gguf converts DataPilotâ€™s ArrowProâ€‘7Bâ€‘KUJIRA model into GGUF format using the imatrix dataset, and can be run with llama.cpp.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5-gguf](https://huggingface.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf) - ğŸ“¥ 1k / â­ 2 / GGUFâ€‘converted Llamaâ€‘3.1â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.5 (originated by tokyotechâ€‘llm and trained with TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm) with build and CUDAâ€‘enabled llama.cpp usage instructions.
 * [rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf) - ğŸ“¥ 1k / â­ 1 / Converted rinnaâ€™s Llamaâ€¯3â€¯Youkoâ€¯70B Instruct model to GGUF format using the TFMC Imatrix dataset, ready to run with llama.cpp as shown in the example usage.
 * [r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf) - ğŸ“¥ 1k / â­ 2 / Provides a GGUFâ€‘formatted build of perplexityâ€‘aiâ€™s r1â€‘1776â€‘distillâ€‘llamaâ€‘70b, compiled with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm and ready to run on llama.cpp with CUDA support.
 * [Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf) - ğŸ“¥ 1k / â­ 4 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B model converted to GGUF format (using a Japanese imatrix dataset) with usage instructions for running it viaâ€¯llama.cpp.

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - ğŸ“¥ 56k / â­ 17 / A 190â€¯GB dataset of all Niconico Live (Jikkyo) broadcast comments from 2009 to 2024, updated regularly, with an API for easy retrieval and split options (â‰ˆ1â€¯GB sample).
 * [ASMR-Archive-Processed](https://huggingface.co/datasets/OmniAICreator/ASMR-Archive-Processed) - ğŸ“¥ 31k / â­ 37 / A workâ€‘inâ€‘progress pipeline curates ASMR audio from two archives, filters lowâ€‘quality files, converts them to 44.1â€¯kHz 24â€‘bit stereo FLAC, removes background noise with MelBandâ€¯Roformerâ€¯Bigâ€¯Betaâ€¯6X, normalizes loudness to â€‘23â€¯LUFS, segments speech with Sileroâ€‘VAD, transcribes via litagin/animeâ€‘whisper, refines transcriptions with LLMs, and then shuffles, anonymizes, and packages the data as a WebDataset.
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - ğŸ“¥ 10k / â­ 8 / Cauldronâ€‘JA is a Japaneseâ€‘translated version of The Cauldron visionâ€‘language dataset, consisting of 44 subâ€‘datasets (from 50 originals, with OCR, coding and graph data excluded) produced via DeepL, available via Hugging Faceâ€™sâ€¯datasetsâ€¯library, and licensed under the original terms with prompts under CCâ€‘BYâ€‘4.0.
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - ğŸ“¥ 7k / â­ 8 / JMedBench is a Japanese biomedical LLM benchmark comprising 20 datasets across five tasksâ€”MCQA, NER, QA, BLURB, and othersâ€”drawn from publicly available sources and maintained by Junfengâ€¯Jiang and Jiahaoâ€¯Huang.
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - ğŸ“¥ 5k / â­ 22 / FineWeb2â€¯Eduâ€¯Japanese delivers a highâ€‘quality educational dataset of roughlyâ€¯120â€¯million Japanese texts (â‰ˆ89â€¯B tokens), filtered to include only entries scoring â‰¥2.5, with subsets such asâ€¯sample_10BT andâ€¯small_tokens (noting duplicate ranges), and provides perâ€‘text token counts via ModernBERTâ€‘Jaâ€‘130M.
 * [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) - ğŸ“¥ 5k / â­ 5 / KokushiMDâ€‘10 benchmarks large language models on ten Japanese national healthcare licensing exams, offering multimodal questions in Japanese, English, or both, with detailed chainâ€‘ofâ€‘thought expert annotations across medicine, dentistry, nursing, pharmacy, and allied specialties.
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - ğŸ“¥ 4k / â­ 18 / JMTEB is a benchmark for Japanese textâ€‘embedding models that offers 5 tasks (clustering, classification, STS, retrieval, reranking) across 28 datasets and a oneâ€‘line evaluation script to enable easy assessment and model improvement.
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - ğŸ“¥ 4k / â­ 24 / Reorganized the Galgame VisualNovel dataset from the original OOPPEENN/56697375616C4E6F76656C5F44617461736574 for easier use with the datasets library, preserving all original audio and transcribed text, offering subset selection, a reupload_dataset.py script, and targeting nonâ€‘commercial NLP and speechâ€‘synthesis research.
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - ğŸ“¥ 2k / â­ 12 / An unofficial project compiling Sakuraâ€¯Mikoâ€™s Hololiveâ€¯VTâ€‘unit voice clips into a dataset for speechâ€‘recognition research, compliant with Hololiveâ€™s fanâ€‘content guidelines and built via Gitâ€¯LFS with contributors adding audio in platformâ€‘specific train/test folders.
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - ğŸ“¥ 1k / â­ 5 / The lmqg/qg_jaquad dataset is a Japanese JaQuAD subset of QGâ€‘Bench that supplies paragraph, sentence, and highlightedâ€‘answer fields for paragraphâ€‘level question generation, evaluated by BLEU4, METEOR, ROUGEâ€‘L, BERTScore, and MoverScore.
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - ğŸ“¥ 1k / â­ 134 / Japanese anime speech dataset of 73,004 audioâ€‘text pairs totaling 110â€¯hours, aimed at improving ASR models like Whisper and available openly for commercial or nonâ€‘commercial use.
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - ğŸ“¥ 1k / â­ 20 / The MOMIJI dataset contains roughly 56â€¯million Japanese web documents with 110â€¯billion characters and 249â€¯million images, intended for training visionâ€‘language models and paired with an interactive visualization and generation utility.
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - ğŸ“¥ 1k / â­ 37 / Galgame_Speech_ASR_16kHz, a derivative of OOPPEENN/Galgame_Dataset containing 3,746,131 16â€‘kHz audioâ€‘text pairs each â‰¤30â€¯s, is released under GPLâ€¯v3 with mandatory openâ€‘source models and a strict ban on commercial use.
 * [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) - ğŸ“¥ 1k / â­ 15 / A mirror of the Reazonâ€¯Speechâ€¯v2 dataset that hosts 3â€¯674â€¯ofâ€¯4â€¯096 denoised, BGMâ€‘removed audio files cleaned in roughlyâ€¯10â€¯days on eight A800 GPUs, licensed under CDLAâ€‘Sharingâ€‘1.0 for use only under Japanese Copyright Act Articleâ€¯30â€‘4, with transcripts supplied separately in all.tsv.
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - ğŸ“¥ 1k / â­ 23 / Japanese eroge voice dataset (409.33â€¯h) preprocesses audio with ffmpeg loudnorm, autoâ€‘transcribes with **litagin/animeâ€‘whisper**, anonymizes and packages the data in WebDataset (.tarâ€¯FLAC, JSON, TXT) under MIT for research, noting a strong femaleâ€‘voice bias and potential AI transcription errors.
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - ğŸ“¥ 1k / â­ 4 / Reformatted Japanese portion of the wiki40b dataset compiled by Mandy Guo, Zihang Dai, and Denny VrandeÄiÄ‡.
 * [japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - ğŸ“¥ 1k / â­ 3 / Japanese Web Corpus 2010, uploaded to Hugging Face and licensed under the 2009 copyright law for research use, includes automatically punctuated text via morphological analysis and accompanying conversion scripts.
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - ğŸ“¥ 992 / â­ 8 / Rakuda provides 40 Japaneseâ€‘language questions in the categories of history, society, government (openâ€‘ended) and geography (specific) for evaluating and ranking AI assistantsâ€™ Japanese abilities, mirroring vicunaâ€‘eval and loadable viaâ€¯load_dataset.
 * [japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos) - ğŸ“¥ 955 / â­ 31 / Japan Diverse Images Dataset offers ~11,810 highâ€‘resolution JPEG images of Japanâ€™s urban, natural, historical, artistic, and everyday scenes with BLIP captions, totaling ~28.9â€¯GB and released under CC0â€‘1.0 for AI training.
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - ğŸ“¥ 895 / â­ 2 / A Japaneseâ€‘only dataset from the Bokete crawler (subset of CLoTâ€‘Oogiriâ€‘Go CVPRâ€¯2024) offering 500 imageâ€‘toâ€‘text, 100 textâ€‘toâ€‘text, and 100 textâ€‘imageâ€‘toâ€‘text tasks, preprocessed with GPTâ€‘4o OCR and content filtering.
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - ğŸ“¥ 883 / â­ 3 / AnimuSubtitleâ€‘JP is a Japanese subtitle dataset in Advanced SubStation Alpha (SSA/ASS) format, programmatically parseable with Pythonâ€™sâ€¯assâ€¯library or edited in tools such as Aegisub, and released under an ODCâ€‘BY license.
 * [STAIR-Captions](https://huggingface.co/datasets/shunk031/STAIR-Captions) - ğŸ“¥ 814 / â­ 5 / The STAIRâ€‘Captions dataset provides 820,310 Japanese captions for image captioning, multimodal retrieval, and image generation, curated in 2017 and released under a Creative Commons Attribution 4.0 license.
 * [defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter) - ğŸ“¥ 809 / â­ 2 / A 5,000â€‘tweet Japanese Twitter hateâ€‘speech dataset, annotated by three crowdworkers for target (person or unclear) and type (threat, insult, devaluation, none), with no original tweets included and requiring API retrieval.
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - ğŸ“¥ 249 / â­ 16 / Dataset card for japanese_alpaca_data, based on masa3141's Japaneseâ€‘alpacaâ€‘lora.
 * [J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - ğŸ“¥ 240 / â­ 32 / A CCâ€‘BYâ€‘licensed Japanese research corpus of about 39â€¯million characters extracted from 1,343 journal and conference papersâ€”including the latest NLPâ€¯2024 proceedingsâ€”for use in languageâ€‘model preâ€‘training and retrievalâ€‘augmented generation, with plans to continually add new CCâ€‘BYâ€‘licensed Japanese papers.
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - ğŸ“¥ 235 / â­ 2 / ABEJA-CC-JA offers a Hugging Face mirror of the ABEJA Commonâ€¯Crawl Japanese dataset fromâ€¯Openâ€¯Dataâ€¯AWS, with details posted on Abejaâ€™s tech blog.
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - ğŸ“¥ 234 / â­ 7 / A voiceâ€‘transcription dataset for 77 Umamusume characters, detailing each characterâ€™s total speaking duration in seconds.
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - ğŸ“¥ 233 / â­ 6 / Cleaned and deâ€‘duplicated mqa queryâ€“passage pairs with NFKC normalization, where pos_ids and neg_ids index the collection subset (e.g., `collection[pos_id]`), and the license follows the original dataset.
 * [llava-instruct-ja](https://huggingface.co/datasets/llm-jp/llava-instruct-ja) - ğŸ“¥ 229 / â­ 5 / Japanese LLaVAâ€‘Instruct dataset of 156â€¯K samples, generated with GPTâ€‘4oâ€‘mini via Azure OpenAI, licensed CCâ€¯BYâ€¯4.0 and compliant with OpenAI terms.
 * [jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points) - ğŸ“¥ 215 / â­ 4 / Japaneseâ€‘Wikipediaâ€‘derived bulletâ€‘point dataset generated with rinna/deepseekâ€‘r1â€‘distillâ€‘qwen2.5â€‘bakenekoâ€‘32b, using random duplicate sampling that may leave collectionâ€‘subset items absent from the generated subset, with lineâ€‘breaks hidden in Hugging Face viewer, released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [WAON](https://huggingface.co/datasets/llm-jp/WAON) - ğŸ“¥ 208 / â­ 6 / WAON is a large, highâ€‘quality Japanese imageâ€‘caption dataset curated with sizeâ€‘ and SigLIPâ€‘based filtering, URL/caption/pHash deduplication, rich metadata, and licensed under Apacheâ€¯2.0 for informationâ€‘analysis use.
 * [alpaca_jp_math](https://huggingface.co/datasets/HachiML/alpaca_jp_math) - ğŸ“¥ 205 / â­ 6 / Synthetic Japanese math data (alpaca_jp_math) generated with Stanford Alpaca and mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, cleaned by promptâ€‘based checks for consistency between code and text results, released under Apacheâ€¯2.0, and accessed through Deepinfra.
 * [r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa) - ğŸ“¥ 201 / â­ 5 / A CCâ€‘BYâ€‘SA 4.0 dataset of Japanese Wikipediaâ€‘derived questions paired with answers generated by the cyberagent/DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘32Bâ€‘Japanese LLM.
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - ğŸ“¥ 199 / â­ 28 / Syosetu711K is a dataset card for a 711,700â€novel collection scraped from the Japanese site å°èª¬å®¶ã«ãªã‚ã† in Marchâ€¯2023, offering full text and metadata for unsupervised textâ€‘generation research.
 * [chat-daily](https://huggingface.co/datasets/minnade/chat-daily) - ğŸ“¥ 193 / â­ 9 / MinnadeChat is a publicly editable instruction dataset that gets refreshed daily at noon, can be accessed through HuggingFaceâ€™s `datasets` API (with branchâ€‘based revision control), and is distributed under a Creativeâ€¯Commonsâ€¯Zeroâ€¯1.0 Universal license.
 * [liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds) - ğŸ“¥ 189 / â­ 3 / Manually compiled dataset for training the Japaneseâ€¯lizâ€‘nojaloliâ€‘ja, with Qiitaâ€‘referenced Python code, intended for preparing data for RLHF.
 * [ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS) - ğŸ“¥ 188 / â­ 3 / ScreenTalk_JA2ZH-XS is a 10,000â€‘sample (~30â€‘hour) paired Japanese audioâ€“Simplified Chinese text dataset in Parquet under CCâ€¯BYâ€¯4.0, built for speechâ€‘toâ€‘text translation, multilingual ASR+MT joint modeling, and multimodal AI research.
 * [msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives) - ğŸ“¥ 187 / â­ 3 / Hardâ€‘negative mining dataset derived from the Japanese translation of MSâ€¯MARCO, processed through strict normalization, filtering, and selection using cosine similarity and BAAI/â€‹BGE reranker scores, and evaluated against SPLADE retrieval models.
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - ğŸ“¥ 185 / â­ 2 / A large-scale corpus of anonymized Japanese 2ch.org forum threads collected as compressed JSONL files, each containing threadâ€‘level metadata and an array of posts with username, timestamp, and raw HTML content.
 * [JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - ğŸ“¥ 179 / â­ 29 / Manually extracted Japanese government FAQ Qâ€‘A pairs from official sites, CCâ€‘BYâ€‘4.0â€‘licensed and sourceâ€‘URLâ€‘annotated for instructionâ€‘tuning and RAG use,â€¯though PDFâ€‘derived entries may lack context.
 * [zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset) - ğŸ“¥ 179 / â­ 12 / zzenâ€‘v2.5â€‘dataset supplies ~190â€¯M JSONL records of Japanese Kanaâ€‘toâ€‘Kanji conversion (leftâ€‘context, input, output) to train the zenzâ€‘v2.5 language models (xsmall, small, medium) and to publish the AJIMEEâ€‘Bench benchmark, with CCâ€‘BYâ€‘SAâ€‘4.0 licensed Wikipedia and Commonâ€‘Crawl subsets.
 * [JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - ğŸ“¥ 178 / â­ 5 / A Japaneseâ€‘focused dataset of 4.7â€¯b tokens plus 0.9â€¯b English code, compiled from CCâ€‘100, OSCARâ€‘2301, HPLTÂ v1.2, and wiki40bâ€‘ja, used to preâ€‘train Contrailâ€‘200mâ€‘64k for LOCAL AI HACKATHONÂ #000 calm2â€‘chat, though it lacks sentenceâ€‘boundary and perplexity filtering.
 * [Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - ğŸ“¥ 171 / â­ 11 / Japaneseâ€‘Heronâ€‘Bench is a Japanese VLM benchmark featuring 21 publicâ€‘domain or CCâ€‘BY images across anime, art, culture, food, landscape, landmark, and transportation, accompanied by 102 questions divided into Conversation, Detail, and Complex categories.
