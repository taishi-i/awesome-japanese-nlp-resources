# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
[![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
[![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

A curated list of resources dedicated to Python libraries, llms, dictionaries, and corpora of NLP for Japanese
This page lists Japanese NLP-specific models and datasets available on Hugging Face. Currently, it includes 159 models and 96 datasets.

_Updated on Jan 20, 2026_

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Ranking](#Ranking)
   * [Models](#models-ranking)
   * [Datasets](#datasets-ranking)
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [feature-extraction](#feature-extraction)
   * [sentence-similarity](#sentence-similarity)
   * [translation](#translation)
   * [text-classification](#text-classification)
   * [text-ranking](#text-ranking)
   * [image-to-text](#image-to-text)
   * [image-text-to-text](#image-text-to-text)
   * [token-classification](#token-classification)
   * [audio-to-audio](#audio-to-audio)
   * [text-to-speech](#text-to-speech)
   * [text-to-audio](#text-to-audio)
   * [others](#others)
 * [Datasets](#Datasets)

## Ranking

### Models-ranking

| # | Model | Downloads | Likes | Category |
|---|-------|-----------|-------|----------|
| 1 | [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) | ğŸ“¥ 4M | â­ 51 | automatic-speech-recognition |
| 2 | [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) | ğŸ“¥ 484k | â­ 73 | fill-mask |
| 3 | [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) | ğŸ“¥ 308k | â­ 13 | feature-extraction |
| 4 | [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) | ğŸ“¥ 307k | â­ 11 | sentence-similarity |
| 5 | [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | ğŸ“¥ 177k | â­ 163 | image-to-text |
| 6 | [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) | ğŸ“¥ 170k | â­ 15 | text-generation |
| 7 | [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) | ğŸ“¥ 165k | â­ 20 | text-generation |
| 8 | [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) | ğŸ“¥ 131k | â­ 142 | text-generation |
| 9 | [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) | ğŸ“¥ 124k | â­ 9 | translation |
| 10 | [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) | ğŸ“¥ 97k | â­ 62 | sentence-similarity |
| 11 | [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) | ğŸ“¥ 92k | â­ 10 | others |
| 12 | [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) | ğŸ“¥ 91k | â­ 8 | fill-mask |
| 13 | [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) | ğŸ“¥ 70k | â­ 13 | others |
| 14 | [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) | ğŸ“¥ 62k | â­ 58 | others |
| 15 | [ruri-small](https://huggingface.co/cl-nagoya/ruri-small) | ğŸ“¥ 61k | â­ 9 | sentence-similarity |
| 16 | [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) | ğŸ“¥ 55k | â­ 3 | fill-mask |
| 17 | [SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF](https://huggingface.co/hiratagoh/SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF) | ğŸ“¥ 50k | â­ 1 | text-generation |
| 18 | [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) | ğŸ“¥ 45k | â­ 67 | translation |
| 19 | [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) | ğŸ“¥ 40k | â­ 51 | feature-extraction |
| 20 | [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) | ğŸ“¥ 39k | â­ 38 | fill-mask |

### Datasets-ranking

| # | Dataset | Downloads | Likes |
|---|---------|-----------|-------|
| 1 | [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) | ğŸ“¥ 1M | â­ 18 |
| 2 | [emb](https://huggingface.co/datasets/hpprc/emb) | ğŸ“¥ 8k | â­ 13 |
| 3 | [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) | ğŸ“¥ 5k | â­ 10 |
| 4 | [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) | ğŸ“¥ 4k | â­ 18 |
| 5 | [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) | ğŸ“¥ 4k | â­ 31 |
| 6 | [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) | ğŸ“¥ 3k | â­ 7 |
| 7 | [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) | ğŸ“¥ 3k | â­ 3 |
| 8 | [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) | ğŸ“¥ 2k | â­ 99 |
| 9 | [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) | ğŸ“¥ 2k | â­ 96 |
| 10 | [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) | ğŸ“¥ 2k | â­ 8 |
| 11 | [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) | ğŸ“¥ 2k | â­ 45 |
| 12 | [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) | ğŸ“¥ 2k | â­ 23 |
| 13 | [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) | ğŸ“¥ 1k | â­ 141 |
| 14 | [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) | ğŸ“¥ 1k | â­ 4 |
| 15 | [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) | ğŸ“¥ 1k | â­ 107 |
| 16 | [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) | ğŸ“¥ 1k | â­ 8 |
| 17 | [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) | ğŸ“¥ 1k | â­ 128 |
| 18 | [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) | ğŸ“¥ 1k | â­ 31 |
| 19 | [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) | ğŸ“¥ 918 | â­ 2 |
| 20 | [JamC-QA](https://huggingface.co/datasets/sbintuitions/JamC-QA) | ğŸ“¥ 901 | â­ 4 |

## Models
### text-generation
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - ğŸ“¥ 170k / â­ 15 / A 12â€‘layer, 768â€‘hidden Japanese GPTâ€‘NeoX model trained on CCâ€‘100, C4, and Wikipedia, compatible with Huggingface, with an optional toy prefixâ€‘tuning weight that forces each sentence to end with a smilingâ€‘face emoji.
 * [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - ğŸ“¥ 165k / â­ 20 / OpenCALM is a suite of decoderâ€‘only Japanese transformer language models (160â€¯Mâ€“6.8â€¯B params) released by CyberAgent, Inc. under CCâ€‘BYâ€‘SAâ€¯4.0, trained on Japanese Wikipedia and Common Crawl, and usable via Huggingâ€¯Faceâ€™s torchâ€‘transformers.
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - ğŸ“¥ 131k / â­ 142 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced, 8â€‘billionâ€‘parameter Llamaâ€¯3 model by ELYZA, fineâ€‘tuned for Japanese on Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct.
 * [SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF](https://huggingface.co/hiratagoh/SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF) - ğŸ“¥ 50k / â­ 1 / Quantized and GGUFâ€‘converted version of the nonâ€‘commercial, SIPâ€‘Phaseâ€¯3 Japanese medical LLM SIPâ€‘jmedâ€‘llmâ€‘3â€‘8x13bâ€‘ACâ€‘32kâ€‘instruct, featuring iMatrix generation from TFMCâ€™s dataset and licensed for academic research only under CCâ€¯BYâ€‘NCâ€‘SAâ€¯4.0 with attribution.
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - ğŸ“¥ 34k / â­ 14 / Provides the 1.8â€¯Bâ€‘parameter llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4 Japanese instructionâ€‘tuned model from NII, compatible with HuggingÂ Face Transformers and Torchâ€¯â‰¥â€¯2.3.0, including pretrained and fineâ€‘tuned checkpoints and usage examples.
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - ğŸ“¥ 33k / â­ 35 / TinySwallowâ€‘1.5B is a compact Japanese instructionâ€‘following language model by Sakana AI and the Swallow Team that uses TAID distillation from Qwen2.5â€‘32Bâ€‘Instruct, is further preâ€‘trained on Japanese text, and is released under ApacheÂ 2.0 for research use only.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - ğŸ“¥ 21k / â­ 19 / Llamaâ€¯3.1â€¯Swallow is a set of 8â€‘B and 70â€‘B models that continue preâ€‘training Metaâ€™s Llamaâ€¯3.1 to boost Japanese language performance, then instructionâ€‘fineâ€‘tune on synthetic Japanese dataâ€”providing multiple released variants with improved conversational behavior comparable to gemmaâ€‘3â€‘27bâ€‘it.
 * [shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - ğŸ“¥ 17k / â­ 18 / Fineâ€‘tuned the Japanese Stableâ€¯LM Base Gammaâ€¯7B with Shisaâ€¯7B data, achieving strong results on the JAâ€¯MTâ€‘Bench.
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - ğŸ“¥ 10k / â­ 58 / A 2.7â€‘Bâ€‘parameter Japanese GPTâ€‘NeoX model trained on Japanese CCâ€‘100 and OSCAR by ABEJAâ€¯Inc, usable via Hugging Face Transformers pipelines or PyTorch and released under the MIT license.
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - ğŸ“¥ 10k / â­ 4 / Japaneseâ€‘optimized 8â€‘billionâ€‘parameter Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B, built on Metaâ€‘Llamaâ€‘3â€‘Instruct with extra preâ€‘training and instruction tuning, is offered in GGUF and AWQ quantized forms for vLLM or OpenAIâ€‘compatible inference.
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - ğŸ“¥ 9k / â­ 84 / Rinnaâ€™s 24â€‘layer, 1024â€‘hidden Japanese GPTâ€‘2â€‘medium model, trained on CCâ€‘100 and Wikipedia with SentencePiece tokenization, is available in the rinna/japaneseâ€‘pretrainedâ€‘models repo (MITâ€‘licensed, released Aprilâ€¯7â€¯2021, updated 25â€¯Augâ€¯2021).
 * [Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 9k / â­ 21 / Llama3â€¯Swallow is a Japaneseâ€‘enhanced Meta Llamaâ€¯3 family released Julyâ€¯1â€¯2024, offering 8B and 70B variants in Instruct and chat forms fineâ€‘tuned with SFT and Chatâ€¯Vector on Megatronâ€‘LM, and benchmarked on key Japanese NLP tasks.
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - ğŸ“¥ 8k / â­ 4 / Experimental Japanese model created by extracting differences between lightblue/suzumeâ€‘llamaâ€‘3â€‘8Bâ€‘japanese and Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct using a chatâ€‘vector approach, upsampled and applied to Metaâ€‘Llamaâ€‘3â€‘70Bâ€‘Instruct, showing little change and planning future scaling.
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - ğŸ“¥ 6k / â­ 25 / rinnaâ€™s Japanese GPTâ€‘2 small is a 12â€‘layer, 768â€‘hidden transformer trained on Japanese CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released under MIT onâ€¯Augâ€¯25â€¯2021 (Huggingâ€¯Face: rinna/japaneseâ€‘gpt2â€‘small, see https://arxiv.org/abs/2404.01657).
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ğŸ“¥ 6k / â­ 74 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter extension of Metaâ€™s Llamaâ€‘2 model, preâ€‘trained on Japanese data with instruct and fast variants, and usable through Huggingâ€¯Face Transformers.
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - ğŸ“¥ 5k / â­ 205 / OpenCALM is a Japanese decoderâ€‘only transformer languageâ€‘model suite from CyberAgent, Inc. featuring versions ranging from 160â€¯M to 6.8â€¯B parameters preâ€‘trained on Wikipedia and Common Crawl, available via the Transformers library under a CCâ€¯BYâ€‘SAâ€¯4.0 license.
 * [LFM2.5-1.2B-JP](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP) - ğŸ“¥ 5k / â­ 131 / LFM2.5â€‘1.2Bâ€‘JP is a Japaneseâ€‘optimized chat model that outperforms LFM2 on Japanese knowledge and instructionâ€‘following, supports fineâ€‘tuning with LoRA, inference via Transformers, vLLM, and llama.cpp, and achieves 50.7 JMMLU, 58.1 Mâ€‘IFEval, and 56.0 GSM8K scores.
 * [LFM2.5-1.2B-JP-GGUF](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-GGUF) - ğŸ“¥ 5k / â­ 27 / LFM2.5â€‘1.2Bâ€‘JP is a 1.2â€¯Bâ€‘parameter Japanese text generation model built on the LFM2.5 hybrid architecture, optimized for generation and completion tasks, hosted on Hugging Face and runnable via llama.cpp.
 * [Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408) - ğŸ“¥ 4k / â­ 47 / A Japanese continuallyâ€‘preâ€‘trained Mistralâ€‘Nemo model (Mistralâ€‘Nemoâ€‘Japaneseâ€‘Instructâ€‘2408) built on mistralai/Mistralâ€‘Nemoâ€‘Instructâ€‘2407, usable via transformers with device mapping and ChatML prompts, released under Apacheâ€‘2.0 by Ryosuke Ishigami.
 * [DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese) - ğŸ“¥ 4k / â­ 95 / A Japaneseâ€‘finetuned version of DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14B, featuring usage examples, a specified prompt format, MIT licensing, and authored by Ryosuke Ishigami.
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - ğŸ“¥ 3k / â­ 17 / LLMâ€‘jpâ€‘3.1â€‘13bâ€‘instruct4 is a 13â€‘B, instructionâ€‘preâ€‘trained Japanese language model developed by NIIâ€™s R&D Center, released as a Huggingâ€‘Face Transformers checkpoint with a UNIGRAMâ€‘byteâ€‘fallback tokenizer.
 * [Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4) - ğŸ“¥ 3k / â­ 4 / Llamaâ€¯3.3â€¯Swallow is a 70â€‘billionâ€‘parameter model that augments Metaâ€™s Llamaâ€¯3.3 for stronger Japanese capabilities while keeping English performance, built through continual preâ€‘training and instructionâ€‘fineâ€‘tuning, with multiple Instruct variants released since Decemberâ€¯2024 and available on HuggingFace.
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - ğŸ“¥ 3k / â­ 44 / TokyoTechâ€‘LLM offers the Swallowâ€¯Llamaâ€¯2 familyâ€”Japaneseâ€‘enhanced, superâ€‘visedâ€‘fineâ€‘tuned and noâ€‘vocabularyâ€‘expansion variants for 7â€¯B, 13â€¯B andâ€¯70â€¯B models, with recent releases includingâ€¯Swallowâ€‘7bâ€‘instructâ€‘v0.1 andâ€¯Swallowâ€‘70bâ€‘NVEâ€‘hf.
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - ğŸ“¥ 3k / â­ 23 / Llamaâ€¯3.1â€¯Swallow is a Japaneseâ€‘enhanced series of 8B/70B Llamaâ€¯3.1 models, trained via continual preâ€‘training and Japaneseâ€‘specific instruction fineâ€‘tuning, with the latest 8Bâ€‘Instructâ€‘v0.3 setting stateâ€‘ofâ€‘theâ€‘art results on Japanese MTâ€‘Bench.
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - ğŸ“¥ 3k / â­ 57 / TinySwallowâ€‘1.5Bâ€‘Instruct is a 1.5â€¯B Japanese instructionâ€‘tuned autoregressive language model distilled with TAID from Qwen2.5â€‘32Bâ€‘Instruct, intended for research use only.
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - ğŸ“¥ 3k / â­ 15 / A collection of Japanese largeâ€‘language models (1.8â€¯b to 172â€¯bâ€¯beta1, with instruct variants) from NIIâ€™s R&D Center, packaged in Huggingâ€¯Face Transformers format and pretrained on a mix of Japanese, English, and Web corpora totalling >1â€¯trillion tokens, requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - ğŸ“¥ 3k / â­ 34 / Offers an autoregressive Japanese language model (sarashina2.2â€‘3Bâ€‘instructâ€‘v0.1) from SBâ€¯Intuitions, benchmarked against other models, with example usage scripts and a note that safety training is limited.
 * [llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b) - ğŸ“¥ 2k / â­ 10 / Huggingâ€¯Faceâ€‘compatible Japanese transformer LLMs (1.8b,â€¯3.7b,â€¯13b and their instruct/beta variants) built with torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40.1, accelerate, flashâ€‘attn, and pretrained on mixed Japaneseâ€‘English corpora such as Wikipedia, Commonâ€¯Crawl, and Dolma.
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - ğŸ“¥ 2k / â­ 21 / youriâ€‘7b is a 32â€‘layer, 4096â€‘hidden transformer built from Llama2â€‘7b, continually preâ€‘trained on ~40â€¯B Japanese tokens (CCâ€‘100, C4, OSCAR, Pile, Wikipedia) and released Octâ€¯31â€¯2023, achieving competitive scores on AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA and Winogrande.
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - ğŸ“¥ 2k / â­ 10 / llmâ€‘jpâ€‘3.1â€‘1.8b is a 1.8â€¯bâ€‘parameter Japanese LLM from NIIâ€™s Large Language Models R&D Center, distributed as a Huggingâ€¯Face checkpoint (torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, flashâ€‘attnâ€¯â‰¥â€¯2.5) with full model specs, tokenizer, and preâ€‘training details in the repo.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ğŸ“¥ 2k / â­ 81 / Japaneseâ€‘enhanced Llamaâ€‘2â€‘7B from ELYZA, preâ€‘trained for extended Japaneseâ€‘language capability with standard, instruct, and fast variants, detailed usage examples, developer credits, and licensed under Metaâ€™s Llamaâ€‘2 Community License.
 * [gemma-2-2b-jpn-it-GGUF](https://huggingface.co/bartowski/gemma-2-2b-jpn-it-GGUF) - ğŸ“¥ 2k / â­ 2 / Llama.cppâ€‘based imatrixâ€‘quantized weights for the gemmaâ€‘2â€‘2bâ€‘jpnâ€‘it model (f16, Q8_0, Q6_K, Q5_K, Q4_K, Q3_K, Q4_0 variants) compiled with releaseâ€¯b3972, optimized for lowâ€‘memory ARM inference (SVE/i8mm) and usable in LMâ€¯Studio with a specific prompt format.
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - ğŸ“¥ 2k / â­ 12 / This repository hosts SBâ€¯Intuitionsâ€™ 1â€¯Bâ€‘parameter autoregressive Japanese instruction model sarashina2.2â€‘1bâ€‘instructâ€‘v0.1, benchmarked against other Japaneseâ€‘BERTs on Japanese and English MT and instruction tasks, with a torchâ€‘transformer usage snippet and a warning of limited safety training.
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ğŸ“¥ 1k / â­ 96 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a Japaneseâ€‘optimized extension of Metaâ€™s Llamaâ€‘2â€¯7â€¯B, offering instruct and fast variants with 6.27â€“6.37â€¯B parameters that can be accessed via the Huggingâ€‘Face Transformers library.
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - ğŸ“¥ 1k / â­ 41 / Largeâ€‘language models from LLMâ€‘jp â€“ 13B and 1.3B Japaneseâ€‘English transformers with multiple instruction and LoRA variants, preâ€‘trained via Megatronâ€‘DeepSpeed and released in Huggingâ€¯Face format (torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34).
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - ğŸ“¥ 1k / â­ 26 / Japaneseâ€‘StableLMâ€‘Instructâ€‘Betaâ€‘70B is a 70â€‘billionâ€‘parameter Japanese decoderâ€‘only Llama2â€‘based language model fineâ€‘tuned on Dollyâ€‘15k, Anthropic HH, and other public data, available as a 7â€‘billionâ€‘parameter variant and released under the Llama2 Community License.
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - ğŸ“¥ 1k / â­ 15 / LLMâ€‘jp offers 13â€¯B and 1.3â€¯B transformer language models, including multiple instructionâ€‘tuned variants, built with Megatronâ€‘DeepSpeed and the Huggingâ€¯Face Transformers ecosystem.
 * [llm-jp-3-13b](https://huggingface.co/llm-jp/llm-jp-3-13b) - ğŸ“¥ 1k / â­ 13 / Repository hosts Huggingâ€¯Face checkpoints for Japanese LLMs (1.8â€¯B,â€¯3.7â€¯B,â€¯13â€¯B,â€¯17.2â€¯B) from the National Institute of Informatics, requiring PyTorchâ€¯2.3+, Transformersâ€¯4.40+ and include sample inference code, a 2.1â€¯Tâ€‘token unigramâ€‘based tokenizer, and preâ€‘training on mixed Japanese and English corpora.
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - ğŸ“¥ 1k / â­ 17 / Japaneseâ€‘StableLMâ€‘Baseâ€‘Betaâ€‘70B is a 70â€‘Bâ€‘parameter Llamaâ€‘2â€‘derived decoderâ€‘only language model fineâ€‘tuned on diverse Japanese data, offering smaller 7â€¯B versions, an instructionâ€‘following variant, and a faster inference release, all licensed under the Llama2 Community License.
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - ğŸ“¥ 1k / â­ 10 / Repository provides GGUFâ€‘formatted, quantised model files for Stability AIâ€™s Japanese StableLMâ€¯Instructâ€¯Gammaâ€¯7B, created with Massedâ€¯Compute hardware and part of TheBlokeâ€™s a16zâ€‘funded LLM work.
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - ğŸ“¥ 1k / â­ 53 / Japanese Stableâ€¯LMâ€¯Instructâ€¯Gammaâ€¯7B is a 7â€‘Bâ€‘parameter decoderâ€‘only Japanese language model fineâ€‘tuned on instruction datasets, built on the Base Gammaâ€¯7B, requires Transformersâ€¯4.34+, is Apacheâ€¯2.0â€‘licensed, and is developed by Stabilityâ€¯AI.
 * [Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 1k / â­ 17 / Llamaâ€¯3.1â€¯Swallow is a set of 8B and 70B Japaneseâ€‘enhanced language models derived from Metaâ€™s Llamaâ€¯3.1 through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning, released in Octâ€¯2024 and hosted on swallowâ€‘llm.github.io.
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 8 / Hosts LLMâ€‘jpâ€™s 13â€¯B and 1.3â€¯B Japaneseâ€‘English instruction and pretrained models, offered in several variant checkpoints for Huggingâ€¯Face Transformers with torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, accelerateâ€¯0.23 and DeepSpeed support.
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - ğŸ“¥ 1k / â­ 25 / A 7â€‘Bâ€‘parameter, autoregressive, decoderâ€‘only Japanese model based on Mistralâ€‘7Bâ€‘v0.1, released by Stabilityâ€¯AI under Apacheâ€¯2.0 for highâ€‘performance Japanese language and downstream tasks and requiring Transformersâ€¯4.34.0+.
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - ğŸ“¥ 1k / â­ 15 / Repositories of 13â€‘B and 1.3â€‘Bâ€‘parameter LLMâ€‘jp instructionâ€‘fineâ€‘tuned modelsâ€”including LoRA variantsâ€”packaged in Huggingâ€¯Face Transformers format and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, and accelerateâ€¯0.23, trained on ~50â€¯k mixed Japanese/English/code examples with Megatronâ€‘DeepSpeed and PEFT.
 * [llm-jp-13b-instruct-full-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 4 / LLMâ€‘jp supplies instructionâ€‘style and pretrained 13B/1.3B Transformer models in Hugging Face and DeepSpeed formats, trained on 50â€¯k+ mixed Japanese/English/sourceâ€‘code data and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34 and accelerateâ€¯0.23.
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ğŸ“¥ 1k / â­ 23 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter Japanese extension of Metaâ€™s Llamaâ€‘2â€‘7B, further preâ€‘trained for Japanese language tasks and offered in base, instruct, fast, and fastâ€‘instruct variants, maintained by the ELYZA team under the Llamaâ€¯2 Community License.
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - ğŸ“¥ 1k / â­ 37 / Offers the Swallowâ€¯Llamaâ€‘2 family of Japaneseâ€‘English LLMsâ€”from 7B,â€¯13B,â€¯andâ€¯70B models with instruct, NVE, and preview variantsâ€”tuned via supervised fineâ€‘tuning, available through Megatronâ€‘LM with a tokenizer, and benchmarked on core Japanese tasks.
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - ğŸ“¥ 1k / â­ 20 / OpenCALM is a family of Japanese decoderâ€‘only Transformer language models (160â€¯Mâ€“6.8â€¯B parameters) from CyberAgent, trained on Japanese Wikipedia and Common Crawl and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - ğŸ“¥ 1k / â­ 13 / An autoGPTQâ€‘quantized 10â€‘Bâ€‘parameter Japaneseâ€‘centric multilingual GPTâ€‘NeoX model (weblabâ€‘10bâ€‘instructionâ€‘sftâ€‘GPTQ) that shrinks the 21.42â€¯GB original to a faster, GPUâ€‘required version, with a 6.03â€¯GB gguf alternative for CPU via llama.cpp, and can be run locally with textâ€‘generationâ€‘webui (~16â€¯tokens/s on an RTXâ€¯3060) or interactively in Colab.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - ğŸ“¥ 1k / â­ 3 / Provides a 4â€‘bit, 4.11â€¯GB quantized version of Metaâ€™s Llamaâ€‘2â€¯7B (ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7bâ€‘fastâ€‘instruct) that cuts memory but degrades instructionâ€‘following, requires a GPU and autoGPTQ, and includes references to alternate AWQ, llama.cpp, and gguf quantizations and benchmark results.
 * [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - ğŸ“¥ 1k / â­ 17 / The TokyoTechâ€‘LLM repository provides the Swallowâ€¯Llamaâ€‘2 family of LLaMAâ€‘2 models, augmented with Japanese data, covering 7B, 13B, and 70B variants that include instructionâ€‘tuned, NVEâ€‘tuned, and a 7B Plus version released since Decemberâ€¯2023.
 * [Wanabi-Novelist-24B-GGUF](https://huggingface.co/kawaimasa/Wanabi-Novelist-24B-GGUF) - ğŸ“¥ 1k / â­ 2 / Repository contains a GGUFâ€‘quantized Wanabiâ€‘Novelistâ€‘24B Japanese novelâ€‘writing LLM optimized for Projectâ€¯Wannabe/koboldcpp, offers experimental imatrix quantization, but lacks general instruction capability and requires the latest Projectâ€¯Wannabe update.
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - ğŸ“¥ 1k / â­ 11 / OpenCALM is a decoderâ€‘only Japanese transformer family from CyberAgent (160â€¯Mâ€¯â€“â€¯6.8â€¯B parameters, from openâ€‘calmâ€‘small to openâ€‘calmâ€‘7b), trained on Japanese Wikipedia and Commonâ€‘Crawl, licensed CCâ€¯BYâ€‘SAâ€¯4.0 and usable through Hugging Face transformers.
 * [shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - ğŸ“¥ 1k / â­ 30 / Shisaâ€¯7B is a Japaneseâ€‘focused language model built on Mistralâ€¯7B, trained with curated airoboros, ultrafeedback, and synthetic ENâ€‘JA data, and includes code for preprocessing, translation, fineâ€‘tuning, and evaluation alongside future research documentation.
 * [ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1) - ğŸ“¥ 1k / â­ 10 / ABEJAâ€‘Qwen2.5â€‘7bâ€‘Japaneseâ€‘v0.1 is a Japaneseâ€‘fineâ€‘tuned Qwenâ€¯2.5â€¯7B model distilled from the 32B Japanese variant and optimized with ChatVector for instructionâ€‘following, available through PyTorch and Huggingâ€¯Face Transformers.
 * [shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - ğŸ“¥ 1k / â­ 16 / shisaâ€‘baseâ€‘7bâ€‘v1 augments Mistralâ€¯7B with 8â€¯B Japanese tokens from MADLADâ€‘400, trainedâ€¯inâ€¯2,400â€¯A100â€‘40 GPUâ€‘hours, and achieves classâ€‘leading Japanese benchmark performance, outperforming comparable 7â€‘B Japaneseâ€‘tuned models such as Japanese Stableâ€¯LM, ELYZA, and Youri.
 * [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ğŸ“¥ 1k / â­ 42 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘13b extends Metaâ€™s Llamaâ€¯2 with additional preâ€‘training for Japanese, offering 13â€¯Bâ€‘parameter models (including instruct and fast variants) that can be loaded with PyTorch and ğŸ¤—â€¯Transformers under the Llamaâ€¯2 Community License.

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - ğŸ“¥ 484k / â­ 73 / Japanese BERTâ€‘base pretrained on 2019 Japanese Wikipedia with IPAâ€‘dictionary and wholeâ€‘word masking, 12â€‘layer 768â€‘dim, 32,000â€‘vocab, 512â€‘token sequences, 1â€¯M steps, available at clâ€‘tohoku/bertâ€‘japanese under CCâ€‘BYâ€‘SA.
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - ğŸ“¥ 91k / â­ 8 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden, 12 heads) pretrained on ~17â€¯M sentences from Japanese Wikipedia (2.6â€¯GB) using MeCab IPA wordâ€‘level tokenization followed by character tokenization into a 4000â€‘word vocabulary, with training code atâ€¯clâ€‘tohoku/bertâ€‘japanese and released under CCâ€¯BYâ€‘SAâ€¯3.0.
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - ğŸ“¥ 55k / â­ 3 / Japanese RoBERTaâ€‘base model pretrained on ~10â€¯M Japanese medical abstracts and 1.4â€¯M body texts from JST, tokenized with a 30â€¯kâ€‘token SentencePiece, released under CCâ€¯BYâ€‘4.0 and usable via Hugging Face pipelines.
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - ğŸ“¥ 39k / â­ 38 / A BERT base model pretrained on ~17â€¯M Japanese Wikipedia sentences (2.6â€¯GB) that tokenizes with the IPA dictionary and WordPiece, has 12 layers/768â€‘dim hidden states/12 heads, a 32â€¯000â€‘token vocabulary, was trained for 1â€¯M steps on Cloud TPUs and is released under CCâ€‘BYâ€‘SAâ€¯3.0.
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - ğŸ“¥ 29k / â­ 19 / ModernBERTâ€‘Jaâ€‘310M is a Japaneseâ€‘language BERT variant that blends localâ€‘global attention with RoPE, trained on 4.09â€¯T tokens of Japanese/English text, supports a 102â€¯400â€‘word vocabulary, 8â€¯192â€‘token sequences, and is optimized for Flashâ€¯Attentionâ€¯2.
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - ğŸ“¥ 29k / â­ 48 / LINE DistilBERT Japanese is a 66â€‘millionâ€‘parameter DistilBERT model preâ€‘trained on 131â€¯GB of Japanese web text using an inâ€‘house BERTâ€‘base teacher, evaluated on JGLUE, tokenized with MeCabâ€¯Unidic and SentencePiece, and released under the Apacheâ€¯2.0 license.
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - ğŸ“¥ 26k / â­ 6 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) trained on 30â€¯M Wikipedia sentences (~4â€¯GB) with Unidicâ€¯2.1.2 wordâ€‘level tokenization followed by characterâ€‘level tokenization and wholeâ€‘word masking, using 512â€‘token sequences, 256 batches, and 1â€¯M training steps.
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - ğŸ“¥ 18k / â­ 6 / ModernBERTâ€‘Jaâ€‘70M is a lightweight Japanese BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯T mixedâ€‘language tokens (vocabâ€¯102â€¯400, maxâ€¯8â€¯192 tokens), supports Flashâ€¯Attentionâ€¯2, and comes in several sizes from 30â€¯M to 310â€¯M parameters.
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - ğŸ“¥ 15k / â­ 8 / Japanese DeBERTaâ€¯V2 large model trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with characterâ€‘level sentencepiece tokenization and wholeâ€‘word masking, ready for downstream fineâ€‘tuning through Huggingâ€¯Face Transformers.
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - ğŸ“¥ 11k / â­ 39 / Japaneseâ€‘Robertaâ€‘Base is a pretrained maskedâ€‘language model from rinna Co.,â€¯Ltd., with guidelines for proper loading, token preprocessing, positionâ€‘id handling, and usage examples emphasizing the need for a leading `[CLS]` token and consistent tokenization.
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - ğŸ“¥ 8k / â­ 30 / Japanese DeBERTaâ€¯V2 base model pretrained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100 and OSCAR data using Juman++ segmentation and SentencePiece tokenization, trained for three weeks on eight NVIDIA A100 GPUs and ready for fineâ€‘tuning.
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - ğŸ“¥ 6k / â­ 27 / Japanese BERTâ€‘base (12â€¯layers, 768 hidden, 12 heads) pretrained on 4â€¯GB of Japanese Wikipedia (â‰ˆ30â€¯M sentences) with Unidicâ€¯2.1.2 wordâ€‘level tokenization, WordPiece subâ€‘tokenization, and wholeâ€‘word masking.
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - ğŸ“¥ 3k / â­ 45 / A 132â€‘millionâ€‘parameter Japanese ModernBERT model that blends localâ€‘global and RoPE attention, trained on 4.39â€¯T tokens (Japanese/English) with a 102â€‘kâ€‘size vocab, 8,192â€‘token max length, and optimized for Flashâ€¯Attentionâ€¯2.
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - ğŸ“¥ 2k / â­ 8 / Japanese RoBERTaâ€‘base model pretrained on Japanese Wikipedia and CCâ€‘100, using Juman++â€‘based SentencePiece tokenization, fineâ€‘tunable via Hugging Face, trained over 700k steps on 8â€¯A100 GPUs with mixedâ€‘precision.
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - ğŸ“¥ 2k / â­ 2 / Japanese DeBERTaâ€¯V2 tiny, pretrained on ~171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR corpora, requires Juman++ word segmentation, was trained in 33â€¯h on 8 NVIDIAâ€¯A100 GPUs, and can be fineâ€‘tuned for downstream tasks.
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - ğŸ“¥ 2k / â­ 4 / DeBERTaV2â€¯base trained on Japanese corpora (CCâ€‘100, mC4, OSCAR2301, Wikipedia, Wikinews) with FPâ€‘16 fineâ€‘tuning for NLU tasks (JSTS, JNLI, JCommonsenseQA), released under CCâ€¯BYâ€‘SAâ€¯4.0 and funded by Japanese research grants.
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - ğŸ“¥ 2k / â­ 9 / Japanese BERTâ€‘large (24 layers, 1024â€‘hidden size, 16 heads, 32â€¯K vocab) pretrained on 30â€¯M Japanese Wikipedia sentences with Unidicâ€‘2.1.2 wordâ€‘level tokenization, WordPiece subwords, and wholeâ€‘word masking over 1â€¯M steps.
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - ğŸ“¥ 2k / â­ 5 / ModernBERTâ€‘Jaâ€‘30M is a Japaneseâ€‘language BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯TB of Japanese/English text, supports 8,192â€‘token sequences, comes in sizes from 30â€¯M to 130â€¯M parameters, and works best with Flash Attentionâ€¯2.

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - ğŸ“¥ 4M / â­ 51 / Japanese wav2vecâ€‘2 XLSRâ€‘53 fineâ€‘tuned on Common Voiceâ€¯6.1, CSS10, and JSUT, requiring 16â€¯kHz audio and usable via HuggingSound or HuggingFace pipelines.
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - ğŸ“¥ 37k / â­ 4 / Fineâ€‘tuned wav2vec2â€‘base Japanese ASR model trained on Common Voiceâ€¯11.0 that predicts only Hiragana, built from rinna/japaneseâ€‘wav2vec2â€‘base with 20â€¯epochs at lrâ€¯1eâ€‘4.
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - ğŸ“¥ 24k / â­ 86 / Kotobaâ€‘Whisperâ€‘v2.2 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated diarization and automatic punctuation via a HuggingFaceâ€‘Transformers pipeline, built in collaboration with Asahiâ€¯Ushio and Kotoba Technologies.
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - ğŸ“¥ 7k / â­ 17 / Kotobaâ€‘Whisperâ€‘Bilingual v1.0 delivers 6.3Ã— faster distilled Whisper models for Japanese and English ASR plus bidirectional speechâ€‘toâ€‘text translation, built from OpenAIâ€™s Whisperâ€¯largeâ€‘v3 via knowledge distillation with crossâ€‘entropy and KLâ€‘divergence loss.
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - ğŸ“¥ 6k / â­ 84 / Kotobaâ€‘Whisperâ€¯v2.0 is a Japanese ASR model distilled from OpenAI Whisper largeâ€‘v3, trained on 7.2â€¯million ReazonSpeech clips, that runs 6.3Ã— faster while matching the teacherâ€™s CER/WER on inâ€‘domain tests and includes stableâ€‘ts/punctuation support and full training code on GitHub.
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - ğŸ“¥ 5k / â­ 35 / reazonspeech-nemo-v2 is a 619â€‘Mâ€‘parameter Japanese longâ€‘form ASR model built on an improved Fastâ€‘Conformer with Linearly Scalable Attention, trained on the ReazonSpeechâ€¯v2.0 corpus, offering multiâ€‘hour inference via a subword RNNâ€‘T decoder (3000â€‘token SentencePiece) and distributed under Apacheâ€¯2.0.
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - ğŸ“¥ 4k / â­ 116 / Anime Whisper is a lightweight Japanese ASR model fineâ€‘tuned on ~5,300â€¯h of animeâ€‘style dialogue that delivers low hallucination, rhythmâ€‘aligned punctuation and accurate transcription of nonâ€‘verbal sounds and NSFW content, and must be run without an initial prompt.
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - ğŸ“¥ 3k / â­ 42 / NVIDIA NeMoâ€™s 0.6â€¯Bâ€‘parameter Hybrid FastConformerâ€‘TDTâ€‘CTC ASR model transcribes Japanese speech with punctuation and is available for inference or fineâ€‘tuning within the NeMo framework.
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - ğŸ“¥ 3k / â­ 19 / Kotobaâ€‘Whisperâ€‘v2.1 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated punctuationâ€‘postprocessing pipelines that maintain comparable CER performance while enabling seamless, punctuationâ€‘aware transcription.
 * [kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - ğŸ“¥ 2k / â­ 58 / Kotobaâ€‘Whisperâ€¯v1.0, a Japanese ASR model distilled from OpenAIâ€™s Whisper largeâ€‘v3, delivers aâ€¯6.3Ã— speedâ€‘up with comparable accuracy, was trained on 1,253â€¯h of ReazonSpeech, and its code (plus a newer v2.0 variant on Huggingâ€¯Face) is publicly available.
 * [japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh) - ğŸ“¥ 2k / â­ 2 / Japaneseâ€‘wav2vec2â€‘baseâ€‘rs35kh is a 96.7â€¯Mâ€‘parameter wav2vecâ€¯2.0 Base model fineâ€‘tuned on the ReazonSpeech v2.0 Japanese ASR corpus, reaching a 13.22â€¯% CER, deployable with Hugging Face transformers, and released under an ApacheÂ 2.0 license.
 * [japanese-hubert-base-k2-rs35kh-bpe](https://huggingface.co/reazon-research/japanese-hubert-base-k2-rs35kh-bpe) - ğŸ“¥ 1k / â­ 4 / Fineâ€‘tuned Japaneseâ€¯Hubertâ€¯Base (k2, ReazonSpeechâ€¯v2.0) attains 11.07â€¯% CER on shortâ€‘form ASR and 27.05â€¯% on longâ€‘form speech, outperforming wav2vecâ€‘2
variants, and is released under the Apacheâ€¯2.0 license.

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - ğŸ“¥ 308k / â­ 13 / Japanese CLOOBâ€‘VIT-B-16, a Visionâ€‘Language model based on vitâ€‘baseâ€‘patch16â€‘224 trained on translated CC12M captions and released by rinna Co., Ltd. on Mayâ€¯12â€¯2022 under Apacheâ€¯2.0.
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - ğŸ“¥ 40k / â­ 51 / A Japanese Sentenceâ€‘BERT v2, fineâ€‘tuned on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘wholeâ€‘wordâ€‘masking with MultipleNegativesRankingLoss, boosts accuracy by ~1.5â€“2â€¯% over v1 and is released asâ€¯sonoisa/sentenceâ€‘bertâ€‘baseâ€‘jaâ€‘meanâ€‘tokensâ€‘v2.
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - ğŸ“¥ 34k / â­ 2 / A Japanese BERTâ€‘base model fineâ€‘tuned with supervised SimCSE on JSNLI, exposed via Sentenceâ€‘Transformers or HuggingFace with CLS pooling, trained on 1â€¯M examples at 512â€‘batch size, 5â€¯Ã—â€¯10â»âµ learning rate, 5â€¯Ã—â€¯10â»âµ temperature, 64â€‘token limit, and BFloat16 precision.
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - ğŸ“¥ 30k / â­ 22 / rinna/japanese-clipâ€‘vitâ€‘bâ€‘16 is an Apacheâ€‘2.0 licensed Japanese CLIP model based on ViTâ€‘B/16, trained on CC12M captions translated to Japanese and released on Mayâ€¯12â€¯2022.
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - ğŸ“¥ 22k / â­ 11 / Japanese Sentenceâ€‘BERT (v1) model for generating sentence embeddings, with an improved v2 available and sample usage via Huggingâ€¯Face Transformers and a custom `SentenceBertJapanese` class.
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - ğŸ“¥ 21k / â­ 19 / Sarashinaâ€‘Embeddingâ€‘v2â€‘1B is a 1,792â€‘dimensional Japanese sentence transformer trained with multiâ€‘stage contrastive learning that achieves stateâ€‘ofâ€‘theâ€‘art JMTEB scores, and can be used for semantic similarity, search, paraphrase mining, classification, and clustering via Sentenceâ€‘Transformers with optional instruction prefixes.
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - ğŸ“¥ 19k / â­ 29 / LY Corporationâ€™s clipâ€‘japaneseâ€‘base is a Japanese CLIP model trained on ~1â€¯B imageâ€‘text pairs, using an Eva02â€‘B transformer image encoder with a 12â€‘layer BERT text encoder, achieving R@1â€¯0.30 on STAIR, 0.89 accuracy on Recruit and 0.58 accuracy on ImageNetâ€‘1K, and supporting zeroâ€‘shot image classification and retrieval.
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - ğŸ“¥ 9k / â­ 2 / ja_ginza_electra is a spaCyâ€¯v3 Python package offering a Japanese ELECTRA model fineâ€‘tuned on mC4 and UD_Japanese_BCCWJâ€¯r2.8 (based on megagonlabs/transformersâ€‘udâ€‘japaneseâ€‘electraâ€‘baseâ€‘discrimininator) with custom bunsetuâ€‘phrase detection, distributed under the MIT license.
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - ğŸ“¥ 3k / â­ 14 / Japanese Sentenceâ€‘LUKE model trained on the same dataset as Sentenceâ€‘BERT, outperforming or matching it, built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite and used via Huggingâ€¯Face Transformersâ€™ MLukeTokenizer and LukeModel.
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - ğŸ“¥ 2k / â­ 54 / A Japaneseâ€‘language T5 model, pretrained on ~100â€¯GB of Wikipedia and OSCAR data with SentencePiece tokenization, surpasses Googleâ€™s multilingual T5 on a newsâ€‘classification benchmark but needs fineâ€‘tuning and may yield biased outputs.
 * [clip-japanese-base-v2](https://huggingface.co/line-corporation/clip-japanese-base-v2) - ğŸ“¥ 2k / â­ 17 / Japanese CLIP model clipâ€‘japaneseâ€‘baseâ€‘v2, upgraded with ~2â€¯B imageâ€‘text pairs and distillation, pairs an Eva02â€‘B image encoder with a 12â€‘layer BERT text encoder to reach higher ImageNetâ€‘1k accuracy (0.708) than its predecessor.
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - ğŸ“¥ 1k / â­ 15 / Supâ€‘simcseâ€‘jaâ€‘large provides a supervised SimCSE fineâ€‘tuned Japanese BERTâ€‘large (clâ€‘tohoku/bertâ€‘largeâ€‘japaneseâ€‘v2) model with CLSâ€‘plusâ€‘MLP pooling, trained on ~1â€¯M JSNLI sentences (lrâ€¯5eâ€‘5, batchâ€¯512, tempâ€¯0.05, maxâ€¯64) and ready for use with Sentenceâ€‘Transformers or Huggingâ€¯Face Transformers.

### sentence-similarity
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - ğŸ“¥ 307k / â­ 11 / Japanese general text embedding models (Ruriâ€‘v3, 30â€‘310â€¯M parameters, 8192â€‘token max, high JMTEB scores) are offered with Sentenceâ€‘Transformers usage examples and benchmark comparisons to other Japanese embeddings.
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - ğŸ“¥ 97k / â­ 62 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token inputs, a 100Kâ€‘token vocabulary, FlashAttentionâ€‘accelerated inference, and multiple size variants for fast sentenceâ€‘transformer usage.
 * [ruri-small](https://huggingface.co/cl-nagoya/ruri-small) - ğŸ“¥ 61k / â­ 9 / Includes Ruriâ€¯v3 Japanese text embeddings (30â€¯Mâ€“310â€¯M parameters, 8192â€‘token limit, JMTEB 74.5â€“77.2), instructions for Sentence Transformers using â€œã‚¯ã‚¨ãƒª:â€ or â€œæ–‡ç« :â€ prefixes, and benchmark results for several Japanese models such as Sup/Unsup SimCSE, GLuCoSE, and LaBSE.
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - ğŸ“¥ 34k / â­ 44 / PLaMoâ€‘Embeddingâ€‘1B is a Japanese textâ€‘embedding model from Preferred Networks that converts Japanese text into vectors for information retrieval, classification, and clustering, shows strong performance on the JMTEB benchmark, and is freely available under an Apacheâ€¯v2.0 license.
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - ğŸ“¥ 30k / â­ 34 / GLuCoSE is a Japanese sentenceâ€‘embedding model built on LUKE that outputs 768â€‘dim meanâ€‘pooled vectors (up to 512 tokens) trained on web and NLI/search data, achievingâ€¯0.864 Spearman andâ€¯0.818 Pearson on similarity benchmarks.
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - ğŸ“¥ 22k / â­ 5 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese text embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192 tokens, a 100â€¯kâ€‘token vocabulary, FlashAttention acceleration, and multiple sizes from 37â€¯M to 315â€¯M parameters.
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - ğŸ“¥ 11k / â­ 16 / JaColBERTv2 is a Japaneseâ€‘only ColBERTâ€‘based retrieval model trained with knowledge distillation on MMarco (31 negatives per positive, 250k steps, batchâ€¯32) that currently outperforms multilingualâ€‘e5â€‘large, BGEâ€‘M3, and JaColBERT, with a full evaluation pending.
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - ğŸ“¥ 10k / â­ 21 / GLuCoSEâ€¯v2 is a CPUâ€‘friendly Japanese textâ€‘embedding model, fineâ€‘tuned by distillation and multiâ€‘stage contrastive learning, that delivers superior semanticâ€‘similarity and retrieval performanceâ€”outperforming comparableâ€‘size models on MIRACL and related benchmarks.
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - ğŸ“¥ 7k / â­ 36 / sbert-jsnliâ€‘lukeâ€‘japaneseâ€‘baseâ€‘lite is a 768â€‘dimensional sentenceâ€‘transformer built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite, trained one epoch on shunk031/jsnli, and includes examples for clustering, semantic search, and both Sentenceâ€‘Transformers and HuggingFace usage.
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - ğŸ“¥ 5k / â­ 2 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8192â€‘token sequences, a 100Kâ€‘token vocabulary, FlashAttention, and released in sizes from 30â€¯M to 310â€¯M parameters for use with sentenceâ€‘transformers.
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - ğŸ“¥ 3k / â­ 1 / Ruriâ€¯v3 delivers highâ€‘performance Japanese text embeddings up to 8192 tokens, a 100kâ€‘token vocabulary, FlashAttention support, and multiple model sizes (30â€¯mâ€“310â€¯m) for efficient inference and fineâ€‘tuning via sentenceâ€‘transformers.
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - ğŸ“¥ 3k / â­ 44 / A collection of releaseâ€‘ready Ruri v3 Japanese text embedding models (30mâ€“310m), complete with SentenceTransformer usage tips, query/passage prefixes, and JMTEB benchmark results showing how they compare to other Japanese and multilingual embeddings.

### translation
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - ğŸ“¥ 124k / â­ 9 / A LLaMAâ€¯3 Youko qlora fineâ€‘tune built on a new VNTL dataset, optimized for accurate, literal translations of Japanese visual novels to English without chat mode, using the default LLaMAâ€¯3 prompt and recommending neutral sampling (temperatureâ€¯0, no repetition penalty).
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - ğŸ“¥ 45k / â­ 67 / Japaneseâ€‘toâ€‘English Transformerâ€‘Align MT model from the Opus corpus, using normalization and SentencePiece preprocessing, achieves 41.7 BLEU and 0.589 chrâ€‘F on the Tatoeba test set.
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - ğŸ“¥ 10k / â­ 14 / Englishâ€‘toâ€‘Japanese transformerâ€‘align MT model with 15.2â€¯BLEU, built on opus+btâ€‘2021â€‘04â€‘10 using normalizationâ€¯+â€¯SentencePiece, hosted on the Tatoeba Challenge.
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - ğŸ“¥ 5k / â­ 28 / Fineâ€‘tuned, GGUFâ€‘quantized LFM2â€‘350M checkpoint for near realâ€‘time biâ€‘directional Japaneseâ€‘English translation of shortâ€‘toâ€‘medium text, usable via llama.cpp.
 * [plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate) - ğŸ“¥ 2k / â­ 112 / PLaMo Translation Model is a largeâ€‘scale language model created by Preferred Networks for translation tasks, available in base, postâ€‘trained, and evaluation variants, released under the PLaMo community license and not instructionâ€‘tuned for chat or other downstream uses.
 * [Sugoi-14B-Ultra-GGUF](https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF) - ğŸ“¥ 1k / â­ 8 / Sugoiâ€¯LLMâ€¯14Bâ€¯Ultra (GGUF) is a Japaneseâ€‘toâ€‘English translation model with a BLEU score of 21.38â€”nearly double its prior 13.67â€”excel at RPGâ€‘Maker bracketed text, strong prompt adherence, and JSON output for interactive chat UIs.

### text-classification
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - ğŸ“¥ 20k / â­ 3 / Japanese BERT Base model fineâ€‘tuned on a 10â€‘label emotion blogâ€‘post dataset (~1,000 sentences) derived fromâ€¯tohokuâ€‘nlp/bert-base-japanese for accurate emotion detection and classification.
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ğŸ“¥ 16k / â­ 43 / A Japanese LUKE model fineâ€‘tuned on the WRIME dataset that classifies which of eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, trustâ€”is expressed in a sentence.
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - ğŸ“¥ 10k / â­ 15 / Japanese sentiment analysis model trained on the chABSA dataset, achieving lossâ€¯0.0001, accuracyâ€¯1.0, and F1â€¯1.0, built with Transformersâ€¯4.24.0 and PyTorchâ€¯1.12.1+cu113, optimized with Adam (learningâ€¯rateâ€¯2eâ€‘05, 10 epochs, batchâ€¯sizeâ€¯16) and evaluated via `model(**inputs)`.
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - ğŸ“¥ 4k / â­ 2 / A Japanese BERTâ€‘based model fineâ€‘tuned on the JGLUE JSTS dataset for semantic similarity scoringâ€”introduced in chapterâ€¯5 ofâ€¯â€œLarge Language Model Introductionâ€â€”with Colab notebooks, transformersâ€‘pipeline usage, and an Apacheâ€¯2.0 license.

### text-ranking
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - ğŸ“¥ 9k / â­ 4 / Japaneseâ€‘trained CrossEncoder rerankers ranging from xsmall (384) to large (1024) plus a BGEâ€‘v2â€‘m3â€‘v1 model, with example code for fineâ€‘tuning, inference, and benchmark scores on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - ğŸ“¥ 8k / â­ 13 / Ruriâ€‘v3 Reranker is a robust Japanese text reranker built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token sequences, a 100kâ€‘token vocabulary, FlashAttention and a SentencePiece tokenizer, and it can be used via sentenceâ€‘transformers.
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - ğŸ“¥ 7k / â­ 7 / Japanese CrossEncoder reranker models ranging from xsmall to large (plus BGE), evaluated on JQaRA, JaCWIR, MIRACL, and JSQuAD, with readyâ€‘toâ€‘use integration examples for sentence_transformers and HuggingFace.
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - ğŸ“¥ 6k / â­ 6 / Fast, lightweight Japanese Rerankerâ€¯v2 models (tiny,â€¯xsmall,â€¯small,â€¯base) with benchmark scores and GPU speeds, usable via sentence_transformersâ€¯CrossEncoder and transformersâ€¯â‰¥â€¯v4.48 (optionally accelerated with flashâ€‘attn) and also available in ONNX/quantized forms for CPU/ARM.

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - ğŸ“¥ 177k / â­ 163 / Mangaâ€¯OCR is a Visionâ€¯Encoderâ€‘Decoder OCR tool that reads vertical and horizontal Japanese manga textâ€”including furiganaâ€”across diverse fonts and lowâ€‘quality images, with the source code freely available.
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - ğŸ“¥ 17k / â­ 3 / meikiocr supplies a Dâ€‘FINEâ€‘based, openâ€‘weight textâ€‘detection model for videoâ€‘games (v0.1 with MobileNetâ€‘v4 backbones, two resolution variants and a 64â€‘box limit) and experimental lowâ€‘latency tiny and small variants trained on Japanese videoâ€‘games and manga.
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - ğŸ“¥ 17k / â­ 4 / Meikiocrâ€™sâ€¯`meiki.text.recognition.v0`â€”a Dâ€‘FINE-based MobileNetV4 model fineâ€‘tuned on Japanese videoâ€‘game textâ€”delivers stateâ€‘ofâ€‘theâ€‘art accuracy and latency for horizontal text by detecting up to 48 characters from 960Ã—32 inputs, outputting each character with its bounding box and confidence score.

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - ğŸ“¥ 4k / â­ 118 / Fineâ€‘tuned from PaddleOCRâ€‘VL, PaddleOCRâ€‘VLâ€‘Forâ€‘Manga achieves 70â€¯% fullâ€‘sentence accuracy on Manga109â€‘s speechâ€‘bubble cropsâ€”over triple the 27â€¯% baselineâ€”using a multiâ€‘language dataset and includes training code and a developer guide.
 * [Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B) - ğŸ“¥ 1k / â­ 14 / Heronâ€‘NVILAâ€‘Liteâ€‘15Bâ€‘hf is a Japaneseâ€‘centric visionâ€‘language model built on the NVILAâ€‘Lite architecture, featuring a PALIGEMMAâ€‘SIGLIP vision encoder, an MLPâ€‘downsample projector, and a Qwen2.5â€‘14Bâ€‘Instruct LLM, supporting Japanese and English with straightforward Hugging Face integration.
 * [llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b) - ğŸ“¥ 1k / â­ 11 / LLMâ€‘jp VILA 14B is a 14â€‘Bâ€‘parameter visionâ€‘language model developed by Japanâ€™s National Institute of Informatics, intended for use with Pythonâ€¯3.10.12 through a cloneâ€‘installâ€‘run workflow that installs required libraries (including flashâ€‘attention) and accepts image inputs and text queries.

### token-classification
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - ğŸ“¥ 8k / â­ 11 / Japanese NER using clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2 that extracts eight entity types (corporations, political/other organizations, facilities, products, events) viaâ€¯`BertForTokenClassification`, trained on the Stockmark Wikipedia dataset and installable with `transformers`, `unidic_lite`, and `fugashi` under a CCâ€¯BYâ€‘SAâ€¯3.0 license.
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - ğŸ“¥ 1k / â­ 25 / Fineâ€‘tuned XLMâ€‘RoBERTaâ€‘base on a Japanese NER corpus (tagsâ€¯PER,â€¯ORG,â€¯LOC,â€¯INS,â€¯PRD,â€¯EVT) using 5â€‘epoch Adam (lrâ€¯5eâ€‘5, batchâ€¯12) to reach a 0.0173 validation loss, released on Transformersâ€¯4.23.1 and PyTorchâ€¯1.12.1.

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - ğŸ“¥ 7k / â­ 13 / Animeâ€‘XCodec2â€‘44.1kHzâ€‘v2 upsamples 16â€¯kHz Japanese speech to 44.1â€¯kHz highâ€‘fidelity audio with a decoderâ€‘only RMSâ€‘loss fineâ€‘tune, keeping the encoder/codebook frozen and preserving identical speech tokens.

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - ğŸ“¥ 2k / â­ 24 / Animeâ€‘Llasaâ€‘3B is a Japanese TTS model built on HKUSTAudio/Llasaâ€‘3B, enhanced with more training data to boost expressiveness and stability, and licensed CCâ€‘BYâ€‘NCâ€‘4.0.

### text-to-audio
 * [japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate) - ğŸ“¥ 1k / â­ 23 / Japaneseâ€‘language TTS trained from Parlerâ€‘TTSâ€‘Largeâ€¯v1 delivers lightweight, highâ€‘quality speech in beta but can be unstable and has limited maleâ€‘voice fidelity, so the Mini version is recommended for reliability.

### others
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - ğŸ“¥ 92k / â­ 10 / Japaneseâ€‘language BERTâ€‘Base (12 layers, 768â€‘dim, 12 heads) pretrained with Unidicâ€‘based wordâ€‘level plus characterâ€‘level tokenization and wholeâ€‘word masking on CCâ€‘100 and 2023 Wikipedia, producing a 7,027â€‘token vocabulary.
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - ğŸ“¥ 70k / â­ 13 / Japaneseâ€‘BERTâ€‘Large trained on CCâ€‘100 and Wikipedia, using Unidicâ€‘lite wordâ€‘level tokenization with WordPiece subwords and wholeâ€‘word masking (24 layers, 1024â€‘dim hidden, 16 heads, 32k vocab), with pretraining code on clâ€‘tohoku/bertâ€‘japanese.
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - ğŸ“¥ 62k / â­ 58 / Japanese BERTâ€‘base (12 layers, 768â€‘dim hidden, 12 heads, 32â€¯k vocab) pretrained with wholeâ€‘word masking on CCâ€‘100 and 2023â€‘Jan Wikipedia, using Unidicâ€¯2.1.2 wordâ€‘level tokenization plus WordPiece, in 2â€¯M training steps.
 * [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) - ğŸ“¥ 32k / â­ 1 / A ggufâ€‘formatted version of cyberagentâ€™s openâ€‘calmâ€‘3b model on the mmngaâ€‘dev branch, ready for llama.cpp testing with usage examples and a note that it may not work once gptneox is integrated.
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - ğŸ“¥ 14k / â­ 17 / Japanese DeBERTaâ€¯V3â€¯base preâ€‘trained on 540â€¯B tokens from LLMâ€‘jpâ€¯v1.0, trained with a modified DeBERTaâ€¯V3 setup, uses a unigram byteâ€‘fallback tokenizer (no morphological analyzer), and is fineâ€‘tuned for JGLUE NLU tasks.
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - ğŸ“¥ 13k / â­ 10 / A Japanese T5â€‘v1.1 model pretrained on â‰ˆ100â€¯GB of Wikipedia and OSCAR CCâ€‘100 data (mixed 10:1 for SentencePiece with byteâ€‘fallback), requiring fineâ€‘tuning for downstream tasks, includes transferâ€‘learning sample code, notes potential bias in outputs, and is licensed CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5) - ğŸ“¥ 4k / â­ 9 / Llamaâ€¯3.1â€¯Swallowâ€¯v0.5 is an 8â€‘billionâ€‘parameter LLM that improves Metaâ€™s Llamaâ€¯3.1 on Japanese language and code/math reasoning while retaining English fluency, achieved through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning on synthetic Japanese data.
 * [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) - ğŸ“¥ 3k / â­ 2 / A temporary test branch offering aâ€¯ggufâ€‘formatted version of CyberAgentâ€™s openâ€‘calmâ€‘7b model forâ€¯llama.cpp, with instructions to clone the dev branch, build, and run the model (note other similar gguf releases exist).
 * [shisa-v2.1-qwen3-8b-UD-japanese-imatrix](https://huggingface.co/dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix) - ğŸ“¥ 3k / â­ 2 / A GGUFâ€‘quantized shisaâ€‘v2.1â€‘qwen3â€‘8b model built with Unsloth Dynamicâ€¯2.0, communityâ€‘patched Qwen3 settings to reduce malfunctions, a larger imatrix for stronger Japanese performance, and a 40K maximum context length.
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - ğŸ“¥ 3k / â­ 71 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced 8â€‘B Llamaâ€¯3 model with GGUF (Q4_K_M) and AWQ quantization, ready to run via llama.cpp, LMâ€¯Studio, or an OpenAIâ€‘compatible API.
 * [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - ğŸ“¥ 3k / â­ 1 / A testâ€‘branch conversion of stockmarkâ€™s gptâ€‘neoxâ€‘japaneseâ€‘1.4b to gguf format, intended for use with llama.cppâ€™sâ€¯mmngaâ€‘dev branch and shown with example inference commands and GPU support.
 * [Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf) - ğŸ“¥ 2k / â­ 3 / ggufâ€‘formatted Qwen3â€‘EZOâ€‘8Bâ€‘beta by AXCXEPT, built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, and run via llama.cppâ€™s CUDAâ€‘enabled build.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - ğŸ“¥ 2k / â­ 38 / GGUFâ€‘formatted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen Japanese 32B model from cyberagent, built with the imatrix dataset and ready to run with llama.cpp.
 * [haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - ğŸ“¥ 2k / â­ 4 / ggufâ€‘formatted conversion of Llamaâ€‘3â€‘8B Japanese Instruct built from the imatrix dataset, ready for inference with llama.cpp.
 * [Tema_Q-R3.1-i1-GGUF](https://huggingface.co/mradermacher/Tema_Q-R3.1-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Offers a comprehensive list of weighted/imatrix GGUF quant versions for the Tema_Qâ€‘R3.1 model, detailing sizes, quality notes, download links, usage guidanceâ€”including GGUF file handlingâ€”and links to the model page, readmes, and FAQ.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - ğŸ“¥ 2k / â­ 55 / Cyberagentâ€™s ggufâ€‘converted DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14Bâ€‘Japanese model (built from the TFMC imatrix dataset) is available under mmnga and can be run with CUDA support using llama.cpp.
 * [Llama-3.1-Swallow-JP-EN-Translator-v1-8B-i1-GGUF](https://huggingface.co/mradermacher/Llama-3.1-Swallow-JP-EN-Translator-v1-8B-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Weighted/imatrix GGUF quantizations (2.1â€¯GBâ€“6.7â€¯GB) of Llamaâ€‘3.1â€‘Swallowâ€‘JPâ€‘ENâ€‘Translatorâ€¯v1â€‘8B are listed for download on Huggingâ€¯Face with usage guidance and a staticâ€“quant page for convenience.
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - ğŸ“¥ 1k / â­ 16 / Highâ€‘performance Japanese SPLADEâ€¯v2 enables sparseâ€‘vector conversion and inference through a WebUI demo, trains with YAST, offers YASEM embedding, and reports JMTEB benchmark results.
 * [llm-jp-3.1-1.8b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-1.8b-instruct4-gguf) - ğŸ“¥ 1k / â­ 1 / GGUFâ€‘formatted llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4, trained on the TFMC/imatrix dataset, with CUDAâ€‘enabled llama.cpp build and run instructions.
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - ğŸ“¥ 1k / â­ 19 / A GGUFâ€‘format release of pfnetâ€™s plamoâ€‘2â€‘translate built from imatrix data based on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with instructions to compile and run it via llama.cpp on CUDAâ€‘enabled hardware.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5-gguf](https://huggingface.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf) - ğŸ“¥ 1k / â­ 2 / GGUF conversion of Llamaâ€‘3.1â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.5 by tokyotechâ€‘llm, incorporating TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with Build/Run instructions forâ€¯llama.cpp.
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - ğŸ“¥ 1k / â­ 7 / A ggufâ€‘format conversion of Vecteusâ€‘v1 from Localâ€‘Novelâ€‘LLM, built using the imatrix dataset, that can be run with llama.cpp via `Vecteusâ€‘v1â€‘Q4_0.gguf` and lists other related models.

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - ğŸ“¥ 1M / â­ 18 / Aggregates overâ€¯150â€¯GB of NicoNico Liveâ€™s comment logs from 2009â€‘2024â€”including preâ€‘transition, postâ€‘transition, and realâ€‘time NXâ€‘Jikkyo capturesâ€”providing an API for easy retrieval of historical TVâ€‘broadcast discussions.
 * [emb](https://huggingface.co/datasets/hpprc/emb) - ğŸ“¥ 8k / â­ 13 / A catalog of Japanese and multilingual QA, NLI and paraphrase datasets, detailing each datasetâ€™s retrieval or QA tasks and its license (Apacheâ€¯2.0, CCâ€‘BYâ€‘SA/CCâ€‘BY, MIT, etc.).
 * [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) - ğŸ“¥ 5k / â­ 10 / Mirror of the Reazon Speechâ€¯v2 dataset on ğŸ¤—, licensedâ€¯CDLAâ€‘Sharingâ€‘1.0 and restricted to Japanese Copyright Actâ€¯Articleâ€¯30â€‘4, with 16â€¯kHz FLAC audio and accompanying metadata.
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - ğŸ“¥ 4k / â­ 18 / JMTEB is a Japanese textâ€‘embedding benchmark featuring 5 tasks (clustering, classification, STS, retrieval, reranking) and 28 datasets, offering a oneâ€‘line evaluation script and inviting community contributions.
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - ğŸ“¥ 4k / â­ 31 / Restructured reupload of the Galgame VisualNovel datasetâ€¯(OOPPEENN/56697375616C4E6F76656C5F44617461736574) for efficient Huggingâ€¯Faceâ€¯datasets loading, preserving all original audio/text and providing an extraction script with multiple gameâ€‘subset options.
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - ğŸ“¥ 3k / â­ 7 / JMedBench is a Japanese biomedical LLM benchmark comprising 20 datasets across five tasks (MCQA, NER, STS, etc.) sourced from MedMCQA, PubMedQA, MMLU, and others, each with its own license, and includes a note that translations may contain biases requiring human review.
 * [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) - ğŸ“¥ 3k / â­ 3 / Japanese Wikipedia sentences are transformed into various embeddings and a FAISS index, offering a Hugging Face Space demo, conversion scripts, and evaluations of search, Q&A, and OpenAIâ€¯textâ€‘embeddingâ€‘3â€‘small for RAG; embeddings are OpenAIâ€‘licensed, others CCâ€‘BYâ€‘SAâ€‘4.0.
 * [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ğŸ“¥ 2k / â­ 99 / A 100â€‘sample Japanese instructionâ€‘tuning evaluation dataset of annotated tasksâ€”ranging from summarization correction and math reasoning to translation, creative generation, and userâ€‘intent understandingâ€”designed for manual or automatic 5â€‘point rating of fineâ€‘tuned models.
 * [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) - ğŸ“¥ 2k / â­ 96 / Nemotronâ€‘Personasâ€‘Japan is an openâ€‘source, CCâ€¯BYâ€¯4.0 dataset of highâ€‘quality, synthetically generated Japanese personasâ€”incorporating name, gender, age, background, marital status, education, occupation and locationâ€”grounded in realâ€‘world demographic, geographic and personality distributions, engineered with probabilistic graphical models and GPTâ€‘OSSâ€‘120B to enhance diversity, reduce bias, prevent model collapse, assist sovereign AI development, and support commercial use.
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - ğŸ“¥ 2k / â­ 8 / Cauldronâ€‘JA is a Japanese visionâ€‘language dataset of 44 subâ€‘datasets translated from The Cauldron using the DeepL API, available via HuggingFaceâ€™s datasets library and licensed identically to the original set, with prompts released under CCâ€‘BYâ€‘4.0.
 * [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - ğŸ“¥ 2k / â­ 45 / Updated JGLUE dataset card and loading script for a Japanese NLP benchmark (created by Yahoo Japan and Waseda University) that covers text classification (MARCâ€‘ja, JCoLA), sentenceâ€‘pair classification (JNLI), and QA (JSQuAD, JCommonsenseQA), with releases linked on GitHub and Hugging Face.
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - ğŸ“¥ 2k / â­ 23 / FineWeb2 Edu Japanese delivers ~120â€¯million highâ€‘quality educational Japanese texts (â‰ˆ89.3â€¯billion tokens) from FineWeb2, filtered by a DeepSeekâ€‘API classifier (scoreâ€¯â‰¥â€¯2.5), tokenized via ModernBERTâ€‘Jaâ€‘130M, and includes a smallâ€‘token subset (â‰¤512 tokens).
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - ğŸ“¥ 1k / â­ 141 / Japanese Anime Speech Dataset offers 73,004 audioâ€‘text pairs (110â€¯hours total, evolving from V1 to V5) to enhance ASR models such as OpenAIâ€™s Whisper, available under an open license for all uses with credit appreciated.
 * [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) - ğŸ“¥ 1k / â­ 4 / Provides a Japanese search/QA dataset with perâ€‘query scores computed by five multilingual/Japanese rerankers (e.g., BAAI/bgeâ€‘rerankerâ€‘v2â€‘m3, Alibabaâ€‘NLP/gteâ€‘multilingualâ€‘rerankerâ€‘base), including average scores for roughly 200 positive and negative example documents per query.
 * [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) - ğŸ“¥ 1k / â­ 107 / ReazonSpeech is a free, FLACâ€‘encoded Japanese speech corpus with transcriptions, offered in five sizes from 8.5â€¯h to 35,000â€¯h, downloadable via Huggingâ€¯Face under the CDLAâ€‘Sharingâ€‘1.0 license and limited to use under Japan Copyright Act Articleâ€¯30â€‘4.
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - ğŸ“¥ 1k / â­ 8 / Rakuda supplies 40 Japanese questionsâ€”openâ€‘ended for history, society, and government, and specific for geographyâ€”for benchmarking Japanese AI assistants, comparable to vicunaâ€‘eval, and can be loaded with `datasets.load_dataset`.
 * [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) - ğŸ“¥ 1k / â­ 128 / Japanese Anime Speech Datasetâ€¯V2 delivers 292,637 cleaned audioâ€‘text pairsâ€”about 397.5â€¯h of SFW and 52.4â€¯h of NSFW contentâ€”in 128â€‘kbps MP3 files split by safety, designed specifically for training automatic speechâ€‘recognition models.
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - ğŸ“¥ 1k / â­ 31 / A 409â€‘hour Japanese eroge voice dataset processed with 2â€‘pass loudnorm (â€‘23â€¯LUFS, â€‘1â€¯dB peak, 11â€¯LRA), transcribed by litagin/anime-whisper, anonymized, stored as WebDataset (FLAC, JSON, TXT), largely featuring female voices with potential AI transcription errors, and MITâ€‘licensed for academic research.
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - ğŸ“¥ 918 / â­ 2 / Large compressed JSONâ€‘Lines dataset of anonymous 2ch.sc/2ch.net threads, including thread IDs, titles, board and region details, reply counts, and full post metadata (author, mail, date, content).
 * [JamC-QA](https://huggingface.co/datasets/sbintuitions/JamC-QA) - ğŸ“¥ 901 / â­ 4 / JamCâ€‘QA is a bilingual benchmark of multipleâ€‘choice questions spanning eight Japaneseâ€‘culture and knowledge categories, with leaderboard metrics comparing stateâ€‘ofâ€‘theâ€‘art models.
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - ğŸ“¥ 895 / â­ 21 / A Japanese web collection of 56â€¯million documents, 110â€¯B characters and 249â€¯million images used to train large visionâ€‘language modelsâ€”offering a momiji_generator for data population, OBELICSâ€‘style visualization, and a sample model (Heronâ€‘NVILAâ€‘Lite).
 * [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) - ğŸ“¥ 889 / â­ 6 / Artificial voice dataset created with VOICEVOX from the ITA, Tsukuyomiâ€‘chan, and ROHAN corpora, containing 445,793 WAV files totaling 577â€¯hâ€¯51â€¯mâ€¯23â€¯s.
 * [llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - ğŸ“¥ 866 / â­ 140 / Japanese instructionâ€‘chat dataset for fineâ€‘tuning LLMs (e.g., with LoRA), 9â€¯M+ samples, recently updated to drop licensed Alpaca data, clean Wikipedia and ALT outputs, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Japanese-Eroge-Voice-V2](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice-V2) - ğŸ“¥ 809 / â­ 6 / Japaneseâ€‘Erogeâ€‘Voiceâ€‘V2 offers 2,657â€¯hours of anonymized 1,033,142 eroge audioâ€“transcription pairs (mostly female, NSFW), MITâ€‘licensed for academic research.
 * [cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - ğŸ“¥ 777 / â­ 21 / cc100-ja is a collection of the Japanese portion of the cc100 dataset, provided as sharded Parquet files.
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - ğŸ“¥ 744 / â­ 2 / Huggingâ€¯Face mirror of the ABEJAâ€¯CCâ€‘JA dataset from AWS Openâ€¯Data, with details posted on ABEJAâ€™s tech blog.
 * [llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - ğŸ“¥ 729 / â­ 32 / Japanese chatbot dataset stripped of English translation data from izumiâ€‘lab/llmâ€‘japaneseâ€‘dataset, offering 2.5â€¯million+ entries (v1.0.0) for fineâ€‘tuning Japanese LLMs on instructionâ€‘response tasks under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) - ğŸ“¥ 727 / â­ 4 / Dataset of dialogues and lore from the Fate/Stayâ€¯Night character â€œEmiliaâ€, formatted for training and evaluating conversational language models.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - ğŸ“¥ 691 / â­ 87 / An automatically translated Japanese version of the databricksâ€‘dollyâ€‘15k dataset, licensed CCâ€‘BYâ€‘SAâ€‘3.0 and last updated on 2023â€‘05â€‘11.
 * [aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - ğŸ“¥ 572 / â­ 37 / A userâ€‘friendly, deduplicated CSV dataset of publicâ€‘domain Japanese texts from Aozoraâ€¯Bunko, processed with globisâ€‘org/aozorabunkoâ€‘extractor and cleaned for modernâ€‘Japanese machineâ€‘learning use.
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - ğŸ“¥ 567 / â­ 3 / AnimuSubtitleâ€‘JP hosts Japanese ASS/SSA subtitle datasets (data_ass, data_TS) that can be parsed with Pythonâ€™sâ€¯ass library or edited in Aegisub, and is released under an ODCâ€‘By license.
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - ğŸ“¥ 563 / â­ 4 / Reformatted Japanese subset of the Wiki40B dataset compiled by Mandy Guo, Zihang Dai, and Denny VrandeÄiÄ‡.
 * [MissingKeys](https://huggingface.co/datasets/RyokoExtra/MissingKeys) - ğŸ“¥ 563 / â­ 2 / MissingKeys is a raw Japaneseâ€‘dominant dataset from the misskey.io network, stored in dateâ€‘compressed JSONL files (â‰ˆ100,000 notes each inside .7z archives) and intended primarily for unsupervised textâ€‘generation training.
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - ğŸ“¥ 550 / â­ 12 / Elite Voice Project compiles Hololive VTuber Sakuraâ€¯Mikoâ€™s audio from Twitch, Twitter and YouTube into a train/testâ€‘organized dataset for speechâ€‘recognition research, using Gitâ€‘LFS, licensed under Hololiveâ€™s fanâ€‘content rules and welcoming community contributions.
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - ğŸ“¥ 536 / â­ 31 / Syosetu711K is a Japanese dataset of ~711,700 novels scraped from å°èª¬å®¶ã«ãªã‚ã† on Marchâ€¯26â€‘27â€¯2023, providing full text and metadata (title, author, NCode, synopsis, etc.) for unsupervised text generation and classification tasks.
 * [paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa) - ğŸ“¥ 536 / â­ 2 / Dataset of LLMâ€‘generated queries and answers from paraphrases of Japanese Wikipedia text, built without using licenseâ€‘restricted models and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU) - ğŸ“¥ 499 / â­ 19 / JMMMU is a Japanese multimodal benchmark expanded over tenfold to 1,320 culturally diverse questions (720 cultureâ€‘agnostic, 600 cultureâ€‘specific) translated by native subject experts, now featuring a public leaderboard.
 * [mc4-ja](https://huggingface.co/datasets/izumi-lab/mc4-ja) - ğŸ“¥ 465 / â­ 6 / Dataset card for the Japanese MC4 dataset (mc4-ja).
 * [callhome-ja-plus](https://huggingface.co/datasets/ayousanz/callhome-ja-plus) - ğŸ“¥ 461 / â­ 2 / Japanese Callhome speech files converted to WAV, accompanied by JSONâ€‘formatted metadata arrays and RTMM speaker label files for evaluation.
 * [oscar_2023_filtered](https://huggingface.co/datasets/if001/oscar_2023_filtered) - ğŸ“¥ 375 / â­ 3 / A 312,396â€‘row filtered subset of the OSCARâ€‘2301 dataset (Hugging Faceâ€¯`if001/oscar_2023_filtered`), with implementation details available at theâ€¯HojiChar_OSCAR_sample GitHub repository.
 * [wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - ğŸ“¥ 368 / â­ 9 / A dataset card for llmâ€‘book/wrimeâ€‘sentiment offering a binary Japanese sentiment analysis set derived from WRIME, labeled as positive or negative based on Avg. Readers_Sentiment (with an option to include neutral cases), and intended as sample data for the book â€œIntroduction to Large Language Models.â€
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - ğŸ“¥ 356 / â­ 38 / Galgame_Speech_ASR_16kHz is a 16â€¯kHz ASR dataset with 3.75â€¯million pairs (â‰ˆ5,354â€¯h), derived from Galgame_Dataset, released under GPLâ€¯v3.0 with commercial use prohibited and requiring any trained models to be openâ€‘source (citation optional).
 * [Galgame_Speech_SER_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_SER_16kHz) - ğŸ“¥ 352 / â­ 13 / A 104â€¯GB dataset of 3,746,131â€¯Galgame audio files (5,353â€¯h), adding LLMâ€‘generated emotion labels (possibly inaccurate) to the existing 16â€¯kHz ASR set, released under GPLâ€¯v3.0 with no commercial use allowed and requiring any trained models to be openâ€‘source.
 * [vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard) - ğŸ“¥ 339 / â­ 40 / The VNTL leaderboard evaluates large language models on translating Japanese visual novels into English by averaging cosineâ€‘similarity scores over 256 samples, ranking preliminary results and benchmarking against tools such as Sugoi Translator, Google Translate, and Naver Papago.
 * [xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - ğŸ“¥ 336 / â­ 6 / Japanese XLâ€‘Sum subset filtered via PaLMâ€‘2 15â€‘gram overlap, containing 4,215 training, 758 validation, and 766 test examples.
 * [kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en) - ğŸ“¥ 333 / â­ 10 / A Japaneseâ€‘toâ€‘English parallel corpus translating the kaken subset of llmâ€‘jpâ€‘corpusâ€‘v3 with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, featuring custom translation columns and licensed under CCâ€‘BYâ€‘4.0.
 * [mc4-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/mc4-ja-filter-ja-normal) - ğŸ“¥ 323 / â­ 5 / Dataset card for the â€œmc4â€‘jaâ€‘filterâ€‘jaâ€‘normalâ€ dataset, with additional information pending.
 * [Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus) - ğŸ“¥ 308 / â­ 4 / Lux Japanese Speech Corpus: a 96â€¯kHz 16â€‘bit WAV dataset of Japanese TTS recordings by characterÂ Lux, including raw and cleaned audio, transcripts inÂ metadata.csv, dataset metadata inÂ dataset_infos.json, and released under CCâ€¯BYâ€¯4.0.
 * [JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR) - ğŸ“¥ 306 / â­ 4 / JFWIR is a 64â€‘millionâ€‘pair Japanese IR dataset built from finewebâ€‘2â€‘edu web content, offering seven query types and hard negatives that raise benchmark scores on JQaRA, MIRACL(ja), jsquad and JaCWIR.
 * [JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA) - ğŸ“¥ 302 / â­ 2 / JCommonsenseQA is a Japanese multipleâ€‘choice dataset adapted from CommonsenseQA, offering 5 answer options per question, labeled indices for the correct choice, and released under a Creative Commonsâ€¯BYâ€‘SAâ€¯4.0 license.
 * [JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA) - ğŸ“¥ 301 / â­ 3 / Japanese Explainable Multiâ€‘hop Question Answering dataset featuring questions, answers, and stepâ€‘byâ€‘step derivations linking Wikipedia articles, with updated derivation formatting and multiple version releases.
 * [sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese) - ğŸ“¥ 300 / â­ 5 / Converted Japanese datasets into SentenceTransformersâ€‘friendly columns, filtering examples by Rerank scores (â‰¥0.7 positive, â‰¤0.3 negative) from multiple HuggingFace sources to support contrastive learning while respecting the original licenses.
 * [RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA) - ğŸ“¥ 297 / â­ 33 / Allganize RAG Leaderboard publishes Japanese RAG performance data and automated endâ€‘toâ€‘end evaluation results across five industry domainsâ€”finance, telecom, manufacturing, public sector, and retailâ€”to help companies benchmark parser, retrieval and generation components where no comprehensive Japanese benchmark yet exists.
 * [JGLUE](https://huggingface.co/datasets/llm-book/JGLUE) - ğŸ“¥ 292 / â­ 15 / Dataset card for the JGLUE dataset used in the book â€œLarge Language Model Introduction,â€ sourced from the original repo, with code licensed CCâ€¯BYâ€‘SAâ€¯4.0, data under the distributorâ€™s license, citing Kurihara & Kawahara (in Japanese), and built on Shunsuke Kitadaâ€™s repository.
 * [Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M) - ğŸ“¥ 292 / â­ 6 / Dataset of 23,212,809 Japanese web novels (~80.8â€¯billion characters) collected personally, for machineâ€‘learning use only and requiring a detailed access request.
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - ğŸ“¥ 285 / â­ 5 / Japanese JaQuAD, a subset of QGâ€‘Bench, provides sentenceâ€‘ and paragraphâ€‘level data with highlighted answer tokens for training Japanese questionâ€‘generation models, evaluated by BLEU4, METEOR, ROUGEâ€‘L, BERTScore, and MoverScore.
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - ğŸ“¥ 282 / â­ 3 / Dataset of Japanese Boketeâ€‘site humor posts (from CLoTâ€‘Oogiriâ€‘Goâ€¯CVPRâ€¯2024) featuring three tasksâ€”textâ€‘toâ€‘text, imageâ€‘toâ€‘text, and textâ€‘imageâ€‘toâ€‘textâ€”with roughly 600â€¯examples, processed via GPTâ€‘4o OCR and HojiChar filtering.
 * [EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench) - ğŸ“¥ 277 / â­ 9 / EDINETâ€‘Bench is a Japanese financial benchmark that evaluates LLMs on tasks such as accounting fraud detection, earnings forecasting, and industry prediction using ten years of EDINETâ€‘API disclosed reports, with construction and evaluation code provided and the dataset relicensed to PDLâ€¯1.0.
 * [jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - ğŸ“¥ 275 / â­ 7 / JHumanEval is a handâ€‘translated Japanese version of the HumanEval benchmark, providing 164 Python programming problems with parallel English and Japanese comments to evaluate Japaneseâ€‘LLM code generation while preserving the original English errors.
 * [bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja) - ğŸ“¥ 264 / â­ 3 / BBHâ€‘ja provides a Japanese translation of the BIGâ€‘Bench Hard dataset, offering evaluation problems in JSONâ€‘L (input, correct target) and Chainâ€‘ofâ€‘Thought prompts in YAML (input, target), translated using the PLaMo model.
 * [JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - ğŸ“¥ 262 / â­ 11 / JMMLU is a Japanese Massive Multitask Language Understanding Benchmark featuring 7,536 teacherâ€‘crafted questions across 56 subjects, including professional medicine, psychology, accounting, philosophy, and diverse highâ€‘school disciplines.
 * [oscar2301-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/oscar2301-ja-filter-ja-normal) - ğŸ“¥ 246 / â­ 6 / Dataset card for the Japaneseâ€‘filtered OSCARâ€¯2301 subset, â€œoscar2301â€‘jaâ€‘filterâ€‘jaâ€‘normal.â€
 * [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) - ğŸ“¥ 234 / â­ 5 / KokushiMDâ€‘10 delivers a multimodal benchmark of Japanese national healthcare licensing exam questionsâ€”spanning ten professions, offered in Japanese, English, and mixed splitsâ€”with expert chainâ€‘ofâ€‘thought annotations for LLM evaluation.
 * [wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - ğŸ“¥ 231 / â­ 4 / Range3â€™sâ€¯wikipediaâ€‘jaâ€‘20230101 repository offers Parquet files containing only Japanese Wikipedia text, extracted from the full Wikipedia dataset and generated with Python code.
 * [anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0) - ğŸ“¥ 228 / â­ 21 / AIâ€‘generated anime illustrations with English prompts and Phiâ€‘3 Visionâ€‘derived captions (English and Japanese) released into the public domain for free use.
 * [JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - ğŸ“¥ 218 / â­ 19 / A Japanese QA dataset for evaluating Retrievalâ€‘Augmented Generation (RAG), built from JAQKET questions and Wikipedia passages with gold retrievalâ€‘relevance labels, released on HuggingFace and GitHub and scored primarily by nDCG@10.
 * [Hachi-Alpaca](https://huggingface.co/datasets/HachiML/Hachi-Alpaca) - ğŸ“¥ 206 / â­ 15 / Hachi-Alpaca delivers Japanese synthetic data derived from Stanford Alpaca, refined and verified by mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, used through Deepinfra, with â€œ_cleanedâ€ versions that have passed modelâ€‘based quality checks.
 * [JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - ğŸ“¥ 205 / â­ 11 / JaQuAD is a 2022 Japanese QA dataset of 39,696 SQuADâ€‘style extractive pairs from Wikipedia, totaling 73.2â€¯MB, that achieves 78.92â€¯% F1 (63.38â€¯% EM) when fineâ€‘tuned with BERTâ€‘Japanese.
 * [Japanese-RAG-Generator-Benchmark](https://huggingface.co/datasets/neoai-inc/Japanese-RAG-Generator-Benchmark) - ğŸ“¥ 205 / â­ 4 / Japanese RAG Generator Benchmark (Jâ€‘RAGBench) supplies a multiâ€‘category QA datasetâ€”covering Integration, Reasoning, Logical, Table, and Abstentionâ€”designed to evaluate Japanese RAG generators, built with human effort and GPTâ€‘4.1, and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) - ğŸ“¥ 204 / â­ 16 / Mirror of the Reazon Speechâ€¯v2 dataset: 3,674 denoised audio files processed with UVR on 8â€¯A800 GPUs over 10â€¯days by Stardustâ€‘minus, released under CDLAâ€‘Sharingâ€‘1.0 and without transcripts.
 * [WAON](https://huggingface.co/datasets/speed/WAON) - ğŸ“¥ 200 / â­ 2 / WAON is a largeâ€‘scale, highâ€‘quality Japanese imageâ€‘text pair dataset for visionâ€‘language models, built through size and SigLIPâ€‘score filtering and deduplication on URLs, captions, and perceptual hashes, and licensed under Apacheâ€¯2.0 with use limited to informational analysis under Japanese law.
 * [livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ğŸ“¥ 185 / â­ 4 / Dataset card details the llm-book/ner-wikinews-dataset, a cleaned collection of livedoor News articles under CCâ€¯BYâ€‘NDâ€¯2.1â€¯JP used in the book *Introduction to Large Language Models* and supplied by LONWIIT.
 * [llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions) - ğŸ“¥ 185 / â­ 5 / llmâ€‘jpâ€‘instructions is a manually curated Japanese instruction dataset offering train, dev, and test splits for languageâ€‘model fineâ€‘tuning.
 * [OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - ğŸ“¥ 182 / â­ 14 / 1.8â€¯million Japaneseâ€‘translated OpenMathInstructâ€‘1 instructionâ€‘tuning examples, generated from GSM8K and MATH benchmark questions with Mixtralâ€‘8x7Bâ€‘derived synthetic solutions verified against the original answers, are released for commercial use under an NVIDIA license that requires license inheritance for redistribution, though modelâ€‘learning licenses need not inherit that license.
 * [jhle](https://huggingface.co/datasets/llm-jp/jhle) - ğŸ“¥ 173 / â­ 96 / Japaneseâ€‘translated Humanityâ€™s Last Exam dataset curated by LLMâ€‘jp, which omits image questions, samples five per raw_subject, is machineâ€‘translated and expertâ€‘reviewed, authored by Yujiâ€¯Tamakoshi,â€¯Koutaâ€¯Nakayama andâ€¯Yusukeâ€¯Miyao, and must never be used in training corpora.
 * [gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset) - ğŸ“¥ 172 / â­ 2 / A dataset of 64,139 Japanese names labeled with biological genderâ€”presented in kanji, hiragana, and romajiâ€”whose 44.9â€¯k training, 6.41â€¯k validation, and 12.8â€¯k test split earned acceptance at ISDAâ€™23.
 * [Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene) - ğŸ“¥ 169 / â­ 16 / Partial voiceâ€‘recording and label dataset for è‰è–™å¯§ã€… (Projectâ€¯Sekai), open for completion and community contribution.
 * [llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval) - ğŸ“¥ 165 / â­ 3 / Dataset card for the jaâ€‘vicunaâ€‘qaâ€‘benchmark used in the book â€œIntroduction to Largeâ€‘Scale LLMÂ IIâ€ and created by llmâ€‘jpâ€‘eval for crossâ€‘dataset Japanese LLM evaluation (ApacheÂ 2.0).
 * [CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - ğŸ“¥ 164 / â­ 21 / Manually curated highâ€‘quality 100â€‘sample Japanese Chainâ€‘ofâ€‘Thought dataset, available as two JSONs: one linking CoT to output and one keeping them separate.
 * [auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - ğŸ“¥ 158 / â­ 24 / AutoWikiQA is Japanâ€™s largest free QA dataset (2â€¯,377â€¯,503 pairs) produced from Wikipedia text using Swallowâ€‘MX and vLLM, delivering diverse, templateâ€‘free questions and answers for knowledge injection and retrievalâ€‘augmented generation.
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - ğŸ“¥ 156 / â­ 8 / Umamusume voice transcriptions dataset listing 77 characters with their total audio duration (e.g., East Commerce 799â€¯s, East Imperial Emperor 1074â€¯s, â€¦).
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - ğŸ“¥ 146 / â­ 6 / Deduplicated, NFKCâ€‘normalized mQA queryâ€“passage pairs, with pos_ids/neg_ids mapping to collection indices for direct retrieval via collection[pos_id], and licensed under the original datasetâ€™s terms.
 * [AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully) - ğŸ“¥ 146 / â­ 48 / AnswerCarefully Dataset supplies Japanese and multilingual data for commercial or nonâ€‘commercial LLM safety enhancement, prohibits any other useâ€”including safety circumventionâ€”allows derivative works with attribution, and carries a creator disclaimer of nonâ€‘liability for harms or service changes.
 * [wrime](https://huggingface.co/datasets/shunk031/wrime) - ğŸ“¥ 145 / â­ 27 / The WRIME dataset is a Japanese collection of 42,200 posts annotated with Plutchikâ€™s eight emotions for the writer, three readers, and their averages, structured into 40â€¯kâ€‘train, 1.2â€¯kâ€‘validation, and 2â€¯kâ€‘test splits for sentimentâ€‘analysis tasks.
 * [STAIR-Captions](https://huggingface.co/datasets/shunk031/STAIR-Captions) - ğŸ“¥ 142 / â­ 5 / STAIRâ€‘Captions is a largeâ€‘scale (820,310) Japanese caption dataset for tasks such as caption generation, multimodal retrieval, and image generation, released under CCâ€¯BYâ€¯4.0.
 * [jsick](https://huggingface.co/datasets/hpprc/jsick) - ğŸ“¥ 137 / â­ 8 / JSICK is a Japanese NLI/STS dataset translated from SICK, offering a stress test that probes wordâ€‘order and caseâ€‘particle handling through multiple transformed sentenceâ€‘pair subsets to support research in multilingual compositional inference.
 * [JaMARD](https://huggingface.co/datasets/elyza/JaMARD) - ğŸ“¥ 127 / â­ 10 / A highâ€‘quality synthetic Japanese math problem dataset with verified chainâ€‘ofâ€‘thought reasoning, built by translating PRM800K and GSM8K via Qwen2â€‘7Bâ€‘Instruct and filtering for correctness, available through the HuggingÂ Face datasets library.
 * [wikipedia-ja-20230720](https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720) - ğŸ“¥ 126 / â­ 13 / Dataset card for the 2023â€‘07â€‘20 release of the Japanese Wikipedia dataset.
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - ğŸ“¥ 125 / â­ 16 / Dataset card for **japanese_alpaca_data**, built on masa3141â€™s Japaneseâ€‘Alpacaâ€‘LoRA work, with additional details in the referenced repository.
 * [JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - ğŸ“¥ 121 / â­ 5 / JAQKET is a Japanese openâ€‘domain QA dataset derived from Wikipedia, offering versionâ€¯1.0 with multipleâ€‘choice quiz questions (13,061 training, 271 validation examples) and versionâ€¯2.0 with only question prompts requiring extracted answers (2,154 training, 1,164 validation), designed to facilitate research on QA systems.
 * [oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - ğŸ“¥ 118 / â­ 26 / Japanese-translated OpenAssistant/oasst1 data with failure flags, ~2,000 manually corrected code translation errors, a released chatâ€‘format subset (oasst1â€‘chatâ€‘44kâ€‘ja), and a script to convert entries into instructionâ€“output pairs for fineâ€‘tuning.
 * [pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset) - ğŸ“¥ 117 / â­ 10 / The sovitsâ€‘emuâ€‘dataset offers 2,735 SEGAâ€‘licensed Emu Otori WAV files for soâ€‘vitsâ€‘svcâ€¯4.0 research, released under CCâ€‘BYâ€‘NCâ€¯4.0 (excluding the voice owners), with mandatory attribution, nonâ€‘commercial use, emailâ€‘only access, and optional pullâ€‘request contributions.
 * [simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon) - ğŸ“¥ 115 / â­ 14 / A simple dataset of Zundamon character settingsâ€”compiled from online sources and admin dataâ€”for testing characterâ€‘LLMs, provided in zmnjp.jsonl and zmn.jsonl formats under a specified license.
 * [livedoor-news-corpus](https://huggingface.co/datasets/shunk031/livedoor-news-corpus) - ğŸ“¥ 110 / â­ 7 / Japanese news articles from livedoor News under a CC BYâ€‘ND license are cleaned of HTML, delivered in 6,567 items split 80/10/10 for training, validation, and testing.
 * [JDocQA](https://huggingface.co/datasets/shunk031/JDocQA) - ğŸ“¥ 103 / â­ 10 / JDocQA is a Japanese PDFâ€‘based QA dataset comprising 5,504 documents and 11,600 questionâ€‘answer pairs that test yes/no, factoid, numerical, openâ€‘ended, and unanswerable comprehension using both visual and textual information.
 * [JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - ğŸ“¥ 102 / â­ 17 / JAâ€‘VGâ€‘VQAâ€‘500 is a 500â€‘sample subset of the Japanese Visual Genome VQA dataset, licensed CCâ€¯BYâ€¯4.0, used to benchmark EvoVLMâ€‘JPâ€‘v1â€‘7B.
