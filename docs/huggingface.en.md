# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
    [![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
    [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
    [![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
    [![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

A curated list of resources dedicated to Python libraries, llms, dictionaries, and corpora of NLP for Japanese
This page lists Japanese NLP-specific models and datasets available on Hugging Face. Currently, it includes 219 models and 173 datasets.

_Updated on Dec 23, 2025_

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Ranking](#Ranking)
   * [Models](#models-ranking)
   * [Datasets](#datasets-ranking)
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [sentence-similarity](#sentence-similarity)
   * [feature-extraction](#feature-extraction)
   * [translation](#translation)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [text-classification](#text-classification)
   * [text-ranking](#text-ranking)
   * [image-to-text](#image-to-text)
   * [token-classification](#token-classification)
   * [text-to-speech](#text-to-speech)
   * [audio-to-audio](#audio-to-audio)
   * [image-text-to-text](#image-text-to-text)
   * [others](#others)
 * [Datasets](#Datasets)

## Ranking

### Models-ranking

| # | Model | Downloads | Likes | Category |
|---|-------|-----------|-------|----------|
| 1 | [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) | ğŸ“¥ 3M | â­ 44 | automatic-speech-recognition |
| 2 | [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) | ğŸ“¥ 600k | â­ 22 | sentence-similarity |
| 3 | [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) | ğŸ“¥ 593k | â­ 13 | feature-extraction |
| 4 | [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) | ğŸ“¥ 364k | â­ 70 | fill-mask |
| 5 | [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) | ğŸ“¥ 338k | â­ 1 | others |
| 6 | [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) | ğŸ“¥ 330k | â­ 11 | sentence-similarity |
| 7 | [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) | ğŸ“¥ 262k | â­ 2 | others |
| 8 | [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) | ğŸ“¥ 244k | â­ 8 | translation |
| 9 | [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | ğŸ“¥ 204k | â­ 162 | image-to-text |
| 10 | [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) | ğŸ“¥ 157k | â­ 25 | token-classification |
| 11 | [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) | ğŸ“¥ 154k | â­ 57 | sentence-similarity |
| 12 | [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) | ğŸ“¥ 147k | â­ 15 | text-generation |
| 13 | [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) | ğŸ“¥ 128k | â­ 6 | fill-mask |
| 14 | [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) | ğŸ“¥ 124k | â­ 80 | automatic-speech-recognition |
| 15 | [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) | ğŸ“¥ 108k | â­ 10 | others |
| 16 | [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) | ğŸ“¥ 107k | â­ 8 | fill-mask |
| 17 | [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) | ğŸ“¥ 102k | â­ 1 | others |
| 18 | [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) | ğŸ“¥ 89k | â­ 66 | translation |
| 19 | [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) | ğŸ“¥ 69k | â­ 13 | others |
| 20 | [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) | ğŸ“¥ 67k | â­ 139 | text-generation |

### Datasets-ranking

| # | Dataset | Downloads | Likes |
|---|---------|-----------|-------|
| 1 | [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) | ğŸ“¥ 115k | â­ 17 |
| 2 | [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) | ğŸ“¥ 11k | â­ 8 |
| 3 | [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) | ğŸ“¥ 7k | â­ 8 |
| 4 | [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) | ğŸ“¥ 6k | â­ 10 |
| 5 | [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) | ğŸ“¥ 6k | â­ 22 |
| 6 | [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) | ğŸ“¥ 6k | â­ 18 |
| 7 | [AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully) | ğŸ“¥ 6k | â­ 47 |
| 8 | [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) | ğŸ“¥ 4k | â­ 126 |
| 9 | [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) | ğŸ“¥ 4k | â­ 99 |
| 10 | [MissingKeys](https://huggingface.co/datasets/RyokoExtra/MissingKeys) | ğŸ“¥ 4k | â­ 2 |
| 11 | [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) | ğŸ“¥ 4k | â­ 6 |
| 12 | [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) | ğŸ“¥ 4k | â­ 26 |
| 13 | [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) | ğŸ“¥ 3k | â­ 94 |
| 14 | [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) | ğŸ“¥ 3k | â­ 44 |
| 15 | [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) | ğŸ“¥ 3k | â­ 4 |
| 16 | [mc4-ja](https://huggingface.co/datasets/izumi-lab/mc4-ja) | ğŸ“¥ 3k | â­ 6 |
| 17 | [vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard) | ğŸ“¥ 3k | â­ 39 |
| 18 | [mc4-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/mc4-ja-filter-ja-normal) | ğŸ“¥ 3k | â­ 5 |
| 19 | [Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene) | ğŸ“¥ 3k | â­ 16 |
| 20 | [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) | ğŸ“¥ 2k | â­ 12 |

## Models
### text-generation
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - ğŸ“¥ 147k / â­ 15 / A 12â€‘layer, 768â€‘hidden Japanese GPTâ€‘NeoX model trained on CCâ€‘100, C4, and Wikipedia, compatible with Huggingface, with an optional toy prefixâ€‘tuning weight that forces each sentence to end with a smilingâ€‘face emoji.
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - ğŸ“¥ 67k / â­ 139 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced, 8â€‘billionâ€‘parameter Llamaâ€¯3 model by ELYZA, fineâ€‘tuned for Japanese on Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct.
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - ğŸ“¥ 48k / â­ 9 / llmâ€‘jpâ€‘3.1â€‘1.8b is a 1.8â€¯bâ€‘parameter Japanese LLM from NIIâ€™s Large Language Models R&D Center, distributed as a Huggingâ€¯Face checkpoint (torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, flashâ€‘attnâ€¯â‰¥â€¯2.5) with full model specs, tokenizer, and preâ€‘training details in the repo.
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - ğŸ“¥ 38k / â­ 13 / Provides the 1.8â€¯Bâ€‘parameter llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4 Japanese instructionâ€‘tuned model from NII, compatible with HuggingÂ Face Transformers and Torchâ€¯â‰¥â€¯2.3.0, including pretrained and fineâ€‘tuned checkpoints and usage examples.
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - ğŸ“¥ 37k / â­ 35 / TinySwallowâ€‘1.5B is a compact Japanese instructionâ€‘following language model by Sakana AI and the Swallow Team that uses TAID distillation from Qwen2.5â€‘32Bâ€‘Instruct, is further preâ€‘trained on Japanese text, and is released under ApacheÂ 2.0 for research use only.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - ğŸ“¥ 31k / â­ 15 / Llamaâ€¯3.1â€¯Swallow is a set of 8â€‘B and 70â€‘B models that continue preâ€‘training Metaâ€™s Llamaâ€¯3.1 to boost Japanese language performance, then instructionâ€‘fineâ€‘tune on synthetic Japanese dataâ€”providing multiple released variants with improved conversational behavior comparable to gemmaâ€‘3â€‘27bâ€‘it.
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - ğŸ“¥ 21k / â­ 15 / A collection of Japanese largeâ€‘language models (1.8â€¯b to 172â€¯bâ€¯beta1, with instruct variants) from NIIâ€™s R&D Center, packaged in Huggingâ€¯Face Transformers format and pretrained on a mix of Japanese, English, and Web corpora totalling >1â€¯trillion tokens, requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - ğŸ“¥ 15k / â­ 82 / Rinnaâ€™s 24â€‘layer, 1024â€‘hidden Japanese GPTâ€‘2â€‘medium model, trained on CCâ€‘100 and Wikipedia with SentencePiece tokenization, is available in the rinna/japaneseâ€‘pretrainedâ€‘models repo (MITâ€‘licensed, released Aprilâ€¯7â€¯2021, updated 25â€¯Augâ€¯2021).
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - ğŸ“¥ 13k / â­ 4 / Japaneseâ€‘optimized 8â€‘billionâ€‘parameter Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B, built on Metaâ€‘Llamaâ€‘3â€‘Instruct with extra preâ€‘training and instruction tuning, is offered in GGUF and AWQ quantized forms for vLLM or OpenAIâ€‘compatible inference.
 * [llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3) - ğŸ“¥ 13k / â­ 4 / Hosts the LLMâ€‘jpâ€‘3â€‘7.2bâ€‘instruct3 7.2â€¯Bâ€‘parameter Japanese language model from the National Institute of Informatics, pretrained on Japanese Wikipedia and Common Crawl, provided in Huggingâ€¯Faceâ€¯Transformers format and requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2) - ğŸ“¥ 12k / â­ 15 / Llamaâ€¯3.1â€¯Swallow delivers 8â€¯B and 70â€¯B Japaneseâ€‘enhanced language models built by continual preâ€‘training and instructionâ€‘fineâ€‘tuning on Metaâ€™s Llamaâ€¯3.1, while preserving the original English capabilities.
 * [Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 9k / â­ 20 / Llama3â€¯Swallow is a Japaneseâ€‘enhanced Meta Llamaâ€¯3 family released Julyâ€¯1â€¯2024, offering 8B and 70B variants in Instruct and chat forms fineâ€‘tuned with SFT and Chatâ€¯Vector on Megatronâ€‘LM, and benchmarked on key Japanese NLP tasks.
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - ğŸ“¥ 9k / â­ 4 / Experimental Japanese model created by extracting differences between lightblue/suzumeâ€‘llamaâ€‘3â€‘8Bâ€‘japanese and Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct using a chatâ€‘vector approach, upsampled and applied to Metaâ€‘Llamaâ€‘3â€‘70Bâ€‘Instruct, showing little change and planning future scaling.
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - ğŸ“¥ 7k / â­ 25 / rinnaâ€™s Japanese GPTâ€‘2 small is a 12â€‘layer, 768â€‘hidden transformer trained on Japanese CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released under MIT onâ€¯Augâ€¯25â€¯2021 (Huggingâ€¯Face: rinna/japaneseâ€‘gpt2â€‘small, see https://arxiv.org/abs/2404.01657).
 * [open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - ğŸ“¥ 5k / â­ 17 / OpenCALM is a suite of Japanese decoderâ€‘only language models from CyberAgent, spanning 160â€¯M to 6.8â€¯B parameters built on GPTâ€‘NeoX and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - ğŸ“¥ 5k / â­ 58 / A 2.7â€‘Bâ€‘parameter Japanese GPTâ€‘NeoX model trained on Japanese CCâ€‘100 and OSCAR by ABEJAâ€¯Inc, usable via Hugging Face Transformers pipelines or PyTorch and released under the MIT license.
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ğŸ“¥ 5k / â­ 74 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter extension of Metaâ€™s Llamaâ€‘2 model, preâ€‘trained on Japanese data with instruct and fast variants, and usable through Huggingâ€¯Face Transformers.
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - ğŸ“¥ 5k / â­ 33 / Offers an autoregressive Japanese language model (sarashina2.2â€‘3Bâ€‘instructâ€‘v0.1) from SBâ€¯Intuitions, benchmarked against other models, with example usage scripts and a note that safety training is limited.
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - ğŸ“¥ 4k / â­ 56 / TinySwallowâ€‘1.5Bâ€‘Instruct is a 1.5â€¯B Japanese instructionâ€‘tuned autoregressive language model distilled with TAID from Qwen2.5â€‘32Bâ€‘Instruct, intended for research use only.
 * [shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - ğŸ“¥ 4k / â­ 18 / Fineâ€‘tuned the Japanese Stableâ€¯LM Base Gammaâ€¯7B with Shisaâ€¯7B data, achieving strong results on the JAâ€¯MTâ€‘Bench.
 * [Llama-3-ELYZA-JP-8B-Heretic-GGUF](https://huggingface.co/ChiKoi7/Llama-3-ELYZA-JP-8B-Heretic-GGUF) - ğŸ“¥ 4k / â­ 1 / A Hereticâ€‘v1.1.0 abliteration of the Japaneseâ€‘enhanced Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B model, producing a decensored version that performs well on Japanese prompts yet exhibits a high refusal rate for English.
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - ğŸ“¥ 3k / â­ 15 / LLMâ€‘jpâ€‘3.1â€‘13bâ€‘instruct4 is a 13â€‘B, instructionâ€‘preâ€‘trained Japanese language model developed by NIIâ€™s R&D Center, released as a Huggingâ€‘Face Transformers checkpoint with a UNIGRAMâ€‘byteâ€‘fallback tokenizer.
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - ğŸ“¥ 3k / â­ 22 / Llamaâ€¯3.1â€¯Swallow is a Japaneseâ€‘enhanced series of 8B/70B Llamaâ€¯3.1 models, trained via continual preâ€‘training and Japaneseâ€‘specific instruction fineâ€‘tuning, with the latest 8Bâ€‘Instructâ€‘v0.3 setting stateâ€‘ofâ€‘theâ€‘art results on Japanese MTâ€‘Bench.
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - ğŸ“¥ 3k / â­ 19 / OpenCALM is a family of Japanese decoderâ€‘only Transformer language models (160â€¯Mâ€“6.8â€¯B parameters) from CyberAgent, trained on Japanese Wikipedia and Common Crawl and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - ğŸ“¥ 3k / â­ 43 / TokyoTechâ€‘LLM offers the Swallowâ€¯Llamaâ€¯2 familyâ€”Japaneseâ€‘enhanced, superâ€‘visedâ€‘fineâ€‘tuned and noâ€‘vocabularyâ€‘expansion variants for 7â€¯B, 13â€¯B andâ€¯70â€¯B models, with recent releases includingâ€¯Swallowâ€‘7bâ€‘instructâ€‘v0.1 andâ€¯Swallowâ€‘70bâ€‘NVEâ€‘hf.
 * [ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0) - ğŸ“¥ 3k / â­ 5 / ABEJAâ€‘Qwen2.5â€‘32bâ€‘Japaneseâ€‘v1.0, built on Qwen2.5â€‘32Bâ€‘Instruct, adds Japaneseâ€‘centric preâ€‘training followed by SFT and DPO fineâ€‘tuning (details on ABEJAâ€™s tech blog).
 * [llm-jp-3-13b](https://huggingface.co/llm-jp/llm-jp-3-13b) - ğŸ“¥ 3k / â­ 13 / Repository hosts Huggingâ€¯Face checkpoints for Japanese LLMs (1.8â€¯B,â€¯3.7â€¯B,â€¯13â€¯B,â€¯17.2â€¯B) from the National Institute of Informatics, requiring PyTorchâ€¯2.3+, Transformersâ€¯4.40+ and include sample inference code, a 2.1â€¯Tâ€‘token unigramâ€‘based tokenizer, and preâ€‘training on mixed Japanese and English corpora.
 * [Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1) - ğŸ“¥ 2k / â­ 2 / Gemmaâ€‘2â€‘Llamaâ€‘Swallow is a family of Gemmaâ€‘2 modelsâ€”2b,â€¯9b,â€¯27bâ€”continually preâ€‘trained and instructionâ€‘tuned on synthetic Japanese data, enhancing Japanese performance while maintaining English capability, and released on Huggingâ€¯Face.
 * [Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1) - ğŸ“¥ 2k / â­ 1 / Japaneseâ€‘enhanced, instructionâ€‘tuned Gemmaâ€‘2 models built on Llama (2b/9b/27b preâ€‘train and instruction versions) released onâ€¯Mayâ€¯19â€¯2025, available via HuggingFace and the Swallow teamâ€™s website.
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - ğŸ“¥ 2k / â­ 21 / youriâ€‘7b is a 32â€‘layer, 4096â€‘hidden transformer built from Llama2â€‘7b, continually preâ€‘trained on ~40â€¯B Japanese tokens (CCâ€‘100, C4, OSCAR, Pile, Wikipedia) and released Octâ€¯31â€¯2023, achieving competitive scores on AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA and Winogrande.
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - ğŸ“¥ 2k / â­ 10 / Repository provides GGUFâ€‘formatted, quantised model files for Stability AIâ€™s Japanese StableLMâ€¯Instructâ€¯Gammaâ€¯7B, created with Massedâ€¯Compute hardware and part of TheBlokeâ€™s a16zâ€‘funded LLM work.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ğŸ“¥ 2k / â­ 81 / Japaneseâ€‘enhanced Llamaâ€‘2â€‘7B from ELYZA, preâ€‘trained for extended Japaneseâ€‘language capability with standard, instruct, and fast variants, detailed usage examples, developer credits, and licensed under Metaâ€™s Llamaâ€‘2 Community License.
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - ğŸ“¥ 2k / â­ 205 / OpenCALM is a Japanese decoderâ€‘only transformer languageâ€‘model suite from CyberAgent, Inc. featuring versions ranging from 160â€¯M to 6.8â€¯B parameters preâ€‘trained on Wikipedia and Common Crawl, available via the Transformers library under a CCâ€¯BYâ€‘SAâ€¯4.0 license.
 * [japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - ğŸ“¥ 2k / â­ 75 / A 3.6â€‘billionâ€‘parameter Japanese GPTâ€‘NeoX model, trained on ~650â€¯GB of Japanese text (C4, CCâ€‘100, Oscar, web crawls), attains a 7.50 perplexity on internal C4 validation and is released under Apacheâ€¯2.0.
 * [sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1) - ğŸ“¥ 2k / â­ 13 / SBâ€¯Intuitionsâ€™ Sarashina2.2â€‘0.5Bâ€¯instructâ€¯v0.1 is a 0.5â€‘billionâ€‘parameter Japanese autoregressive model that performs well on Japanese and English MT benchmarks and is ready to load via torchâ€‘transformers.
 * [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - ğŸ“¥ 2k / â­ 17 / The TokyoTechâ€‘LLM repository provides the Swallowâ€¯Llamaâ€‘2 family of LLaMAâ€‘2 models, augmented with Japanese data, covering 7B, 13B, and 70B variants that include instructionâ€‘tuned, NVEâ€‘tuned, and a 7B Plus version released since Decemberâ€¯2023.
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - ğŸ“¥ 2k / â­ 12 / This repository hosts SBâ€¯Intuitionsâ€™ 1â€¯Bâ€‘parameter autoregressive Japanese instruction model sarashina2.2â€‘1bâ€‘instructâ€‘v0.1, benchmarked against other Japaneseâ€‘BERTs on Japanese and English MT and instruction tasks, with a torchâ€‘transformer usage snippet and a warning of limited safety training.
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ğŸ“¥ 2k / â­ 23 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter Japanese extension of Metaâ€™s Llamaâ€‘2â€‘7B, further preâ€‘trained for Japanese language tasks and offered in base, instruct, fast, and fastâ€‘instruct variants, maintained by the ELYZA team under the Llamaâ€¯2 Community License.
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ğŸ“¥ 2k / â­ 96 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a Japaneseâ€‘optimized extension of Metaâ€™s Llamaâ€‘2â€¯7â€¯B, offering instruct and fast variants with 6.27â€“6.37â€¯B parameters that can be accessed via the Huggingâ€‘Face Transformers library.
 * [karasu-1.1B](https://huggingface.co/lightblue/karasu-1.1B) - ğŸ“¥ 2k / â­ 7 / Pretrained TinyLlama in Japanese (â‰ˆ50â€¯k steps) built on ~3â€¯B OSCAR/mC4 tokens, usable via HuggingFace Transformers or VLLM, created by Peterâ€¯Devine, Shoâ€¯Higuchi, Yuukiâ€¯Yamanaka, Atomâ€¯Sonoda, Shunichiâ€¯Taniguchi, Tomiokaâ€¯Wataru, and Renjuâ€¯Aoki.
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - ğŸ“¥ 2k / â­ 15 / LLMâ€‘jp offers 13â€¯B and 1.3â€¯B transformer language models, including multiple instructionâ€‘tuned variants, built with Megatronâ€‘DeepSpeed and the Huggingâ€¯Face Transformers ecosystem.
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - ğŸ“¥ 1k / â­ 41 / Largeâ€‘language models from LLMâ€‘jp â€“ 13B and 1.3B Japaneseâ€‘English transformers with multiple instruction and LoRA variants, preâ€‘trained via Megatronâ€‘DeepSpeed and released in Huggingâ€¯Face format (torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34).
 * [Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2) - ğŸ“¥ 1k / â­ 4 / Llamaâ€¯3.1â€¯Swallow delivers 8B and 70B Japaneseâ€‘enhanced language models, created by continual preâ€‘training and instructionâ€‘tuned synthetic Japanese data, with publicly released v0.1â€“v0.3 versions and strong performance on benchmarks such as JCom, JEMHopQA, NIILC, and JSQuAD.
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - ğŸ“¥ 1k / â­ 53 / Japanese Stableâ€¯LMâ€¯Instructâ€¯Gammaâ€¯7B is a 7â€‘Bâ€‘parameter decoderâ€‘only Japanese language model fineâ€‘tuned on instruction datasets, built on the Base Gammaâ€¯7B, requires Transformersâ€¯4.34+, is Apacheâ€¯2.0â€‘licensed, and is developed by Stabilityâ€¯AI.
 * [ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1) - ğŸ“¥ 1k / â­ 10 / ABEJAâ€‘Qwen2.5â€‘7bâ€‘Japaneseâ€‘v0.1 is a Japaneseâ€‘fineâ€‘tuned Qwenâ€¯2.5â€¯7B model distilled from the 32B Japanese variant and optimized with ChatVector for instructionâ€‘following, available through PyTorch and Huggingâ€¯Face Transformers.
 * [llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - ğŸ“¥ 1k / â­ 62 / A Japaneseâ€‘focused variant of Metaâ€‘Llamaâ€‘3â€‘8B, called Llamaâ€¯3â€¯Youkoâ€¯8B, was continually preâ€‘trained and instructionâ€‘tuned on ~22â€¯B tokens from Japanese corpora (CCâ€‘100, C4, OSCAR, Theâ€¯Pile, Wikipedia) and released onâ€¯Mayâ€¯1â€¯2024.
 * [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ğŸ“¥ 1k / â­ 42 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘13b extends Metaâ€™s Llamaâ€¯2 with additional preâ€‘training for Japanese, offering 13â€¯Bâ€‘parameter models (including instruct and fast variants) that can be loaded with PyTorch and ğŸ¤—â€¯Transformers under the Llamaâ€¯2 Community License.
 * [Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1) - ğŸ“¥ 1k / â­ 4 / Gemmaâ€‘2â€‘Llamaâ€‘Swallow is a Japaneseâ€‘focused extension of Gemmaâ€¯2 that offers 2B,â€¯9B, andâ€¯27B models preâ€‘trained via continual learning and instructionâ€‘tuned through SFT on synthetic Japanese data, and is distributed on HuggingFace and the Swallow teamâ€™s website.
 * [Sakura-13B-Galgame-GGUF](https://huggingface.co/QuantFactory/Sakura-13B-Galgame-GGUF) - ğŸ“¥ 1k / â­ 2 / A CCâ€¯BYâ€‘NCâ€‘SAâ€¯4.0â€‘licensed, offline quantized Japaneseâ€‘English galgame/lightâ€‘novel translation model (Sakuraâ€‘13Bâ€‘Galgame) built with llama.cpp, delivering near GPTâ€‘3.5 performance, offered in multiple sizes and an OpenAIâ€‘APIâ€‘compatible backend.
 * [llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b) - ğŸ“¥ 1k / â­ 10 / Huggingâ€¯Faceâ€‘compatible Japanese transformer LLMs (1.8b,â€¯3.7b,â€¯13b and their instruct/beta variants) built with torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40.1, accelerate, flashâ€‘attn, and pretrained on mixed Japaneseâ€‘English corpora such as Wikipedia, Commonâ€¯Crawl, and Dolma.
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - ğŸ“¥ 1k / â­ 37 / Offers the Swallowâ€¯Llamaâ€‘2 family of Japaneseâ€‘English LLMsâ€”from 7B,â€¯13B,â€¯andâ€¯70B models with instruct, NVE, and preview variantsâ€”tuned via supervised fineâ€‘tuning, available through Megatronâ€‘LM with a tokenizer, and benchmarked on core Japanese tasks.
 * [Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B) - ğŸ“¥ 1k / â­ 6 / Llamaâ€¯3.1â€¯Futureâ€¯Codeâ€¯Ja is an 8â€‘Bâ€‘parameter model built on Metaâ€™s Llamaâ€¯3.1, trained on Japanese code and naturalâ€‘language data (Stackâ€¯V2, LLMâ€‘jp Corpus), fineâ€‘tuned with SFT/DPO, supports both causal and Fillâ€‘inâ€‘theâ€‘Middle inference, and outperforms the vanilla Llamaâ€¯3.1 and Qwen families on Japanese and English codeâ€‘completion tasks.
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - ğŸ“¥ 1k / â­ 13 / An autoGPTQâ€‘quantized 10â€‘Bâ€‘parameter Japaneseâ€‘centric multilingual GPTâ€‘NeoX model (weblabâ€‘10bâ€‘instructionâ€‘sftâ€‘GPTQ) that shrinks the 21.42â€¯GB original to a faster, GPUâ€‘required version, with a 6.03â€¯GB gguf alternative for CPU via llama.cpp, and can be run locally with textâ€‘generationâ€‘webui (~16â€¯tokens/s on an RTXâ€¯3060) or interactively in Colab.
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - ğŸ“¥ 1k / â­ 17 / Japaneseâ€‘StableLMâ€‘Baseâ€‘Betaâ€‘70B is a 70â€‘Bâ€‘parameter Llamaâ€‘2â€‘derived decoderâ€‘only language model fineâ€‘tuned on diverse Japanese data, offering smaller 7â€¯B versions, an instructionâ€‘following variant, and a faster inference release, all licensed under the Llama2 Community License.
 * [llm-jp-13b-instruct-full-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 4 / LLMâ€‘jp supplies instructionâ€‘style and pretrained 13B/1.3B Transformer models in Hugging Face and DeepSpeed formats, trained on 50â€¯k+ mixed Japanese/English/sourceâ€‘code data and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34 and accelerateâ€¯0.23.
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - ğŸ“¥ 1k / â­ 15 / Repositories of 13â€‘B and 1.3â€‘Bâ€‘parameter LLMâ€‘jp instructionâ€‘fineâ€‘tuned modelsâ€”including LoRA variantsâ€”packaged in Huggingâ€¯Face Transformers format and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, and accelerateâ€¯0.23, trained on ~50â€¯k mixed Japanese/English/code examples with Megatronâ€‘DeepSpeed and PEFT.
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - ğŸ“¥ 1k / â­ 26 / Japaneseâ€‘StableLMâ€‘Instructâ€‘Betaâ€‘70B is a 70â€‘billionâ€‘parameter Japanese decoderâ€‘only Llama2â€‘based language model fineâ€‘tuned on Dollyâ€‘15k, Anthropic HH, and other public data, available as a 7â€‘billionâ€‘parameter variant and released under the Llama2 Community License.
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 8 / Hosts LLMâ€‘jpâ€™s 13â€¯B and 1.3â€¯B Japaneseâ€‘English instruction and pretrained models, offered in several variant checkpoints for Huggingâ€¯Face Transformers with torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, accelerateâ€¯0.23 and DeepSpeed support.
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - ğŸ“¥ 1k / â­ 25 / A 7â€‘Bâ€‘parameter, autoregressive, decoderâ€‘only Japanese model based on Mistralâ€‘7Bâ€‘v0.1, released by Stabilityâ€¯AI under Apacheâ€¯2.0 for highâ€‘performance Japanese language and downstream tasks and requiring Transformersâ€¯4.34.0+.
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - ğŸ“¥ 1k / â­ 11 / OpenCALM is a decoderâ€‘only Japanese transformer family from CyberAgent (160â€¯Mâ€¯â€“â€¯6.8â€¯B parameters, from openâ€‘calmâ€‘small to openâ€‘calmâ€‘7b), trained on Japanese Wikipedia and Commonâ€‘Crawl, licensed CCâ€¯BYâ€‘SAâ€¯4.0 and usable through Hugging Face transformers.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - ğŸ“¥ 1k / â­ 3 / Provides a 4â€‘bit, 4.11â€¯GB quantized version of Metaâ€™s Llamaâ€‘2â€¯7B (ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7bâ€‘fastâ€‘instruct) that cuts memory but degrades instructionâ€‘following, requires a GPU and autoGPTQ, and includes references to alternate AWQ, llama.cpp, and gguf quantizations and benchmark results.
 * [Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 1k / â­ 17 / Llamaâ€¯3.1â€¯Swallow is a set of 8B and 70B Japaneseâ€‘enhanced language models derived from Metaâ€™s Llamaâ€¯3.1 through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning, released in Octâ€¯2024 and hosted on swallowâ€‘llm.github.io.
 * [japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - ğŸ“¥ 1k / â­ 106 / A 1.3â€‘Bâ€‘parameter, 24â€‘layer transformer GPTâ€‘1B, trained on Japanese C4, CCâ€‘100, and Wikipedia, was released by rinna Co. on Januaryâ€¯26â€¯2022 and is available under the MIT license.
 * [llm-jp-3-1.8b-instruct](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct) - ğŸ“¥ 1k / â­ 25 / Hugging Faceâ€‘compatible Japaneseâ€‘centric transformer models (llmâ€‘jpâ€‘3â€‘1.8b, 1.8bâ€‘instruct, 3.7b, 3.7bâ€‘instruct, 13b, 13bâ€‘instruct, 17.2bâ€‘beta1, 17.2bâ€‘beta1â€‘instruct) from the National Institute of Informatics, preâ€‘trained on diverse Japanese and English corpora (including Wikipedia, Common Crawl, WARP, Kaken, Dolma) and requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerate, and flashâ€‘attn.
 * [shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - ğŸ“¥ 1k / â­ 16 / shisaâ€‘baseâ€‘7bâ€‘v1 augments Mistralâ€¯7B with 8â€¯B Japanese tokens from MADLADâ€‘400, trainedâ€¯inâ€¯2,400â€¯A100â€‘40 GPUâ€‘hours, and achieves classâ€‘leading Japanese benchmark performance, outperforming comparable 7â€‘B Japaneseâ€‘tuned models such as Japanese Stableâ€¯LM, ELYZA, and Youri.
 * [shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - ğŸ“¥ 1k / â­ 30 / Shisaâ€¯7B is a Japaneseâ€‘focused language model built on Mistralâ€¯7B, trained with curated airoboros, ultrafeedback, and synthetic ENâ€‘JA data, and includes code for preprocessing, translation, fineâ€‘tuning, and evaluation alongside future research documentation.
 * [ALMA-7B-Ja-V2](https://huggingface.co/webbigdata/ALMA-7B-Ja-V2) - ğŸ“¥ 1k / â­ 20 / C3TRâ€‘Adapter applies a 4â€‘bit QLoRA to gemmaâ€‘7b, enabling 8.1â€¯GB GPU usage on free Colab, while ALMAâ€‘7Bâ€‘Jaâ€‘V2 delivers Japaneseâ€‘English translation (and German, Chinese, Icelandic, Czech support) evaluated with BLEU and chrF++ metrics.
 * [Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) - ğŸ“¥ 1k / â­ 4 / Llamaâ€¯3.1â€¯Swallow delivers 8â€‘B and 70â€‘B Japaneseâ€‘enhanced Llamaâ€¯3.1 models, continuously preâ€‘trained on Meta Llamaâ€¯3.1 and instructionâ€‘tuned with synthetic Japanese data, released by the Swallow team under Megatronâ€‘LM with a web portal, model index, and benchmark results.

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - ğŸ“¥ 364k / â­ 70 / Japanese BERTâ€‘base pretrained on 2019 Japanese Wikipedia with IPAâ€‘dictionary and wholeâ€‘word masking, 12â€‘layer 768â€‘dim, 32,000â€‘vocab, 512â€‘token sequences, 1â€¯M steps, available at clâ€‘tohoku/bertâ€‘japanese under CCâ€‘BYâ€‘SA.
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - ğŸ“¥ 128k / â­ 6 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) trained on 30â€¯M Wikipedia sentences (~4â€¯GB) with Unidicâ€¯2.1.2 wordâ€‘level tokenization followed by characterâ€‘level tokenization and wholeâ€‘word masking, using 512â€‘token sequences, 256 batches, and 1â€¯M training steps.
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - ğŸ“¥ 107k / â­ 8 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden, 12 heads) pretrained on ~17â€¯M sentences from Japanese Wikipedia (2.6â€¯GB) using MeCab IPA wordâ€‘level tokenization followed by character tokenization into a 4000â€‘word vocabulary, with training code atâ€¯clâ€‘tohoku/bertâ€‘japanese and released under CCâ€¯BYâ€‘SAâ€¯3.0.
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - ğŸ“¥ 66k / â­ 38 / A BERT base model pretrained on ~17â€¯M Japanese Wikipedia sentences (2.6â€¯GB) that tokenizes with the IPA dictionary and WordPiece, has 12 layers/768â€‘dim hidden states/12 heads, a 32â€¯000â€‘token vocabulary, was trained for 1â€¯M steps on Cloud TPUs and is released under CCâ€‘BYâ€‘SAâ€¯3.0.
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - ğŸ“¥ 37k / â­ 18 / ModernBERTâ€‘Jaâ€‘310M is a Japaneseâ€‘language BERT variant that blends localâ€‘global attention with RoPE, trained on 4.09â€¯T tokens of Japanese/English text, supports a 102â€¯400â€‘word vocabulary, 8â€¯192â€‘token sequences, and is optimized for Flashâ€¯Attentionâ€¯2.
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - ğŸ“¥ 37k / â­ 3 / Japanese RoBERTaâ€‘base model pretrained on ~10â€¯M Japanese medical abstracts and 1.4â€¯M body texts from JST, tokenized with a 30â€¯kâ€‘token SentencePiece, released under CCâ€¯BYâ€‘4.0 and usable via Hugging Face pipelines.
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - ğŸ“¥ 35k / â­ 48 / LINE DistilBERT Japanese is a 66â€‘millionâ€‘parameter DistilBERT model preâ€‘trained on 131â€¯GB of Japanese web text using an inâ€‘house BERTâ€‘base teacher, evaluated on JGLUE, tokenized with MeCabâ€¯Unidic and SentencePiece, and released under the Apacheâ€¯2.0 license.
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - ğŸ“¥ 16k / â­ 30 / Japanese DeBERTaâ€¯V2 base model pretrained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100 and OSCAR data using Juman++ segmentation and SentencePiece tokenization, trained for three weeks on eight NVIDIA A100 GPUs and ready for fineâ€‘tuning.
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - ğŸ“¥ 13k / â­ 27 / Japanese BERTâ€‘base (12â€¯layers, 768 hidden, 12 heads) pretrained on 4â€¯GB of Japanese Wikipedia (â‰ˆ30â€¯M sentences) with Unidicâ€¯2.1.2 wordâ€‘level tokenization, WordPiece subâ€‘tokenization, and wholeâ€‘word masking.
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - ğŸ“¥ 10k / â­ 8 / Japanese DeBERTaâ€¯V2 large model trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with characterâ€‘level sentencepiece tokenization and wholeâ€‘word masking, ready for downstream fineâ€‘tuning through Huggingâ€¯Face Transformers.
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - ğŸ“¥ 9k / â­ 39 / Japaneseâ€‘Robertaâ€‘Base is a pretrained maskedâ€‘language model from rinna Co.,â€¯Ltd., with guidelines for proper loading, token preprocessing, positionâ€‘id handling, and usage examples emphasizing the need for a leading `[CLS]` token and consistent tokenization.
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - ğŸ“¥ 8k / â­ 45 / A 132â€‘millionâ€‘parameter Japanese ModernBERT model that blends localâ€‘global and RoPE attention, trained on 4.39â€¯T tokens (Japanese/English) with a 102â€‘kâ€‘size vocab, 8,192â€‘token max length, and optimized for Flashâ€¯Attentionâ€¯2.
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - ğŸ“¥ 7k / â­ 2 / Japanese DeBERTaâ€¯V2 tiny, pretrained on ~171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR corpora, requires Juman++ word segmentation, was trained in 33â€¯h on 8 NVIDIAâ€¯A100 GPUs, and can be fineâ€‘tuned for downstream tasks.
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - ğŸ“¥ 7k / â­ 5 / ModernBERTâ€‘Jaâ€‘30M is a Japaneseâ€‘language BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯TB of Japanese/English text, supports 8,192â€‘token sequences, comes in sizes from 30â€¯M to 130â€¯M parameters, and works best with Flash Attentionâ€¯2.
 * [splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - ğŸ“¥ 6k / â­ 10 / Zeroâ€‘shot evaluation of Japanese SPLADE variants and other retrieval models on the MIRACL and hotchpotch/JQaRA datasets shows SPLADEâ€‘v3 achieving NDCG@10â€¯=â€¯0.604, SPLADEâ€‘v2â€‘doc requiring no query encoder, and includes sample code for inspecting tokenâ€‘level expansion.
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - ğŸ“¥ 3k / â­ 6 / ModernBERTâ€‘Jaâ€‘70M is a lightweight Japanese BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯T mixedâ€‘language tokens (vocabâ€¯102â€¯400, maxâ€¯8â€¯192 tokens), supports Flashâ€¯Attentionâ€¯2, and comes in several sizes from 30â€¯M to 310â€¯M parameters.
 * [llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base) - ğŸ“¥ 3k / â­ 10 / A ModernBERTâ€‘base model trained on the 3.4â€¯TB Japanese llmâ€‘jpâ€‘corpus v4, fineâ€‘tuned in two stages (max_seq_len 1024 â†’ 8192), achieves 0.92â€¯JSTS, 0.91â€¯JNLI, and 0.88â€¯JCoLA.
 * [deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - ğŸ“¥ 2k / â­ 9 / Japanese DeBERTaâ€¯V2 large is preâ€‘trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR (using Juman++ segmentation and SentencePiece tokenization) and was trained for 36â€¯days on 8 NVIDIAâ€¯A100 GPUs via Huggingâ€¯Face Transformers.
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - ğŸ“¥ 2k / â­ 4 / DeBERTaV2â€¯base trained on Japanese corpora (CCâ€‘100, mC4, OSCAR2301, Wikipedia, Wikinews) with FPâ€‘16 fineâ€‘tuning for NLU tasks (JSTS, JNLI, JCommonsenseQA), released under CCâ€¯BYâ€‘SAâ€¯4.0 and funded by Japanese research grants.
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - ğŸ“¥ 2k / â­ 8 / Japanese RoBERTaâ€‘base model pretrained on Japanese Wikipedia and CCâ€‘100, using Juman++â€‘based SentencePiece tokenization, fineâ€‘tunable via Hugging Face, trained over 700k steps on 8â€¯A100 GPUs with mixedâ€‘precision.
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - ğŸ“¥ 2k / â­ 9 / Japanese BERTâ€‘large (24 layers, 1024â€‘hidden size, 16 heads, 32â€¯K vocab) pretrained on 30â€¯M Japanese Wikipedia sentences with Unidicâ€‘2.1.2 wordâ€‘level tokenization, WordPiece subwords, and wholeâ€‘word masking over 1â€¯M steps.
 * [roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - ğŸ“¥ 1k / â­ 3 / A 512â€‘token Japanese RoBERTaâ€‘large preâ€‘trained on Japanese Wikipedia and CCâ€‘100, employing Juman++â€¯+â€¯SentencePiece tokenization, trained for 670â€¯k steps on eight A100 GPUs with a 6Ã—10â»âµ learning rate.

### sentence-similarity
 * [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) - ğŸ“¥ 600k / â­ 22 / Weights for the final JaColBERTv2.5 checkpoint, trained on just 40â€¯% of JaColBERTv2 data with a new recipe, outperform all previous modelsâ€”including JaColBERTV2 multilingual variants such as BGEâ€‘M3â€”across every dataset.
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - ğŸ“¥ 330k / â­ 11 / Japanese general text embedding models (Ruriâ€‘v3, 30â€‘310â€¯M parameters, 8192â€‘token max, high JMTEB scores) are offered with Sentenceâ€‘Transformers usage examples and benchmark comparisons to other Japanese embeddings.
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - ğŸ“¥ 154k / â­ 57 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token inputs, a 100Kâ€‘token vocabulary, FlashAttentionâ€‘accelerated inference, and multiple size variants for fast sentenceâ€‘transformer usage.
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - ğŸ“¥ 40k / â­ 3 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese text embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192 tokens, a 100â€¯kâ€‘token vocabulary, FlashAttention acceleration, and multiple sizes from 37â€¯M to 315â€¯M parameters.
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - ğŸ“¥ 40k / â­ 34 / GLuCoSE is a Japanese sentenceâ€‘embedding model built on LUKE that outputs 768â€‘dim meanâ€‘pooled vectors (up to 512 tokens) trained on web and NLI/search data, achievingâ€¯0.864 Spearman andâ€¯0.818 Pearson on similarity benchmarks.
 * [sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - ğŸ“¥ 23k / â­ 13 / Japanese Sentenceâ€‘BERT base model fineâ€‘tuned on the Japanese SNLI dataset (523â€¯k train, 10â€¯k validation, 3.9â€¯k test) using the colorfulscoop/bertâ€‘baseâ€‘ja backbone, achieving 85.3â€¯% test accuracy, and deployable via sentenceâ€‘transformersâ€™ encode after installing dependencies.
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - ğŸ“¥ 15k / â­ 16 / JaColBERTv2 is a Japaneseâ€‘only ColBERTâ€‘based retrieval model trained with knowledge distillation on MMarco (31 negatives per positive, 250k steps, batchâ€¯32) that currently outperforms multilingualâ€‘e5â€‘large, BGEâ€‘M3, and JaColBERT, with a full evaluation pending.
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - ğŸ“¥ 13k / â­ 2 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8192â€‘token sequences, a 100Kâ€‘token vocabulary, FlashAttention, and released in sizes from 30â€¯M to 310â€¯M parameters for use with sentenceâ€‘transformers.
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - ğŸ“¥ 9k / â­ 36 / sbert-jsnliâ€‘lukeâ€‘japaneseâ€‘baseâ€‘lite is a 768â€‘dimensional sentenceâ€‘transformer built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite, trained one epoch on shunk031/jsnli, and includes examples for clustering, semantic search, and both Sentenceâ€‘Transformers and HuggingFace usage.
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - ğŸ“¥ 8k / â­ 21 / GLuCoSEâ€¯v2 is a CPUâ€‘friendly Japanese textâ€‘embedding model, fineâ€‘tuned by distillation and multiâ€‘stage contrastive learning, that delivers superior semanticâ€‘similarity and retrieval performanceâ€”outperforming comparableâ€‘size models on MIRACL and related benchmarks.
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - ğŸ“¥ 6k / â­ 44 / PLaMoâ€‘Embeddingâ€‘1B is a Japanese textâ€‘embedding model from Preferred Networks that converts Japanese text into vectors for information retrieval, classification, and clustering, shows strong performance on the JMTEB benchmark, and is freely available under an Apacheâ€¯v2.0 license.
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - ğŸ“¥ 6k / â­ 1 / Ruriâ€¯v3 delivers highâ€‘performance Japanese text embeddings up to 8192 tokens, a 100kâ€‘token vocabulary, FlashAttention support, and multiple model sizes (30â€¯mâ€“310â€¯m) for efficient inference and fineâ€‘tuning via sentenceâ€‘transformers.
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - ğŸ“¥ 4k / â­ 44 / A collection of releaseâ€‘ready Ruri v3 Japanese text embedding models (30mâ€“310m), complete with SentenceTransformer usage tips, query/passage prefixes, and JMTEB benchmark results showing how they compare to other Japanese and multilingual embeddings.
 * [sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b) - ğŸ“¥ 2k / â­ 38 / Sarashinaâ€‘Embeddingâ€‘v1â€‘1B is a 1.2â€¯Bâ€‘parameter Japanese textâ€‘embedding model built on Sarashina2.1â€‘1B, trained with multiâ€‘stage contrastive learning to reach stateâ€‘ofâ€‘theâ€‘art scores on JMTEB while producing 1,792â€‘dimensional dense vectors for semantic similarity, search, and classification under a nonâ€‘commercial license.
 * [RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja) - ğŸ“¥ 1k / â­ 8 / RoSEtta is a Japanese sentenceâ€‘embedding model built on RoFormer with RoPE, pretrained by MLM followed by weak supervision, distillation, and contrastive learning to excel at retrieval up to 1024 tokens, requiring â€œquery:â€ or â€œpassage:â€ prefixes and usable via Sentenceâ€¯Transformers or Huggingâ€¯Face Transformers.
 * [simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - ğŸ“¥ 1k / â­ 15 / Japanese SimCSE based on BERTâ€‘baseâ€‘japaneseâ€‘v2, fineâ€‘tuned on the JSNLI dataset for sentence embeddings, compatible with Sentenceâ€‘Transformers and trained with cosineâ€‘similarity loss to maximize Spearman correlation.

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - ğŸ“¥ 593k / â­ 13 / Japanese CLOOBâ€‘VIT-B-16, a Visionâ€‘Language model based on vitâ€‘baseâ€‘patch16â€‘224 trained on translated CC12M captions and released by rinna Co., Ltd. on Mayâ€¯12â€¯2022 under Apacheâ€¯2.0.
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - ğŸ“¥ 56k / â­ 51 / A Japanese Sentenceâ€‘BERT v2, fineâ€‘tuned on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘wholeâ€‘wordâ€‘masking with MultipleNegativesRankingLoss, boosts accuracy by ~1.5â€“2â€¯% over v1 and is released asâ€¯sonoisa/sentenceâ€‘bertâ€‘baseâ€‘jaâ€‘meanâ€‘tokensâ€‘v2.
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - ğŸ“¥ 30k / â­ 22 / rinna/japanese-clipâ€‘vitâ€‘bâ€‘16 is an Apacheâ€‘2.0 licensed Japanese CLIP model based on ViTâ€‘B/16, trained on CC12M captions translated to Japanese and released on Mayâ€¯12â€¯2022.
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - ğŸ“¥ 24k / â­ 11 / Japanese Sentenceâ€‘BERT (v1) model for generating sentence embeddings, with an improved v2 available and sample usage via Huggingâ€¯Face Transformers and a custom `SentenceBertJapanese` class.
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - ğŸ“¥ 21k / â­ 29 / LY Corporationâ€™s clipâ€‘japaneseâ€‘base is a Japanese CLIP model trained on ~1â€¯B imageâ€‘text pairs, using an Eva02â€‘B transformer image encoder with a 12â€‘layer BERT text encoder, achieving R@1â€¯0.30 on STAIR, 0.89 accuracy on Recruit and 0.58 accuracy on ImageNetâ€‘1K, and supporting zeroâ€‘shot image classification and retrieval.
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - ğŸ“¥ 11k / â­ 2 / ja_ginza_electra is a spaCyâ€¯v3 Python package offering a Japanese ELECTRA model fineâ€‘tuned on mC4 and UD_Japanese_BCCWJâ€¯r2.8 (based on megagonlabs/transformersâ€‘udâ€‘japaneseâ€‘electraâ€‘baseâ€‘discrimininator) with custom bunsetuâ€‘phrase detection, distributed under the MIT license.
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - ğŸ“¥ 3k / â­ 53 / A Japaneseâ€‘language T5 model, pretrained on ~100â€¯GB of Wikipedia and OSCAR data with SentencePiece tokenization, surpasses Googleâ€™s multilingual T5 on a newsâ€‘classification benchmark but needs fineâ€‘tuning and may yield biased outputs.
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - ğŸ“¥ 3k / â­ 14 / Japanese Sentenceâ€‘LUKE model trained on the same dataset as Sentenceâ€‘BERT, outperforming or matching it, built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite and used via Huggingâ€¯Face Transformersâ€™ MLukeTokenizer and LukeModel.
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - ğŸ“¥ 2k / â­ 14 / Supâ€‘simcseâ€‘jaâ€‘large provides a supervised SimCSE fineâ€‘tuned Japanese BERTâ€‘large (clâ€‘tohoku/bertâ€‘largeâ€‘japaneseâ€‘v2) model with CLSâ€‘plusâ€‘MLP pooling, trained on ~1â€¯M JSNLI sentences (lrâ€¯5eâ€‘5, batchâ€¯512, tempâ€¯0.05, maxâ€¯64) and ready for use with Sentenceâ€‘Transformers or Huggingâ€¯Face Transformers.
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - ğŸ“¥ 2k / â­ 17 / Sarashinaâ€‘Embeddingâ€‘v2â€‘1B is a 1,792â€‘dimensional Japanese sentence transformer trained with multiâ€‘stage contrastive learning that achieves stateâ€‘ofâ€‘theâ€‘art JMTEB scores, and can be used for semantic similarity, search, paraphrase mining, classification, and clustering via Sentenceâ€‘Transformers with optional instruction prefixes.
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - ğŸ“¥ 1k / â­ 2 / A Japanese BERTâ€‘base model fineâ€‘tuned with supervised SimCSE on JSNLI, exposed via Sentenceâ€‘Transformers or HuggingFace with CLS pooling, trained on 1â€¯M examples at 512â€‘batch size, 5â€¯Ã—â€¯10â»âµ learning rate, 5â€¯Ã—â€¯10â»âµ temperature, 64â€‘token limit, and BFloat16 precision.

### translation
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - ğŸ“¥ 244k / â­ 8 / A LLaMAâ€¯3 Youko qlora fineâ€‘tune built on a new VNTL dataset, optimized for accurate, literal translations of Japanese visual novels to English without chat mode, using the default LLaMAâ€¯3 prompt and recommending neutral sampling (temperatureâ€¯0, no repetition penalty).
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - ğŸ“¥ 89k / â­ 66 / Japaneseâ€‘toâ€‘English Transformerâ€‘Align MT model from the Opus corpus, using normalization and SentencePiece preprocessing, achieves 41.7 BLEU and 0.589 chrâ€‘F on the Tatoeba test set.
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - ğŸ“¥ 11k / â­ 14 / Englishâ€‘toâ€‘Japanese transformerâ€‘align MT model with 15.2â€¯BLEU, built on opus+btâ€‘2021â€‘04â€‘10 using normalizationâ€¯+â€¯SentencePiece, hosted on the Tatoeba Challenge.
 * [LFM2-350M-ENJP-MT](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT) - ğŸ“¥ 5k / â­ 79 / LFM2â€‘350Mâ€‘ENJPâ€‘MT is a fineâ€‘tuned LFM2â€‘350M checkpoint that delivers near realâ€‘time, bidirectional Japanese/English translation for shortâ€‘toâ€‘medium inputs with quality comparable to models over ten times larger, as illustrated across everyday, technical, business, and news domains, and emphasizes collaborative humanâ€‘AI use.
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - ğŸ“¥ 5k / â­ 27 / Fineâ€‘tuned, GGUFâ€‘quantized LFM2â€‘350M checkpoint for near realâ€‘time biâ€‘directional Japaneseâ€‘English translation of shortâ€‘toâ€‘medium text, usable via llama.cpp.
 * [opus-mt-ja-ru](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ru) - ğŸ“¥ 3k / â­ 3 / Japaneseâ€‘toâ€‘Russian transformerâ€‘align MT model trained on Opusâ€¯2020â€‘06â€‘17 with normalization plus SentencePiece, part of Helsinkiâ€‘NLPâ€™s Tatoeba Challenge, scoring BLEUâ€¯23.2 and chrF2â€¯0.441 on the Tatoeba test set.
 * [plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate) - ğŸ“¥ 2k / â­ 106 / PLaMo Translation Model is a largeâ€‘scale language model created by Preferred Networks for translation tasks, available in base, postâ€‘trained, and evaluation variants, released under the PLaMo community license and not instructionâ€‘tuned for chat or other downstream uses.
 * [Sugoi-14B-Ultra-GGUF](https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF) - ğŸ“¥ 2k / â­ 8 / Sugoiâ€¯LLMâ€¯14Bâ€¯Ultra (GGUF) is a Japaneseâ€‘toâ€‘English translation model with a BLEU score of 21.38â€”nearly double its prior 13.67â€”excel at RPGâ€‘Maker bracketed text, strong prompt adherence, and JSON output for interactive chat UIs.
 * [elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en) - ğŸ“¥ 2k / â­ 9 / ElanMTâ€‘BTâ€‘jaâ€‘en is a Marian MT Japaneseâ€‘toâ€‘English model fineâ€‘tuned solely on openly licensed and backâ€‘translated Wikipedia data, matching the performance of other public models while explicitly avoiding webâ€‘crawled or machineâ€‘translated corpora, and released under a CCâ€‘BYâ€‘SAâ€‘4.0 license.
 * [fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja) - ğŸ“¥ 1k / â­ 54 / FuguMT is a Marianâ€‘NMT based Englishâ€‘toâ€‘Japanese translation model built with Huggingâ€¯Face Transformers and SentencePiece, achieving a BLEU score of 32.7 on Tatoeba.
 * [fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en) - ğŸ“¥ 1k / â­ 32 / FuguMT is a Japaneseâ€‘toâ€‘English Marianâ€‘NMT translation model built with transformers and SentencePiece, scoring 39.1 BLEU on Tatoeba.

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - ğŸ“¥ 3M / â­ 44 / Japanese wav2vecâ€‘2 XLSRâ€‘53 fineâ€‘tuned on Common Voiceâ€¯6.1, CSS10, and JSUT, requiring 16â€¯kHz audio and usable via HuggingSound or HuggingFace pipelines.
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - ğŸ“¥ 124k / â­ 80 / Kotobaâ€‘Whisperâ€¯v2.0 is a Japanese ASR model distilled from OpenAI Whisper largeâ€‘v3, trained on 7.2â€¯million ReazonSpeech clips, that runs 6.3Ã— faster while matching the teacherâ€™s CER/WER on inâ€‘domain tests and includes stableâ€‘ts/punctuation support and full training code on GitHub.
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - ğŸ“¥ 66k / â­ 4 / Fineâ€‘tuned wav2vec2â€‘base Japanese ASR model trained on Common Voiceâ€¯11.0 that predicts only Hiragana, built from rinna/japaneseâ€‘wav2vec2â€‘base with 20â€¯epochs at lrâ€¯1eâ€‘4.
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - ğŸ“¥ 18k / â­ 19 / Kotobaâ€‘Whisperâ€‘v2.1 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated punctuationâ€‘postprocessing pipelines that maintain comparable CER performance while enabling seamless, punctuationâ€‘aware transcription.
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - ğŸ“¥ 17k / â­ 83 / Kotobaâ€‘Whisperâ€‘v2.2 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated diarization and automatic punctuation via a HuggingFaceâ€‘Transformers pipeline, built in collaboration with Asahiâ€¯Ushio and Kotoba Technologies.
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - ğŸ“¥ 10k / â­ 114 / Anime Whisper is a lightweight Japanese ASR model fineâ€‘tuned on ~5,300â€¯h of animeâ€‘style dialogue that delivers low hallucination, rhythmâ€‘aligned punctuation and accurate transcription of nonâ€‘verbal sounds and NSFW content, and must be run without an initial prompt.
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - ğŸ“¥ 7k / â­ 35 / reazonspeech-nemo-v2 is a 619â€‘Mâ€‘parameter Japanese longâ€‘form ASR model built on an improved Fastâ€‘Conformer with Linearly Scalable Attention, trained on the ReazonSpeechâ€¯v2.0 corpus, offering multiâ€‘hour inference via a subword RNNâ€‘T decoder (3000â€‘token SentencePiece) and distributed under Apacheâ€¯2.0.
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - ğŸ“¥ 3k / â­ 38 / NVIDIA NeMoâ€™s 0.6â€¯Bâ€‘parameter Hybrid FastConformerâ€‘TDTâ€‘CTC ASR model transcribes Japanese speech with punctuation and is available for inference or fineâ€‘tuning within the NeMo framework.
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - ğŸ“¥ 2k / â­ 17 / Kotobaâ€‘Whisperâ€‘Bilingual v1.0 delivers 6.3Ã— faster distilled Whisper models for Japanese and English ASR plus bidirectional speechâ€‘toâ€‘text translation, built from OpenAIâ€™s Whisperâ€¯largeâ€‘v3 via knowledge distillation with crossâ€‘entropy and KLâ€‘divergence loss.

### text-classification
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - ğŸ“¥ 44k / â­ 3 / Japanese BERT Base model fineâ€‘tuned on a 10â€‘label emotion blogâ€‘post dataset (~1,000 sentences) derived fromâ€¯tohokuâ€‘nlp/bert-base-japanese for accurate emotion detection and classification.
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ğŸ“¥ 13k / â­ 43 / A Japanese LUKE model fineâ€‘tuned on the WRIME dataset that classifies which of eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, trustâ€”is expressed in a sentence.
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - ğŸ“¥ 12k / â­ 14 / Japanese sentiment analysis model trained on the chABSA dataset, achieving lossâ€¯0.0001, accuracyâ€¯1.0, and F1â€¯1.0, built with Transformersâ€¯4.24.0 and PyTorchâ€¯1.12.1+cu113, optimized with Adam (learningâ€¯rateâ€¯2eâ€‘05, 10 epochs, batchâ€¯sizeâ€¯16) and evaluated via `model(**inputs)`.
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - ğŸ“¥ 6k / â­ 2 / A Japanese BERTâ€‘based model fineâ€‘tuned on the JGLUE JSTS dataset for semantic similarity scoringâ€”introduced in chapterâ€¯5 ofâ€¯â€œLarge Language Model Introductionâ€â€”with Colab notebooks, transformersâ€‘pipeline usage, and an Apacheâ€¯2.0 license.
 * [bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - ğŸ“¥ 2k / â­ 14 / Fineâ€‘tuned Japanese BERT (clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2) on Amazon product reviews for sentiment classification, achieving ~81â€¯% accuracy and 0.73â€¯F1 after 6 epochs with a 2â€¯Ã—â€¯10â»âµ learning rate.
 * [bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - ğŸ“¥ 1k / â­ 6 / A Japanese BERT BASE fineâ€‘tuned on the WRIME dataset predicts 0â€‘4 intensity scores for eight emotions (joy, sadness, anticipation, surprise, anger, fear, disgust, trust) for writers and readers, with code available, trained in 3â€¯hrs on a K80, achieving about 0.6â€¯MSE for writers and 0.2â€¯MSE for readers.

### text-ranking
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - ğŸ“¥ 13k / â­ 5 / Fast, lightweight Japanese Rerankerâ€¯v2 models (tiny,â€¯xsmall,â€¯small,â€¯base) with benchmark scores and GPU speeds, usable via sentence_transformersâ€¯CrossEncoder and transformersâ€¯â‰¥â€¯v4.48 (optionally accelerated with flashâ€‘attn) and also available in ONNX/quantized forms for CPU/ARM.
 * [japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - ğŸ“¥ 11k / â­ 16 / Japanese-language CrossEncoder reranker modelsâ€”from xsmall to largeâ€”trained on Japanese text, exposed through sentence_transformers, with evaluation on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - ğŸ“¥ 10k / â­ 4 / Japaneseâ€‘trained CrossEncoder rerankers ranging from xsmall (384) to large (1024) plus a BGEâ€‘v2â€‘m3â€‘v1 model, with example code for fineâ€‘tuning, inference, and benchmark scores on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - ğŸ“¥ 7k / â­ 12 / Ruriâ€‘v3 Reranker is a robust Japanese text reranker built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token sequences, a 100kâ€‘token vocabulary, FlashAttention and a SentencePiece tokenizer, and it can be used via sentenceâ€‘transformers.
 * [japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - ğŸ“¥ 7k / â­ 15 / A Japanese CrossEncoder reranker suiteâ€”including xsmall, small, base, large, and japaneseâ€‘bgeâ€‘rerankerâ€‘v2â€‘m3â€‘v1â€”paired with example usage, evaluation metrics on several benchmarks, and supporting documentation.
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - ğŸ“¥ 6k / â­ 7 / Japanese CrossEncoder reranker models ranging from xsmall to large (plus BGE), evaluated on JQaRA, JaCWIR, MIRACL, and JSQuAD, with readyâ€‘toâ€‘use integration examples for sentence_transformers and HuggingFace.

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - ğŸ“¥ 204k / â­ 162 / Mangaâ€¯OCR is a Visionâ€¯Encoderâ€‘Decoder OCR tool that reads vertical and horizontal Japanese manga textâ€”including furiganaâ€”across diverse fonts and lowâ€‘quality images, with the source code freely available.
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - ğŸ“¥ 31k / â­ 3 / meikiocr supplies a Dâ€‘FINEâ€‘based, openâ€‘weight textâ€‘detection model for videoâ€‘games (v0.1 with MobileNetâ€‘v4 backbones, two resolution variants and a 64â€‘box limit) and experimental lowâ€‘latency tiny and small variants trained on Japanese videoâ€‘games and manga.
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - ğŸ“¥ 30k / â­ 3 / Meikiocrâ€™sâ€¯`meiki.text.recognition.v0`â€”a Dâ€‘FINE-based MobileNetV4 model fineâ€‘tuned on Japanese videoâ€‘game textâ€”delivers stateâ€‘ofâ€‘theâ€‘art accuracy and latency for horizontal text by detecting up to 48 characters from 960Ã—32 inputs, outputting each character with its bounding box and confidence score.
 * [sarashina2.2-vision-3b](https://huggingface.co/sbintuitions/sarashina2.2-vision-3b) - ğŸ“¥ 3k / â­ 13 / Sarashina2.2â€‘Visionâ€‘3B is a 3â€‘B parameter Japanese large visionâ€‘language model built on Sarashina2.2â€‘3Bâ€‘Instruct and a SigLIP image encoder, achieving strong performance on Japanese VQA benchmarks.
 * [sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b) - ğŸ“¥ 1k / â­ 10 / Sarashina2â€‘Visionâ€‘8B is a Japanese large visionâ€‘language model built on Sarashina2â€‘7B and the Qwen2â€‘VLâ€‘7B image encoder that tops four benchmarks as of Marchâ€¯2025, with openâ€‘source inference scripts and training details included.

### token-classification
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - ğŸ“¥ 157k / â­ 25 / Fineâ€‘tuned XLMâ€‘RoBERTaâ€‘base on a Japanese NER corpus (tagsâ€¯PER,â€¯ORG,â€¯LOC,â€¯INS,â€¯PRD,â€¯EVT) using 5â€‘epoch Adam (lrâ€¯5eâ€‘5, batchâ€¯12) to reach a 0.0173 validation loss, released on Transformersâ€¯4.23.1 and PyTorchâ€¯1.12.1.
 * [MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - ğŸ“¥ 27k / â­ 4 / A Huggingâ€‘Faceâ€‘compatible NER model trained on the MedTxtâ€‘CRâ€‘JA Japanese medical dataset, accompanied by a predict script that normalizes entity outputs, produces XMLâ€‘tagged text, and uses an external `id_to_tags.pkl` to map label IDs to real tags.
 * [bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - ğŸ“¥ 9k / â­ 10 / Fineâ€‘tuned Japanese BERTâ€‘Base for namedâ€‘entity recognition on a Wikipedia dataset, presented in Chapterâ€¯6 of the *Large Language Model Introduction* book and deployable via a Huggingâ€¯Face transformers pipeline (Apacheâ€¯2.0 licensed).
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - ğŸ“¥ 9k / â­ 11 / Japanese NER using clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2 that extracts eight entity types (corporations, political/other organizations, facilities, products, events) viaâ€¯`BertForTokenClassification`, trained on the Stockmark Wikipedia dataset and installable with `transformers`, `unidic_lite`, and `fugashi` under a CCâ€¯BYâ€‘SAâ€¯3.0 license.

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - ğŸ“¥ 3k / â­ 23 / Animeâ€‘Llasaâ€‘3B is a Japanese TTS model built on HKUSTAudio/Llasaâ€‘3B, enhanced with more training data to boost expressiveness and stability, and licensed CCâ€‘BYâ€‘NCâ€‘4.0.
 * [Anime-Llasa-3B-Captions](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-Captions) - ğŸ“¥ 2k / â­ 13 / Animeâ€‘Llasaâ€‘3Bâ€‘Captions is a Japanese textâ€‘toâ€‘speech model built on Animeâ€‘Llasaâ€‘3B and fineâ€‘tuned with Geminiâ€¯2.5â€¯Proâ€‘generated audio metadata, enabling controllable speech synthesis via prompt tags and inâ€‘text markers, though it cannot always perfectly reflect the specified attributes.

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - ğŸ“¥ 6k / â­ 11 / Animeâ€‘XCodec2â€‘44.1kHzâ€‘v2 upsamples 16â€¯kHz Japanese speech to 44.1â€¯kHz highâ€‘fidelity audio with a decoderâ€‘only RMSâ€‘loss fineâ€‘tune, keeping the encoder/codebook frozen and preserving identical speech tokens.

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - ğŸ“¥ 2k / â­ 114 / Fineâ€‘tuned from PaddleOCRâ€‘VL, PaddleOCRâ€‘VLâ€‘Forâ€‘Manga achieves 70â€¯% fullâ€‘sentence accuracy on Manga109â€‘s speechâ€‘bubble cropsâ€”over triple the 27â€¯% baselineâ€”using a multiâ€‘language dataset and includes training code and a developer guide.

### others
 * [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) - ğŸ“¥ 338k / â­ 1 / A ggufâ€‘formatted version of cyberagentâ€™s openâ€‘calmâ€‘3b model on the mmngaâ€‘dev branch, ready for llama.cpp testing with usage examples and a note that it may not work once gptneox is integrated.
 * [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) - ğŸ“¥ 262k / â­ 2 / A temporary test branch offering aâ€¯ggufâ€‘formatted version of CyberAgentâ€™s openâ€‘calmâ€‘7b model forâ€¯llama.cpp, with instructions to clone the dev branch, build, and run the model (note other similar gguf releases exist).
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - ğŸ“¥ 108k / â­ 10 / Japaneseâ€‘language BERTâ€‘Base (12 layers, 768â€‘dim, 12 heads) pretrained with Unidicâ€‘based wordâ€‘level plus characterâ€‘level tokenization and wholeâ€‘word masking on CCâ€‘100 and 2023 Wikipedia, producing a 7,027â€‘token vocabulary.
 * [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - ğŸ“¥ 102k / â­ 1 / A testâ€‘branch conversion of stockmarkâ€™s gptâ€‘neoxâ€‘japaneseâ€‘1.4b to gguf format, intended for use with llama.cppâ€™sâ€¯mmngaâ€‘dev branch and shown with example inference commands and GPU support.
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - ğŸ“¥ 69k / â­ 13 / Japaneseâ€‘BERTâ€‘Large trained on CCâ€‘100 and Wikipedia, using Unidicâ€‘lite wordâ€‘level tokenization with WordPiece subwords and wholeâ€‘word masking (24 layers, 1024â€‘dim hidden, 16 heads, 32k vocab), with pretraining code on clâ€‘tohoku/bertâ€‘japanese.
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - ğŸ“¥ 37k / â­ 57 / Japanese BERTâ€‘base (12 layers, 768â€‘dim hidden, 12 heads, 32â€¯k vocab) pretrained with wholeâ€‘word masking on CCâ€‘100 and 2023â€‘Jan Wikipedia, using Unidicâ€¯2.1.2 wordâ€‘level tokenization plus WordPiece, in 2â€¯M training steps.
 * [sarashina2.2-0.5b](https://huggingface.co/sbintuitions/sarashina2.2-0.5b) - ğŸ“¥ 37k / â­ 10 / Sarashina2.2 provides 0.5â€‘B, 1â€‘B, and 3â€‘B language models trained by SB Intuitions through a threeâ€‘phase pipeline and synthetic data, achieving top Japanese QA, math, and coding scores while offering preâ€‘trained weights that are not instructionâ€‘tuned and may produce biased outputs.
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - ğŸ“¥ 16k / â­ 17 / Japanese DeBERTaâ€¯V3â€¯base preâ€‘trained on 540â€¯B tokens from LLMâ€‘jpâ€¯v1.0, trained with a modified DeBERTaâ€¯V3 setup, uses a unigram byteâ€‘fallback tokenizer (no morphological analyzer), and is fineâ€‘tuned for JGLUE NLU tasks.
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - ğŸ“¥ 15k / â­ 10 / A Japanese T5â€‘v1.1 model pretrained on â‰ˆ100â€¯GB of Wikipedia and OSCAR CCâ€‘100 data (mixed 10:1 for SentencePiece with byteâ€‘fallback), requiring fineâ€‘tuning for downstream tasks, includes transferâ€‘learning sample code, notes potential bias in outputs, and is licensed CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Llama-3-ELYZA-JP-8B-Heretic-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-ELYZA-JP-8B-Heretic-i1-GGUF) - ğŸ“¥ 8k / â­ 1 / Repository provides a full set of weighted/imatrix GGUF quantizations for Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8Bâ€‘Heretic at varying quality and size levels (e.g., i1â€‘IQ1_S, i1â€‘IQ2_M, i1â€‘Q4_K_M), downloadable from Huggingâ€¯Face with usage guidance linked to TheBlokeâ€™s READMEs.
 * [shisa-v2.1-qwen3-8b-UD-japanese-imatrix](https://huggingface.co/dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix) - ğŸ“¥ 8k / â­ 1 / A GGUFâ€‘quantized shisaâ€‘v2.1â€‘qwen3â€‘8b model built with Unsloth Dynamicâ€¯2.0, communityâ€‘patched Qwen3 settings to reduce malfunctions, a larger imatrix for stronger Japanese performance, and a 40K maximum context length.
 * [bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - ğŸ“¥ 6k / â­ 10 / Japanese BARTâ€‘Base pretrained on 18â€¯M Japanese Wikipedia sentences, segmented with Juman++ and tokenized by SentencePiece, fineâ€‘tunable, trained for 500k steps on 4â€¯Teslaâ€¯V100 GPUs with 6â€‘layer encoder/decoder and 768â€‘dim hidden size.
 * [shisa-v2.1-unphi4-14b-i1-GGUF](https://huggingface.co/mradermacher/shisa-v2.1-unphi4-14b-i1-GGUF) - ğŸ“¥ 5k / â­ 1 / Hosted here are weighted/imatrix and GGUF quantizations of Shisaâ€‘V2.1â€‘UNPhi4â€‘14B (static GGUF releases on HuggingÂ Face), listing download links, file sizes and quality notes, with usage guidance linked to TheBlokeâ€™s READMEs.
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - ğŸ“¥ 5k / â­ 69 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced 8â€‘B Llamaâ€¯3 model with GGUF (Q4_K_M) and AWQ quantization, ready to run via llama.cpp, LMâ€¯Studio, or an OpenAIâ€‘compatible API.
 * [gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf) - ğŸ“¥ 5k / â­ 12 / Gemmaâ€‘2â€‘2bâ€‘jpnâ€‘itâ€‘translateâ€‘gguf is a 2â€‘billionâ€‘parameter, ~2â€¯GB small language model that delivers Japaneseâ€‘English translation quality comparable to 7â€‘billionâ€‘parameter models, performs best on sentenceâ€‘byâ€‘sentence input, and includes Colab and llama.cpp usage examples.
 * [Tema_Q-R3.1-i1-GGUF](https://huggingface.co/mradermacher/Tema_Q-R3.1-i1-GGUF) - ğŸ“¥ 4k / â­ 1 / Offers a comprehensive list of weighted/imatrix GGUF quant versions for the Tema_Qâ€‘R3.1 model, detailing sizes, quality notes, download links, usage guidanceâ€”including GGUF file handlingâ€”and links to the model page, readmes, and FAQ.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - ğŸ“¥ 3k / â­ 55 / Cyberagentâ€™s ggufâ€‘converted DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14Bâ€‘Japanese model (built from the TFMC imatrix dataset) is available under mmnga and can be run with CUDA support using llama.cpp.
 * [Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf) - ğŸ“¥ 3k / â­ 5 / A ggufâ€‘formatted Ninjaâ€‘v1â€‘NSFW model for Japanese LLM, built from the imatrix dataset and usable with llama.cpp (clone, compile, run with the provided prompt).
 * [c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - ğŸ“¥ 2k / â­ 4 / GGUFâ€‘formatted versions of CohereForAIâ€™s c4aiâ€‘commandâ€‘râ€‘plus, built with Japanese LLM data from TFMC/imatrix, plus instructions for concatenating split files and running the model via llama.cpp for Japanese dialogue.
 * [Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf) - ğŸ“¥ 2k / â­ 11 / A repository providing a GGUFâ€‘format conversion of the Ninjaâ€‘v1â€‘NSFWâ€‘128k model built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with usage instructions for running it in llama.cpp to generate Japanese novel text.
 * [Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5) - ğŸ“¥ 2k / â­ 7 / Llamaâ€¯3.1â€¯Swallowâ€¯v0.5 is an 8â€‘billionâ€‘parameter LLM that improves Metaâ€™s Llamaâ€¯3.1 on Japanese language and code/math reasoning while retaining English fluency, achieved through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning on synthetic Japanese data.
 * [t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - ğŸ“¥ 2k / â­ 2 / A T5â€¯v1.1 model preâ€‘trained on Japanese Wikipedia andâ€¯mC4/ja, featuring GEGLU activations, no dropout during preâ€‘training, no embeddingâ€‘classifier sharing, released under CCâ€‘BYâ€‘SAâ€¯4.0 (commercial use requires prior contact).
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - ğŸ“¥ 2k / â­ 7 / A ggufâ€‘format conversion of Vecteusâ€‘v1 from Localâ€‘Novelâ€‘LLM, built using the imatrix dataset, that can be run with llama.cpp via `Vecteusâ€‘v1â€‘Q4_0.gguf` and lists other related models.
 * [lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - ğŸ“¥ 2k / â­ 2 / Japanese 8â€‘B LLaMAâ€¯3 converted to GGUF format by lightblue, built with theâ€¯TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm.
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - ğŸ“¥ 2k / â­ 16 / Highâ€‘performance Japanese SPLADEâ€¯v2 enables sparseâ€‘vector conversion and inference through a WebUI demo, trains with YAST, offers YASEM embedding, and reports JMTEB benchmark results.
 * [r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf) - ğŸ“¥ 2k / â­ 2 / A GGUFâ€‘format conversion of perplexityâ€‘AIâ€™s r1â€‘1776â€‘distillâ€‘llamaâ€‘70b, built with imatrix data fromâ€¯TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, ready for CUDAâ€‘enabled use with llama.cpp.
 * [haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - ğŸ“¥ 2k / â­ 4 / ggufâ€‘formatted conversion of Llamaâ€‘3â€‘8B Japanese Instruct built from the imatrix dataset, ready for inference with llama.cpp.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - ğŸ“¥ 2k / â­ 39 / GGUFâ€‘formatted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen Japanese 32B model from cyberagent, built with the imatrix dataset and ready to run with llama.cpp.
 * [umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf) - ğŸ“¥ 2k / â­ 7 / A ggufâ€‘formatted version of Umievoâ€‘itr012â€‘Gleipnirâ€‘7B (trained on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm) ready to run with llama.cpp.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5-gguf](https://huggingface.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf) - ğŸ“¥ 2k / â­ 2 / GGUF conversion of Llamaâ€‘3.1â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.5 by tokyotechâ€‘llm, incorporating TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with Build/Run instructions forâ€¯llama.cpp.
 * [Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf) - ğŸ“¥ 2k / â­ 2 / Provides a ggufâ€‘format conversion of the Ninjaâ€‘v1â€‘128k model from the Localâ€‘Novelâ€‘LLMâ€‘project, built using TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm data, and includes llama.cpp usage instructions.
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - ğŸ“¥ 2k / â­ 18 / A GGUFâ€‘format release of pfnetâ€™s plamoâ€‘2â€‘translate built from imatrix data based on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with instructions to compile and run it via llama.cpp on CUDAâ€‘enabled hardware.
 * [gemma-3-JP-EN-Translator-v1-4B-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-JP-EN-Translator-v1-4B-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Weighted/imatrix quantizations of the Gemmaâ€‘3 JPâ€‘EN translator (v1â€‘4B) are provided in multiple GGUF and static formats with Hugging Face links, fileâ€‘size/quality notes, a qualityâ€‘vsâ€‘size comparison graph, and usage guidance for a visionâ€‘modelâ€‘compatible mmproj file.
 * [rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf) - ğŸ“¥ 2k / â­ 6 / GGUFâ€‘formatted conversion of rinnaâ€™s llamaâ€‘3â€‘youkoâ€‘8b, trained with the imatrix dataset, including usage instructions and links to related models.
 * [Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf) - ğŸ“¥ 2k / â­ 9 / A gguf-format release of moonshotaiâ€™s Moonlightâ€‘16Bâ€‘A3Bâ€‘Instruct, trained on TFMCâ€™s imatrix Japanese dataset, ready for use with llama.cpp (CUDAâ€‘enabled) and shown by running a recipeâ€‘request prompt.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ğŸ“¥ 2k / â­ 44 / Converted GGUF of ELYZAâ€™s 7â€¯b Japanese Llamaâ€‘2 instruct model, adding Japanese vocab for a 1.8Ã— speedâ€‘up and ready to run with llama.cpp under the Llamaâ€¯2 license.
 * [aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf) - ğŸ“¥ 2k / â­ 1 / A gguf-format conversion of CohereForAIâ€™s ayaâ€‘23â€‘35B model, built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, runable via llama.cpp with `./main -m 'aya-23-35B-Q4_0.gguf'`.
 * [Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf) - ğŸ“¥ 1k / â­ 7 / A ggufâ€‘formatted version of cyberagentâ€™s Llamaâ€‘3.1â€‘70Bâ€‘Japaneseâ€‘Instructâ€‘2407, built using TFMC/imatrixâ€‘dataset-forâ€‘japaneseâ€‘llm data and run with llama.cppâ€™s CLI.
 * [DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf) - ğŸ“¥ 1k / â­ 10 / A gguf-format conversion of DataPilotâ€™s ArrowProâ€‘7Bâ€‘KUJIRA model, built with TFMCâ€™s imatrix Japanese LLM dataset and run with llama.cpp.
 * [rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf) - ğŸ“¥ 1k / â­ 1 / A ggufâ€‘formatted copy of rinnaâ€™s Llamaâ€‘3â€‘Youkoâ€‘70Bâ€‘Instruct model built from imatrix data, ready for inference with llama.cpp.
 * [lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf) - ğŸ“¥ 1k / â­ 6 / Repository hosts a gguf-converted version of lightblueâ€™s DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘7Bâ€‘Japanese model, built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, and ready for inference with llama.cpp.
 * [QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf) - ğŸ“¥ 1k / â­ 3 / A ggufâ€‘formatted conversion of Qwenâ€™s QwQâ€‘32Bâ€‘Preview model built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, including instructions to run it via llama.cpp.
 * [tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf) - ğŸ“¥ 1k / â­ 1 / A ggufâ€‘formatted TokyoTechâ€‘LLM Swallowâ€‘13bâ€‘instructâ€‘v0.1 model built with the imatrix Japanese dataset, ready for use with llama.cpp as demonstrated.
 * [karakuri-lm-8x7b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf) - ğŸ“¥ 1k / â­ 4 / A ggufâ€‘formatted release of karakuriâ€‘lmâ€‘8x7bâ€‘chatâ€‘v0.1, trained on the TFMC/imatrix Japanese LLM dataset and runnable with llama.cpp using a Q4_0 quantized model.
 * [umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - ğŸ“¥ 1k / â­ 3 / GGUFâ€‘formatted Japanese chat model â€œumiyukiâ€‘Japaneseâ€‘Chatâ€‘Umievoâ€‘itr001â€‘7bâ€ built from the imatrix dataset, runnable through llama.cpp.
 * [aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf) - ğŸ“¥ 1k / â­ 1 / CohereForAIâ€™s ayaâ€‘23â€‘8B model in gguf format, built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, includes a llama.cpp usage example.
 * [gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf) - ğŸ“¥ 1k / â­ 1 / GGUFâ€‘formatted conversion of Googleâ€™s gemmaâ€‘2â€‘2bâ€‘it model using imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, with llama.cpp usage instructions.
 * [c4ai-command-r7b-12-2024-gguf](https://huggingface.co/mmnga/c4ai-command-r7b-12-2024-gguf) - ğŸ“¥ 1k / â­ 2 / Provides a ggufâ€‘format conversion of CohereForAIâ€™s c4aiâ€‘commandâ€‘r7bâ€‘12â€‘2024 model, built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with instructions for compiling and running it via llama.cpp using CUDA.
 * [pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf) - ğŸ“¥ 1k / â­ 1 / A GGUFâ€‘format conversion of pfnetâ€™s nekomataâ€‘14bâ€‘pfnâ€‘qfin Japanese largeâ€‘language model, built from the TFMC/imatrixâ€‘dataset and licensed under Tongyiâ€‘Qianwen, ready for use with llama.cpp.
 * [Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf) - ğŸ“¥ 1k / â­ 4 / GGUFâ€‘converted Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B by elyza, built with the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, ready for use with llama.cpp.
 * [llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf) - ğŸ“¥ 1k / â­ 5 / GGUF conversion of llmâ€‘jpâ€‘3â€‘7.2bâ€‘instruct3 built from the TFMC/imatrix dataset, usable with llama.cpp (without custom chat templates) and covering several mmngaâ€‘llmâ€‘jp model variants.
 * [ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf) - ğŸ“¥ 1k / â­ 1 / GGUFâ€‘converted ArrowProâ€‘7Bâ€‘KillerWhale, built with TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM and released by DataPilot, is ready for inference with llama.cpp using main â€‘m â€œArrowProâ€‘7Bâ€‘KillerWhaleâ€‘Q4_0.ggufâ€.
 * [Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf) - ğŸ“¥ 1k / â­ 3 / GGUFâ€‘converted Llamaâ€‘3â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.1 from tokyotechâ€‘llm, built with the TFMC/imatrix Japanese LLM dataset, is ready to run via llama.cppâ€™s inference tool.
 * [ELYZA-Thinking-1.0-Qwen-32B-gguf](https://huggingface.co/mmnga/ELYZA-Thinking-1.0-Qwen-32B-gguf) - ğŸ“¥ 1k / â­ 1 / A gguf-format conversion of ELYZAâ€™s Thinkingâ€‘1.0 Qwenâ€‘32B model (built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm), with instructions to compile and run it via llama.cpp using CUDA.
 * [Llama-3-ELYZA-JP-8B-Heretic-GGUF](https://huggingface.co/mradermacher/Llama-3-ELYZA-JP-8B-Heretic-GGUF) - ğŸ“¥ 1k / â­ 1 / Provides a selection of GGUFâ€‘quantized static models and weighted/imatrix variants for Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8Bâ€‘Heretic, ranging from Q2_K (3.3â€¯GB) to Q8_0 (8.6â€¯GB) with recommended fast options, along with usage guidance and links for model requests.
 * [tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf) - ğŸ“¥ 1k / â­ 2 / GGUFâ€‘formatted Llamaâ€‘3.1â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.2 from tokyotechâ€‘llm, built with the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm and accompanied by llama.cpp usage instructions.
 * [WabiSabi-V1-i1-GGUF](https://huggingface.co/mradermacher/WabiSabi-V1-i1-GGUF) - ğŸ“¥ 1k / â­ 1 / Weighted/imatrix quantizations of the WabiSabiâ€‘V1 modelâ€”available as GGUF files of varied sizes and quality, alongside static quants on Huggingâ€¯Faceâ€”include usage guidance, a comparison graph, and a FAQ/modelâ€‘request hub.

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - ğŸ“¥ 115k / â­ 17 / Aggregates overâ€¯150â€¯GB of NicoNico Liveâ€™s comment logs from 2009â€‘2024â€”including preâ€‘transition, postâ€‘transition, and realâ€‘time NXâ€‘Jikkyo capturesâ€”providing an API for easy retrieval of historical TVâ€‘broadcast discussions.
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - ğŸ“¥ 11k / â­ 8 / JMedBench is a Japanese biomedical LLM benchmark comprising 20 datasets across five tasks (MCQA, NER, STS, etc.) sourced from MedMCQA, PubMedQA, MMLU, and others, each with its own license, and includes a note that translations may contain biases requiring human review.
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - ğŸ“¥ 7k / â­ 8 / Cauldronâ€‘JA is a Japanese visionâ€‘language dataset of 44 subâ€‘datasets translated from The Cauldron using the DeepL API, available via HuggingFaceâ€™s datasets library and licensed identically to the original set, with prompts released under CCâ€‘BYâ€‘4.0.
 * [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) - ğŸ“¥ 6k / â­ 10 / Mirror of the Reazon Speechâ€¯v2 dataset on ğŸ¤—, licensedâ€¯CDLAâ€‘Sharingâ€‘1.0 and restricted to Japanese Copyright Actâ€¯Articleâ€¯30â€‘4, with 16â€¯kHz FLAC audio and accompanying metadata.
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - ğŸ“¥ 6k / â­ 22 / FineWeb2 Edu Japanese delivers ~120â€¯million highâ€‘quality educational Japanese texts (â‰ˆ89.3â€¯billion tokens) from FineWeb2, filtered by a DeepSeekâ€‘API classifier (scoreâ€¯â‰¥â€¯2.5), tokenized via ModernBERTâ€‘Jaâ€‘130M, and includes a smallâ€‘token subset (â‰¤512 tokens).
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - ğŸ“¥ 6k / â­ 18 / JMTEB is a Japanese textâ€‘embedding benchmark featuring 5 tasks (clustering, classification, STS, retrieval, reranking) and 28 datasets, offering a oneâ€‘line evaluation script and inviting community contributions.
 * [AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully) - ğŸ“¥ 6k / â­ 47 / AnswerCarefully Dataset supplies Japanese and multilingual data for commercial or nonâ€‘commercial LLM safety enhancement, prohibits any other useâ€”including safety circumventionâ€”allows derivative works with attribution, and carries a creator disclaimer of nonâ€‘liability for harms or service changes.
 * [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) - ğŸ“¥ 4k / â­ 126 / Japanese Anime Speech Datasetâ€¯V2 delivers 292,637 cleaned audioâ€‘text pairsâ€”about 397.5â€¯h of SFW and 52.4â€¯h of NSFW contentâ€”in 128â€‘kbps MP3 files split by safety, designed specifically for training automatic speechâ€‘recognition models.
 * [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ğŸ“¥ 4k / â­ 99 / A 100â€‘sample Japanese instructionâ€‘tuning evaluation dataset of annotated tasksâ€”ranging from summarization correction and math reasoning to translation, creative generation, and userâ€‘intent understandingâ€”designed for manual or automatic 5â€‘point rating of fineâ€‘tuned models.
 * [MissingKeys](https://huggingface.co/datasets/RyokoExtra/MissingKeys) - ğŸ“¥ 4k / â­ 2 / MissingKeys is a raw Japaneseâ€‘dominant dataset from the misskey.io network, stored in dateâ€‘compressed JSONL files (â‰ˆ100,000 notes each inside .7z archives) and intended primarily for unsupervised textâ€‘generation training.
 * [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) - ğŸ“¥ 4k / â­ 6 / Artificial voice dataset created with VOICEVOX from the ITA, Tsukuyomiâ€‘chan, and ROHAN corpora, containing 445,793 WAV files totaling 577â€¯hâ€¯51â€¯mâ€¯23â€¯s.
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - ğŸ“¥ 4k / â­ 26 / Restructured reupload of the Galgame VisualNovel datasetâ€¯(OOPPEENN/56697375616C4E6F76656C5F44617461736574) for efficient Huggingâ€¯Faceâ€¯datasets loading, preserving all original audio/text and providing an extraction script with multiple gameâ€‘subset options.
 * [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) - ğŸ“¥ 3k / â­ 94 / Nemotronâ€‘Personasâ€‘Japan is an openâ€‘source, CCâ€¯BYâ€¯4.0 dataset of highâ€‘quality, synthetically generated Japanese personasâ€”incorporating name, gender, age, background, marital status, education, occupation and locationâ€”grounded in realâ€‘world demographic, geographic and personality distributions, engineered with probabilistic graphical models and GPTâ€‘OSSâ€‘120B to enhance diversity, reduce bias, prevent model collapse, assist sovereign AI development, and support commercial use.
 * [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - ğŸ“¥ 3k / â­ 44 / Updated JGLUE dataset card and loading script for a Japanese NLP benchmark (created by Yahoo Japan and Waseda University) that covers text classification (MARCâ€‘ja, JCoLA), sentenceâ€‘pair classification (JNLI), and QA (JSQuAD, JCommonsenseQA), with releases linked on GitHub and Hugging Face.
 * [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) - ğŸ“¥ 3k / â­ 4 / Dataset of dialogues and lore from the Fate/Stayâ€¯Night character â€œEmiliaâ€, formatted for training and evaluating conversational language models.
 * [mc4-ja](https://huggingface.co/datasets/izumi-lab/mc4-ja) - ğŸ“¥ 3k / â­ 6 / Dataset card for the Japanese MC4 dataset (mc4-ja).
 * [vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard) - ğŸ“¥ 3k / â­ 39 / The VNTL leaderboard evaluates large language models on translating Japanese visual novels into English by averaging cosineâ€‘similarity scores over 256 samples, ranking preliminary results and benchmarking against tools such as Sugoi Translator, Google Translate, and Naver Papago.
 * [mc4-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/mc4-ja-filter-ja-normal) - ğŸ“¥ 3k / â­ 5 / Dataset card for the â€œmc4â€‘jaâ€‘filterâ€‘jaâ€‘normalâ€ dataset, with additional information pending.
 * [Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene) - ğŸ“¥ 3k / â­ 16 / Partial voiceâ€‘recording and label dataset for è‰è–™å¯§ã€… (Projectâ€¯Sekai), open for completion and community contribution.
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - ğŸ“¥ 2k / â­ 12 / Elite Voice Project compiles Hololive VTuber Sakuraâ€¯Mikoâ€™s audio from Twitch, Twitter and YouTube into a train/testâ€‘organized dataset for speechâ€‘recognition research, using Gitâ€‘LFS, licensed under Hololiveâ€™s fanâ€‘content rules and welcoming community contributions.
 * [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) - ğŸ“¥ 2k / â­ 3 / Japanese Wikipedia sentences are transformed into various embeddings and a FAISS index, offering a Hugging Face Space demo, conversion scripts, and evaluations of search, Q&A, and OpenAIâ€¯textâ€‘embeddingâ€‘3â€‘small for RAG; embeddings are OpenAIâ€‘licensed, others CCâ€‘BYâ€‘SAâ€‘4.0.
 * [oscar_2023_filtered](https://huggingface.co/datasets/if001/oscar_2023_filtered) - ğŸ“¥ 2k / â­ 3 / A 312,396â€‘row filtered subset of the OSCARâ€‘2301 dataset (Hugging Faceâ€¯`if001/oscar_2023_filtered`), with implementation details available at theâ€¯HojiChar_OSCAR_sample GitHub repository.
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - ğŸ“¥ 2k / â­ 3 / AnimuSubtitleâ€‘JP hosts Japanese ASS/SSA subtitle datasets (data_ass, data_TS) that can be parsed with Pythonâ€™sâ€¯ass library or edited in Aegisub, and is released under an ODCâ€‘By license.
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - ğŸ“¥ 2k / â­ 3 / Dataset of Japanese Boketeâ€‘site humor posts (from CLoTâ€‘Oogiriâ€‘Goâ€¯CVPRâ€¯2024) featuring three tasksâ€”textâ€‘toâ€‘text, imageâ€‘toâ€‘text, and textâ€‘imageâ€‘toâ€‘textâ€”with roughly 600â€¯examples, processed via GPTâ€‘4o OCR and HojiChar filtering.
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - ğŸ“¥ 2k / â­ 136 / Japanese Anime Speech Dataset offers 73,004 audioâ€‘text pairs (110â€¯hours total, evolving from V1 to V5) to enhance ASR models such as OpenAIâ€™s Whisper, available under an open license for all uses with credit appreciated.
 * [aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - ğŸ“¥ 2k / â­ 36 / A userâ€‘friendly, deduplicated CSV dataset of publicâ€‘domain Japanese texts from Aozoraâ€¯Bunko, processed with globisâ€‘org/aozorabunkoâ€‘extractor and cleaned for modernâ€‘Japanese machineâ€‘learning use.
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - ğŸ“¥ 2k / â­ 5 / Japanese JaQuAD, a subset of QGâ€‘Bench, provides sentenceâ€‘ and paragraphâ€‘level data with highlighted answer tokens for training Japanese questionâ€‘generation models, evaluated by BLEU4, METEOR, ROUGEâ€‘L, BERTScore, and MoverScore.
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - ğŸ“¥ 2k / â­ 20 / A Japanese web collection of 56â€¯million documents, 110â€¯B characters and 249â€¯million images used to train large visionâ€‘language modelsâ€”offering a momiji_generator for data population, OBELICSâ€‘style visualization, and a sample model (Heronâ€‘NVILAâ€‘Lite).
 * [sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus) - ğŸ“¥ 2k / â­ 5 / An 81â€‘yearâ€‘old Japanese womanâ€™s â€œFusicâ€¯Saâ€‘yoâ€‘jiâ€ voice corpus, downloadable as a zip from GoogleÂ Drive, offers raw noisy and cleanedâ€¯.wav files together with phoneme and Kana labels and prosodic symbols, is free for nonâ€‘explicit commercial use with credit, prohibits direct audio links, and mandates redistributing the README with any reâ€‘distribution.
 * [japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - ğŸ“¥ 2k / â­ 3 / Japanese Web Corpus 2010 data, automatically punctuated with morphology analysis, uploaded to Hugging Face for research use only under the 2009 copyright amendment, and including conversion scripts.
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - ğŸ“¥ 1k / â­ 30 / A 409â€‘hour Japanese eroge voice dataset processed with 2â€‘pass loudnorm (â€‘23â€¯LUFS, â€‘1â€¯dB peak, 11â€¯LRA), transcribed by litagin/anime-whisper, anonymized, stored as WebDataset (FLAC, JSON, TXT), largely featuring female voices with potential AI transcription errors, and MITâ€‘licensed for academic research.
 * [japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos) - ğŸ“¥ 1k / â­ 32 / A 11,810â€‘image, 28.9â€¯GB dataset of ~4k JPEGs displaying Japanâ€™s urban, natural, historical, artistic, and everyday scenes, each paired with BLIP captions and metadata, released under CC0â€¯1.0 for AI training.
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - ğŸ“¥ 1k / â­ 4 / Reformatted Japanese subset of the Wiki40B dataset compiled by Mandy Guo, Zihang Dai, and Denny VrandeÄiÄ‡.
 * [JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA) - ğŸ“¥ 1k / â­ 2 / JCommonsenseQA is a Japanese multipleâ€‘choice dataset adapted from CommonsenseQA, offering 5 answer options per question, labeled indices for the correct choice, and released under a Creative Commonsâ€¯BYâ€‘SAâ€¯4.0 license.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - ğŸ“¥ 1k / â­ 87 / An automatically translated Japanese version of the databricksâ€‘dollyâ€‘15k dataset, licensed CCâ€‘BYâ€‘SAâ€‘3.0 and last updated on 2023â€‘05â€‘11.
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - ğŸ“¥ 1k / â­ 8 / Rakuda supplies 40 Japanese questionsâ€”openâ€‘ended for history, society, and government, and specific for geographyâ€”for benchmarking Japanese AI assistants, comparable to vicunaâ€‘eval, and can be loaded with `datasets.load_dataset`.
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - ğŸ“¥ 1k / â­ 2 / Huggingâ€¯Face mirror of the ABEJAâ€¯CCâ€‘JA dataset from AWS Openâ€¯Data, with details posted on ABEJAâ€™s tech blog.
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - ğŸ“¥ 1k / â­ 37 / Galgame_Speech_ASR_16kHz is a 16â€¯kHz ASR dataset with 3.75â€¯million pairs (â‰ˆ5,354â€¯h), derived from Galgame_Dataset, released under GPLâ€¯v3.0 with commercial use prohibited and requiring any trained models to be openâ€‘source (citation optional).
 * [defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter) - ğŸ“¥ 978 / â­ 2 / A 5,000â€‘tweet Japanese Twitter defamation detection dataset, annotated for target (A1â€“A3) and content (B1â€“B4) by three crowdworkers and collected Febâ€‘Junâ€¯2022, requiring API access to retrieve the original tweets.
 * [llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - ğŸ“¥ 975 / â­ 139 / Japanese instructionâ€‘chat dataset for fineâ€‘tuning LLMs (e.g., with LoRA), 9â€¯M+ samples, recently updated to drop licensed Alpaca data, clean Wikipedia and ALT outputs, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Hachi-Alpaca](https://huggingface.co/datasets/HachiML/Hachi-Alpaca) - ğŸ“¥ 970 / â­ 15 / Hachi-Alpaca delivers Japanese synthetic data derived from Stanford Alpaca, refined and verified by mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, used through Deepinfra, with â€œ_cleanedâ€ versions that have passed modelâ€‘based quality checks.
 * [JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - ğŸ“¥ 961 / â­ 11 / JaQuAD is a 2022 Japanese QA dataset of 39,696 SQuADâ€‘style extractive pairs from Wikipedia, totaling 73.2â€¯MB, that achieves 78.92â€¯% F1 (63.38â€¯% EM) when fineâ€‘tuned with BERTâ€‘Japanese.
 * [STAIR-Captions](https://huggingface.co/datasets/shunk031/STAIR-Captions) - ğŸ“¥ 961 / â­ 5 / STAIRâ€‘Captions is a largeâ€‘scale (820,310) Japanese caption dataset for tasks such as caption generation, multimodal retrieval, and image generation, released under CCâ€¯BYâ€¯4.0.
 * [kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en) - ğŸ“¥ 960 / â­ 9 / A Japaneseâ€‘toâ€‘English parallel corpus translating the kaken subset of llmâ€‘jpâ€‘corpusâ€‘v3 with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, featuring custom translation columns and licensed under CCâ€‘BYâ€‘4.0.
 * [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) - ğŸ“¥ 905 / â­ 5 / KokushiMDâ€‘10 delivers a multimodal benchmark of Japanese national healthcare licensing exam questionsâ€”spanning ten professions, offered in Japanese, English, and mixed splitsâ€”with expert chainâ€‘ofâ€‘thought annotations for LLM evaluation.
 * [JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU) - ğŸ“¥ 844 / â­ 19 / JMMMU is a Japanese multimodal benchmark expanded over tenfold to 1,320 culturally diverse questions (720 cultureâ€‘agnostic, 600 cultureâ€‘specific) translated by native subject experts, now featuring a public leaderboard.
 * [wikipedia-ja-20230720](https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720) - ğŸ“¥ 786 / â­ 13 / Dataset card for the 2023â€‘07â€‘20 release of the Japanese Wikipedia dataset.
 * [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) - ğŸ“¥ 721 / â­ 4 / Provides a Japanese search/QA dataset with perâ€‘query scores computed by five multilingual/Japanese rerankers (e.g., BAAI/bgeâ€‘rerankerâ€‘v2â€‘m3, Alibabaâ€‘NLP/gteâ€‘multilingualâ€‘rerankerâ€‘base), including average scores for roughly 200 positive and negative example documents per query.
 * [Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus) - ğŸ“¥ 714 / â­ 3 / Lux Japanese Speech Corpus: a 96â€¯kHz 16â€‘bit WAV dataset of Japanese TTS recordings by characterÂ Lux, including raw and cleaned audio, transcripts inÂ metadata.csv, dataset metadata inÂ dataset_infos.json, and released under CCâ€¯BYâ€¯4.0.
 * [cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - ğŸ“¥ 680 / â­ 21 / cc100-ja is a collection of the Japanese portion of the cc100 dataset, provided as sharded Parquet files.
 * [jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella) - ğŸ“¥ 643 / â­ 5 / JaCappella offers sixâ€‘part (lead, soprano, alto, tenor, bass, vocal percussion) scores and parallel audio tracks for Japanese a cappella ensembles across multiple genres, downloadable from Huggingâ€¯Face.
 * [paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa) - ğŸ“¥ 638 / â­ 3 / Dataset of LLMâ€‘generated queries and answers from paraphrases of Japanese Wikipedia text, built without using licenseâ€‘restricted models and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [JaMARD](https://huggingface.co/datasets/elyza/JaMARD) - ğŸ“¥ 632 / â­ 9 / A highâ€‘quality synthetic Japanese math problem dataset with verified chainâ€‘ofâ€‘thought reasoning, built by translating PRM800K and GSM8K via Qwen2â€‘7Bâ€‘Instruct and filtering for correctness, available through the HuggingÂ Face datasets library.
 * [JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA) - ğŸ“¥ 621 / â­ 3 / Japanese Explainable Multiâ€‘hop Question Answering dataset featuring questions, answers, and stepâ€‘byâ€‘step derivations linking Wikipedia articles, with updated derivation formatting and multiple version releases.
 * [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) - ğŸ“¥ 612 / â­ 104 / ReazonSpeech is a free, FLACâ€‘encoded Japanese speech corpus with transcriptions, offered in five sizes from 8.5â€¯h to 35,000â€¯h, downloadable via Huggingâ€¯Face under the CDLAâ€‘Sharingâ€‘1.0 license and limited to use under Japan Copyright Act Articleâ€¯30â€‘4.
 * [pvc](https://huggingface.co/datasets/p1atdev/pvc) - ğŸ“¥ 597 / â­ 7 / PVC figure product dataset covering goodsmile (947), goodsmileâ€‘nendoroid (3,378), goodsmileâ€‘scale (2,203), kotobukiya (864), myethos, spiritale, and tokyofigures (394) with corresponding source URLs.
 * [cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents) - ğŸ“¥ 558 / â­ 3 / Documentâ€‘level concatenation of the cc100â€‘ja dataset from HuggingFace, licensed under the original cc100 terms.
 * [xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - ğŸ“¥ 555 / â­ 6 / Japanese XLâ€‘Sum subset filtered via PaLMâ€‘2 15â€‘gram overlap, containing 4,215 training, 758 validation, and 766 test examples.
 * [sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese) - ğŸ“¥ 555 / â­ 5 / Converted Japanese datasets into SentenceTransformersâ€‘friendly columns, filtering examples by Rerank scores (â‰¥0.7 positive, â‰¤0.3 negative) from multiple HuggingFace sources to support contrastive learning while respecting the original licenses.
 * [JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - ğŸ“¥ 552 / â­ 5 / JAQKET is a Japanese openâ€‘domain QA dataset derived from Wikipedia, offering versionâ€¯1.0 with multipleâ€‘choice quiz questions (13,061 training, 271 validation examples) and versionâ€¯2.0 with only question prompts requiring extracted answers (2,154 training, 1,164 validation), designed to facilitate research on QA systems.
 * [JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - ğŸ“¥ 548 / â­ 10 / JMMLU is a Japanese Massive Multitask Language Understanding Benchmark featuring 7,536 teacherâ€‘crafted questions across 56 subjects, including professional medicine, psychology, accounting, philosophy, and diverse highâ€‘school disciplines.
 * [auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - ğŸ“¥ 542 / â­ 24 / AutoWikiQA is Japanâ€™s largest free QA dataset (2â€¯,377â€¯,503 pairs) produced from Wikipedia text using Swallowâ€‘MX and vLLM, delivering diverse, templateâ€‘free questions and answers for knowledge injection and retrievalâ€‘augmented generation.
 * [SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText) - ğŸ“¥ 530 / â­ 2 / Randomly extracted passages from Wikibooks, Wikipedia, Cosmopedia, and caseâ€‘law were regenerated with Ï†â€‘3, using tensâ€‘ofâ€‘GB parquet datasets (the datasets library loads only the first few GB, so GitÂ LFS is needed) and computation was performed on Tokyoâ€¯Techâ€™s TSUBAME4.0 supercomputer.
 * [CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - ğŸ“¥ 523 / â­ 2 / Japanese CallHome corpus contains 200 USâ€‘based 30â€‘minute telephone audio recordings from 120 speakers, with 80 training, 20 development and 100 evaluation transcriptions (DOI:â€¯10.21415/T5H59V).
 * [llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - ğŸ“¥ 521 / â­ 32 / Japanese chatbot dataset stripped of English translation data from izumiâ€‘lab/llmâ€‘japaneseâ€‘dataset, offering 2.5â€¯million+ entries (v1.0.0) for fineâ€‘tuning Japanese LLMs on instructionâ€‘response tasks under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [jsick](https://huggingface.co/datasets/hpprc/jsick) - ğŸ“¥ 510 / â­ 8 / JSICK is a Japanese NLI/STS dataset translated from SICK, offering a stress test that probes wordâ€‘order and caseâ€‘particle handling through multiple transformed sentenceâ€‘pair subsets to support research in multilingual compositional inference.
 * [anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0) - ğŸ“¥ 495 / â­ 21 / AIâ€‘generated anime illustrations with English prompts and Phiâ€‘3 Visionâ€‘derived captions (English and Japanese) released into the public domain for free use.
 * [jawiki](https://huggingface.co/datasets/hpprc/jawiki) - ğŸ“¥ 490 / â­ 18 / A structured text dataset from Wikipediaâ€™s Januaryâ€¯2024 HTML dump that preserves paragraph structure without markup, provides metadata (abstracts, dates, disambiguation/sexual/violent flags, templates) for each article, and is ready for NLP experiments.
 * [livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ğŸ“¥ 473 / â­ 4 / Dataset card details the llm-book/ner-wikinews-dataset, a cleaned collection of livedoor News articles under CCâ€¯BYâ€‘NDâ€¯2.1â€¯JP used in the book *Introduction to Large Language Models* and supplied by LONWIIT.
 * [JMMMU-Pro](https://huggingface.co/datasets/JMMMU/JMMMU-Pro) - ğŸ“¥ 469 / â­ 6 / JMMMUâ€‘Pro is a lowâ€‘cost, imageâ€‘based Japanese multimodal benchmark built by generating visual questions with Nanoâ€¯Bananaâ€¯Pro and human verification, showing that current openâ€‘source LMMs struggle and guiding future Japanese VQA research.
 * [llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval) - ğŸ“¥ 457 / â­ 3 / Dataset card for the jaâ€‘vicunaâ€‘qaâ€‘benchmark used in the book â€œIntroduction to Largeâ€‘Scale LLMÂ IIâ€ and created by llmâ€‘jpâ€‘eval for crossâ€‘dataset Japanese LLM evaluation (ApacheÂ 2.0).
 * [japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized) - ğŸ“¥ 442 / â­ 3 / Japanese web corpora such as mc4â€‘ja are cleaned and clustered into ~10,000 groups by an unsupervised model, available for lawful analysis; only some files are in parquet format with the file list in the out folder, and it should be downloaded using gitâ€‘lfs.
 * [llava-instruct-ja](https://huggingface.co/datasets/llm-jp/llava-instruct-ja) - ğŸ“¥ 441 / â­ 5 / Japanese LLaVAâ€‘Instruct dataset of 156K samples, generated with GPTâ€‘4oâ€‘mini via Azure OpenAI, licensed CCâ€¯BYâ€¯4.0 and compliant with OpenAI terms.
 * [JGLUE](https://huggingface.co/datasets/llm-book/JGLUE) - ğŸ“¥ 426 / â­ 14 / Dataset card for the JGLUE dataset used in the book â€œLarge Language Model Introduction,â€ sourced from the original repo, with code licensed CCâ€¯BYâ€‘SAâ€¯4.0, data under the distributorâ€™s license, citing Kurihara & Kawahara (in Japanese), and built on Shunsuke Kitadaâ€™s repository.
 * [oscar2301-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/oscar2301-ja-filter-ja-normal) - ğŸ“¥ 422 / â­ 6 / Dataset card for the Japaneseâ€‘filtered OSCARâ€¯2301 subset, â€œoscar2301â€‘jaâ€‘filterâ€‘jaâ€‘normal.â€
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - ğŸ“¥ 415 / â­ 6 / Deduplicated, NFKCâ€‘normalized mQA queryâ€“passage pairs, with pos_ids/neg_ids mapping to collection indices for direct retrieval via collection[pos_id], and licensed under the original datasetâ€™s terms.
 * [JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - ğŸ“¥ 403 / â­ 19 / A Japanese QA dataset for evaluating Retrievalâ€‘Augmented Generation (RAG), built from JAQKET questions and Wikipedia passages with gold retrievalâ€‘relevance labels, released on HuggingFace and GitHub and scored primarily by nDCG@10.
 * [Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M) - ğŸ“¥ 400 / â­ 6 / Dataset of 23,212,809 Japanese web novels (~80.8â€¯billion characters) collected personally, for machineâ€‘learning use only and requiring a detailed access request.
 * [relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation) - ğŸ“¥ 396 / â­ 3 / A tool that uses text2dataset with vLLM and the openâ€‘weight Gemmaâ€¯2.9bâ€‘it for rapid Englishâ€‘toâ€‘Japanese translation, generating a 1.5â€¯billionâ€‘pair Japanese imageâ€‘text dataset to support CLIPâ€‘based visionâ€‘language models.
 * [gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset) - ğŸ“¥ 390 / â­ 2 / A dataset of 64,139 Japanese names labeled with biological genderâ€”presented in kanji, hiragana, and romajiâ€”whose 44.9â€¯k training, 6.41â€¯k validation, and 12.8â€¯k test split earned acceptance at ISDAâ€™23.
 * [J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - ğŸ“¥ 390 / â­ 32 / A highâ€‘quality Japanese research paper corpus (~39â€¯M characters) licensed under CCâ€‘BYâ€‘* from ACL 2021â€‘2024 proceedings, the *NLP* journal, and other journals, released for LLM preâ€‘training and RAG with ongoing additions.
 * [oscor-2301-ja-text-content](https://huggingface.co/datasets/ayousanz/oscor-2301-ja-text-content) - ğŸ“¥ 390 / â­ 2 / Converts the Japanese OSCORâ€‘2301 datasetâ€™s JSON files into plainâ€‘text by extracting only the â€œcontentâ€ field from each entry.
 * [OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - ğŸ“¥ 368 / â­ 14 / 1.8â€¯million Japaneseâ€‘translated OpenMathInstructâ€‘1 instructionâ€‘tuning examples, generated from GSM8K and MATH benchmark questions with Mixtralâ€‘8x7Bâ€‘derived synthetic solutions verified against the original answers, are released for commercial use under an NVIDIA license that requires license inheritance for redistribution, though modelâ€‘learning licenses need not inherit that license.
 * [EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench) - ğŸ“¥ 359 / â­ 9 / EDINETâ€‘Bench is a Japanese financial benchmark that evaluates LLMs on tasks such as accounting fraud detection, earnings forecasting, and industry prediction using ten years of EDINETâ€‘API disclosed reports, with construction and evaluation code provided and the dataset relicensed to PDLâ€¯1.0.
 * [jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - ğŸ“¥ 356 / â­ 7 / JHumanEval is a handâ€‘translated Japanese version of the HumanEval benchmark, providing 164 Python programming problems with parallel English and Japanese comments to evaluate Japaneseâ€‘LLM code generation while preserving the original English errors.
 * [janli](https://huggingface.co/datasets/hpprc/janli) - ğŸ“¥ 354 / â­ 6 / JaNLI is a Japanese Adversarial NLI dataset modeled on HANS, comprising 13,680 training and 720 test sentence pairs annotated with entailment labels, structural heuristics (e.g., subsequence, constituent), nounâ€‘phrase counts, and semantic tags to probe Japanese linguistic phenomena and model vulnerabilities.
 * [oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - ğŸ“¥ 350 / â­ 26 / Japanese-translated OpenAssistant/oasst1 data with failure flags, ~2,000 manually corrected code translation errors, a released chatâ€‘format subset (oasst1â€‘chatâ€‘44kâ€‘ja), and a script to convert entries into instructionâ€“output pairs for fineâ€‘tuning.
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - ğŸ“¥ 336 / â­ 2 / Large compressed JSONâ€‘Lines dataset of anonymous 2ch.sc/2ch.net threads, including thread IDs, titles, board and region details, reply counts, and full post metadata (author, mail, date, content).
 * [RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA) - ğŸ“¥ 333 / â­ 33 / Allganize RAG Leaderboard publishes Japanese RAG performance data and automated endâ€‘toâ€‘end evaluation results across five industry domainsâ€”finance, telecom, manufacturing, public sector, and retailâ€”to help companies benchmark parser, retrieval and generation components where no comprehensive Japanese benchmark yet exists.
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - ğŸ“¥ 332 / â­ 8 / Umamusume voice transcriptions dataset listing 77 characters with their total audio duration (e.g., East Commerce 799â€¯s, East Imperial Emperor 1074â€¯s, â€¦).
 * [Swallow-Instruct-v0.1](https://huggingface.co/datasets/tokyotech-llm/Swallow-Instruct-v0.1) - ğŸ“¥ 326 / â­ 10 / Swallowâ€¯Instructâ€¯v0.1 is a fineâ€‘tuning dataset for Swallowâ€‘model series (e.g., Llamaâ€‘3â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.1, Swallowâ€‘13Bâ€‘Instructâ€‘v0.1) comprising 5â€¯334 English and ~42â€¯000 Japanese OpenAssistant conversations built from OpenAssistant2, with Japanese prompts added for the â€œbetaâ€ subset, and created by Tokyo Techâ€™s Okazaki/YOKOTA labs and AIST.
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - ğŸ“¥ 325 / â­ 16 / Dataset card for **japanese_alpaca_data**, built on masa3141â€™s Japaneseâ€‘Alpacaâ€‘LoRA work, with additional details in the referenced repository.
 * [ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - ğŸ“¥ 324 / â­ 76 / Versionâ€¯2 of a sentenceâ€‘aligned Japanese webâ€‘novel to English fanâ€‘translation dataset (106â€¯k chapters), updated alignment, added series metadata, no quality filtering, released under fairâ€‘use and Apacheâ€¯2.0 with a Huggingâ€¯Face takedown procedure.
 * [jsnli](https://huggingface.co/datasets/shunk031/jsnli) - ğŸ“¥ 322 / â­ 5 / JSNLI is a Japanese translation of the SNLI NLI benchmark released by the KUROHASHIâ€‘CHUâ€‘MURAWAKI LAB, offering 548â€¯k training pairs (3â€¯916 validation) in TSV format with JUMAN++â€‘morphed premises and hypotheses, plus a 533â€¯k filtered subset, all distributed under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [Galgame_Speech_SER_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_SER_16kHz) - ğŸ“¥ 321 / â­ 11 / A 104â€¯GB dataset of 3,746,131â€¯Galgame audio files (5,353â€¯h), adding LLMâ€‘generated emotion labels (possibly inaccurate) to the existing 16â€¯kHz ASR set, released under GPLâ€¯v3.0 with no commercial use allowed and requiring any trained models to be openâ€‘source.
 * [wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - ğŸ“¥ 320 / â­ 9 / A dataset card for llmâ€‘book/wrimeâ€‘sentiment offering a binary Japanese sentiment analysis set derived from WRIME, labeled as positive or negative based on Avg. Readers_Sentiment (with an option to include neutral cases), and intended as sample data for the book â€œIntroduction to Large Language Models.â€
 * [JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR) - ğŸ“¥ 320 / â­ 4 / JFWIR is a 64â€‘millionâ€‘pair Japanese IR dataset built from finewebâ€‘2â€‘edu web content, offering seven query types and hard negatives that raise benchmark scores on JQaRA, MIRACL(ja), jsquad and JaCWIR.
 * [JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - ğŸ“¥ 316 / â­ 16 / JAâ€‘VGâ€‘VQAâ€‘500 is a 500â€‘sample subset of the Japanese Visual Genome VQA dataset, licensed CCâ€¯BYâ€¯4.0, used to benchmark EvoVLMâ€‘JPâ€‘v1â€‘7B.
 * [oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - ğŸ“¥ 313 / â­ 13 / A 68kâ€‘record Japaneseâ€‘chat version of OpenAssistant/oasst2, DeepLâ€‘translated, is released together with conversion code that turns it into an Instructionâ€‘Output format usable for fineâ€‘tuning.
 * [bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja) - ğŸ“¥ 308 / â­ 2 / BBHâ€‘ja provides a Japanese translation of the BIGâ€‘Bench Hard dataset, offering evaluation problems in JSONâ€‘L (input, correct target) and Chainâ€‘ofâ€‘Thought prompts in YAML (input, target), translated using the PLaMo model.
 * [callhome-ja-plus](https://huggingface.co/datasets/ayousanz/callhome-ja-plus) - ğŸ“¥ 291 / â­ 2 / Japanese Callhome speech files converted to WAV, accompanied by JSONâ€‘formatted metadata arrays and RTMM speaker label files for evaluation.
 * [swallow-gemma-magpie-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-gemma-magpie-v0.1) - ğŸ“¥ 287 / â­ 3 / Swallowâ€‘Gemmaâ€‘Magpieâ€‘v0.1 is a 148â€¯kâ€‘sample synthetic Japanese Q&A dataset generated with Googleâ€¯Gemmaâ€‘2â€‘27bâ€‘IT, designed for instructionâ€‘tuning TokyoTechâ€™s LLaMAâ€‘3.1â€‘Swallowâ€¯70B/8B models across diverse subjects.
 * [japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - ğŸ“¥ 286 / â­ 12 / A variant of the kunishou/hhâ€‘rlhfâ€‘49kâ€‘ja dataset that omits entries where ng_translation equalsâ€¯1.
 * [swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1) - ğŸ“¥ 285 / â­ 5 / 42â€¯k Japaneseâ€“English instructionâ€‘tuning pairs from Swallowâ€‘Magpieâ€‘Ultraâ€‘v0.1, labeled â€œaverageâ€‘goodâ€ or â€œexcellent,â€ are released for training Llamaâ€‘3.1â€‘Swallow models.
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - ğŸ“¥ 282 / â­ 28 / Syosetu711K is a Japanese dataset of ~711,700 novels scraped from å°èª¬å®¶ã«ãªã‚ã† on Marchâ€¯26â€‘27â€¯2023, providing full text and metadata (title, author, NCode, synopsis, etc.) for unsupervised text generation and classification tasks.
 * [AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset) - ğŸ“¥ 282 / â­ 4 / Compiled from many openâ€‘source corpora totaling 1.56â€¯B tokens, this dataset preâ€‘trains the AKUâ€‘d_msâ€‘0.5Bâ€‘chatâ€‘v0.1 model, includes processing scripts, and will make raw data public later.
 * [anim400k](https://huggingface.co/datasets/davidchan/anim400k) - ğŸ“¥ 269 / â­ 39 / Iâ€™m sorry, but I canâ€™t view or access the linked Google Docs, so Iâ€™m unable to read the repository description to create a summary.
 * [jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points) - ğŸ“¥ 265 / â­ 4 / Dataset of Japanese Wikipedia bullet points generated by the rinna/deepseekâ€‘r1â€‘distillâ€‘qwen2.5â€‘bakenekoâ€‘32b model, sampled randomly (allowing duplicates), with lineâ€‘break formatting not fully shown in the Hugging Face viewer, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - ğŸ“¥ 264 / â­ 4 / Range3â€™sâ€¯wikipediaâ€‘jaâ€‘20230101 repository offers Parquet files containing only Japanese Wikipedia text, extracted from the full Wikipedia dataset and generated with Python code.
 * [guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja) - ğŸ“¥ 256 / â­ 5 / Japanese subset of the Guanaco dataset, with references to similar datasets such asâ€¯inuâ€‘ai/alpacaâ€‘guanacoâ€‘japaneseâ€‘gptâ€‘1b.
 * [Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - ğŸ“¥ 251 / â­ 11 / Japaneseâ€‘Heronâ€‘Bench evaluates Japanese VLMs with 21 publicâ€‘domain or CCâ€‘BY images across 7 subâ€‘categories, each paired with 1â€“2 questions in Conversation, Detail and Complex categories, totaling 102 questions.
 * [nekopara-speech](https://huggingface.co/datasets/grider-transwithai/nekopara-speech) - ğŸ“¥ 251 / â­ 15 / Nekopara Audio Dataset supplies 44.1â€¯kHz clips labeled with speaker name, volume, transcription, and an adultâ€‘content flag, but warnings indicate that file names and the adult flag should not be trusted alone for classification.
 * [pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset) - ğŸ“¥ 247 / â­ 10 / The sovitsâ€‘emuâ€‘dataset offers 2,735 SEGAâ€‘licensed Emu Otori WAV files for soâ€‘vitsâ€‘svcâ€¯4.0 research, released under CCâ€‘BYâ€‘NCâ€¯4.0 (excluding the voice owners), with mandatory attribution, nonâ€‘commercial use, emailâ€‘only access, and optional pullâ€‘request contributions.
 * [wrime](https://huggingface.co/datasets/shunk031/wrime) - ğŸ“¥ 241 / â­ 27 / The WRIME dataset is a Japanese collection of 42,200 posts annotated with Plutchikâ€™s eight emotions for the writer, three readers, and their averages, structured into 40â€¯kâ€‘train, 1.2â€¯kâ€‘validation, and 2â€¯kâ€‘test splits for sentimentâ€‘analysis tasks.
 * [Japanese-RAG-Generator-Benchmark](https://huggingface.co/datasets/neoai-inc/Japanese-RAG-Generator-Benchmark) - ğŸ“¥ 236 / â­ 4 / Japanese RAG Generator Benchmark (Jâ€‘RAGBench) supplies a multiâ€‘category QA datasetâ€”covering Integration, Reasoning, Logical, Table, and Abstentionâ€”designed to evaluate Japanese RAG generators, built with human effort and GPTâ€‘4.1, and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - ğŸ“¥ 229 / â­ 21 / Manually curated highâ€‘quality 100â€‘sample Japanese Chainâ€‘ofâ€‘Thought dataset, available as two JSONs: one linking CoT to output and one keeping them separate.
 * [cv-corpus-17.0-ja-client_id-grouped](https://huggingface.co/datasets/masuidrive/cv-corpus-17.0-ja-client_id-grouped) - ğŸ“¥ 222 / â­ 2 / Common Voice subset with 649 speaker groups (client IDs) each 30â€“300 samples, 45,668 recordings split 8:2 train/validation, batched into 1,000â€‘sample Parquet files, CC0 licensed.
 * [alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python) - ğŸ“¥ 215 / â­ 8 / alpaca_jp_python is a Japanese synthetic Alpaca dataset created and cleaned with mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, hosted on Deepinfra, and distributed via the datasets library with cleanâ€‘labeled â€œ_cleanedâ€ splits and promptâ€‘based curation.
 * [livedoor-news-corpus](https://huggingface.co/datasets/shunk031/livedoor-news-corpus) - ğŸ“¥ 214 / â­ 6 / Japanese news articles from livedoor News under a CC BYâ€‘ND license are cleaned of HTML, delivered in 6,567 items split 80/10/10 for training, validation, and testing.
 * [ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS) - ğŸ“¥ 212 / â­ 3 / ScreenTalk_JA2ZHâ€‘XS is a 10,000â€‘sample, ~30â€‘hour paired Japanese audio / Simplified Chinese text dataset (Parquet, CCâ€¯BYâ€¯4.0) used for speechâ€‘toâ€‘text translation, multilingual ASR, and multimodal AI research.
 * [liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds) - ğŸ“¥ 210 / â­ 3 / MITâ€‘licensed, handâ€‘written dataset for training ebisuke/liz-nojaloli-ja, with Python code that may reference Qiita, and intended for RLHF data preparation.
 * [alpaca_jp_math](https://huggingface.co/datasets/HachiML/alpaca_jp_math) - ğŸ“¥ 210 / â­ 6 / alpaca_jp_math is a Japanese synthetic math dataset crafted with Stanford Alpaca and mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, cleaned and validated for consistency between code and text outputs, and distributed under the Apacheâ€¯2.0 license.
 * [msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives) - ğŸ“¥ 208 / â­ 3 / A hardâ€‘negative mining pipeline applied to the Japanese MSâ€¯MARCO translationâ€”including normalization, highâ€‘cosineâ€‘similarity filtering, BAAI/BGE rerankerâ€‘based selection, and random samplingâ€”was created and shown by chiâ€‘square tests to have a statistically higher positive rate than the SPLADEâ€‘trained mMARCO baseline.
 * [JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - ğŸ“¥ 207 / â­ 5 / JetCopperâ€‘10B is a 4.7â€¯Bâ€‘token Japanese dataset (plus 0.9â€¯B English code) compiled from CCâ€‘100, OSCARâ€‘2301, HPLTâ€¯v1.2, and wiki40bâ€‘ja, used to preâ€‘train Contrailâ€‘200mâ€‘64k for the LOCAL AI HACKATHON #000 calm2â€‘chat, but it has not yet undergone sentenceâ€‘boundary or perplexity filtering.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - ğŸ“¥ 205 / â­ 18 / Japanese translation of the Databricks Dollyâ€‘15k instructionâ€‘tuning dataset, produced with DeepL by the LLMâ€‘jp collaborative project in Japan.
 * [lima-ja](https://huggingface.co/datasets/zan/lima-ja) - ğŸ“¥ 202 / â­ 3 / LIMAâ€‘JA is a Japaneseâ€‘translated, ChatGPTâ€‘edited version of Metaâ€™s LIMA dataset (â‰ˆ100 changes) for language models, accessible withâ€¯load_dataset('zan/lima-ja',â€¯'v1') and licensed under CCâ€¯BYâ€‘NCâ€‘SA unless the original LIMA source requires a stricter license.
 * [wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - ğŸ“¥ 201 / â­ 9 / Japaneseâ€‘only Wiki40B subset, packaged as three Parquet files and generated via Python/Beam code.
 * [HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja) - ğŸ“¥ 199 / â­ 3 / Japanese autoâ€‘translated HelpSteer dataset for NVIDIAâ€¯SteerLM alignment trials, with reference URLs for LLM training.
 * [ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2) - ğŸ“¥ 198 / â­ 5 / Provides a Windows executable for running ggmlâ€‘japaneseâ€‘gpt2 that needs matchingâ€¯*.bin and SentencePiece models, with an example command and a note that the xsmall model format is currently broken.
 * [chat-daily](https://huggingface.co/datasets/minnade/chat-daily) - ğŸ“¥ 196 / â­ 9 / MinnadeChat is a collaboratively constructed instructionâ€‘style dataset updated daily at noon, available via HuggingFaceâ€¯datasets with dateâ€‘specific revisions and released under CCâ€¯0â€¯1.0 Universal.
 * [JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - ğŸ“¥ 193 / â­ 29 / A CCâ€‘BYâ€‘4.0 licensed, manually compiled dataset of Japanese government FAQ questionâ€“answer pairs, complete with source URLs, designed for instructionâ€‘tuning and RAG of largeâ€‘languageâ€‘model systems.
 * [shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1) - ğŸ“¥ 192 / â­ 7 / A preâ€‘training dataset for shisaâ€‘baseâ€‘7bâ€‘v1, built from DSIRâ€‘sampled MADLADâ€‘400 tokens in a 90â€¯% Japanese / 10â€¯% English ratio.
 * [covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese) - ğŸ“¥ 188 / â­ 2 / Japanese Twitter dataset of COVIDâ€‘19 tweets, stored as a CSV of tweet IDs and assessmentâ€‘option IDs (63â€‘68), for textâ€‘classification tasks that distinguish COVID relevance, factual versus personal content, opinions, uncertain, and unrelated posts.
 * [wikipedia-qa-ja-100k](https://huggingface.co/datasets/alfredplpl/wikipedia-qa-ja-100k) - ğŸ“¥ 186 / â­ 3 / Dataset card for the Japanese QA set â€œwikipediaâ€‘qaâ€‘jaâ€‘100kâ€, sourced from hpprc/wikipediaâ€‘20240101, with RAGâ€‘style prompt guidelines for use with CALMâ€¯2â€‘7Bâ€¯Chat.
 * [AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics) - ğŸ“¥ 186 / â­ 4 / An Apacheâ€‘2.0 licensed Anime Songs Lyrics Dataset in Parquet format containing ~23,000 entries, each with title, artist, anime, release date, views, lyrics, URLs, creator credits, and arrangement details, fully documented and the source code hosted on the authorâ€™s GitHub.
 * [MGSM_ja](https://huggingface.co/datasets/sbintuitions/MGSM_ja) - ğŸ“¥ 180 / â­ 2 / Offers a reproducible clone of SB Intuitions and a Japaneseâ€‘only subset of the MGSM multilingual chainâ€‘ofâ€‘thought reasoning dataset, licensed under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset) - ğŸ“¥ 178 / â­ 12 / A 190â€‘millionâ€‘pair JSONL dataset for kanaâ€‘toâ€‘kanji conversion, released under CCâ€¯BYâ€‘SAâ€¯4.0, is used to train the zenzâ€‘v2.5 series (medium, small, xsmall) and supplies the AJIMEEâ€‘Bench evaluation benchmark.
 * [JDocQA](https://huggingface.co/datasets/shunk031/JDocQA) - ğŸ“¥ 173 / â­ 9 / JDocQA is a Japanese PDFâ€‘based QA dataset comprising 5,504 documents and 11,600 questionâ€‘answer pairs that test yes/no, factoid, numerical, openâ€‘ended, and unanswerable comprehension using both visual and textual information.
 * [humaneval-ja-v0.6](https://huggingface.co/datasets/HachiML/humaneval-ja-v0.6) - ğŸ“¥ 171 / â­ 3 / Dataset card for the â€œhumaneval-jaâ€ dataset, with further details pending.
 * [CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned) - ğŸ“¥ 169 / â­ 15 / CCâ€‘newsâ€‘2024â€‘Julyâ€‘Octoberâ€‘cleaned contains Japanese news articles from the Common Crawl news subset (Julyâ€“Octoberâ€¯2024), processed and cleaned with Uzushio using the pipeline_03a.conf configuration.
 * [extraction-wiki-ja](https://huggingface.co/datasets/llm-jp/extraction-wiki-ja) - ğŸ“¥ 169 / â­ 2 / An instructionâ€‘tuning dataset from LLMâ€‘jpâ€”built on a Japanese Wikipedia subset (llmâ€‘jpâ€‘corpusâ€‘v3), filtered with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, and offering twoâ€‘turn and fourâ€‘turn dialogue formatsâ€”created by Hirokazuâ€¯Kiyomaru and Takashiâ€¯Kodama.
 * [nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - ğŸ“¥ 163 / â­ 2 / A MITâ€‘licensed, 100â€‘word Japanese stopâ€‘word list derived from the CCâ€‘100 Wikipedia dump, curated to match nagisaâ€™s tokenization rules for use in text preprocessing, feature extraction, and modeling.
 * [simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon) - ğŸ“¥ 154 / â­ 14 / A simple dataset of Zundamon character settingsâ€”compiled from online sources and admin dataâ€”for testing characterâ€‘LLMs, provided in zmnjp.jsonl and zmn.jsonl formats under a specified license.
 * [WAON](https://huggingface.co/datasets/llm-jp/WAON) - ğŸ“¥ 154 / â­ 7 / WAON is a large, highâ€‘quality Japanese imageâ€‘text pair dataset for visionâ€‘language models, built through rigorous filtering of image size and SigLIP scores and deduplication via URLs, captions and pHash, providing rich metadata (captions, page URLs, safety scores, image hashes) under an Apacheâ€¯2.0 license for informational analysis.
 * [snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus) - ğŸ“¥ 152 / â­ 21 / A 50â€¯kâ€‘sentence Japanese corpus with aligned original, simplified Japanese (core 2â€¯kâ€‘word vocabulary) and English translations, plus a 35â€¯kâ€‘sentence expansion set, designed for textâ€‘simplification and bidirectional Japaneseâ€“English translation tasks.
 * [r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa) - ğŸ“¥ 150 / â­ 5 / Japanese Wikipediaâ€“derived questions and their corresponding pages were automatically generated, answered with cyberagent/DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘32Bâ€‘Japanese, and released under CCâ€‘BYâ€‘SAÂ 4.0.
 * [TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese) - ğŸ“¥ 148 / â­ 4 / A ~3000â€‘story Japanese childâ€‘reading dataset, synthetically generated by GPTâ€‘4oâ€‘mini using only simple words, created following the method in https://arxiv.org/abs/2305.07759.
 * [ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai) - ğŸ“¥ 147 / â­ 2 / Dataset of every NHK â€œkeitaiâ€¯oogiriâ€ prompt and answer scraped from a blog archive, with metadata on episode, score, awards, respondent prefecture and rank, noting possible parse errors and usage restrictions.
 * [SocialStigmaQA-JA](https://huggingface.co/datasets/ibm-research/SocialStigmaQA-JA) - ğŸ“¥ 141 / â­ 4 / Japanese release of SocialStigmaQA featuring 93 stigmas and 37 translated question templates, each with a biased_answer field and four prompt styles (original, positive, doubt, etc.) to probe social bias in large language models.
 * [amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct) - ğŸ“¥ 139 / â­ 17 / Adds 180 new professional Java and JaxTon records to a 5.2â€‘Kâ€‘instruction dataset, providing 1,050 codeâ€‘generation, 150 behaviorâ€‘check, and 4,000 bugâ€‘fix examples sourced from commercially licensed programming materials that were translated and manually corrected.
 * [JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA) - ğŸ“¥ 136 / â­ 4 / JICâ€‘VQA is a Japanese visionâ€‘language benchmark that adds multipleâ€‘choice questions to a Japanese imageâ€‘classification dataset, featuring 101 foods, 30 flowers, 20 facilities, and 10 landmarks under CCâ€‘BYâ€‘2.0, CCâ€‘BYâ€‘NCâ€‘2.0, or publicâ€‘domain licenses.
 * [ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset) - ğŸ“¥ 135 / â­ 2 / Japanese namedâ€‘entity recognition dataset (Versionâ€¯2.0) created by Stockmark, used in the book *Intro to Large Language Models*, derived from stockmarkteam/nerâ€‘wikipediaâ€‘dataset, and licensed CCâ€‘BYâ€‘SAâ€¯3.0 like Japanese Wikipedia.
 * [aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - ğŸ“¥ 134 / â­ 4 / Fork of the Hugging Face dataset **globis-university/aozorabunko-clean**, filtered to include only rows where `meta["æ–‡å­—é£ã„ç¨®åˆ¥"]` equals `"æ–°å­—æ–°ä»®å"`.
 * [llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions) - ğŸ“¥ 133 / â­ 5 / llmâ€‘jpâ€‘instructions is a manually curated Japanese instruction dataset offering train, dev, and test splits for languageâ€‘model fineâ€‘tuning.
 * [aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats) - ğŸ“¥ 132 / â­ 12 / A dataset of conversational excerpts extracted heuristically from AozoraÂ Bunko publicâ€‘domain Japanese books, grouped into utterance sequences and released under CCâ€‘BYâ€‘4.0.
 * [llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken) - ğŸ“¥ 128 / â­ 6 / Converted the kaken subset of llmâ€‘jpâ€‘corpusâ€‘v3 to Hugging Face format, enriching each entry with retrieved article titles where possible and licensing the data under CCâ€‘BYâ€‘4.0.
 * [japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset) - ğŸ“¥ 125 / â­ 7 / A Japanese imageâ€‘classification dataset released by Recruit Co., Ltd under CCâ€‘BYâ€‘4.0, containing four tasksâ€”101 foods, 30 flowers, 20 facilities, and 10 landmarksâ€”to evaluate Japaneseâ€‘CLIP models.
 * [orca_dpo_pairs_ja](https://huggingface.co/datasets/yongtae-jp/orca_dpo_pairs_ja) - ğŸ“¥ 125 / â­ 7 / Japanese translations of the Intel/orca_dpo_pairs dataset generated with Palmâ€¯2 (textâ€‘bisonâ€‘32k@002) for Japanese LLM developers, preserving original nonâ€‘English text while ensuring natural, conversational Japanese.
 * [Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0) - ğŸ“¥ 123 / â­ 20 / A JSONâ€‘formatted Japanese instructionâ€‘fineâ€‘tuning corpus compiled by merging 16 distinct datasetsâ€”spanning dialogue, reasoning, and RLHF tasksâ€”plus GitHubâ€‘published tools for processing the data.
 * [modern_haiku](https://huggingface.co/datasets/p1atdev/modern_haiku) - ğŸ“¥ 121 / â­ 3 / The Modern Haiku Dataset is a curated collection of 37,158 Japanese modern haiku, each annotated with ID, text, author, source, reviewer comments, season, and kigo (seasonal word) data, and organized into seasonal subsets and a separate kigo set.
 * [Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230) - ğŸ“¥ 115 / â­ 8 / Malumâ€‘230 is a humanâ€‘crafted Japanese dataset of multiâ€‘turn conversations and passages for logical reasoning, intended for both preâ€‘training and postâ€‘training, and has been evaluated on Japanese MTâ€‘Bench with Qwen2.5â€‘7B.
 * [CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA) - ğŸ“¥ 113 / â­ 6 / CAMERA is a Japanese adâ€‘text generation dataset from CyberAgent, offering 12,395 training, 3,098 validation, and 872 test examples to support advanced multimodal adâ€generation research.
 * [JA-Multi-Image-VQA](https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA) - ğŸ“¥ 113 / â­ 10 / JAâ€‘Multiâ€‘Imageâ€‘VQA offers 39 images with 55 manually crafted Japanese Q&A pairs for multiâ€‘image VQA evaluation, available viaâ€¯load_dataset, licensed Apacheâ€¯2.0 (excluding images) and restricted from commercial sale or service replication.
 * [ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun) - ğŸ“¥ 113 / â­ 10 / ChouBun is a Japanese longâ€‘context benchmark for LLMs that includes extractive QA (wiki_qa, edinet_qa) and abstractive summarization (corp_sec_qa, corp_sec_sum) tasks, formatted identically to THUDM/LongBench.
 * [bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated) - ğŸ“¥ 112 / â­ 3 / Japaneseâ€‘translated subset of the Bluemoonâ€¯Fandom roleâ€‘play dataset, built with OpenRouterâ€™s commandâ€‘Râ€‘08â€‘2024 for fast, uncensored translation, containing 467 conversations and 8,372 messages.
 * [LiquidAI-Hackathon-Tokyo-SFT-Data](https://huggingface.co/datasets/Aratako/LiquidAI-Hackathon-Tokyo-SFT-Data) - ğŸ“¥ 108 / â­ 2 / Dataset used for supervised fineâ€‘tuning of models built during the Liquid AI Hackathon Tokyo.
 * [wikipedia-20240101](https://huggingface.co/datasets/hpprc/wikipedia-20240101) - ğŸ“¥ 105 / â­ 4 / Preâ€‘processed Wikipedia dataset created with Apacheâ€¯Beam and mwparserfromhell, including metadata (language, date, beam_runner, trust_remote_code, max_shard_size) and distributed to replace the slow original preprocessing while respecting Wikipediaâ€™s terms and licenses.
 * [ja-rag-cot](https://huggingface.co/datasets/jaeyong2/ja-rag-cot) - ğŸ“¥ 105 / â­ 2 / A 209,496â€‘item Japanese Wikipedia dataset generated with Qwen/Qwen2â€‘72Bâ€‘Instruct using chainâ€‘ofâ€‘thought, licensed under Qwen, CCâ€‘BYâ€‘SAâ€‘3.0, and GFDL, and supported by TPU Research Cloud.
 * [gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim) - ğŸ“¥ 105 / â­ 2 / A slim Japanese translation of GSM8K, built from openai/gsm8k and nejumi/phiâ€‘4â€‘GPTQâ€‘Int4â€‘calibâ€‘jaâ€‘1k, contains some invalid data and a surnameâ€‘based word puzzle.
 * [WildGuardTestJP](https://huggingface.co/datasets/sbintuitions/WildGuardTestJP) - ğŸ“¥ 105 / â­ 3 / Japanese translation of WildGuardTest with 1,725 samples for evaluating guardrail models, produced via Seedâ€‘Xâ€‘PPOâ€‘7B and refined by GPTâ€‘OSSâ€‘120B, Qwen2.5â€‘72Bâ€‘Instruct, and Gemmaâ€‘3â€‘27Bâ€‘it, and released under ODCâ€‘BY.
 * [sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - ğŸ“¥ 104 / â­ 19 / Sakura_dataset is a freeâ€‘commercial ultraâ€‘small, highâ€‘quality Japanese dataset that aggregates commonsense QA, a 210â€¯kâ€‘item math collection (Calcâ€‘ape210k) and a selfâ€‘made Japanese commonsense set, all licensed under the DbCLâ€¯v1.0 and accompanied by example LoRA finetuning code for Rinnaâ€™s Japaneseâ€¯GPTâ€‘NeoXâ€‘3.6â€¯B model.
 * [abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice) - ğŸ“¥ 104 / â­ 4 / The abcâ€‘multipleâ€‘choice datasetâ€”created from fourâ€‘choice questions in the abc competitionâ€”provides a multiâ€‘choice Japanese QA resource, includes evaluation scripts in this repo, and is licensed for research use only, prohibiting commercial exploitation.
 * [JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR) - ğŸ“¥ 102 / â­ 6 / JaCWIR is a 5,000â€‘query Japanese casual webâ€‘search evaluation dataset, built from ~500â€¯k Hatenaâ€‘Bookmark titles and descriptions with ChatGPTâ€¯3.5â€‘generated queries and one positive plus 99 hard negatives per query, released on HuggingFace and GitHub for IR and rerank research.
