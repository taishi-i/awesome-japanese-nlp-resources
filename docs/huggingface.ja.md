# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
    [![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
    [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
    [![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
    [![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

日本語向けのNLPに関する、Pythonライブラリ、LLM、辞書、コーパスに特化したリソースを厳選してまとめた一覧です。
このページでは、Hugging Faceで利用可能な日本語NLP特化のモデルとデータセットを掲載しています。現在、207件のモデルと43件のデータセットが含まれています。

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [日本語 (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [繁體中文 (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [简体中文 (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [sentence-similarity](#sentence-similarity)
   * [feature-extraction](#feature-extraction)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [translation](#translation)
   * [text-classification](#text-classification)
   * [text-ranking](#text-ranking)
   * [image-to-text](#image-to-text)
   * [token-classification](#token-classification)
   * [text-to-speech](#text-to-speech)
   * [question-answering](#question-answering)
   * [image-text-to-text](#image-text-to-text)
   * [audio-to-audio](#audio-to-audio)
   * [Others](#Others)
 * [Datasets](#Datasets)

## Models
### text-generation
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - 📥 217k / ⭐ 15 / 軽量な12層、768ユニットの日本語 GPT‑NeoX モデルで、CC‑100、C4、Wikipedia 上でトレーニングされ、Hugging Face 経由でデプロイ可能です。また、文末を笑顔の絵文字で終わらせる toy prefix‑tuning weight（プレフィックス・チューニング重み）を組み合わせています。
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - 📥 59k / ⭐ 8 / LLM‑jp‑3.1‑1.8b は、情報学研究所がリリースした 1.8 b の日本語言語モデルであり、Hugging Face Transformers で事前学習され、ユニグラムトークナイザーを使用し、Hugging Face‑compatible checkpoint として配布されています。
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - 📥 48k / ⭐ 137 / Llama‑3‑ELYZA‑JP‑8BはMeta Llama 3 8B Instructに基づく日本語対応大型言語モデルで、ELYZA株式会社によって追加の事前学習と指示チューニングを施して訓練され、Meta Llama 3 Community Licenseの下でリリースされています。
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - 📥 40k / ⭐ 35 / TinySwallow‑1.5B は1.5 Bパラメータを持つ日本語コンパクト自己回帰言語モデルで、Qwen2.5‑32B‑Instruct から TAID によって蒸留され、日本語テキストで事前学習されました。研究目的で Apache 2.0 の下でリリースされ、商業的支援はありません。
 * [Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3) - 📥 21k / ⭐ 14 / 日本語強化版 Llama 3.1 モデル（8B と 70B）は、Meta の Llama 3.1 に対する継続的プレトレーニングで構築され、70B‑Instruct v0.3 などの指示チューニング済みバリアントを備え、詳細で会話に応じた応答を提供し、MT‑Bench のスコアも向上、2024年12月までにリリースされます。
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - 📥 17k / ⭐ 14 / Llama 3.1 Swallowは、MetaのLlama 3.1の日本語強化版8Bおよび70Bを提供し、合成日本語データを用いた継続的な事前学習と指示ファインチューニングで構築され、最近のリリースでは会話能力が向上し、Gemma‑3‑27b‑itを模倣するようになった。
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - 📥 17k / ⭐ 15 / National Institute of Informatics から公開された Large‑language models (1.8–13 B パラメータ、transformer‑based)、Hugging Face checkpoints (instructional と vanilla バリアント) としてリリースされ、日本語および多言語コーパスで事前学習され、標準の torch‑transformers と Japanese tokenizer を使って利用可能です。
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - 📥 16k / ⭐ 12 / LLM‑jp‑3.1‑1.8b‑instruct4 は、NII の LLM‑jp‑3.1 シリーズからの 18億パラメータの日本語命令事前学習済み言語モデルで、Hugging Face Transformers 形式で提供され、指定された依存関係と詳細なアーキテクチャおよびトークナイザー情報が含まれています。
 * [Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2) - 📥 13k / ⭐ 15 / Llama 3.1 Swallowは、MetaのLlama 3.1の日本語強化版である8 Bおよび70 Bを提供し、継続的なプレトレーニングと合成日本語データによるインストラクション調整で訓練されています。複数のリリースバリアント（v0.1–v0.3）があるほか、Swallowチームのウェブサイトに詳細情報が掲載されています。
 * [llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3) - 📥 11k / ⭐ 4 / NIIのLLM‑jp‑3シリーズからの7.2 Bパラメータを持つ日本語LLM‑jp‑3‑7.2b‑instruct3は、Hugging Face Transformers（torch ≥ 2.3.0、transformers ≥ 4.40.1）のパッケージとして提供され、日本語WikipediaとCommon Crawlで事前学習されています。
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - 📥 11k / ⭐ 82 / 24層、1024ユニットの隠れサイズを持つ日本語GPT‑2（Medium）モデルで、CC‑100とWikipediaでトレーニングされ、SentencePieceでトークナイズ済み。2021年4月7日リリース（2021年8月25日更新）rinnaによってMITライセンスの下で公開（Hugging FaceおよびarXivで確認可）。
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - 📥 11k / ⭐ 25 / 12層、768ユニットの隠れ層を持つ transformer ベースの日本語 GPT‑2 Small モデルで、CC‑100 と Wikipedia で学習され、SentencePiece でトークナイズされ、2021年8月25日に MIT ライセンスでリリースされた。
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - 📥 9k / ⭐ 4 / 実験的日本語 Llama‑3 8B モデルは、Meta‑Llama‑3 8B‑Instruct からチャット‑ベクターアプローチで差分を抽出・アップサンプリングし、それを Meta‑Llama‑3 70B‑Instruct に適用することで作成され、Hugging Face model card が付属しています。
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - 📥 8k / ⭐ 4 / ELYZAからの日本語強化8B Llama‑3、Meta Llama 3をファインチューニングしたもので、vLLMとOpenAI‑style inference用にGGUFとAWQ量子化で利用可能です。
 * [llm-jp-3-13b](https://huggingface.co/llm-jp/llm-jp-3-13b) - 📥 7k / ⭐ 13 / NIIのLarge Language Model R&D Centerが開発した日本語Transformerモデル（llm‑jp‑3‑1.8b、3‑3.7b、3‑13b 及びその instruct バリエーション）は、torch ≥ 2.3、transformers ≥ 4.40、accelerate ≥ 0.29 を必要とする Hugging Face チェックポイントとして公開され、約1.3 兆トークン規模の日本語‑英語混合コーパスで訓練されています。
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - 📥 6k / ⭐ 74 / ELYZA‑japanese‑Llama‑2‑7b は、Meta の Llama‑2 7 B を日本語向けに拡張した 6.27 B パラメータのバリエーションです。追加の日本語コーパスで事前学習（pre‑trained）され、Llama‑2 Community License の下で instruct と fast バージョンがリリースされています。
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - 📥 5k / ⭐ 32 / SB Intuitions の「Japanese‑language, autoregressive LLM “Sarashina2.2‑3B‑instruct‑v0.1”」は、日本語と英語の MT ベンチマークで評価され、同系列の小規模兄弟モデルを上回り、Qwen2.5‑3B‑instruct や Gemma‑2‑2b‑jpn‑it などの大規模モデルと肩を並べるが、セーフティチューニングは限定的である。
 * [Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4) - 📥 5k / ⭐ 12 / Llama 3.3 Swallow は、Meta の Llama 3.3 を拡張した 70B LLM で、改良された日本語サポートとインストラクションチューニング済みバリアントを備えています。12月 2024 から 3月 2025 までに 8B と 70B のバージョンがリリースされ、HuggingFace で入手可能です。
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - 📥 5k / ⭐ 58 / ABEJAが日本語CC‑100、Wikipedia、OSCARデータで訓練した2.7 BパラメータのGPT‑NeoXモデルで、テキスト生成にHugging Face Transformersでアクセスでき、MITライセンスの下でリリースされています。
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - 📥 5k / ⭐ 56 / TinySwallow-1.5B-Instructは、日本語指示調整された1.5Bパラメータの自己回帰型言語モデルで、Qwen2.5-32B-InstructからTAIDで蒸留され、研究目的でのみ使用されることを意図しています。
 * [open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - 📥 4k / ⭐ 17 / OpenCALM は CyberAgent が開発した日本語向けデコーダーのみの Transformer スイートで、パラメータ数は 160 M から 6.8 B まであり、Wikipedia と Common Crawl で事前学習され、CC BY‑SA 4.0 ライセンスの下で公開され、Hugging Face Transformers を通じて利用可能です。
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - 📥 4k / ⭐ 22 / Llama 3.1 Swallowは、8Bと70Bのインストラクション調整済みLlama 3.1モデルのセットで、継続的にプリトレーニングとSFTで日本語データを微調整し、日本語MT‑Benchで最先端の性能を発揮します。
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - 📥 4k / ⭐ 43 / TokyoTech‑LLMのSwallow Llama‑2ファミリーは、日本語に特化した7B、13B、70Bのモデル（インストラクション・チューニング版およびNVEバリアントを含む）を提供し、2023年12月から2024年4月にかけて詳細なバージョンノートとダウンロード可能な重みのインデックス付きでリリースしています。
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - 📥 3k / ⭐ 15 / LLM‑jp‑3.1‑13b‑instruct4 は NII の研究開発センターから提供される日本語の指示調整済み言語モデルで、130億パラメータを持ち、Hugging Face Transformers 形式で配布されています。指定された依存関係と un‑gram‑byte‑fallback トークナイザーが備わっています。
 * [Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1) - 📥 3k / ⭐ 1 / SwallowシリーズはGemma-2を強化し、英語性能を維持しつつ日本語性能を向上させました。2B、9B、27Bの事前学習済みかつ指示調整済みモデルを提供し、これらはHuggingFaceにてリリースされています。モデルは合成日本語データで訓練され、JCom、JSQuAD、JMMLU などのベンチマークで評価されています。
 * [ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0) - 📥 3k / ⭐ 5 / ABEJA‑Qwen2.5‑32b‑Japanese‑v1.0 は、ABEJA の開発者による継続的なプレトレーニングと追加の SFT と DPO ファインチューニングを経て、Qwen2.5‑32B‑Instruct から派生した日本語中心の言語モデルです。
 * [llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m) - 📥 3k / ⭐ 1 / LLM‑jp‑3‑150m は NII の LLM R&D Center が Hugging‑Face 形式でリリースした 150 M パラメータの日本語トランスフォーマーで、torch ≥ 2.3.0 と transformers ≥ 4.40.1 が必要です。日本語 Wikipedia、Common Crawl、WARP、Kaken のデータセットでトレーニングされています。
 * [Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1) - 📥 2k / ⭐ 2 / Gemma‑2 Swallow は、2B、9B、27B の言語モデルからなるスイートで、英語の機能を保持したまま日本語データに対して pretrained および instruction‑tuned されています。2025年5月19日にリリースされ、Hugging Face で入手可能です。
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - 📥 2k / ⭐ 19 / OpenCALM は CyberAgent が開発した、160 M 〜 6.8 B パラメータの transformer‑based decoder‑only 日本語モデル群で、日本語ウィキペディアと Common Crawl を訓練データとして使用し、CC‑BY‑SA 4.0 の下で公開され、GPT‑NeoX ライブラリ経由でアクセスできます。
 * [Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2) - 📥 2k / ⭐ 4 / Llama 3.1 Swallow は、Meta の Llama 3.1 をベースに、継続的なプレトレーニングと合成日本語データによる命令チューニングを経て作成された、8 B および 70 B の日本語向け拡張版を提供します。これらは、Swallow チームによって日本語と英語の両方で利用可能なように、v0.1 から v0.3 までの連続版としてリリースされました。
 * [llm-jp-3.1-13b](https://huggingface.co/llm-jp/llm-jp-3.1-13b) - 📥 2k / ⭐ 2 / 13 BパラメータのLLM‑jp‑3.1 日本語言語モデルを NII から提供し、事前学習済みおよびファインチューニング済みのチェックポイント、Unigram‑byte‑fallback トークナイザー、Hugging Face/torch の依存関係仕様を含みます。
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - 📥 2k / ⭐ 10 / GGUFフォーマットの、Massed Compute-quantised モデルファイルは、Stability AI の日本語 StableLM Instruct Gamma 7B 向けに作成されています。a16z の資金提供によりサポートされ、TheBloke の Discord と Patreon チャンネルで配布されています。
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - 📥 2k / ⭐ 21 / rinna/youri‑7b は 32 層、4096 隠れユニットサイズのトランスフォーマーで、EleutherAI/gpt‑neox のコードを使用して約 40 B の日本語トークン上で LLaMA‑2‑7B を継続的に事前学習し、LLaMA‑2 ライセンスの下でリリースされた。
 * [ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1) - 📥 2k / ⭐ 10 / ABEJA‑Qwen2.5‑7b‑Japanese‑v0.1 は、 ABEJA‑Qwen2.5‑32b‑Japanese‑v0.1 から蒸留された日本語で訓練された Qwen2.5‑7B‑Instruct モデルであり、ChatVector によってインストラクションフォローが改善され、ポストトレーニングは行われていません。
 * [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - 📥 2k / ⭐ 17 / Swallow Llama 2 は、日本語と英語向けに教師付き学習でファインチューニングされた 7B、13B、70B LLaMA‑2 ベースのモデルを包含する TokyoTech‑LLM コレクションで、NVE および「plus」バリアントを備え、定期的に更新されるリリースが特徴です。
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - 📥 2k / ⭐ 96 / Japanese‑augmented Llama‑2‑7B (6.27‑6.37 B パラメータ) モデル（I​nstruct および Fast バリエーションを含む）は、追加の日本語事前学習で微調整され、ELYZA によって Llama 2 Community License の下でリリースされ、サンプル使用コードとデベロッパークレジットが付属しています。
 * [sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1) - 📥 2k / ⭐ 13 / 日本語の自己回帰型言語モデル（sarashina2.2 – 0.5B‑instruct、1B‑instruct、3B‑instruct）は、SB Intuitionsによってトレーニングされ、日本語/英語タスクでベンチマークを行い、torch‑transformers使用ガイドと安全性トレーニングの制限に関する備考が付載されています。
 * [Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) - 📥 2k / ⭐ 17 / Llama 3.1 Swallow は、Meta の Llama 3.1 を継続的に事前学習し、合成日本語データで教師付き微調整を行ったことで構築された 8‑B および 70‑B の日本語強化言語モデルです。命令調整版は 2024年10月にリリースされました。
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - 📥 2k / ⭐ 81 / ELYZA‑japanese‑Llama‑2‑7Bは、追加のpre‑trainingを施した日本語最適化版のLlama‑2‑7Bモデルで、instructとfastのバリエーションがあります。Llama 2 Community Licenseの下でリリースされています。
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - 📥 2k / ⭐ 205 / OpenCALM は CyberAgent, Inc. が提供する日本語重視のデコーダーのみのトランスフォーマー言語モデルスイートで、160 M から 6.8 B パラメータのモデルを備え、Japanese Wikipedia と CommonCrawl で訓練され、CC BY‑SA 4.0 の下でリリースされています。
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - 📥 2k / ⭐ 53 / Japanese Stable LM Instruct Gamma 7B は、Stability AI が開発した 7 Bパラメータのデコーダー専用・インストラクション調整済み日本語言語モデルで、Base Gamma 7B をベースに構築され、Transformers ≥ 4.34.0 が必要で、Apache 2.0 ライセンスで公開されています。
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - 📥 2k / ⭐ 41 / LLM‑jpからの日本語中心の指示・生成LLM（13 B と 1.3 B パラメータモデルは、Megatron‑DeepSpeed で混合日本語/英語/ソースコードデータを学習）は、torch ≥ 2.0、transformers ≥ 4.34、accelerate = 0.23 を必要とする Hugging Face Transformers チェックポイントとしてリリースされています。
 * [japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - 📥 1k / ⭐ 104 / 1.3 BパラメータのJapanese GPTモデル。日本語C4、CC‑100、Wikipediaで訓練され、24層のTransformerとSentencePieceトークン化を使用。2022年1月26日にrinnaからMITライセンスでリリース。
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - 📥 1k / ⭐ 15 / LLM‑jp は、1.3B と 13B の日本語に特化したトランスフォーマー モデルを Hugging Face 形式でリリースし、instruction‑fine‑tuned、LoRA、pre‑trained variants を含む。これらは混成の日本語―英語―コードデータで学習され、PyTorch ≥ 2.0、Transformers ≥ 4.34、Accelerate 0.23 で使用可能です。
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - 📥 1k / ⭐ 25 / Japanese Stable LM Base Gamma 7B は、Mistral‑7B をベースにした 70億パラメータのデコーダーオンリー型言語モデルです。日本語タスクに最適化されており、日本語ウィキペディア、mc4、CC‑100 及び OSCAR コーパスで学習されています。Apache 2.0 ライセンスでリリースされ、Transformers ≥ 4.34.0 が必要です。
 * [Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B) - 📥 1k / ⭐ 6 / 8‑ビリオンドパラメータのLlama 3.1ベースモデルが、大規模な日本語コードおよび自然言語コーパスで微調整され、指示追従、因果推論およびFill‑in‑the‑Middle推論を備えている。日本語と英語のコード補完および生成タスクにおいて、ベースLlama 3.1およびQwenを上回る性能を示す。
 * [Sakura-13B-Galgame-GGUF](https://huggingface.co/QuantFactory/Sakura-13B-Galgame-GGUF) - 📥 1k / ⭐ 2 / llama.cppで量子化された、非商用ライセンスのSakura-13B-Galgame翻訳モデルで、オフラインにGPT-3.5レベルの日本語-日本語/英語 Galgameテキストを提供し、APIサポート、ライトノベルデータによる継続的なプリトレーニング、継続的な実験的アップデートを備えています。
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - 📥 1k / ⭐ 37 / TokyoTech‑LLMは、7B・13B・70Bのサイズを持つ日本語強化LLaMA‑2モデルで構成されたSwallow‑Llamaシリーズをリリースし、教師ありファインチューニング、NVEバリエーション、2023年12月から2024年4月までの新バージョンで更新しました。
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - 📥 1k / ⭐ 17 / Japanese‑StableLM‑Base‑Beta‑70Bは、70億パラメータのLlama‑2ベースのデコードオンリー言語モデルで、多様な日本語データでファインチューニングされ、強力な下流日本語性能を実現します。小規模な7Bバリアントと指示追従型の対応バージョンもあります。
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - 📥 1k / ⭐ 23 / 日本語強化版 Llama‑2‑7B モデル（ELYZA‑japanese‑Llama‑2‑7b とその派生）は追加事前学習で構築され、Llama 2 ライセンスの下で配布され、使用例と詳細が記載されています。
 * [Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1) - 📥 1k / ⭐ 3 / 日本語強化Gemma‑2言語モデル（2b、9b、27b）が提供され、合成日本語データに対する継続的な事前学習と指示調整による微調整を経て構築され、Hugging FaceとSwallowチームのウェブサイトで公開されています。
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - 📥 1k / ⭐ 26 / 70 billionパラメータの日本語専用デコーダーのみのLlama‑2モデルで、Databricks Dolly‑15k、Anthropic HH、その他公開データでファインチューニング（7 billionパラメータ版も含む）され、テキスト生成の使用例も付属しています。
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - 📥 1k / ⭐ 13 / 21.42 GB 本体に比べて小型で高速な、量子化された日本語中心の多言語10‑B GPT‑NeoXモデル（weblab‑10b‑instruction‑sft‑GPTQ）は、autoGPTQを搭載したGPUが必要で、CPU上でllama.cppを使用できる6 GB GGUFバージョンを含み、ローカルまたはColabでtext‑generation‑webuiを実行するための手順を提供します。
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - 📥 1k / ⭐ 15 / LLM‑jpの13Bと1.3Bのインストラクション調整済み言語モデルは、Megatron‑DeepSpeedに基づき、Hugging Face互換である。これらは日本語、英語、コードのプリトレーニングデータを提供し、torch ≥ 2.0.0、transformers ≥ 4.34.0、および accelerate 0.23.0 を必要とします。
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - 📥 1k / ⭐ 8 / LLM‑jpはHugging‑Face‑Transformers形式の13Bと1.3Bの日本語／英語の指示スタイル言語モデルを提供し、混在した日本語‑英語コードデータで事前学習されており、使用にはtorch≥2.0、transformers≥4.34、accelerate、DeepSpeedが必要です。
 * [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - 📥 1k / ⭐ 42 / Meta の Llama 2（13 Bパラメータ）を日本語に対応させた拡張バージョンで、Instruct と Fast バリアントを備え、ELYZA チームによって制作され、Hugging‑Face Transformers を経由で使用可能で、Llama 2 Community License の下で配布されます。
 * [Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1) - 📥 1k / ⭐ 3 / Gemma‑2‑Llama‑Swallowは、Gemma‑2をベースに、日本語に特化した継続的事前学習とインストラクション微調整を追加し、2B/9B/27Bの事前学習済みおよびインストラクション微調整済みモデルをHuggingFaceおよびSwallow LLM websiteで公開しています。
 * [japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - 📥 1k / ⭐ 15 / 6層、512次隠れ層のトランスフォーマーモデルで、Japanese CC‑100 と Wikipedia 上で SentencePiece トークナイゼーションを使用して訓練され、2021年8月25日に MIT ライセンスの下でリリースされました（https://huggingface.co/rinna/japanese‑gpt2‑xsmall と arXiv 2404.01657 を参照）。
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - 📥 1k / ⭐ 11 / OpenCALMは、160 M から 6.8 B パラメータまである日本語のデコーダーオンリートランスフォーマー言語モデルのスイートであり、日本語WikipediaとCommon Crawlで訓練され、CyberAgentによってCC BY‑SA 4.0ライセンスで公開されています。
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - 📥 1k / ⭐ 3 / ELYZA‑japanese‑Llama‑2‑7b‑fast‑instruct は、4.11 GB の GPU 専用、autoGPTQ 必須の日本語指示調整済み量子化モデルで、Meta 社の Llama 2 をベースにしています。メモリと速度を削減する代わりに、指示追従性能が低下します。最新の AWQ バリアントではより高い準拠性を提供し、CPU 用の gguf バージョンも用意されています。
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - 📥 1k / ⭐ 12 / SB Intuitions（Sarashina2 1B–3B）による日本語自己回帰型言語モデルは、日本語/英語タスクに関するベンチマーク結果、PyTorch 使用例スクリプト、そして安全性トレーニングが限定的である旨の注意書きとともに提供されます。
 * [llm-jp-3-1.8b-instruct](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct) - 📥 1k / ⭐ 25 / 国立情報学研究所の Japanese large‑language models (llm‑jp‑3‑*) は、1.8 b、3.7 b、13 b（「-instruct」を含む）バリアントを備えた Hugging Face チェックポイントとしてリリースされ、torch ≥ 2.3、transformers ≥ 4.40、accelerate、flash‑attn を必要とし、日本語 Wikipedia、Common Crawl、複数の混合コーパスで事前学習されています。
 * [shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b) - 📥 1k / ⭐ 2 / Shisa AI のバイリンガル日本語‑英語チャットモデル（Qwen2.5、Llama 3.1/3.3、Mistral Nemo、Unphi4）は、同一のトレーニングデータを共有し、日本語タスクで優れた性能を発揮しつつ、堅実な英語スキルも維持し、Apache 2.0 もしくは MIT ライセンスで公開されています。
 * [shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - 📥 1k / ⭐ 16 / Shisa‑base‑7B‑v1 は、MADLAD‑400 からの 8 B 日本語トークン事前学習で拡張された Mistral‑7B モデルで、2,400 A100‑40 GPU‑hours の学習時間でトレーニングされ、7‑B パラメータモデルの中で日本語ベンチマークでクラスリーディングな性能を達成しています。
 * [ALMA-7B-Ja-V2](https://huggingface.co/webbigdata/ALMA-7B-Ja-V2) - 📥 1k / ⭐ 20 / C3TR‑Adapter は、Google‑gemma‑7b 用の 4‑ビット QLoRA アダプタであり、ALMA‑7B‑Ja‑V2 モデル（日本語–英語翻訳、オプションでドイツ語・中国語・アイスランド語・チェコ語をサポート）を駆動します。8.1 GB GPU が必要ですが、無料の Colab でも実行できます。ベンチマークスコアが向上しています。
 * [ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - 📥 1k / ⭐ 24 / ELYZA-japanese-LLama-2-13b-fast-instructは、LLAMA 2をベースに追加の事前学習を施した13.14 Bパラメータ、44,581トークンの日本語言語モデルで、LLAMA 2 Community Licenseの下でリリースされています。
 * [ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - 📥 1k / ⭐ 22 / ELYZA‑Japanese‑Llama‑2‑13bは13.02 B‑parameterのLlama 2モデルで、日本語向けにファインチューニングされています。13.14 B‑parameterのfast variantも存在します。MetaのLLAMA 2 Community Licenseの下で Akira Sasaki、Masato Hirakawa、Shintaro Horie、Tomoaki Nakamura、Sam Passaglia、Daisuke Oba によって作成されています。
 * [stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - 📥 1k / ⭐ 39 / Stockmark Inc. の 13 B パラメータの日本語言語モデル（stockmark‑13b および instruction‑tuned バリアント stockmark‑13b‑instruct）は、AWS Trainium と neuronx‑neometegron を使用して約 220 B トークンで学習され、MIT license でリリースされています。
 * [ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - 📥 1k / ⭐ 7 / ELYZA‑Japanese‑Llama‑2‑13b‑fast は、約13 Bパラメータを持つ日本語強化 Llama‑2 モデルで、base、instruct、fast、fast‑instruct の各バリエーションがあります。このモデルは Akira Sasaki、Masato Hirakawa、Shintaro Horie、Tomoaki Nakamura、Sam Passaglia、Daisuke Oba によって Llama 2 Community License の下でリリースされました。
 * [Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) - 📥 1k / ⭐ 3 / Llama 3.1 Swallowは、継続的な事前学習とJapanese SFTによって構築されたMetaのLlama 3.1の8‑Bおよび70‑Bの日本語強化バリアントをリリースし、主要な日本語タスクでのベンチマーク結果付きでオープンソースのMegatron‑LMモデルとして入手可能です。
 * [llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct) - 📥 1k / ⭐ 31 / Repositoryは、日本語中心のLLM（1.8B‑3.1.8B、3.3.7B、13B、172B β1バリエーション）をNII’s Large Language Model R&D Centerからリリースし、Hugging Face兼容のチェックポイント、Byte‑fallback tokenizerを提供するとともに、Japanese Wikipedia、Common Crawl、WARP、Kaken、English Wikipedia、Dolma corporaでのプレトレーニングを実施します。

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - 📥 245k / ⭐ 70 / BERT‑base日本語モデル（12層、768次元の隠れ状態、12ヘッド、32,000語彙）は、2019年9月にMeCab IPA辞書の単語レベルトークン化を用い、WordPieceサブワードと全単語マスキング（MLM）で事前学習されました。CC BY‑SA 3.0で公開され、コードは cl‑tohoku/bert‑japanese で入手できます。
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - 📥 131k / ⭐ 6 / Japanese BERT‑baseは、Unidic 2.1.2 tokenization、character‑level subwords、whole‑word maskingを使用し、約30M件のWikipedia文書で事前学習され、12 layers、768‑dim hidden states、12 heads、そして6144‑token vocabularyを備えています。
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - 📥 101k / ⭐ 8 / BERT‑base Japanese は、17 M の Wikipedia 文章（約 2.6 GB）で事前学習され、IPA ベースの単語トークン化を用いて文字レベルの分割を行い、4 000 トークン語彙、12 層 / 768 次元の隠れ状態、12 ヘッドを備え、Creative Commons BY‑SA 3.0 の下で公開され、コードは cl‑tohoku/bert‑japanese にあります。
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - 📥 76k / ⭐ 38 / BERT‑base Japanese model（12層、768次元の隠れ状態、12ヘッド）で、IPA‑dictionary word tokenizationとWordPieceを組み合わせ、32k語彙を備え、2.6 GB（約17 M文）の日本語Wikipediaで事前学習されました。モデルはCloud TPUs上で1 Mステップ学習され、コードはcl‑tohoku/bert‑japaneseにあり、CC BY‑SA 3.0でリリースされています。
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - 📥 48k / ⭐ 48 / LINE DistilBERT Japaneseは、66Mパラメータ、6レイヤーのDistilBERTで、131 GBの日本語ウェブテキストを用い、LINE BERT‑base教師の指導下で訓練され、JGLUEで高いスコアを達成しています。MeCab/UnidicとSentencePieceを用いてトークン化され、Apache 2.0ライセンスの下で公開されています。
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - 📥 37k / ⭐ 3 / 約10 百万件のJST医学要旨と140万件の本文テキスト文で事前学習されたJapanese RoBERTaベースモデル。30,000トークンのSentencePiece (Unigram) 語彙を使用し、CC 4.0の下でリリースされ、ファインチューニングまたはtransformer-pipeline での使用に準備完了。
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - 📥 27k / ⭐ 30 / 日本語の DeBERTa V2 ベースモデル。171 GB の日本語 Wikipedia、CC‑100、OSCAR からなるデータセットで、Juman++ で分かち書きされたテキストと SentencePiece サブワードで事前学習されました。Hugging Face Transformers ライブラリを使い、8 台の NVIDIA A100 GPU で 3 週間トレーニングを行い、下流タスクへのファインチューニングが可能な状態です。
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - 📥 17k / ⭐ 8 / Japanese DeBERTa V2 large は、約171 GB の日本語Wikipedia、CC‑100、OSCAR を対象に、文字レベルトークナイズ、全単語マスキングを用い、16枚の A100 GPU で26日間訓練されました。
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - 📥 11k / ⭐ 27 / 12層、768次元のBERT‑baseモデルで、30百万件の日本語Wikipedia文を使って事前学習済み。Unidic‑liteの語レベルトークナイズを採用し、次にWordPieceサブワードと全語マスキングを適用。32 768語彙を持ち、トレーニングコードは cl-tohoku/bert-japanese でホストされている。
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - 📥 10k / ⭐ 18 / ModernBERT‑Ja‑310M は、SB Intuitions が開発した日本語‑英語 BERT バリアントで、ローカル + グローバル注意を RoPE と組み合わせています。4.09 trillion トークン、102,400 語の語彙、8,192 トークンシーケンスで学習され、効率的な推論のために Flash Attention 2 をサポートしています。
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - 📥 10k / ⭐ 39 / rinnaが事前学習したJapanese RoBERTa‑baseで、モデルをロードする詳細な手順と、[CLS]を先頭に追加してマスクトークン予測を確実に実行し、適切なトークン化と明示的なposition_idsを使用する方法、およびHuggingFace inference APIとの違いを明示する手順を提供します。
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - 📥 7k / ⭐ 2 / リポジトリは171 GBのWikipedia、CC‑100、OSCARデータで事前学習された日本語 DeBERTa V2 tiny モデルをホストしており、Juman++ 区切りと SentencePiece トークン化を必要とし、33 時間を8 A100 GPUでトレーニングしており、下流タスクへのファインチューニング準備が整っています。
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - 📥 7k / ⭐ 5 / ModernBERT‑Ja‑30M は SB Intuitions の日本語 BERT バリアントで、ローカルとグローバルアテンションを RoPE と組み合わせ、4.39 兆トークンで訓練され、102,400トークンの語彙と8,192トークンの制限を備え、複数のサイズ（30 M、70 M、130 M）と Flash‑Attention 2 との互換性を提供します。
 * [splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - 📥 6k / ⭐ 10 / 日本語検索モデル（splade‑japanese‑v3、JaColBERTv2、BGE‑m3、様々なm‑e5サイズなど）の評価指標（nDCG、recall、MRR）は、MIRACL と hotchpotch/JQaRA ベンチマークで報告されており、splade‑japanese‑v2‑doc はクエリエンコーダフリーとして記載されています。
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - 📥 3k / ⭐ 45 / Japanese ModernBERT‑Ja‑130Mは、ローカル+グローバルアテンションとRoPEを組み合わせた130 millionパラメータのBERTバリエーションです。4.39 Tトークンの日本語/英語コーパスで訓練され、102,400語彙と8,192トークン長さを備え、GPU Flash Attention 2に最適化されています。
 * [llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base) - 📥 2k / ⭐ 10 / llm‑jp‑modernbert‑base は 3.4 TB の日本語データで最大シーケンス長 8,192 を対象にトレーニングされた modernBERT‑base モデルで、2 段階 (1024→8192) にわたって構築され、JSTS で 0.92、JNLI で 0.912、JCoLA で 0.88 を達成しています。すべてのコードと評価スクリプトは GitHub で公開されています。
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - 📥 2k / ⭐ 8 / JavaScriptベースの日本語RoBERTaベースモデルで、WikipediaとCC‑100で学習し、Juman++をBertJapaneseTokenizer経由でトークン化し、8本のA100 GPUで70万ステップ以上でファインチューニングされた。
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - 📥 2k / ⭐ 4 / DeBERTaV2 base Japaneseは、CC‑100、mC4、OSCAR2301、Wikipedia、およびWikinewsで1Mステップ（Adam/FP16）で事前学習され、JGLUE NLUベンチマークでトップの結果を達成し、早稲田RoBERTaに近い性能を示す。
 * [deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - 📥 2k / ⭐ 9 / 日本語版 DeBERTa V2 Large は、Wikipedia、CC‑100、OSCAR から収集した合計 171 GB の日本語テキストで事前学習され、Juman++/sentencepiece トークン化を使用し、8 台の A100 GPU で 36 日間トレーニングされました。
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - 📥 2k / ⭐ 9 / 24層、1024次元のBERT‑Largeモデルで、Unidic 2.1.2の単語トークン化に続きWordPieceサブワード化を行い、全単語マスキングを採用しています。語彙は32,768トークンで、日本語Wikipedia4 GB（約30M文）を用いて事前学習され、512トークンのインスタンス、256バッチ、1 Mステップで訓練されました。
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - 📥 1k / ⭐ 6 / ModernBERT‑Ja‑70M は、4.39 T のトークンを使ってトレーニングされた 70 M パラメータを持つ日本語 BERT モデルで、combined local/global attention と RoPE を組み合わせて 8,192‑token sequences を効率的にモデリングし、Flash Attention 2 をサポートして最適な GPU パフォーマンスを実現します。
 * [bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - 📥 1k / ⭐ 4 / 日本語ウィキペディアを対象に、IPA文字トークナイゼーションと全単語マスキングを使用して訓練された BERT base model は、12レイヤー、768ヒドゥンユニット、12ヘッド、4,000トークン語彙を備えており、Creative Commons BY‑SA 3.0 で公開されています。
 * [roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - 📥 1k / ⭐ 32 / 日本語 RoBERTa‑base は日本語 Wikipedia と CC‑100 で事前学習され、Juman++ による分割が必要です。ファインチューニング可能な形でデプロイされ、Adam（lr = 1 × 10⁻⁴）で 8 台の A100 GPU を用いて 700 k ステップ学習しました。
 * [roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - 📥 1k / ⭐ 3 / Japanese RoBERTa‑Large は Japanese Wikipedia と CC‑100 を使って最大 512‑token のシーケンスで事前学習され、8 台の NVIDIA A100 GPU 上で 670 k ステップで訓練され、自動 Juman++/SentencePiece トークナイズでファインチューニング可能です。

### sentence-similarity
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - 📥 354k / ⭐ 11 / Ruriは、Sentence Transformers と簡単に連携できる新しい v3 日本語テキスト埋め込みモデル（30M〜310M パラメータ、最大長 8192、JMTEB スコア 74.51〜77.24）を提供し、JMTEB スイートで他の日本語および多言語モデルとベンチマークします。
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - 📥 159k / ⭐ 56 / Ruri v3は、ModernBERT‑Jaをベースにした高性能な日本語埋め込みモデルで、最大8 192トークンのシーケンス、10万語語彙、FlashAttentionをサポートし、sentence‑transformersを介して利用可能な複数のサイズを備えています。
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - 📥 73k / ⭐ 33 / GLuCoSE‑base‑jaは、LUKEをベースにした日本語の文埋め込みモデルで、Web、NLI、検索データを混合して訓練されています。768次元の平均プーリングベクトル（512トークン max）を生成し、コサイン類似度と意味検索タスクに最適です。競合ベースラインよりもSpearman/ Pearson指標で高いスコアを示します。
 * [sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - 📥 49k / ⭐ 13 / Japanese Sentence‑BERT モデルは colorfulscoop/bert-base-ja をベースに構築され、523,005件の SNLI 文でファインチューニングされ、3‑label ソフトマックス分類器を使用し、0.8529 のテスト精度を達成し、512 次元の埋め込みを生成します。
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - 📥 44k / ⭐ 3 / Ruri v3 は、ModernBERT‑Ja をベースにした最先端の日本語テキスト埋め込みを提供し、最大 8,192 トークン、10 万トークン語彙、FlashAttention を利用した高速化、および 30 M から 310 M パラメータまでの複数サイズのモデルをサポートします。
 * [simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - 📥 26k / ⭐ 15 / cl‑tohoku/bert‑base‑japanese‑v2 をベースに構築された日本語SimCSEモデル。JSNLI上で20エポックにわたりファインチューニング（Spearman係数が最高のチェックポイント）され、sentence‑transformers で使用可能です（fugashi/unidic‑lite が必要）。CC‑BY‑SA 4.0 の下でライセンスされています。
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - 📥 26k / ⭐ 2 / Ruri v3は、ModernBERT‑Jaをベースに構築された最先端の日本語テキスト埋め込みモデルであり、100kトークンの語彙、8,192トークンのシーケンスサポート、FlashAttentionによるアクセラレーション、およびsentence‑transformersと互換性のある複数のサイズバリエーションを提供します。
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - 📥 9k / ⭐ 43 / PLaMo‑Embedding‑1B は Preferred Networks が開発した日本語テキスト埋め込みモデルで、検索・分類・クラスタリングに高品質なベクトルを生成し、JMTEB ベンチマークで優れた性能を発揮します。また、商用利用が許可された Apache v2.0 ライセンスのもとで自由に利用できます。
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - 📥 8k / ⭐ 35 / studio‑ousia/luke‑japanese‑base‑lite に基づく日本語文変換器は、JSNLI で1エポックだけ学習し、文や段落を 768 次元の密ベクトルにマップしてクラスタリングと意味検索に利用します。
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - 📥 8k / ⭐ 21 / GLuCoSE v2はCPU向けに最適化された日本語テキスト埋め込みモデルで、取得（retrieval）と語義類似性（semantic‑similarity）タスクに優れています。モデルは“query:” / “passage:” というプレフィックスを使用し、蒸留ベース（distillation‑based）ファインチューニング後にMIRACLで同程度のサイズのモデルを上回る性能を示します。
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - 📥 7k / ⭐ 1 / Ruri v3は、100k語彙、8,192語サポート、FlashAttention統合を備えたModernBERT‑Jaベースの日本語埋め込みモデルで、30 M〜310 Mパラメータの4種サイズバリアントがあり、JMTEBベンチマークで最先端のスコアを達成しています。
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - 📥 7k / ⭐ 16 / MMarco上で250k ステップの知識蒸留を通じて学習させた日本語専用ドキュメント検索モデル、JaColBERTv2は、早期テストでmultilingual‑e5‑large、BGE‑M3、JaColBERTをすでに上回っており、強力なドメイン外一般化を実現しています。
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - 📥 3k / ⭐ 44 / Ruriは30 M–310 Mパラメータの日本語一般テキスト埋め込み v3 モデルをリリースしました。これらは sentence_transformers（「クエリ:」/「文章:」プレフィックスを使用）でロード可能で、JMTEB で同等のモデルを上回る性能を示しています。ベンチマーク表にその結果が示されています。
 * [ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m) - 📥 3k / ⭐ 1 / Ruriは、ModernBERT‑Ja をベースにした日本語文埋め込みシリーズで、30 M から 310 M パラメータのファインチューニング済みモデルを提供し、sentence‑transformers や FlashAttention 2 と統合でき、JMTEB スコアで最大 77.2 に達し、すべて Apache 2.0 ライセンスで公開されています。
 * [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) - 📥 3k / ⭐ 22 / 最終の JaColBERTv2.5 の重みは、改良されたレシピで JaColBERTv2 データの40 %で訓練され、すべてのデータセットで JaColBERTV2 の多言語バリエーション（BGE‑M3 など）を含む以前のすべてのモデルを上回っています。
 * [sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b) - 📥 2k / ⭐ 38 / Sarashina‑Embedding‑v1‑1B は、1.2 B パラメータを持つ日本語の文変換モデルで、テキストを1,792 次元ベクトルにマッピングします。マルチステージ対照学習でトレーニングされ、JMTEB ベンチマークで最先端の性能を達成し、非商用ライセンスの下でセマンティック類似度、検索、パラフレーズマイニング、分類、クラスタリングをサポートします。
 * [RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja) - 📥 1k / ⭐ 8 / RoSEttaは、RoFormerをベースとした日本語の文埋め込みモデルで、1024トークンまで対応し、検索と意味的類似性タスクに優れています。Sentence TransformersまたはHugging Face Transformersを通じてアクセスでき、クエリ/パッセージ前置詞が必須です。

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - 📥 767k / ⭐ 13 / 日本語CLOOBビジョン・ランゲージモデル (vit‑base‑patch16‑224) は、CC12Mのキャプションを日本語に翻訳したデータで訓練され、2022年5月12日リンナ株式会社によって Apache 2.0 ライセンスの下でリリースされ、Hugging Faceでホストされています。
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - 📥 55k / ⭐ 50 / Japanese sentence‑BERT v2 は、MultipleNegativesRankingLoss を使って cl‑tohoku/bert‑base‑japanese‑whole‑word‑masking でファインチューニングされた結果、プライベートデータセットで v1 を約 1.5〜2 点上回ります。
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - 📥 29k / ⭐ 22 / 日本語 CLIP（ViT‑B‑16）モデルは、CC12M キャプション翻訳で訓練され、2022年5月に rinna によって Apache 2.0 ライセンスの下でリリースされました。
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - 📥 26k / ⭐ 10 / Japanese Sentence‑BERT v1モデル、より新しいv2は約1.5ポイントの精度向上を提供し、リンクされたQiita記事に示された Hugging‑Face ベースの SentenceBertJapanese 例。
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - 📥 22k / ⭐ 29 / LY株式会社が開発した日本語CLIPモデルは、約10億組のウェブ画像‑テキストペアで訓練され、EVA‑02‑B画像エンコーダと12層BERTテキストエンコーダを使用して、STAIR Captions、Recruit、そしてJapanese ImageNet‑1Kで高いゼロショット画像分類と検索性能を発揮します。
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - 📥 11k / ⭐ 2 / Repo ja_ginza_electra には、mC4 Japanese 上で ELECTRA ディスクリミネータをファインチューニングする spaCy v3 日本語 NLP モデルがバンドルされており（megagonlabs/transformers‑ud‑japanese‑electra‑base‑discrimininator をベースに構築）、GiNZA v5 の文節フレーズコンポーネントを含み、MIT ライセンスで NINJAL‑Megagon Labs の共同プロジェクトとして公開されています。
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - 📥 4k / ⭐ 53 / 300百万パラメータの日本語T5モデルは、約100 GBのWikipedia（2020）とOSCAR CC‑100データで事前学習されており、Google の mT5‑small より25 % 小さく、livedoor news‑classification ベンチマークで6ポイント高いという特徴があります。したがって、ファインチューニングが必要で、バイアスリスクについての警告が付随します。
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - 📥 4k / ⭐ 17 / Sarashina2.2‑1B は、28 の JMTEB データセットで最先端（state‑of‑the‑art）なスコアを獲得し、語義類似度、検索、パラフレーズマイニング、分類、クラスタリングを可能にする、マルチステージ対照学習で訓練された 1,792 次元の日本語 Sentence Transformer です。
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - 📥 3k / ⭐ 14 / Sentence‑BERT と同じデータセットで訓練された日本語 Sentence‑LUKE モデルで、量的に 0.5 pt の前列、質的により高い精度を実現。studio‑ousia/luke‑japanese‑base‑lite を基盤に構築され、Transformer‑style の SentenceLukeJapanese クラスを通じて簡単に推論できるように公開されている。
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - 📥 2k / ⭐ 14 / 微調整された日本語用大型BERT（cl‑tohoku/bert‑large‑japanese‑v2）のSupervised‑SimCSEモデル（CLS+MLP pooling、1024‑dim、5e‑5 lr、batch 512、BFloat16、max‑seq 64、1 M training examples）は、sentence‑transformersまたはraw HuggingFace Transformers で利用可能です。
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - 📥 1k / ⭐ 2 / 日本語BERT‑baseをJSNLIで教師付きSimCSE（CLSプーリング + 訓練のみMLP）で微調整し、sup‑simcse‑ja‑baseとして公開。768次元の文埋め込み、100万件の訓練例、学習率5e‑5、バッチ512、温度0.05を採用し、sentence‑transformersまたはHuggingFace Transformers経由で利用可能。

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - 📥 6M / ⭐ 44 / 日本語に微調整されたwav2vec2‑large XLSR‑53音声認識モデルで、Common Voice、CSS10、JSUTで訓練され、16 kHzの音声に対応し、HuggingFace経由で利用可能です。
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - 📥 55k / ⭐ 4 / Fine‑tuned Japanese‑wav2vec2‑base ASRモデル（ひらがなのみ予測）をCommon Voice 11.0（20エポック、lr 1e‑4、バッチ16）で訓練し、7 kステップ後に0.1418 WERを達成。
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - 📥 54k / ⭐ 78 / Kotoba‑Whisper v2.0 は、OpenAI の Whisper‑large‑v3 から蒸留された日本語音声認識モデルで、7.2 M ReazonSpeech クリップで訓練され、教師モデルよりも 6.3 × 速く実行されつつ、日本語 CER/WER が低い。さらに、すべてのトレーニングコードと評価データが公開されています。
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - 📥 16k / ⭐ 82 / Kotoba‑Whisper‑v2.2 は、kotoba‑whisper‑v2.0 を拡張した日本語 ASR モデルで、Transformers ベースのパイプラインを用いて pyannote ツールでスピーカー分離（ディアライゼーション）と句読点を追加します。
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - 📥 11k / ⭐ 112 / Anime Whisper は、Galgame_Speech_ASR_16kHz データセットから約5,300時間のアニメ風音声で微調整された軽量な日本語 ASR モデルです。実装は kotoba‑whisper‑v2.0 をベースにしており、高精度で句読点付きの文字起こしを実現します。最小限の幻覚化（hallucinations）を抑え、非言語音を忠実に再現し、NSFW オーディオに対しても堅牢な性能を発揮します。初期プロンプトを回避することで品質劣化を防止しています。
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - 📥 7k / ⭐ 35 / reazonspeech‑nemo‑v2 は 619 Mパラメータからなる subword ベースの RNN‑T Conformer モデルで、ReazonSpeech v2.0 日本語コーパスでトレーニングされています。Fast‑Conformer の Linearly Scalable Attention と Longformer encoder を用いることで、数時間にわたる音声を文字起こしできます。Apache 2.0 ライセンスの reazonspeech ライブラリ経由で利用可能です。
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - 📥 6k / ⭐ 19 / Kotoba‑Whisper‑v2.1 は、kotoba‑whisper‑v2.0 を拡張した日本語 ASR モデルで、後処理パイプラインにより文字列に句読点を追加しつつ、前モデルと同等の誤り率を実現しています。
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - 📥 3k / ⭐ 37 / Parakeet TDT‑CTC 0.6B (ja) は、NVIDIA NeMo の 6億パラメータ Hybrid FastConformer‑TDT‑CTC ASR モデルで、16 kHzモノラルの日本語音声を句読点付きで文字起こしでき、NeMo フレームワークで実行またはファインチューニングが可能です。
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - 📥 2k / ⭐ 17 / Kotoba‑Whisper‑Bilingual v1.0は、6.3倍速い蒸留 Whisper モデルで、日英間の ASR と双方向音声→テキスト翻訳を行い、Whisper large‑v3 からの擬似ラベリングと外部 LLM 生成転写を用いて訓練され、OpenAI およびカスケードベースラインと対照的に評価されます。
 * [whisper-large-v2-translate-zh-v0.2-st](https://huggingface.co/chickenrice0721/whisper-large-v2-translate-zh-v0.2-st) - 📥 1k / ⭐ 1 / 日本語から中国語への翻訳用にfine‑tunedしたopenai/whisper‑large‑v2は、5,000 hの日本語音声（中国語字幕付き）で訓練され、TER 60.48、BLEU 29.37、CER 0.66という性能を発揮しています。

### translation
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - 📥 113k / ⭐ 66 / Opus‑mt‑ja‑en は、正規化と SentencePiece 前処理を施した opus データセットで訓練された日本語–英語 transformer‑align モデルを提供し、Tatoeba ja‑en テストセットで 41.7 BLEU (0.589 chrF) を達成しています。
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - 📥 88k / ⭐ 8 / 新しい VNTL データセットで LLaMA 3 (Youko QLoRA) をファインチューニングし、より正確でかつ文字通りの日本語から英語へのビジュアルノベル翻訳を提供します。デフォルトプロンプト、マルチラインサポート、および推奨される中立サンプリング（温度 0）を使用しています。
 * [fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en) - 📥 11k / ⭐ 32 / FuguMTは、Marian‑NMT、Transformers、SentencePieceを用いて構築された日本語‑英語ニューラル翻訳モデルであり、Tatoebaベンチマークで39.1 BLEUを獲得しています。
 * [fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja) - 📥 11k / ⭐ 54 / FuguMTは、トランスフォーマー、SentencePiece、およびオプションでpySBDを用いたセグメンテーションを活用した英語‑日本語のMarian‑NMT翻訳モデルで、TatoebaデータセットでBLEUスコア32.7を獲得しました。
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - 📥 9k / ⭐ 14 / 英日 transformer‑align MT モデルが Opus‑BT 2021 データで訓練され、正規化と SentencePiece 前処理を使用し、Tatoeba テストセットで 15.2 BLEU と 0.258 chrF を達成しました。
 * [LFM2-350M-ENJP-MT](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT) - 📥 6k / ⭐ 79 / LFM2‑350M‑ENJP‑MTは、微調整されたLFM2‑350Mチェックポイントで、短〜中程度のテキストに対し、10倍以上大きいモデルと同等の品質で、ほぼリアルタイムの双方向日本語/英語翻訳を提供し、人間とAIの協働を強調します。
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - 📥 2k / ⭐ 27 / 近リアルタイムで短〜中規模入力の双方向日英翻訳にファインチューニングされた量子化済み LFM2‑350M チェックポイントで、Hugging Face で入手可能かつ llama.cpp で使用できます。
 * [Sugoi-14B-Ultra-GGUF](https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF) - 📥 1k / ⭐ 8 / Sugoi LLM 14B Ultra (GGUF) は BLEU スコア 21.38 を達成し（前世代の13.67のほぼ2倍）、RPG Maker の括弧付きテキストに対するプロンプトへの従順性が高く、日英ゲームダイアログ翻訳に優れ、LM Studio のような対話型チャットインターフェース向けに実験的なツール連携と JSON 出力を最適化しています。
 * [aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - 📥 1k / ⭐ 5 / bert-japaneseエンコーダーとkogpt2デコーダーで構築された日本語→朝鮮語翻訳モデルで、Hugging Faceデモ、PyTorch / Transformers依存関係を提供し、Korean AI Hub datasetsで訓練されています。

### text-classification
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - 📥 20k / ⭐ 3 / 日本語BERT Baseを約1,000件のブログ投稿でファインチューニングし、10種類の感情カテゴリに対応。ラベル9は「shame」（恥ずかしい）にマッピング。
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - 📥 15k / ⭐ 14 / loss 0.0001、accuracy 1.0、F1 1.0の日本語感情分析モデル。chABSA データセットで、Transformers 4.24.0 と PyTorch 1.12.1+cu113 を用い、learning_rate 2e‑05、batch 16、10 エポック、Adam オプティマイザ、linear scheduler、seed 42 で学習。`model(**inputs)` で呼び出されます。
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - 📥 11k / ⭐ 43 / 日本語テキストで8つの感情（joy、sadness、anticipation、surprise、anger、fear、disgust、trust）を予測するためにwrimeデータセットで微調整されたLUKE‑Japanese‑large‑liteモデル。
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - 📥 6k / ⭐ 2 / bert‑base‑japanese‑v3‑jsts は、JGLUE JSTS similarity dataset で fine‑tuned され、semantic similarity tasks に使用できる日本語 BERT モデルです。Hugging Face の transformers pipeline で利用でき、Apache 2.0 license の下で提供されています。
 * [luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - 📥 3k / ⭐ 5 / studio-ousia/luke-japanese-largeからファインチューニングされたこの日本語モデルは、自動的に名誉毀損を検出し、発話を4つのカテゴリ（中立、脅迫、虐げ、評判毀損）に分類します。CC BY‑SA 4.0の下でリリースされています。
 * [bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - 📥 2k / ⭐ 13 / Fine‑tuned Japanese BERT を Amazon 商品レビューに適用すると、6 epoch、2e‑05 学習率、16 サンプルバッチで約 81 % の精度（F1≈0.73）を達成し、Hugging Face Transformers 4.27.4 と PyTorch 2.0 を使用しています。
 * [bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - 📥 1k / ⭐ 8 / BERT‑base‑japanese‑v3 は、感情分析（日本語 LLM チュートリアル第 5章）用に MARC‑ja JGLUE データセットでファインチューニングされたモデルで、Colab ノートブックまたは transformers パイプラインから利用可能で、Apache 2.0 のライセンスでリリースされています。

### text-ranking
 * [japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - 📥 18k / ⭐ 15 / 日本語でトレーニングされたCrossEncoderリランカー（xsmall から large‑v1 までで隠れサイズは 384–1024、または japanese‑bge‑reranker‑v2‑m3‑v1）は、日本語QAタスクに利用可能で、JQaRA、JaCWIR、MIRACL、JSQuADにおけるベンチマークスコアが付随しています。
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - 📥 15k / ⭐ 4 / Japanese‑reranker‑xsmall‑v2は、tiny、xsmall、small、base、cross‑encoderの超軽量日本語リラッカーを提供し、ベンチマークスコアとGPU速度を備えています。transformers v4.48+で利用でき、さらにFlash‑Attention 2またはONNX/quantizationでCPU/ARM上の高速化も可能です。
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - 📥 10k / ⭐ 3 / 日本語CrossEncoderリランカーモデル（384–1024 hidden units）は、日本語データで訓練され、JQaRA、JaCWIR、MIRACL、JSQuADでのベンチマーク結果で最先端の関連性スコアリングを実現しています。
 * [japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - 📥 7k / ⭐ 16 / 日本語のCrossEncoderリレイナモデリングライブラリ（xsmall から large（隠れ層サイズ 384–1024））と BGE Reranker v2 m3、JQaRA、JaCWIR、MIRACL、JSQuAD での評価結果を含む。
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - 📥 6k / ⭐ 7 / Japanese CrossEncoder Reranker モデル（xsmall、small、base、large、および BGE‑v2）は、hotchpotch feature の hidden size が 384–1024 で、JQaRA、JaCWIR、MIRACL、JSQuAD で 0.61–0.98 の性能を達成します。
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - 📥 5k / ⭐ 12 / Ruri‑Rerankerは、ModernBERT‑Jaをベースにした最先端の日本語汎用リランカーで、最大8,192トークンのシーケンス、10万語彙、FlashAttentionの統合を備え、sentence‑transformers経由で利用可能です。

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - 📥 184k / ⭐ 158 / Manga OCRは、Vision Encoder‑Decoderベースの日本語 OCR システムで、振り仮名付きの高品質な縦書き・横書きの漫画テキストを提供し、多様なフォントに対応し、低品質の画像にも対応します。また、ソースコードは無料で入手可能です。
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - 📥 22k / ⭐ 2 / meikiocr パイプラインのコアコンポーネントで、オープンウェイトモデルを提供します。  
- v0.1（MobileNetV4‑small ベースの D‑FINE デテクタで、64 ボックスに制限された 960×544 / 320×192 の2種類があり、ビデオゲームの文字列で PaddleOCR を上回ります）  
- 低レイテンシーの v0 tiny / small バリアント（CPU で約 30/70 ms、GPU で 3/7 ms、少量または多数のテキストラインに対応）
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - 📥 22k / ⭐ 2 / meikiocr は、D‑FINE/MobileNetV4 に基づく効率的な文字検出モデルで、最新鋭の精度と低遅延で日本語のビデオゲームテキストを処理します。1枚あたり最大 48 文字ボックスを 960×32 画像に対して出力し、CPU／GPU 推論は Hugging Face を通じて利用できます。
 * [sarashina2.2-vision-3b](https://huggingface.co/sbintuitions/sarashina2.2-vision-3b) - 📥 2k / ⭐ 13 / Sarashina2.2‑Vision‑3Bのリポジトリ：3 Bパラメータの日本語ビジョン‑ランゲージモデルで、SigLIPとSarashina2.2‑3B‑Instructをベースに構築。VQAとQAタスクにおける日本語と英語のベンチマーク結果を報告し、推論用のインストラクションとスクリプトを含む。
 * [sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b) - 📥 1k / ⭐ 10 / 日本語8‑B VLM Sarashina2‑Vision‑8Bは、Sarashina2‑7BをベースにQwen2‑VL‑7Bイメージエンコーダを搭載しており、2025年3月7日時点で4つの日本語VLMベンチマークでトップの成績を収め、SB Intuitionsによって訓練されています。

### token-classification
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - 📥 540k / ⭐ 25 / Fine‑tuned XLM‑RoBERTa‑Base for Japanese NER on a dataset with PER, ORG, LOC, INS, PRD, EVT labels, trained 5 epochs (lr 5e‑05, batch 12) to reach a final loss of 0.0173.
 * [bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - 📥 29k / ⭐ 10 / Wikipedia データセットの固有表現抽出のためにファインチューニングされた BERT-Japanese (cl‑tohoku/bert‑base‑japanese‑v3) は、『Large Language Model Introduction』本から公開され、Apache 2.0 ライセンスでリリースされています。
 * [MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - 📥 27k / ⭐ 4 / MedTxt‑CR‑JAで訓練された日本語医療NER。予測スクリプトはXMLタグ付きテキストを出力し、エンティティを正規化し、id_to_tagsマッピング経由でHuggingFaceのロードをサポート。さらにトレーニングユーティリティも備えており、すべてPython 3.9用に構築されています。
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - 📥 8k / ⭐ 11 / BertForTokenClassificationを使用した日本語NERシステムは、企業、政治、その他組織、施設、製品、イベントなど8種類のエンティティタイプを抽出します。バックボーンはcl-tohoku/bert-base-japanese-v2で、StockMark Wikipedia NERデータセットでファインチューニングされています（Python、transformers、unidic_lite、fugashi経由、CC-BY‑SA 3.0）。

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - 📥 3k / ⭐ 20 / Anime‑Llasa‑3B は、HKUSTAudio/Llasa‑3B から改良された日本語の TTS モデルで、拡張されたトレーニングデータにより表現力と安定性が向上し、CC‑BY‑NC‑4.0 の下でリリースされています。
 * [Anime-Llasa-3B-Captions](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-Captions) - 📥 2k / ⭐ 11 / Anime‑Llasa‑3B‑Captionsは、Gemini 2.5 Proで生成した音声メタデータを用いてAnime‑Llasa‑3Bを微調整した日本語TTSモデルで、system prompt tagsとfull-width in-text markersを通じて制御可能な合成を実現します。

### question-answering
 * [bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - 📥 2k / ⭐ 2 / BERTベースの日本語モデルで、JaQuAD質問応答データセットでファインチューニングされたもので、トレーニングコードと使用例は SkelterLabsInc/JaQuAD にホストされ、CC BY‑SA 3.0でリリースされています。

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - 📥 2k / ⭐ 110 / PaddleOCR‑VL‑For‑Manga は、PaddleOCR‑VL を用いて構築されたファインチューニング済み OCR モデルで、27 %（PaddleOCR‑VL）から 70 % へと日本語マンガの吹き出しにおける全文句の精度を向上させます。109 言語で訓練された模型で、Manga109‑s で評価され、カスタム OCR プロジェクト向けの訓練コードとチュートリアルを提供します。

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - 📥 1k / ⭐ 9 / Anime‑XCodec2‑44.1kHz‑v2 は、デコーダのみのアップサンプリング拡張版で、新しいアップサンプリング層とRMS loss を使用して 16 kHz の日本語音声トークンを 44.1 kHz の高忠実度オーディオに変換します。元のエンコーダ、コードブック、トークンセットは変更されません。

### Others
 * [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) - 📥 114k / ⭐ 1 / CyberAgent の open‑calm‑3b モデルの gguf‑format バリアントで、llama.cpp の mmnga‑dev ブランチをベースに構築され、例として使用方法が示され、将来の gptneox 統合により互換性が破損する可能性がある旨の警告付きです。
 * [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) - 📥 106k / ⭐ 2 / CyberAgent の open‑calm‑7b モデルを llama.cpp 用に GGUF 形式に変換したものです。現在はテストブランチにあり、将来の GPT‑NeoX ベースのアップデートとは互換性がない可能性があります。
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - 📥 102k / ⭐ 10 / 日本語の BERT‑base モデル（12層、768‑次元の隠れ状態、12ヘッド）で、Unidic‑2.1.2 の word‑level tokenization を用い、さらに character‑level tokenization と whole‑word masking を行って日本語 CC‑100 コーパスと 2023 Wikipedia で事前学習し、7027‑token vocabulary を得た。
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - 📥 71k / ⭐ 13 / BERT‑Large 日本語モデル（24層、1024‑dim hidden、16 head、32 768‑token vocab）を提供します。CC‑100とWikipediaで訓練され、Unidic‑lite word‑level tokenization に加えて WordPiece と whole‑word masking を使用し、2 M training steps を行ったモデルです（コードは cl‑tohoku/bert‑japanese にあります）。
 * [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - 📥 47k / ⭐ 1 / stockmarkの日本語 GPT‑NeoX 1.4b を llama.cpp 用に mmnga‑dev ブランチ経由で gguf 変換実験。※ネイティブ GPT‑NeoX サポートが追加されると使用不能になる可能性があります。
 * [sarashina2.2-0.5b](https://huggingface.co/sbintuitions/sarashina2.2-0.5b) - 📥 45k / ⭐ 10 / Sarashina2.2‑0.5B は 5億パラメータの日本語言語モデルで、三段階でトレーニングされました――数学とコーディングの合成データでの学習、続いてアプリケーションタスク用に微調整――日本語 QA、数学、コーディングベンチマークでベースラインスコアを示しますが、インストラクション調整は行われておらず、信頼性の低い、偏った出力を生成する可能性があります。
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - 📥 37k / ⭐ 56 / BERT‑base Japanese（12層、768 hidden、12ヘッド、32,768トークン語彙）は、CC‑100と日本語ウィキペディアで事前学習され、Unidic‑liteの単語レベルトークン化と全語マスキングを使用し、事前学習コードが含まれています。
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - 📥 28k / ⭐ 17 / Japanese DeBERTa V3 base は、LLM‑jp v1.0 からの 540 B トークンと日本語ウィキペディア、mC4、英語ウィキペディア、The Pile、Code The Stack からのデータで事前学習されており、形態素前分割を行わない unigram byte‑fallback トークナイザーを使用しており、下流タスクでファインチューニングが可能です。
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - 📥 14k / ⭐ 9 / 約100 GB の日本語テキスト（Wikipedia ダンプ + CC‑100 コーパス）で事前学習された T5 v1.1 モデルで、バイトフォールバックを使用した SentencePiece トークナイザーを備えており、下流タスクでのファインチューニングを想定しているが、データセットのバイアスを反映している可能性があります。CC‑BY‑SA 4.0 の下で提供されます。
 * [bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - 📥 6k / ⭐ 10 / 日本語　BART base モデルは、1800万件の Wikipedia 文章で事前学習され、Juman++ による形態素分割を必要とし、微調整可能です。4×V100 GPUs 上で 500,000 ステップにわたり学習され、6‑layer encoder/decoder と 768‑dim hidden size の構成を持っています。
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - 📥 3k / ⭐ 16 / 高性能日本語 SPLADE v2 デモは、512トークンまでのテキストを疎ベクトルに変換し、推論用に WebUI および YASEM API を提供します。YAST で学習され、JMTEB 検索ベンチマークを報告します。
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - 📥 3k / ⭐ 68 / ELYZA, Inc.が提供する日本語強化版8B Llama‑3モデルは、Meta‑Llama‑3‑8B‑Instructをベースに追加のプレトレーニングと指示チューニングを施しており、GGUF（Q4_K_M）とAWQ量子化ウェイトを採用しています。これにより、llama.cppまたはLM Studioで使用でき、GPT‑4で約3.6というスコアを記録します。
 * [Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf) - 📥 3k / ⭐ 5 / ggufフォーマットのNinja‑v1‑NSFWモデルで、Local‑Novel‑LLMプロジェクトを元に、TFMC/imatrixデータセットから構築され、llama.cppで使用できる状態です。
 * [Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf) - 📥 3k / ⭐ 11 / 日本語小説向け LLM のために GGUF に変換された Ninja‑v1‑NSFW‑128k モデル。imatrix データで構築され、llama.cpp で実行します（例：main -m 'Ninja‑v1‑NSFW‑128k‑Q4_0.gguf'）。
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - 📥 3k / ⭐ 55 / gguf‑converted DeepSeek‑R1‑Distill‑Qwen 日本語言語モデル（14B と 32B）は、imatrix データセットから構築され、llama.cpp で使用できる状態です。
 * [aquif-3.5-Max-42B-A3B-i1-GGUF](https://huggingface.co/mradermacher/aquif-3.5-Max-42B-A3B-i1-GGUF) - 📥 2k / ⭐ 2 / Repo は Aquif‑3.5‑Max‑42B‑A3B の weighted/imatrix GGUF 量子化ファイルを提供し、サイズをリストアップし、Hugging Face のリンクと使用方法のノート、性能グラフ、および FAQ /model‑request リソースを提供します。
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - 📥 2k / ⭐ 17 / pfnet の plamo-2-translate モデルを TFMC/imatrix 日本語 LLM データセットから構築し、GGUF‑形式変換を提供して、llama.cpp での使用方法を記載しています。
 * [t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - 📥 2k / ⭐ 2 / Japanese T5 v1.1 モデルは、日本語 Wikipedia と mC4/ja で事前学習され、GEGLU 活性化、事前学習中のドロップアウト無効化、パラメータ共有なしを特徴とし、CC‑BY‑SA 4.0 の下で公開され、事前の連絡により商用利用が許可されています。
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - 📥 2k / ⭐ 7 / TFMC/imatrix-dataset-for-japanese‑llmをベースに構築されたggufフォーマットのVecteus‑v1で、llama.cppとNinja‑v1‑128k‑ggufなどの関連モデルと共に使用できる状態です。
 * [gemma-3-JP-EN-Translator-v1-4B-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-JP-EN-Translator-v1-4B-i1-GGUF) - 📥 2k / ⭐ 1 / Gemma‑3‑JP‑EN‑Translator‑v1‑4B モデル用に、加重／IMatrix GGUF 定量化を提供します。サイズ／品質表、mradermacher の静的 GGUF リリースへのダウンロードリンク、および定量化の使用または作成に関するガイダンスを掲載しています。
 * [c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - 📥 2k / ⭐ 4 / GGUF形式でビルドされたCohereForAIのc4ai‑command‑R‑plusモデルを提供します。モデルはTFMC imatrix Japanese LLMデータセットから作成され、q6_kとq8_0の分割ファイルの統合手順と、llama.cppで実行する方法が記載されています。
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - 📥 2k / ⭐ 44 / GGUF変換版、7 B日本語Llama‑2モデル（ELYZA 提供）で、fast instruct バリアントが日本語語彙を追加し、トークンコストを削減して速度を1.8×向上させます。さらに CodeLlama と GPTQ バージョンもあり、Meta の LLAMA 2 Community License の下で llama.cpp と併用可能です。
 * [Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf) - 📥 2k / ⭐ 2 / Ninja‑v1‑128k日本語LLMのgguf‑フォーマット版。Local‑Novel‑LLM‑projectからimatrix datasetを使って再構築され、llama.cppでデプロイ可能。
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - 📥 2k / ⭐ 39 / 14‑Bと32‑Bの日本語gguf形式のDeepSeek‑R1‑Distill‑Qwenモデルが、TFMC imatrix datasetでトレーニングされ、CUDA‑enabled llama.cpp推論使用指示が提供されています。
 * [lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - 📥 2k / ⭐ 2 / GGUF‑変換版の lightblue の 8B 日本語 Llama‑3 モデル、TFMC/imatrix データセットで構築され、llama.cpp を通じて実行するための手順付き。
 * [haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - 📥 1k / ⭐ 4 / Llama‑3‑8B Japanese Instruct を imatrix データセットを使って GGUF 形式に変換し、llama.cpp で使用できるようにしました。
 * [umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf) - 📥 1k / ⭐ 7 / TFMC/imatrix-dataset-for-japanese‑llm から構築された Umievo‑itr012‑Gleipnir‑7B 日本語LLM の GGUF 変換。llama.cpp の使用手順（ビルド、`…-Q4_0.gguf` の読み込み、128 トークン制限、プロンプト例「こんにちわ」）。
 * [rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf) - 📥 1k / ⭐ 6 / rinna の GGUF フォーマットで提供される日本語 LLM、TFMC Imatrix データで構築された Llama‑3‑Youko‑8B と、併せて提供される rinna‑nekomata‑7B/14B モデルは、llama.cpp フレームワークを介して利用できます。
 * [aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf) - 📥 1k / ⭐ 1 / CohereForAIの aya‑23‑8B を llama.cpp 用に GGUF フォーマットに変換し、TFMC/imatrix の日本語LLMデータでビルド、使用例の説明も含めました。
 * [umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - 📥 1k / ⭐ 3 / Umiyuki によって作られた GGUF 変換済みの Japanese‑Chat‑Umievo‑itr001‑7b は、TFMC/imatrix‑dataset‑for‑japanese‑llm データで訓練され、llama.cpp で推論可能です。
 * [karakuri-lm-8x7b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf) - 📥 1k / ⭐ 4 / GGUF形式の8×7B Karakuri‑LMチャットモデル、karakuri‑lm‑8x7b‑chat‑v0.1をimatrixデータセットで派生し、llama.cppで使用できるようになっています。
 * [tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf) - 📥 1k / ⭐ 1 / gguf形式のSwallow‑13b‑instruct‑v0.1モデル（tokyotech‑llm製）は、TFMC/imatrixデータセットでトレーニングされ、llama.cppで実行されます。
 * [pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf) - 📥 1k / ⭐ 1 / GGUF‑フォーマット変換：pfnet の nekomata‑14b‑pfn‑qfin モデルは TFMC の imatrix dataset で構築され、tongyi‑qianwen ライセンスの下で提供されています。llama.cpp の使用方法も含まれています。
 * [DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf) - 📥 1k / ⭐ 10 / DataPilot‑ArrowPro‑7B‑KUJIRA‑ggufは、imatrixデータセットを使用してDataPilotのArrowPro‑7B‑KUJIRAモデルをGGUF形式に変換し、llama.cppで実行できます。
 * [Llama-3.1-Swallow-8B-Instruct-v0.5-gguf](https://huggingface.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf) - 📥 1k / ⭐ 2 / GGUF‑converted Llama‑3.1‑Swallow‑8B‑Instruct‑v0.5（tokyotech‑llm によってオリジネートされ、TFMC/imatrix‑dataset‑for‑japanese‑llm で訓練済み）で、ビルドおよび CUDA 対応 llama.cpp 使用手順付き。
 * [rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf) - 📥 1k / ⭐ 1 / rinnaのLlama 3 Youko 70B Instructモデルを、TFMC Imatrixデータセットを使用してGGUF形式に変換し、llama.cppで実行できるようにしました。例を示した使用例の通りです。
 * [r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf) - 📥 1k / ⭐ 2 / perplexity‑ai の r1‑1776‑distill‑llama‑70b を GGUF‑formatted build として提供し、TFMC/imatrix‑dataset‑for‑japanese‑llm の imatrix データでコンパイル済み、llama.cpp 上で CUDA サポート付きで動作可能です。
 * [Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf) - 📥 1k / ⭐ 4 / Llama‑3‑ELYZA‑JP‑8Bモデルを日本語imatrixデータセットを使用して GGUF 形式に変換し、llama.cpp 経由で実行するための使用手順を添付。

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - 📥 56k / ⭐ 17 / 2009年から2024年までのすべてのNiconico Live（Jikkyo）放送コメントを含む190 GBのデータセットで、定期的に更新され、簡単に取得できるAPIと分割オプション（≈1 GBサンプル）を備えています。
 * [ASMR-Archive-Processed](https://huggingface.co/datasets/OmniAICreator/ASMR-Archive-Processed) - 📥 31k / ⭐ 37 / 進行中のパイプラインは、2つのアーカイブからASMR音声をキュレーションし、低品質ファイルをフィルタリングし、44.1 kHz 24 bit ステレオFLACに変換し、MelBand Roformer Big Beta 6Xで背景ノイズを除去し、音量を ‑23 LUFS に正規化し、Silero‑VADで音声をセグメント化し、litagin/anime‑whisperで文字起こしし、LLMで文字起こしを洗練させ、最後にシャッフル、匿名化し、データをWebDatasetとしてパッケージします。
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - 📥 10k / ⭐ 8 / Cauldron‑JA は The Cauldron 視覚言語データセットの日本語訳版で、44 個のサブデータセット（元の 50 個から OCR、コーディング、グラフデータを除いた）から構成され、DeepL によって生成され、Hugging Face の datasets ライブラリを通じて利用可能です。ライセンスはオリジナルの条件に基づき、プロンプトは CC‑BY‑4.0 で提供されます。
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - 📥 7k / ⭐ 8 / JMedBench は、日本語のバイオメディカル LLM ベンチマークで、20 のデータセットが 5 つのタスク（MCQA、NER、QA、BLURB など）にわたり構成され、公開されているソースから採取され、Junfeng Jiang と Jiahao Huang によって管理されています。
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - 📥 5k / ⭐ 22 / FineWeb2 Edu Japanese は、約1億2000万件の日本語テキスト（≈89 Bトークン）からなる高品質な教育データセットを提供しています。スコアが ≥ 2.5 のエントリのみをフィルタリングし、sample_10BT や small_tokens（重複範囲に注意）などのサブセットを含めています。また、ModernBERT‑Ja‑130M を介してテキストごとのトークン数を提供します。
 * [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) - 📥 5k / ⭐ 5 / KokushiMD‑10 は、10 の日本国内の国家医療免許試験で大規模言語モデルをベンチマークし、医療・歯科・看護・薬学・関連専門分野にわたる詳細な Chain‑of‑Thought 専門家注釈付きマルチモーダル問題を日本語、英語、または両方で提供します。
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - 📥 4k / ⭐ 18 / JMTEBは、28のデータセットにわたる5つのタスク（クラスタリング、分類、STS、リトリーバル、再ランク付け）を提供する日本語テキスト埋め込みモデル向けのベンチマークであり、1行の評価スクリプトで簡単な評価とモデル改善を可能にします。
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - 📥 4k / ⭐ 24 / Galgame VisualNovel データセットを、元の OOPPEENN/56697370656F4E76616C5F44617461736574 から再整理し、datasets ライブラリでの使用を容易にしました。すべてのオリジナル音声と文字起こしテキストを保持し、サブセット選択、reupload_dataset.py スクリプトを提供、非商業的な NLP および音声合成研究を対象としています。
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - 📥 2k / ⭐ 12 / 非公式プロジェクトで、Sakura MikoのHololive VT‑unitの音声クリップを音声認識研究用のデータセットにまとめ、Hololiveのファンコンテンツガイドラインに準拠し、Git LFSを使って構築、参加者がプラットフォーム別のtrain/testフォルダーに音声を追加する。
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - 📥 1k / ⭐ 5 / lmqg/qg_jaquad データセットは QG‑Bench の日本語 JaQuAD サブセットであり、段落、文、ハイライトされた回答のフィールドを提供し、段落レベルの質問生成を行います。評価には BLEU4、METEOR、ROUGE‑L、BERTScore、そして MoverScore が用いられます。
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - 📥 1k / ⭐ 134 / 73,004の音声‑テキストペアから構成され、合計110時間の日本アニメ音声データセット。WhisperなどのASRモデルの改善を目的としており、商業利用・非商業利用のいずれでもオープンに利用可能です。
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - 📥 1k / ⭐ 20 / MOMIJIデータセットには、約5600万件の日本語ウェブドキュメントと1100億文字、2490万枚の画像が収録されており、ビジョン‑ランゲージモデルのトレーニングに使用され、インタラクティブな可視化と生成ユーティリティとペアリングされています。
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - 📥 1k / ⭐ 37 / Galgame_Speech_ASR_16kHz は、OOPPEENN/Galgame_Dataset の派生で、3,746,131 本の 16‑kHz オーディオ‑テキストペア（各ペアは ≤30 s）を含み、GPL v3 の下で公開されます。必須のオープンソースモデルと商用利用の厳格な禁止が付随します。
 * [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) - 📥 1k / ⭐ 15 / Reazon Speech v2 データセットのミラーで、8 本の A800 GPU を使って約 10 日でクリーンアップされた 4,096 本中 3,674 本のノイズ除去・BGM 削除済み音声ファイルをホストしています。利用は CDLA‑Sharing‑1.0 のライセンスの下で、日本著作権法第 30‑4 条に基づく場合に限ります。トランスクリプトは別途 all.tsv で提供されます。
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - 📥 1k / ⭐ 23 / 日本のエロゲ音声データセット（409.33 h）は、ffmpeg loudnormで音声を前処理し、**litagin/anime‑whisper**で自動文字起こしを行い、匿名化してデータをWebDataset（.tar FLAC, JSON, TXT）にパッケージ化し、MITライセンスの下で研究用途に公開しています。強い女性声のバイアスとAI文字起こしのエラーの可能性があることに注意してください。
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - 📥 1k / ⭐ 4 / Mandy Guo、Zihang Dai、そして Denny Vrandečić によってコンパイルされた Wiki40b データセットの日本語パートを再フォーマットしました。
 * [japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - 📥 1k / ⭐ 3 / Japanese Web Corpus 2010 は、Hugging Face にアップロードされ、研究利用向けに 2009 年の著作権法の元でライセンスされています。このコーパスには、形態素解析によって自動的に句読点が付与されたテキストと、対応する変換スクリプトが含まれています。
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - 📥 992 / ⭐ 8 / Rakuda は、歴史・社会・政府（自由回答）および地理（具体的）というカテゴリの日本語質問を40問提供し、AI アシスタントの日本語能力を評価し順位付けします。vicuna‑eval を模倣しており、load_dataset で読み込むことができます。
 * [japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos) - 📥 955 / ⭐ 31 / Japan Diverse Images Dataset は、都市・自然・歴史的・芸術的・日常的な日本のシーンを含む約11,810枚の高解像度JPEG画像を提供し、BLIPキャプション付きで合計約28.9 GBとなっており、AIトレーニング用に CC0‑1.0 でリリースされています。
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - 📥 895 / ⭐ 2 / Bokete crawler（CLoT‑Oogiri‑Go CVPR 2024のサブセット）から収集された日本語のみのデータセット。画像 → テキスト 500件、テキスト → テキスト 100件、テキスト画像 → テキスト 100件のタスクを提供し、GPT‑4o OCR とコンテンツフィルタリングで前処理済み。
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - 📥 883 / ⭐ 3 / AnimuSubtitle‑JPは、日本語字幕データセットで、Advanced SubStation Alpha (SSA/ASS) 形式で提供され、Pythonのassライブラリを使ってプログラム的に解析でき、Aegisubなどのツールで編集可能です。ODC‑BY ライセンスの下で公開されています。
 * [STAIR-Captions](https://huggingface.co/datasets/shunk031/STAIR-Captions) - 📥 814 / ⭐ 5 / STAIR‑Captions データセットは、画像キャプション作成、マルチモーダル検索、画像生成のために820,310件の日本語キャプションを提供します。このデータセットは2017年にキュレーションされ、Creative Commons Attribution 4.0 ライセンスの下で公開されました。
 * [defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter) - 📥 809 / ⭐ 2 / 5,000ツイートの日本語Twitterヘイトスピーチデータセット、ターゲット（person or unclear）とタイプ（threat、insult、devaluation、none）を3名のクラウドワーカーがアノテートしており、オリジナルツイートは含まれていないためAPI取得が必要です
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - 📥 249 / ⭐ 16 / japanese_alpaca_data のデータセットカードは、masa3141 の Japanese‑alpaca‑lora をベースにしています。
 * [J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - 📥 240 / ⭐ 32 / 約3900万文字の日本語研究語料集で、CC‑BY ライセンスの下にある1,343件のジャーナルおよび会議論文（最新の NLP 2024 Proceedings を含む）から抽出。言語モデルの事前学習（language‑model pre‑training）と検索拡張生成（retrieval‑augmented generation）への利用を想定し、将来的には新たな CC‑BY ライセンスの日本語論文を継続的に追加予定。
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - 📥 235 / ⭐ 2 / ABEJA-CC-JA は、Open Data AWS の ABEJA Common Crawl Japanese データセットの Hugging Face ミラーを提供しており、詳細は Abeja のテックブログに掲載されています。
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - 📥 234 / ⭐ 7 / 77匹のウマ娘キャラクターの音声文字起こしデータセットで、各キャラクターの合計発話時間（秒）が記載されています。
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - 📥 233 / ⭐ 6 / NFKC正規化を用いてクリーンアップおよび重複排除したmqaクエリ–パッセージペアで、pos_idsとneg_idsはコレクションのサブセットをインデックス化します（例：`collection[pos_id]`）。ライセンスは元のデータセットに準じます。
 * [llava-instruct-ja](https://huggingface.co/datasets/llm-jp/llava-instruct-ja) - 📥 229 / ⭐ 5 / 156 KサンプルのJapanese LLaVA‑Instruct データセットは、Azure OpenAIを介してGPT‑4o‑miniで生成され、CC BY 4.0のライセンスに準拠し、OpenAIの利用規約にも適合しています。
 * [jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points) - 📥 215 / ⭐ 4 / Japanese‑Wikipedia を派生した箇条書きデータセット。rinna/deepseek‑r1‑distill‑qwen2.5‑bakeneko‑32b で生成され、ランダムな重複サンプリングを使用しているため、生成されたサブセットにコレクションのサブセット項目が欠落していることがあります。改行は Hugging Face viewer で隠されています。CC‑BY‑SA 4.0 の下でリリース。
 * [WAON](https://huggingface.co/datasets/llm-jp/WAON) - 📥 208 / ⭐ 6 / WAON は、サイズと SigLIP ベースのフィルタリング、URL／caption／pHash による重複排除、豊富なメタデータを備えた大規模・高品質な日本語画像キャプションデータセットで、情報分析用途向けに Apache 2.0 ライセンス下で公開されています。
 * [alpaca_jp_math](https://huggingface.co/datasets/HachiML/alpaca_jp_math) - 📥 205 / ⭐ 6 / Synthetic Japanese math data (alpaca_jp_math) は、Stanford Alpaca と mistralai/Mixtral‑8x22B‑Instruct‑v0.1 を用いて生成され、コードとテキストの結果の一貫性を確認するプロンプトベースのチェックによってクリーンアップされ、Apache 2.0 ライセンスの下でリリースされ、Deepinfra を通じてアクセス可能です。
 * [r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa) - 📥 201 / ⭐ 5 / 日本語Wikipedia由来の質問と、cyberagent/DeepSeek‑R1‑Distill‑Qwen‑32B‑Japanese LLM によって生成された回答をペアにした CC‑BY‑SA 4.0 データセット。
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - 📥 199 / ⭐ 28 / Syosetu711K は、2023年3月に日本のサイト「小説家になろう」からスクレイピングされた 711,700 本の小説コレクションのデータセットカードです。全文とメタデータを提供し、教師なしテキスト生成研究に利用できます。
 * [chat-daily](https://huggingface.co/datasets/minnade/chat-daily) - 📥 193 / ⭐ 9 / MinnadeChatは、昼正午に毎日更新される公に編集可能な指示データセットで、HuggingFaceの`datasets` API（ブランチベースのリビジョンコントロール付き）を通じてアクセスでき、Creative Commons Zero 1.0 Universal ライセンスの下で配布されています。
 * [liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds) - 📥 189 / ⭐ 3 / 日本語版 liz‑nojaloli‑ja を訓練するために手作業で作成したデータセット。Qiita で参照された Python コードを含み、RLHF 用データの準備を目的としています。
 * [ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS) - 📥 188 / ⭐ 3 / ScreenTalk_JA2ZH-XSは、ParquetフォーマットでCC BY 4.0の下に配布される、約30時間相当の10,000サンプル（日本語音声–簡体字中国語テキストをペアにしたデータセットです。スピーチツーテキスト翻訳、マルチリンガルASR+MTの統合モデリング、マルチモーダルAI研究のために構築されています。
 * [msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives) - 📥 187 / ⭐ 3 / MS MARCOの日本語翻訳から派生したハードネガティブマイニングデータセットで、厳格な正規化、フィルタリング、選択をコサイン類似度と BAAI/BGE リランカー スコアを使用して行い、SPLADE リトリーブモデルに対して評価しました。
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - 📥 185 / ⭐ 2 / 匿名化された日本語の2ch.orgフォーラムスレッドを大規模コーパスとして、圧縮されたJSONLファイルで収集します。各ファイルにはスレッドレベルのメタデータと、ユーザー名、タイムスタンプ、元のHTMLコンテンツを含む投稿の配列が含まれています。
 * [JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - 📥 179 / ⭐ 29 / 公式サイトから手動で抽出した日本政府のFAQ Q‑Aペアは、CC‑BY‑4.0 ライセンスで提供され、ソースURLが注記されており、インストラクションチューニングとRAG使用のために利用できる。ただし、PDFから抽出されたエントリは文脈が欠けている場合がある。
 * [zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset) - 📥 179 / ⭐ 12 / zzen‑v2.5‑dataset は、約190 M の JSONL レコード（左文脈、入力、出力）を含む日本語カナ→漢字変換データを提供し、zenz‑v2.5 言語モデル（xsmall、small、medium）を訓練し、AJIMEE‑Bench ベンチマークを公開します。データは CC‑BY‑SA‑4.0 ライセンス付き Wikipedia と Common‑Crawl のサブセットから取得されています。
 * [JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - 📥 178 / ⭐ 5 / 日本語中心の4.7 b トークンと0.9 b 英語コードを含むデータセットは、CC‑100、OSCAR‑2301、HPLT v1.2、wiki40b‑ja からコンパイルされ、LOCAL AI HACKATHON #000 calm2‑chat 用に Contrail‑200m‑64k の事前学習に使用されたが、センテンス境界やパープレキシティフィルタリングは行われていない。
 * [Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - 📥 171 / ⭐ 11 / Japanese‑Heron‑Benchは、日本語のVLMベンチマークで、アニメ、アート、文化、食べ物、風景、ランドマーク、交通にわたる21枚のパブリックドメインまたはCC‑BY画像を特徴とし、会話・詳細・複雑の3カテゴリに分かれた102問の質問が付随しています。
