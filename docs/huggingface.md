# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1388 models and 543 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èªž (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## ðŸ“– Contents

Released [a tool ðŸ”Ž](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search) for searching through a large number of repository information.

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).
Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Multimodality](#Multimodality)
	 * [Text Generation](#Text-Generation)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Reasoning](#Reasoning)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## ðŸŽ‰ The latest additions

**Models**
13 models have been added.

- [reazon-research/japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh)
- [Tetsuo3003/ner-medical-japanese](https://huggingface.co/Tetsuo3003/ner-medical-japanese)
- [dahara1/gemma-3-1b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-1b-it-qat-japanese-imatrix)
- [EQUES/JPharmatron-7B](https://huggingface.co/EQUES/JPharmatron-7B)
- [stockmark/Stockmark-2-VL-100B-beta](https://huggingface.co/stockmark/Stockmark-2-VL-100B-beta)
- [shisa-ai/shisa-v2-llama3.1-405b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b)
- [mmnga/llm-jp-3.1-8x13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf)
- [yasu-oh/Llama-3-Swallow-Infused-R1776-70B](https://huggingface.co/yasu-oh/Llama-3-Swallow-Infused-R1776-70B)
- [tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF](https://huggingface.co/tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF)
- [Aratako/MistralPrism-24B](https://huggingface.co/Aratako/MistralPrism-24B)
- [ayousanz/modernbert-vtuber-finetuned](https://huggingface.co/ayousanz/modernbert-vtuber-finetuned)
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated)
- [OmeletRice/japanese-wav2vec2-base-bf16-asmr](https://huggingface.co/OmeletRice/japanese-wav2vec2-base-bf16-asmr)


**Datasets**
8 datasets have been added.

- [UEC-InabaLab/KokoroChat](https://huggingface.co/datasets/UEC-InabaLab/KokoroChat)
- [DataPilot/Zero_SFT_Ja_v3.5](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3.5)
- [OmniAICreator/Japanese-Wikipedia-202506](https://huggingface.co/datasets/OmniAICreator/Japanese-Wikipedia-202506)
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k)
- [clnine/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-nikkei225)
- [clnine/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-financial-terms)
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted)
- [nguyenthanhasia/japanese-bar-exam-qa](https://huggingface.co/datasets/nguyenthanhasia/japanese-bar-exam-qa)


## ðŸ§  Models

This list is sorted by downloads as of June 10, 2025.
1388 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 2,926,527
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from xlm-roberta-base using a dataset from Japanese Wikipedia, classifying tokens into PER, ORG, or other categories.
  - Downloads: 658,703
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M parameter text embedding model by Retrieva, built on modernbert-ja-310m, and designed for both Japanese and English text.
  - Downloads: 435,709
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri provides Japanese general text embeddingsâ€”including v3 models up to 315M parameters with a max length of 8192â€”for use with the Sentence Transformers library and requires `fugashi`, `sentencepiece`, and `unidic-lite` for installation.
  - Downloads: 314,990
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - Rinnaâ€™s Japanese CLOOB model, built upon CLIP, enables image-text matching for Japanese language tasks and is installable via pip.
  - Downloads: 265,073
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained on Japanese text using IPA dictionary-based word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 197,966
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B is a top-performing Japanese text embedding model by Preferred Networks for tasks like information retrieval, classification, and clustering, achieving state-of-the-art results on the JMTEB benchmark.
  - Downloads: 197,323
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This repository provides a refined Japanese Sentence-BERT model (v2) â€“ built on cl-tohoku/bert-base-japanese-whole-word-masking and utilizing MultipleNegativesRankingLoss for improved accuracy over v1, requiring fugashi and ipadic for inference.
  - Downloads: 167,177
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - This repository provides a Japanese BERT model pre-trained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved language modeling.
  - Downloads: 132,476
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - This repository provides a Japanese BERT model pre-trained on CC-100 and JA Wiki data using Unidic-lite word-level tokenization and whole word masking.
  - Downloads: 129,239
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - This repository provides a Japanese BERT base model pre-trained on Japanese text using a word/character tokenization scheme based on the IPA dictionary, mirroring the original BERT base architecture.
  - Downloads: 107,533
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - This repository provides a Japanese BERT model pre-trained on CC-100 and JAWIKI datasets, utilizing character and word-level tokenization with Unidic and whole word masking for improved language understanding.
  - Downloads: 106,484
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - This repository provides a pre-trained Japanese DeBERTa V2 tiny modelâ€”trained on Japanese Wikipedia, CC-100, and OSCARâ€”for masked language modeling tasks.
  - Downloads: 79,649
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - This repository provides a Japanese BERT base model pretrained on Japanese text with word-level tokenization using the IPA dictionary, featuring a standard 12-layer, 768-dimension architecture.
  - Downloads: 56,673
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This repository provides a Japanese Sentence-BERT model (version 1) for generating sentence embeddings, with a more accurate version 2 also available, utilizing the `transformers` library and PyTorch.
  - Downloads: 53,008
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and aims to respect word boundaries during inference.
  - Downloads: 49,050
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri provides pre-trained Japanese text embeddings using Sentence Transformers, requiring installation of supporting libraries and a specific prefix ("ã‚¯ã‚¨ãƒª:" or "æ–‡ç« :") for input texts.
  - Downloads: 46,496
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese sentence embedding model, built on LUKE and trained on diverse data, designed for tasks like semantic similarity and search.
  - Downloads: 44,581
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 38,452
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 33,297
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced 8B language model built on Meta Llama 3, optimized through pre-training and instruction tuning by ELYZA, Inc.
  - Downloads: 29,764
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - This repository provides a BERT-based model fine-tuned for Japanese sentiment analysis of Amazon product reviews, classifying sentence polarity as positive or negative.
  - Downloads: 19,712
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow are 8B/70B large language models continually pre-trained on a massive Japanese web corpus, enhancing Japanese language capabilities while maintaining English proficiency based on Metaâ€™s Llama 3.1.
  - Downloads: 18,145
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository hosts a medium-sized Japanese GPT-2 language model trained by rinna Co., Ltd., and readily usable with the `transformers` library.
  - Downloads: 16,802
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jnli) fine-tuned on the JGLUE MARC-ja dataset for natural language inference tasks, as demonstrated in the book â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 16,253
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - This repository provides a Japanese BERT model pretrained on Jawiki+20200831 using Unidic-lite word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 15,385
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - This repository provides a Japanese DeBERTa V2 large language model pre-trained on diverse Japanese text corpora using character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 15,085
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14,385
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering longer sequence support (8192 tokens) and an expanded vocabulary for improved performance and efficiency.
  - Downloads: 13,864
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitionsâ€™ repository hosts sarashina2.2-3b-instruct-v0.1, a Japanese autoregressive language model evaluated on both Japanese and English tasks, alongside performance comparisons with other models.
  - Downloads: 13,368
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring installation of `sentence-transformers`, `fugashi`, `sentencepiece`, and `unidic-lite` and the addition of specific prefixes ("ã‚¯ã‚¨ãƒª:" or "æ–‡ç« :") for effective query/passage encoding.
  - Downloads: 11,754
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese is a pre-trained DistilBERT model for Japanese natural language processing, built by LINE Corporation on 131GB of web text and based on their in-house BERT-base model.
  - Downloads: 11,445
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - This repository provides a Japanese sentence-transformers model, `sbert-jsnli-luke-japanese-base-lite`, for embedding sentences into 768-dimensional vectors suitable for semantic search and clustering, fine-tuned on JSNLI data.
  - Downloads: 10,854
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model built on Whisper v2.0, enhanced with speaker diarization, punctuation, and integrated into Hugging Face Transformers (v4.39+).
  - Downloads: 10,832
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - This repository provides a Japanese BERT large model, pre-trained on CC-100 and JAWIKI with Unidic-lite word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 10,654
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a multilingual BERT-based model providing sentence embeddings for 109 languages, trained with masked and translation language modeling for tasks like bi-text retrieval, and converted from TensorFlow to PyTorch.
  - Downloads: 10,372
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, trained on 312.5B Japanese tokens with a perplexity of 8.68, based on the EleutherAI/gpt-neox architecture.
  - Downloads: 10,216
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository hosts a base-sized Japanese RoBERTa model trained by rinna Co., Ltd., and loadable via the `transformers` library for masked language modeling tasks.
  - Downloads: 9,596
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jsts) fine-tuned on the JSTS dataset for calculating semantic similarity, as demonstrated in Chapter 5 of â€œIntroduction to Large Language Models.â€
  - Downloads: 8,089
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and distributed as a PyPI package with GiNZA v5 for enhanced Japanese NLP pipelines.
  - Downloads: 7,870
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 7,127
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a continually pre-trained language model built on Metaâ€™s Llama 3, enhanced with Japanese language data and available in 8B and 70B Instruct/Chat versions released July 1, 2024.
  - Downloads: 6,379
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - This repository hosts the llm-jp-3-7.2b-instruct3, a 7.2 billion parameter Japanese large language model developed by NII, utilizing Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 6,316
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - This repository details an experiment applying differences extracted via a chat-vector approach between the smaller suzume-llama-3-8B-japanese and meta-llama/Meta-Llama-3-8B-Instruct models to the larger meta-llama/Meta-Llama-3-70B-Instruct, yielding minimal change and suggesting future exploration of scaling factors.
  - Downloads: 6,237
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - This repository details a 90M parameter Japanese character-level GPT-2 Small language model pre-trained on diverse Japanese text data and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 6,034
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri provides Japanese general text embeddings with v3 models (30M-315M parameters, max length 8192) achieving up to 77.24 JMTEB, utilizing Sentence Transformers, Fugashi, Sentencepiece, and Unidic-lite.
  - Downloads: 5,960
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, offering models for improved Japanese text retrieval and ranking.
  - Downloads: 5,837
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - This repository offers a 3.6 billion parameter Japanese GPT-NeoX model finetuned for instruction-following conversation using translated Anthropic HH and FLAN datasets.
  - Downloads: 5,729
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M is a computationally efficient Japanese BERT model trained on 4.09T tokens, utilizing local & global attention and RoPE for improved long sequence handling with a 102,400 vocabulary.
  - Downloads: 5,679
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1 billion parameter English/Japanese language model utilizing a hybrid Mamba/attention architectureâ€”like Samba, but with added normalization for improved training stability.
  - Downloads: 5,641
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Metaâ€™s Llama 3.1, significantly enhancing Japanese language capabilities alongside retained English proficiency using a 200 billion token corpus.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository hosts a small Japanese GPT-2 model trained by rinna Co., Ltd., readily usable with the `transformers` library for causal language modeling.
  - Downloads: 4,489
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a state-of-the-art Japanese text embedding modelâ€”based on Sarashina2.1-1Bâ€”that generates 1792-dimensional dense vectors for tasks like semantic search and similarity.
  - Downloads: 4,423
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-14B and 32B Japanese language models, utilizing the TFMC/imatrix dataset, and intended for use with llama.cpp.
  - Downloads: 4,360
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 tiny model pre-trained on large Japanese text corpora for masked language modeling tasks.
  - Downloads: 4,317
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - Rinnaâ€™s Japanese HuBERT Base model is a 12-layer transformer trained on 19,000 hours of Reazon Japanese speech data, replicating the original HuBERT Base architecture.
  - Downloads: 4,200
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - This repository provides a GGUF-formatted version of the AXCXEPT-phi-4-deepseek-R1K-RL-EZO language model, trained with imatrix data, and intended for use with llama.cpp.
  - Downloads: 4,108
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly improved accuracy, particularly in qualitative evaluations, and requiring SentencePiece for inference.
  - Downloads: 3,989
- [reazon-research/japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh)
  - This repository provides a Japanese wav2vec 2.0 base model fine-tuned on the ReazonSpeech v2.0 corpus for automatic speech recognition (ASR), usable via the transformers library with optimized settings.
  - Downloads: 3,939
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - Rinnaâ€™s Japanese wav2vec2.0 Base model is a 12-layer transformer trained on ~19,000 hours of Japanese speech data, replicating the original wav2vec 2.0 Base architecture.
  - Downloads: 3,934
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - This repository details a v1.1 T5 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 3,892
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - llava-calm2-siglip is an experimental Vision Language Model enabling Japanese-language question answering about images using PIL, Transformers, and PyTorch.
  - Downloads: 3,879
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the deepseek-r1-distill-qwen2.5-bakeneko-32b language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 3,795
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper provides fast, distilled Japanese Automatic Speech Recognition (ASR) modelsâ€”based on OpenAI's Whisper large-v3 and compatible with whisper.cppâ€”developed by Asahi Ushio and Kotoba Technologies.
  - Downloads: 3,624
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings from Wikipedia to generate contextualized representations of words and entities.
  - Downloads: 3,515
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository provides a series of Japanese cross-encoder rerankersâ€”including xsmall, small, base, and large models with varying layer and hidden sizesâ€”for improved text retrieval performance.
  - Downloads: 3,441
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - Rinna's japanese-gpt-1b is a 1.3B-parameter Japanese GPT model for causal language generation, readily usable with Hugging Face Transformers.
  - Downloads: 3,399
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - This repository provides a 3.6 billion parameter Japanese GPT-NeoX language model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 3,317
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - This repository provides a Japanese DeBERTa V3 base model pre-trained on the LLM-jp corpus, suitable for masked language modeling tasks using the `transformers` library.
  - Downloads: 3,309
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri provides large Japanese text embeddings using Sentence Transformers, requiring `fugashi`, `sentencepiece`, and `unidic-lite`, and necessitates prefixes like "ã‚¯ã‚¨ãƒª: " or "æ–‡ç« : " for proper inference.
  - Downloads: 3,293
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 is a Japanese language model distilled from ABEJA-Qwen2.5-32b-Japanese-v0.1 and based on Qwen2.5-7B-Instruct, enhanced for instruction following using ChatVector differences.
  - Downloads: 3,261
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter large language model based on Meta Llama 3, provided in quantized GGUF (Q4_K_M) format for use with llama.cpp.
  - Downloads: 3,215
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository hosts rinna's extra-small Japanese GPT-2 model for causal language modeling, readily usable with the `transformers` library.
  - Downloads: 3,201
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides quantized GGUF versions of the rinna/qwen2.5-bakeneko-32b-instruct model, compatible with llama.cpp applications, and including variations like AWQ and GPTQ quantization.
  - Downloads: 3,003
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M is a Japanese BERT model variant, trained on 4.39T tokens, that utilizes local and global attention with RoPE for efficient handling of long sequences and boasts a 102,400 vocabulary size.
  - Downloads: 2,670
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with word-level and character-level tokenization, and whole word masking for improved language understanding.
  - Downloads: 2,662
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following.
  - Downloads: 2,559
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Metaâ€™s Llama 3, enhanced with Japanese language data and available in 8B and 70B Instruct and Chat versions released July 1, 2024.
  - Downloads: 2,556
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - This repository provides a high-performance Japanese SPLADE v2 model for converting text to sparse vectors, offering a WebUI demo and utilizing YAST/YASEM for inference, token inspection, and training.
  - Downloads: 2,407
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including the hotchpotch/japanese-bge-reranker-v2-m3-v1 model, with varying layer and hidden size configurations for improved sentence re-ranking performance.
  - Downloads: 2,314
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository provides a Japanese-fine-tuned version of DeepSeek-R1-Distill-Qwen-14B, enabling it to output thought processes and generate textâ€”like novel introductionsâ€”directly in Japanese.
  - Downloads: 2,277
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - This repository provides a fine-tuned Japanese BERT model (llm-book/bert-base-japanese-v3-marc_ja) for sentiment analysis, trained on the JGLUE MARC-ja dataset and detailed in the book â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 2,275
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to improve search relevance by scoring sentence pairs based on their semantic similarity.
  - Downloads: 2,230
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - Youri-7b is a continually pre-trained 7 billion parameter language model based on Llama 2, enhanced with 40B Japanese/English tokens to improve performance on Japanese tasks.
  - Downloads: 2,214
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the CyberAgent DeepSeek-R1-Distill-Qwen-32B Japanese language model, trained with the TFMC/imatrix dataset, for use with llama.cpp.
  - Downloads: 2,212
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Metaâ€™s Llama 3.3, significantly enhancing Japanese language capabilities while maintaining English proficiency via a 315 billion token corpus.
  - Downloads: 2,209
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - This repository hosts the llm-jp-3-1.8b-instruct3, a 1.8 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 2,185
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) fine-tuned on JAWiki for unsupervised SimCSE sentence similarity tasks, as detailed in chapter 8 of â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 2,118
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - Rinna's Japanese HuBERT Large is a 24-layer transformer model trained on 19,000 hours of Japanese speech data using the official HuBERT training code and corpus.
  - Downloads: 2,111
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a 1 billion parameter decoder-only language model pre-trained on Japanese data by CyberAgent, Inc., and accessible via Hugging Face Transformers.
  - Downloads: 2,104
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, offering extended sequence length (8192 tokens) and a larger vocabulary (100K tokens) with FlashAttention for improved performance and efficiency.
  - Downloads: 2,042
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruction-tuned variants) developed by NII, formatted for Hugging Face Transformers and requiring specific library versions.
  - Downloads: 1,995
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese sentence embedding model, distilled from a large language model, designed for semantic similarity measurement and long text retrieval with a 1024 token limit.
  - Downloads: 1,979
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a 13B-parameter Japanese large language model, fine-tuned from RakutenAI-2.0-8x7B, to improve instruction-following and overall AI capabilities.
  - Downloads: 1,950
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - Cyberagentâ€™s DeepSeek-R1-Distill-Qwen-14B-Japanese is a Japanese-finetuned 14B language model based on DeepSeek-R1-Distill-Qwen, optimized for use with the `transformers` library.
  - Downloads: 1,930
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 1,909
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base is a Japanese language model based on modernBERT, trained on a 3.4TB corpus with an 8192 token max length, requiring the transformers library for use.
  - Downloads: 1,860
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri provides pre-trained Japanese text embeddings (v3 models ranging from 30M to 315M parameters) for use with the Sentence Transformers library, achieving up to 77.24 JMTEB scores with a max sequence length of 8192.
  - Downloads: 1,844
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - ABEJA's repository hosts a large Japanese GPT-2 model for text generation, requiring sentencepiece installation and utilizing the `transformers` pipeline.
  - Downloads: 1,824
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 base model pre-trained on large Japanese text corpora for masked language modeling tasks.
  - Downloads: 1,795
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,779
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - This repository provides a 32K vocabulary T5 model pre-trained on large-scale Japanese web text data (mC4, Wiki40b) with accompanying training code.
  - Downloads: 1,712
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - StableLM-3B-4E1T is a 3 billion-parameter Japanese language model built upon StableLM-3B-4E1T via continued pretraining on Japanese data for enhanced performance.
  - Downloads: 1,710
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering extended sequence length (8192 tokens) and a larger 100K vocabulary for improved performance and efficiency.
  - Downloads: 1,686
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - This repository provides GGUF format conversions of ELYZAâ€™s Japanese Llama 2 and CodeLlama models, including a faster, token-cost-reduced 7b instruct version.
  - Downloads: 1,683
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese language model developed by LINE Corporation, offering code examples for text generation using the Transformers library.
  - Downloads: 1,670
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - LINE Corporationâ€™s repository hosts a 1.7B parameter Japanese language model, fine-tuned for instruction-following and dialogue performance.
  - Downloads: 1,616
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - LINE Corporationâ€™s repository hosts a 3.6B parameter Japanese language model fine-tuned for instruction-following and improved conversational ability.
  - Downloads: 1,613
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B is a continually pre-trained 8B parameter Llama 3 modelâ€”enhanced with 22B Japanese/English tokensâ€”demonstrating improved performance on Japanese language tasks, available in base and instruction-tuned versions with HF and GPTQ formats.
  - Downloads: 1,602
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - This repository hosts ABEJA's 2.7B-parameter Japanese GPT-NeoX model, compatible with transformers v4.23+, for text generation tasks.
  - Downloads: 1,568
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository provides a Japanese Sentence BERT base model, pretrained on colorfulscoop/bert-base-ja v1.0 and fine-tuned using the Japanese SNLI dataset for sentence similarity tasks.
  - Downloads: 1,565
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to improve relevance by scoring sentence pairs given an input query.
  - Downloads: 1,554
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - This repository details a 717M parameter Japanese character-level GPT-2 language model pre-trained on large Japanese text corpora and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 1,545
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,541
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - KoichiYasuoka/roberta-small-japanese-luw-upos is a RoBERTa-small model pre-trained on Aozora Bunko texts for Japanese POS-tagging and dependency parsing with Universal Part-of-Speech tagging of long-unit words.
  - Downloads: 1,501
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, accessible via Hugging Face Transformers.
  - Downloads: 1,482
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,457
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - This repository provides a Japanese SimCSE model (pkshatech/simcse-ja-bert-base-clcmlp) for generating sentence embeddings from Japanese text, built on cl-tohoku/bert-base-japanese-v2 and trained with the JSNLI dataset, requiring fugashi and unidic-lite for tokenization.
  - Downloads: 1,456
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model, fine-tuned for instruction-following using datasets like Dolly-15k, with smaller 7B and optimized versions also available.
  - Downloads: 1,451
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter decoder-only language model fine-tuned on diverse Japanese data, built upon Llama-2-70b, for optimal performance on Japanese language tasks.
  - Downloads: 1,435
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, enhanced with additional pre-training for improved Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 1,423
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B is a 7B parameter decoder-only language model pre-trained on 1.3T Japanese and English tokens, requiring transformers >= 4.34.1 for usage.
  - Downloads: 1,422
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository offers Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned variants) developed by NIIâ€™s R&D Center for LLMs, utilizing Hugging Face Transformers and requiring PyTorch >= 2.3.0.
  - Downloads: 1,418
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained for improved instruction-following and assistant-like responses.
  - Downloads: 1,408
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow are enhanced 8B and 70B large language models continually pre-trained on Llama 3.1, significantly improving Japanese language capabilities alongside retained English proficiency using a 200 billion token corpus.
  - Downloads: 1,399
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8B parameter language model, pre-trained on 1.3T tokens and fine-tuned for dialogue using SFT and DPO, with available quantized versions like AWQ, GPTQ, and GGUF (though GGUF performance is cautioned).
  - Downloads: 1,393
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter large language model pretrained on a 220B Japanese token corpus by Stockmark Inc., with an instruction-tuned version available and supported by AWS.
  - Downloads: 1,377
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese and multilingual data, with a fine-tuned chat version (Karakuri LM Chat) leveraging continual learning and the SteerLM technique.
  - Downloads: 1,370
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built on Llama 2 with additional pre-training to enhance its Japanese capabilities, optimized for fast inference.
  - Downloads: 1,366
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x1.8b-instruct3 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 1,364
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX model, finetuned with a new data split for improved instruction-following conversational ability.
  - Downloads: 1,326
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - This repository details a Japanese-pre-trained T5 v1.1 Transformer modelâ€”featuring GEGLU activation and no pre-training dropoutâ€”with updated size designations ("xl", "xxl").
  - Downloads: 1,325
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - Stockmark-100b is a 100 billion parameter language model pretrained by Stockmark Inc. on a 910 billion token Japanese/English corpus, with an instruction-tuned version (stockmark-100b-instruct-v0.1) available and supported by GENIAC.
  - Downloads: 1,324
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B Japanese language model, trained on the imatrix dataset and usable with llama.cpp.
  - Downloads: 1,302
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - LINE Corporationâ€™s japanese-large-lm-1.7b is a 1.7 billion parameter Japanese language model for text generation, readily usable with Hugging Face Transformers.
  - Downloads: 1,283
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - This repository provides Stability AI's Japanese Stable LM Instruct Gamma 7B, a 7-billion parameter decoder-only language model fine-tuned for instruction following, built upon the Japanese Stable LM Base Gamma 7B.
  - Downloads: 1,283
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-7B is a 7 billion parameter Llama-2-based language model fine-tuned on Japanese data with an expanded vocabulary to enhance performance on Japanese language tasks.
  - Downloads: 1,275
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese and multilingual data, with a fine-tuned chat version (Karakuri LM Chat) leveraging SteerLM and continual learning.
  - Downloads: 1,270
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is a 13 billion parameter, instruction-tuned Japanese LLM developed by Stockmark Inc., utilizing data from the Project of Development of Japanese Instruction data for LLM.
  - Downloads: 1,257
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Nekomata-7b is a continually pre-trained version of Qwen-7b on 30B Japanese/English tokens, enhancing performance on Japanese tasks with an expanded vocabulary and long sequence support.
  - Downloads: 1,250
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-7B is a 7B-parameter language model fine-tuned for instruction following, utilizing an expanded Japanese vocabulary for improved performance.
  - Downloads: 1,248
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 1,247
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository offers a small Japanese GPT-NeoX language model, trained with EleutherAIâ€™s code and compatible with Hugging Faceâ€™s transformers library for causal language modeling.
  - Downloads: 1,242
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - Stockmarkâ€™s GPT-NeoX-Japanese-1.4B is a 1.4 billion parameter language model pre-trained on a 20 billion token Japanese corpus, offering efficient inference with `bfloat16` or `float16` precision.
  - Downloads: 1,235
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B is a 7B-parameter decoder-only language model fine-tuned for instruction-following, based on japanese-stablelm-base-beta-7b and publicly available datasets, with larger 70B and faster versions also available.
  - Downloads: 1,216
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of Mistral-7B-Instruct-v0.3, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and demonstrating usage with llama.cpp for Japanese language tasks.
  - Downloads: 1,213
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
  - LLM-jp-3.1-13b-instruct4 is a 13 billion parameter Japanese large language model from NII, improved with instruction pre-training for enhanced instruction following capabilities, building on the LLM-jp-3 series.
  - Downloads: 1,211
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - Rinna's nekomata-14b is a continually pre-trained version of Qwen-14b on 66B Japanese and English tokens, enhancing performance on Japanese tasks with an expanded vocabulary for improved efficiency.
  - Downloads: 1,211
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M is a computationally efficient Japanese BERT model trained on 4.39T tokens with RoPE and a 102,400 vocabulary, designed for handling long sequences.
  - Downloads: 1,207
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - This repository hosts LLM-jpâ€™s collaborative, Japanese-focused large language models, including v1.0 and v1.1 instruction-tuned variants like llm-jp-13b, built using techniques like DPO, LoRA, and RLHF.
  - Downloads: 1,198
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B is a 7B-parameter language model fine-tuned on diverse Japanese data, built upon Llama-2-7b, and designed for high performance in Japanese language tasks, with a matching instruction-following version available.
  - Downloads: 1,184
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - This repository provides a GGUF formatted conversion of the aixsatoshi Llama-3-8b-Cosmopedia-japanese model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and related models from aixsatoshi and mmnga.
  - Downloads: 1,166
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - This repository provides a GGUF conversion of the AIBunCho-developed japanese-novel-gpt-j-6b model, intended for use with llama.cpp, though compatibility may vary with future updates.
  - Downloads: 1,143
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B is a 7-billion parameter decoder-only language model pretrained on Japanese data, building upon Mistral-7B-v0.1 to enhance Japanese language performance.
  - Downloads: 1,127
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - This repository provides a pretrained Japanese RoBERTa base model, trained on Japanese Wikipedia and CC-100, for tasks like masked language modeling.
  - Downloads: 1,116
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - Cyberagentâ€™s Mistral-Nemo-Japanese-Instruct-2408 is a continually pre-trained Japanese language model built upon Mistral-Nemo-Instruct-2407, designed for instruction-following tasks and usable with the `transformers` library.
  - Downloads: 1,106
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring extended sequence length support (8192 tokens) and a larger 100K vocabulary for improved performance and efficiency.
  - Downloads: 1,087
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x13b-instruct3 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 1,078
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, a Japanese language model optimized for speed and reduced token cost, alongside other related models like CodeLlama versions.
  - Downloads: 1,070
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7B-parameter language model pre-trained on Japanese and English data to excel in Japanese language modeling and downstream tasks, with an instruction-following version also available.
  - Downloads: 1,050
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,040
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is a 13B LLaMA-based language model pre-trained on English and Japanese datasets and released by Preferred Networks under the Apache 2.0 license, utilizing libraries like transformers and offering text generation capabilities.
  - Downloads: 1,032
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.2ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,020
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large, including a BGE model) with varying layer and hidden sizes for improved information retrieval, alongside technical reports and usage examples.
  - Downloads: 1,007
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the Moonshot AI Moonlight-16B-A3B-Instruct model, trained with TFMC/imatrix-dataset-for-japanese-llm, and intended for testing with llama.cpp.
  - Downloads: 999
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository hosts the llm-jp-3-13b-instruct3, a 13 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 999
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 982
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository provides GGUF format conversions of the open-calm-7b language model, alongside versions of 3b and 1b, for use with llama.cpp, acknowledging potential future incompatibility with mainline llama.cpp updates.
  - Downloads: 978
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese Llama 2 7B models, including instruct and fast variants, and CodeLlama versions for improved performance and reduced token costs.
  - Downloads: 972
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - This repository provides a GGUF-formatted version of Stability AI's japanese-stablelm-2-instruct-1_6b model, trained with the imatrix dataset, requiring agreement to its terms of use and membership for commercial applications.
  - Downloads: 928
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 919
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - This repository provides GGUF format conversions of the Vecteus-v1 language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp for Japanese language processing.
  - Downloads: 893
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B language model, supported by a16z and enabled by hardware from Massed Compute.
  - Downloads: 893
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - This repository provides a Japanese BERT base model fine-tuned on the WRIME dataset for predicting emotion intensities (eight emotions) in Japanese tweets, as detailed in a related research paper.
  - Downloads: 888
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - Cyberagentâ€™s DeepSeek-R1-Distill-Qwen-32B-Japanese is a finetuned 32B parameter language model for Japanese, easily usable with the `transformers` library.
  - Downloads: 877
- [EQUES/JPharmatron-7B](https://huggingface.co/EQUES/JPharmatron-7B)
  - JPharmatron-7B is a 7B language model built on Qwen2.5, pre-trained on Japanese & English pharmaceutical data, and enhanced for conversational abilities.
  - Downloads: 863
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B is a Japanese language model created by evolutionarily merging four powerful base modelsâ€”Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004-7b.
  - Downloads: 856
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, offering various quantizations (including IQ1_S at 2.0GB) for use with tools like those detailed in TheBloke's READMEs.
  - Downloads: 847
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides a llama.cpp quantized version of the Mistral-Nemo-Japanese-Instruct-2408 model, a continually pre-trained Japanese language model based on Mistral-Nemo-Instruct-2407.
  - Downloads: 760
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - This repository provides a GGUF-formatted version of the r1-1776-distill-llama-70b language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 754
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository details a Japanese typo detection modelâ€”based on RoBERTaâ€”that identifies and scores the probability of various character-level errors like deletions, insertions, substitutions, and kanji misconversions within text.
  - Downloads: 745
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct, enhanced for instruction following using ChatVector and detailed in the linked ABEJA tech blog.
  - Downloads: 732
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF æ¦‚è¦ Aratako/calm3-22b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 731
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of the TokyoTech Llama-3.1-Swallow-8B-Instruct-v0.3 language model, trained with the imatrix Japanese dataset and usable with llama.cpp.
  - Downloads: 723
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - This repository provides GGUF-formatted versions of the c4ai-command-r-plus language model, trained on the imatrix dataset, requiring file concatenation for larger quantized versions like Q6_K and Q8_0, and usable with llama.cpp.
  - Downloads: 714
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - This repository provides a 3B-parameter Japanese decoder-only language model, fine-tuned for instruction following, based on Japanese StableLM-3B-4E1T Base.
  - Downloads: 699
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 70B model, supported by a16z and enabled by Massed Compute hardware.
  - Downloads: 695
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository hosts GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B Japanese language model, including a potentially better 32B version.
  - Downloads: 685
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - This repository provides a Japanese-specific DeBERTa V3 model designed for inference without morphological analysis, and with improved handling of word boundaries.
  - Downloads: 672
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a 13B parameter, Apache 2.0 licensed, Japanese instruction-tuned language model built on PLaMo-13B with an 8192 context length.
  - Downloads: 663
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-base Japanese model pre-trained on the Aozora corpus for dependency parsing and question answering, specifically designed to handle long-unit words with masked context for disambiguation.
  - Downloads: 660
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
  - ABEJA-Qwen2.5-32b-Japanese-v1.0 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct and further refined with SFT and DPO training.
  - Downloads: 657
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese Llama 2 and CodeLlama 7b models, including a fast variant with reduced token cost and increased speed.
  - Downloads: 649
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of Microsoftâ€™s Phi-3-mini-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 634
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2's Japanese language capabilities via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English proficiency.
  - Downloads: 623
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - This repository provides a GGUF quantized version of the Aratako/calm3-22b-RP-v2 model, distributed under a CC-BY-NC-SA 4.0 license due to training data derived from OpenAIâ€™s GPT-4o-mini and Anthropicâ€™s Claude 3.5 Sonnet.
  - Downloads: 620
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-base model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 615
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training to enhance its Japanese capabilities, designed for assisting with code and general tasks.
  - Downloads: 610
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - This repository provides a pre-trained Japanese ALBERT model (â€œken11/albert-base-japanese-v1â€) intended for fine-tuning on downstream tasks, with specific instructions for handling the [MASK] token when using Sentencepiece tokenizer.
  - Downloads: 599
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA's Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter large language model based on Meta Llama 3, available in AutoAWQ quantized format.
  - Downloads: 590
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - This repository provides a T5-base model fine-tuned on the Livedoor News corpus for text summarization, as featured in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 583
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a non-commercial, 13B parameter language model fine-tuned on Japanese datasets for instruction following, built upon the 8192-context PLaMo-13B base model and released under CC-BY-NC-4.0.
  - Downloads: 583
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - This repository provides a fine-tuned Japanese BERT model (â€œbert-base-japanese-jsnliâ€) for zero-shot text classification, achieving 92.88% accuracy on the JSNLI dataset.
  - Downloads: 576
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository hosts the llm-jp-3-980m-instruct3, a Japanese large language model developed by NII's R&D Center for LLMs, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 569
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - This repository provides a Waseda RoBERTa model finetuned for evaluating truthfulness of generated answers on the JTruthfulQA dataset.
  - Downloads: 547
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1 to significantly enhance Japanese language capabilities while preserving English proficiency, using a 200 billion token corpus.
  - Downloads: 547
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 is a 7B parameter language model fine-tuned for instruction following, building upon the Japanese-StableLM-Base-Alpha-7B and specializing in Japanese text generation.
  - Downloads: 546
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-70b-chat-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 545
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the qwq-bakeneko-32b language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 538
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository provides GGUF-formatted versions of the Fugaku-LLM-13B-instruct large language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and requiring acceptance of the provided terms of use.
  - Downloads: 525
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - This repository hosts llm-jp-3-440m-instruct3, a 440M parameter Japanese language model from NIIâ€™s LLM-jp-3 series, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 524
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-T2-2B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 516
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-Preferred-MedSwallow-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 503
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - This repository hosts the llm-jp-3-150m-instruct3 large language model, developed by NII's R&D Center for LLMs, in Hugging Face Transformers format with required dependencies like torch and transformers.
  - Downloads: 499
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - This repository provides GGUF formatted weights and instructions for running the ELYZA-japanese-Llama-2-13b-fast-instruct model with LlamaEdge, utilizing a 5120 context size and specific prompt formatting.
  - Downloads: 496
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - This repository hosts llm-jp-3-980m, a 980 million parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers using torch and tokenizers.
  - Downloads: 485
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-13b-instruct3 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp (excluding custom chat templates/`-cvn` flag).
  - Downloads: 473
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - This repository provides a 6B parameter Japanese GPT-2 language model, fine-tuned on 6.93 million *okashi* (comedic response) examples and pre-trained on a 47.7 billion token Japanese corpus, optimized for AWS trn1 instances using the Megatron-LM/Neuron framework under the Apache 2.0 license.
  - Downloads: 456
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
  - This repository provides a GGUF formatted version of the Gemma-2-Llama-Swallow-9b-it-v0.1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 456
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-small-japanese) for improved speech recognition, trained on Japanese datasets and requiring 16kHz input audio.
  - Downloads: 455
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-mini-instruct language model, trained on the TFMC/imatrix dataset, and intended for use with llama.cpp.
  - Downloads: 445
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 431
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èªž SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 426
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 410
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - This repository hosts llm-jp-3-150m, a 150 million parameter Japanese large language model developed by NII, utilizing Hugging Face Transformers with specified torch, transformers, and tokenizers versions.
  - Downloads: 408
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - This repository provides a Japanese BERT model, pretrained on Japanese Wikipedia data, with a small architecture (12 layers, 256 dimensions, 4 attention heads) suitable for finance-related natural language processing.
  - Downloads: 406
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight, transformer-based Japanese language foundation model optimized for resource-constrained environments, serving as a base for instruct models like RakutenAI-2.0-mini-instruct.
  - Downloads: 406
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides GGUF conversions of Japanese-Starling-ChatV-7B, a 7B Japanese chat model built upon Mistral-7B-v0.1 and enhanced with chat vectors derived from Starling-LM-7B-beta.
  - Downloads: 398
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Beta 7B model, created with support from a16z and hardware from Massed Compute.
  - Downloads: 397
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - This repository provides GGUF quantized versions of Googleâ€™s Gemma-2-2b-jpn-it model, offering compatibility with tools like llama.cpp, LM Studio, and LLMFarm, built using a process based on npaka's LLM-jp-3.
  - Downloads: 390
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - JMedRoBERTa-base-sentencepiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 379
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This repository provides a Japanese language model based on XLNet, requiring Mecab and SentencePiece, and utilizing NFKD normalization which removes Japanese accent marks for simplified text processing.
  - Downloads: 377
- [stockmark/Stockmark-2-VL-100B-beta](https://huggingface.co/stockmark/Stockmark-2-VL-100B-beta)
  - Stockmark-2-VL-100B-beta is a 100B-parameter Japanese visual language model, built on Qwen2.5-VL-72B synthetic data and designed for document reading comprehension with Chain-of-Thought reasoning, currently available as a beta release.
  - Downloads: 375
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - This repository provides a Japanese BERT large model, pretrained on Jawiki-20200831 using Unidic-lite word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 372
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitionsâ€™ repository hosts sarashina2.2-1b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English tasks alongside other models like Qwen and RakutenAI.
  - Downloads: 368
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA's Japanese-CodeLlama-7b-instruct model, alongside related Japanese Llama 2 models, offering faster performance and reduced token costs.
  - Downloads: 367
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances the Japanese language capabilities of the Gemma 2 model through continual pre-training on a 200 billion token Japanese web corpus, while preserving English proficiency.
  - Downloads: 367
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M is a computationally efficient Japanese BERT model leveraging local & global attention, RoPE, and trained on 4.39T tokens with a 102,400 vocabulary for handling long sequences.
  - Downloads: 366
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-13b-instruct-v0.1 language model, trained with the imatrix dataset, alongside links to other related Swallow model variations.
  - Downloads: 362
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - This repository provides a GGUF-formatted conversion of the karakuri-lm-32b-thinking-2501-exp language model, trained with imatrix data, for use with llama.cpp.
  - Downloads: 353
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 350
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository hosts llm-jp-3-3.7b-instruct3, a 3.7 billion parameter Japanese large language model developed by NII, utilizing the Hugging Face Transformers format and requiring PyTorch 2.3.0+ and Transformers 4.40+.
  - Downloads: 349
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides GGUF quantized versions of the rinna/qwen2.5-bakeneko-32b-instruct-v2 Japanese language model, compatible with llama.cpp applications, including merged reasoning variants.
  - Downloads: 344
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 338
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese Llama 2 and CodeLlama 7b models, including standard, fast, and instruct variants optimized for performance and reduced token cost.
  - Downloads: 333
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AIâ€™s Japanese StableLM Base Beta 70B language model, optimized for CPU inference with support from Massed Compute and funding from a16z.
  - Downloads: 331
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - This repository provides a Japanese BERT base model further pretrained on financial text, utilizing the architecture of BERT small and building upon Tohoku University's base Japanese model.
  - Downloads: 323
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters) developed by NII, utilizing the Hugging Face Transformers format and requiring specific library versions (PyTorch, Transformers, Tokenizers, Accelerate).
  - Downloads: 321
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - This repository provides a Japanese BERT large model pre-trained on CC-100 and JAWIKI datasets, utilizing character and word (Unidic 2.1.2) tokenization with whole word masking for improved language understanding.
  - Downloads: 301
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - This repository provides GGUF-formatted conversions of the Ninja-v1 Japanese language model, trained on the TFMC/imatrix dataset, and includes instructions for use with llama.cpp.
  - Downloads: 297
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - This repository provides a large Japanese DeBERTa V2 model pre-trained on extensive Japanese text data for masked language modeling tasks.
  - Downloads: 296
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - Luke-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities from text, utilizing Wikipedia data.
  - Downloads: 290
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 269
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - This repository provides a GGUF-formatted version of the open-calm-3b language model, compatible with llama.cpp, but subject to potential incompatibility following future updates to llama.cppâ€™s gptneox implementation.
  - Downloads: 269
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - Luke-Japanese-large-lite is a lightweight, pre-trained Japanese language model utilizing knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 266
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - This repository provides a Japanese Llama-3-ELYZA-JP-8B model finetuned on a specific dataset, trained faster using Unsloth and TRL, and licensed under Apache 2.0.
  - Downloads: 266
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - This repository provides GGUF-formatted conversions of the Honyaku-13b language model and other models by aixsatoshi, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset.
  - Downloads: 263
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B language model, trained with the imatrix Japanese dataset and usable with llama.cpp.
  - Downloads: 259
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation's japanese-large-lm-1.7b-instruction-sft language model, enabling its use with llama.cpp.
  - Downloads: 258
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository provides GGUF versions of Stability AI's Japanese-StableLM-3B-4E1T-Instruct language model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 257
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-7b-instruct-v0.1 model, along with related models and datasets used for Japanese language model training.
  - Downloads: 254
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model featuring GEGLU activation and no pre-training dropout, optimized for fine-tuning with potential dropout re-enablement.
  - Downloads: 251
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 248
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides a GGUF quantized version of the Llama-3-8B-Japanese-Instruct model, optimized for use with LlamaEdge (v0.10.1+) and employing a specific llama-3-chat prompt template.
  - Downloads: 246
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - This repository provides GGUF-formatted versions of Ryota39â€™s Phi-3-mini-4k-instruct-DPO model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and demonstrating usage with llama.cpp.
  - Downloads: 246
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech Swallow-70b-instruct-v0.1 large language model, trained with the imatrix dataset, alongside links to other related Swallow model variants.
  - Downloads: 243
- [mmnga/llm-jp-3.1-8x13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3.1-8x13b-instruct4 Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 237
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLS-R-53 model for Japanese speech recognition, trained on public datasets like Common Voice and JSUT, requiring 16kHz sampled input audio.
  - Downloads: 236
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3.1-13b-instruct4 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp for inference.
  - Downloads: 234
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - This repository details a 4.11GB quantized version of the Japanese-enhanced Meta Llama 2 7B model, optimized for speed at the cost of some performance.
  - Downloads: 231
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository provides GGUF versions of Stability AIâ€™s japanese-stablelm-3b-4e1t-base model, noting Llama.cpp compatibility limitations with GPU offloading (max 34 layers).
  - Downloads: 228
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of the qwen2.5-bakeneko-32b-instruct language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and compatible with llama.cpp.
  - Downloads: 226
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B is a 14B vision language model from Japan's National Institute of Informatics, requiring Python 3.10.12 and specific library installations including flash-attention.
  - Downloads: 223
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - This repository provides a GGUF formatted version of the TokyoTech-LLM Swallow-MS-7b-instruct-v0.1 language model, trained with the TFMC/imatrix dataset for Japanese LLMs, alongside links to other related model versions.
  - Downloads: 220
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - This repository hosts the llm-jp-3-8x13b-instruct3, a large language model developed by NIIâ€™s Research and Development Center for Large Language Models, utilizing the Hugging Face Transformers format with PyTorch support.
  - Downloads: 219
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model with GEGLU activation and optimized dropout settings for improved performance.
  - Downloads: 217
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - This repository hosts the llm-jp-3-8x1.8b-instruct3, a Japanese large language model from the National Institute of Informatics, utilizing the Hugging Face Transformers format and requiring PyTorch 2.3.0+.
  - Downloads: 217
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the RakutenAI-2.0-8x7B-instruct Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 216
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B is an instruction-tuned, Japanese language model built upon Qwen2.5-Bakeneko-32B and optimized for reasoning tasks using Chat Vector and ORPO, available in multiple quantization formats.
  - Downloads: 214
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - This repository provides a GGUF-formatted conversion of the Alfredplpl Llama-3-8B-Instruct-Ja model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 213
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is a 47B parameter language model, fine-tuned for dialogue using SFT and DPO, with available AWQ, GPTQ, and GGUF (though GGUF performance is potentially degraded) quantized versions, requiring flash attention for inference.
  - Downloads: 212
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - This repository provides GGUF formatted conversions of ELYZA's Japanese Llama 2 modelsâ€”including 7b, 13b, fast, instruct, and CodeLlama versionsâ€”optimized for speed and reduced token cost.
  - Downloads: 211
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - This repository provides a Japanese-optimized, quantized GGUF version of Gemma-2-2b-it, leveraging iMatrix and supporting faster inference with speculative decoding via llama.cpp.
  - Downloads: 204
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 202
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - CyberAgentâ€™s Llama-3.1-70B-Japanese-Instruct-2407 is a continually pre-trained 70B parameter Japanese language model built upon Metaâ€™s Llama-3.1-70B-Instruct, accessible via the `transformers` library.
  - Downloads: 197
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - This repository provides a GGUF-formatted conversion of the DeepSeek-R1-Distill-Qwen-14B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 197
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - AMBER is a 132M parameter text embedding model by Retrieva, Inc., built on modernbert-ja-130m, and optimized for both Japanese and English text.
  - Downloads: 195
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - This repository provides a Japanese BERT model pretrained on Wikipedia data for dependency parsing and question answering, built upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 187
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, with links to various quant sizes and a reference to TheBloke's documentation for usage.
  - Downloads: 187
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - This repository provides a 6.3GB GPTQ-quantized version of the 10 billion parameter, Japanese-centric multilingual Weblab-10B-instruction-sft model, offering faster speed with a slight performance trade-off.
  - Downloads: 186
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - This repository provides a RoBERTa-based named entity recognition (NER) model, fine-tuned on MedTxt-CR, for extracting medical-specific entities (diseases, symptoms, drugs, etc.) from Japanese text using the IOB2 tagging format.
  - Downloads: 182
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B large language model built upon Metaâ€™s Llama 3.3, enhanced for Japanese language capabilities through continual pre-training on a 315 billion token corpus including Japanese web data, Wikipedia, and code.
  - Downloads: 180
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the Haqishen Llama-3-8B-Japanese-Instruct model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 169
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - This repository provides GGUF-quantized versions of the LLM-jp-3-3.7b-instruct large language model, compatible with llama.cpp, LM Studio, and LLMFarm for local use.
  - Downloads: 160
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - This repository provides a GGUF-formatted version of the EZO-phi-4-v2_900 language model, trained with imatrix data, and designed for use with llama.cpp.
  - Downloads: 158
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with a 128k context window and enhanced long-context memory, including NSFW versions.
  - Downloads: 156
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - This repository provides a GGUF-formatted version of the stockmark-gpt-neox-japanese-1.4b language model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation within llama.cpp.
  - Downloads: 155
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - This repository provides a fine-tuned wav2vec 2.0 Large model for Japanese automatic speech recognition, trained on the ReazonSpeech v2.0 corpus and usable via the `transformers` library.
  - Downloads: 154
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the DeepSeek-R1-Distill-Qwen-32B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 153
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - This repository provides a pre-trained RoBERTa-small model for Japanese, specifically trained on the Aozora Bunko corpus using a character-level tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 149
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - This repository provides a pre-trained Japanese BART base model, `ku-nlp/bart-base-japanese`, for conditional text generation tasks, requiring word segmentation with Juman++ for input.
  - Downloads: 147
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - This repository provides a GGUF formatted conversion of the DeepSeek-R1-Distill-Qwen-1.5B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - This repository provides GGUF quantized versions of the Japanese-Italian Gemma 2B model, built using npaka's LLM-jp-3 and compatible with tools like llama.cpp and LM Studio.
  - Downloads: 147
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides quantized GGUF versions of the Llama-3-8B-Japanese-Instruct model, optimized for use with GaiaNet, including a smallest 3.18GB Q2_K quantization with a 4096 context size.
  - Downloads: 146
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository provides a series of Japanese cross-encoder rerankersâ€”including small, base, and large models with varying layer and hidden sizesâ€”for improved text retrieval, alongside technical reports and usage instructions.
  - Downloads: 145
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, potentially including a 14B version, under the MIT License.
  - Downloads: 145
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - This repository details a v1.1 T5 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 143
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - This repository provides a GGUF-formatted conversion of the cyberagent-open-calm-1b language model, compatible with llama.cpp, though potentially subject to future incompatibility with updates to llama.cppâ€™s gptneox implementation.
  - Downloads: 143
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot's ArrowMint-Gemma3-4B-YUKI-v0.1 is a Japanese language model fine-tuned from Google's Gemma-3-4b-it, specializing in multi-turn, prompt-following conversational ability for AI VTubers with a focus on lightweight performance.
  - Downloads: 141
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - This repository provides a GGUF-formatted version of AXCXEPT's phi-4-open-R1-Distill-EZOv1 model, trained with imatrix data, for use with llama.cpp.
  - Downloads: 135
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - This repository provides GGUF conversions of rinna's japanese-gpt-neox-3.6b model, intended for use with llama.cpp, though compatibility may change with future llama.cpp updates.
  - Downloads: 134
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository hosts Japanese large language modelsâ€”including 1.8b to 172b parameter variants, both base and instruction-tunedâ€”developed by NII's R&D Center for LLMs with GENIAC support.
  - Downloads: 134
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository provides a fine-tuned luke-japanese-base model for binary sentiment classification on the JGLUE MARC-ja dataset, achieving 0.9 accuracy.
  - Downloads: 132
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository provides GGUF conversions of LINE Corporationâ€™s 1.7B Japanese large language model, along with instructions for use with llama.cpp.
  - Downloads: 129
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - This repository provides a RoBERTa-large Japanese language model, pre-trained on Aozora texts, for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 128
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri provides pretrained and fine-tuned Japanese text embedding models (Ruri-v3) built on ModernBERT-Ja, offering various sizes for different performance/parameter trade-offs as measured by JMTEB.
  - Downloads: 127
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - This repository provides a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 125
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - Luke-Japanese is a pre-trained, lightweight Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 122
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 121
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - LLM-jp-3-7.2b-instruct provides a 7.2 billion parameter, Japanese large language model developed by NII, utilizing Hugging Face Transformers and requiring specific library versions for usage.
  - Downloads: 121
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-large model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 120
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIæ§˜ã® EZO-Common-T2-2B-gemma-2-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 117
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2â€™s Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - This repository provides a 32B Qwen2.5-Bakeneko instruct model quantized to 8-bit using AutoGPTQ for reduced memory usage and faster inference.
  - Downloads: 113
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
  - This repository details a Japanese language model finetuned from OpenAI's Whisper-large-v3-turbo, prioritizing speed with a smaller vocabulary, achieving state-of-the-art performance on short-form Japanese but suffering from quality issues and hallucinations in longer contexts.
  - Downloads: 112
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - This repository provides a GGUF-formatted version of the Qwen1.5-110B-Chat large language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and licensed under Tongyi-Qianwen.
  - Downloads: 111
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - This repository provides an 8-layer reduced version of the oshizo/japanese-e5-mistral-7b_slerp model, trained on 800,000 Japanese sentences, with usage details linked to intfloat/e5-mistral-7b-instruct.
  - Downloads: 110
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow improves the Japanese language capabilities of the Gemma 2 model through continual pre-training on a large Japanese web corpus, while preserving English proficiency.
  - Downloads: 110
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - This repository provides a Japanese ELECTRA base model, pretrained on Japanese Wikipedia data, with a 12-layer, 768-dimensional architecture for language understanding tasks.
  - Downloads: 109
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - This repository provides a small, pre-trained RoBERTa model for Japanese, specifically trained on the Aozora corpus and suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 108
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Asagi-14B is a 14B-parameter Japanese Vision & Language Model trained on a large, diverse datasetâ€”including synthetically generated data from CALM3 and Phi3.5-visionâ€”with openly usable outputs.
  - Downloads: 108
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - This repository provides a Japanese BERT small model pretrained on Wikipedia and financial texts, mirroring the ELECTRA small architecture with 12 layers and 256 hidden dimensions.
  - Downloads: 107
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Japanese-StableLM-base-gamma-7B-Mistral-7B-Instruct-v0.1 model, enabling its use with various inference engines.
  - Downloads: 105
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - This repository provides a Japanese BigBird base model pretrained on large-scale Japanese text datasets for masked language modeling tasks.
  - Downloads: 104
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Llama-8B language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and compatible with llama.cpp.
  - Downloads: 104
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned Wav2Vec2-large-xlsr-53 model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets with a 16kHz sampling rate requirement.
  - Downloads: 101
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - This repository provides a RoBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 101
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
  - LLM-jp-3.1-1.8b-instruct4 is an instruction-following large language model developed by NII, building on the LLM-jp-3 series with improved capabilities through mid-training instruction pre-training.
  - Downloads: 100
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-1.8b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for creating the quantizations.
  - Downloads: 100
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - This repository provides a GGUF-formatted conversion of the WabiSabi-V1 Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 99
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - This repository provides a Japanese extractive question answering model, fine-tuned from rinna/japanese-roberta-base on the JaQuAD dataset, for use with pipelines like Hugging Face Transformers.
  - Downloads: 97
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-base-japanese) for improved speech recognition, trained on Japanese datasets and requiring 16kHz audio input.
  - Downloads: 95
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - This repository provides a Japanese RoBERTa-large model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 93
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted versions of the japanese-stablelm-instruct-gamma-7b model, based on Mistral 7B, for easier local deployment.
  - Downloads: 92
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This repository merges intfloatâ€™s E5-Mistral-7B-Instruct and Stability AIâ€™s Japanese-StableLM-Base-Gamma-7B models, detailing the necessary steps to combine them due to class compatibility issues.
  - Downloads: 90
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - This repository provides a pretrained DeBERTa V2 base model for Japanese masked language modeling, with associated pretraining code available elsewhere.
  - Downloads: 90
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Asagi-8B is a large-scale Japanese Vision & Language Model trained on a diverse, synthesized datasetâ€”primarily using CALM3-22B-Chat and Phi3.5-vision-instructâ€”with openly usable outputs.
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository hosts llm-jp-3-13b-instruct2, a 13 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the cyberagent/Mistral-Nemo-Japanese-Instruct-2408 model, optimized for llama.cpp and runnable via the TensorBlock client.
  - Downloads: 87
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - This repository provides a question encoder based on the `bert-base-japanese-v3` model, fine-tuned with `aio-retriever` for the BPR document retrieval model described in chapter 9 of â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 83
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - This repository releases a 4-bit quantized derivative of the llm-jp-3-172b-instruct3 large language model, reducing GPU/memory usage while maintaining performance through BitsAndBytes (fp4) quantization with float16/float32 computation.
  - Downloads: 83
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - This repository provides a JaQuAD-fine-tuned RoBERTa base model for Japanese question answering, offering code and usage examples via the `transformers` library.
  - Downloads: 82
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed through a collaborative project utilizing the Fugaku supercomputer, allowing both commercial and non-commercial use including modification, redistribution, and service implementation.
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 79
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM Base 7B is a 7B-parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the `transformers` library for implementation.
  - Downloads: 79
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese CodeLlama-7b and Llama-2-7b models, including both base and instruct versions, optimized for speed and reduced token cost.
  - Downloads: 77
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository hosts a base-sized Japanese BART model, pre-trained by Stockmark Inc. for sequence-to-sequence tasks, detailed in the provided article.
  - Downloads: 77
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - This repository provides a pre-trained Japanese GPT2 model (â€œskytnt/gpt2-japanese-lyric-mediumâ€) for generating Japanese song lyrics from a title and/or prompt text.
  - Downloads: 73
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - This repository provides a GPT-2 small model trained on a 540M token Japanese Wikipedia dataset for Japanese language generation.
  - Downloads: 71
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - This repository provides an ESPnet2 text-to-speech (TTS) modelâ€”jsut_transformerâ€”trained by kan-bayashi using the jsut/tts1 recipe, based on data from Zenodo.
  - Downloads: 70
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a Japanese language model based on a Switch Transformer with a Prefix-LM structure, utilizing a unique "Spout" vector for controllable text generation and fine-tuning.
  - Downloads: 69
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from Kyoto University's BERT Japanese Pretrained model using the ner-wikipedia-dataset, requiring separate downloads of the tokenizer and installation of Juman++ & pyknp.
  - Downloads: 68
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model, optimized for various VRAM capacities (8GB-16GB+).
  - Downloads: 68
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - This repository provides a Japanese BERT model (bert-base-japanese-v3) fine-tuned on the JCommonsenseQA dataset for multiple-choice question answering, as detailed in the book â€œå…¥é–€ Large Language Modelsâ€.
  - Downloads: 66
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed through a collaborative project utilizing the Fugaku supercomputer, allowing both commercial and non-commercial use including modification, redistribution, and service implementation.
  - Downloads: 66
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a Japanese-language Transformer Encoder pre-trained with Megatron-LM, featuring PreNorm and addressing a recent bias initialization bug in its parameters.
  - Downloads: 66
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - This repository provides a QLoRA-finetuned version of cyberagent/calm3-22b-chat optimized for role-playing using the ChatML format, with a GGUF version and a live demo available.
  - Downloads: 65
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - This repository hosts Llama-3.1-70B-EZO-1.1-it, a Japanese language model fine-tuned from Metaâ€™s Llama 3.1, achieving top performanceâ€”surpassing gpt-4o-miniâ€”on the ElyzaTasks-100 benchmark, under the Llama 3 Community License.
  - Downloads: 65
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This repository provides a ChatML modelâ€”created via SLERP mergingâ€”combining yamatazen/Himeyuri-Magnum-12B and shisa-ai/shisa-v2-mistral-nemo-12b (HMS).
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-liteã®é‡ã¿ã®åå‰ã‚’XLMRobertaå½¢å¼ã«ç½®ãæ›ãˆã€XLMRobertaãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸç‰©ã§ã™ã€‚
  - Downloads: 62
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech-to-text, outputting continuous character sequences from 16kHz audio input.
  - Downloads: 61
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository likely contains research and analysis data related to the Tsukuba region, potentially involving university students and scientific projects conducted on or near its campus in Ibaraki Prefecture, Kanto region.
  - Downloads: 60
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - This repository provides a Japanese instruction-tuned, AWQ-quantized (3.89GB) version of Metaâ€™s Llama 2 7B model, optimized for use with NVIDIA A100 or RTX 3000 series GPUs.
  - Downloads: 59
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository provides a medium-sized Japanese GPT-2 model and tokenizer (Unidic-lite) built on PyTorch and Hugging Face Transformers for text generation.
  - Downloads: 58
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - This repository provides a GGUF-formatted version of the Stockmark-2-100B-Instruct-beta large language model, trained with the imatrix dataset and optimized for use with llama.cpp.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 58
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - This repository provides a Japanese question answering model fine-tuned on the JaQuAD dataset using BERT base, achieving F1 scores of 77.35/78.92 and exact match scores of 61.01/63.38 on development/test sets.
  - Downloads: 57
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference (NLI) model, built with SentenceTransformers and based on bert-base-japanese-v3, that classifies sentence pairs into entailment, neutral, or contradiction.
  - Downloads: 57
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, for masked language modeling tasks.
  - Downloads: 57
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a 32B parameter language model continually pre-trained on Japanese and English data to enhance performance on Japanese tasks.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 57
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained on Wikipedia for Universal POS-tagging and dependency parsing using the UniDic long-unit-word vocabulary.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Asagi-4B is a Japanese Vision & Language Model trained on a large, diverse datasetâ€”including synthetically generated data from CALM3 and Phi3.5-visionâ€”with openly usable outputs.
  - Downloads: 56
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - This repository provides a fine-tuned Qwen3-8B modelâ€”Qwen3-8B-RP-v0.1â€”optimized for role-playing, configurable via system prompts to define character settings and dialogue scenarios, with an example using Ollama.
  - Downloads: 54
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - This repository provides a Japanese novel-focused language model, GPT-J-6B, pre-trained on Japanese data and fine-tuned on novel text, runnable via Google Colab.
  - Downloads: 53
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository details a fine-tuned Japanese language model (based on sonoisa/sentence-luke-japanese-base-lite) for classifying the aggressiveness of social media comments, achieving a 71.3% F1-score on a manually labeled dataset, and presented at NLP2024.
  - Downloads: 53
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - Rinnaâ€™s Nekomata-7B-instruction model is provided in GGUF format for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 52
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - This repository provides a pre-trained Japanese character-level GPT-2 Medium model (310M parameters) for text generation, trained on Japanese Wikipedia, CC-100, and OSCAR data.
  - Downloads: 52
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - This repository hosts a merged language modelâ€”built upon shisa-ai/shisa-v2-mistral-nemo-12b using the TIES methodâ€”incorporating natong19/Mistral-Nemo-Instruct-2407-abliterated for enhanced performance.
  - Downloads: 52
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - This repository hosts weighted and static GGUF quantizations of the japanese-llama-3-8b-instruct-v2 model, offering various size/quality options including IQ1_S, with usage guidance available from TheBlokeâ€™s READMEs.
  - Downloads: 52
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository provides a Japanese-adapted version of the Llama 3 8B model, commercially usable under the Llama 3 license, with instructions for quick testing via demo/Colab or local implementation using transformers and accelerate.
  - Downloads: 50
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 49
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - This repository provides a BERT-large model pre-trained on Japanese Wikipedia for dependency parsing and question answering, built upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 49
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - This project provides code and a LoRA-tuned, Unsloth-based Japanese language modelâ€”optimized with GRPOâ€”to transform standard text into the characteristic "Ojisan" (middle-aged man) style of online communication, featuring specific phrasing, emoji, and a friendly tone.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia for Universal Part-of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 47
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training to enhance its Japanese capabilities, designed for instruction-following tasks.
  - Downloads: 47
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallow-70B-Instruct-v0.1 is a mergekit-created, 70B parameter language model demonstrated with Japanese-RP conversation examples, showcasing dialogue between this model and Gemini-2.0-flash-exp.
  - Downloads: 47
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - This repository provides a 32B Japanese language model, quantized with AWQ, based on DeepSeek-R1-Distill-Qwen and calibrated using TFMC/imatrix data.
  - Downloads: 45
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard is a high-performing, 7 billion parameter Japanese language model, fine-tuned from Metaâ€™s Llama-2-7b, achieving scores exceeding ChatGPT-3.5 on the JGLUE benchmark without using JGLUE or ChatGPT data in its training.
  - Downloads: 44
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-large Japanese question-answering model pre-trained on the Aozora corpus for dependency parsing and optimized for handling ambiguous words using masked input.
  - Downloads: 44
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - This repository provides a Japanese BERT model pre-trained for Universal Part-of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia data.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built on a base model trained with 42 billion Japanese tokens from the Cultura-X dataset.
  - Downloads: 42
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 is a Japanese LLaMA-based language model, trained on primarily Wikipedia data with 76,000 steps, designed to run on 24GB VRAM and utilizing Flash Attention.
  - Downloads: 42
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - This repository provides DataPilot/Llama3.1-ArrowSE-v0.4, a Japanese language model fine-tuned from Llama 3.1-8B-instruct using Mergekit to improve performance.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 42
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - This repository provides GGUF-formatted conversions of Meta Llama 3 8B Instruct, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 41
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - This repository provides a Japanese instruction-following model, fine-tuned from Meta's Llama-3-8B-Instruct using QLoRA on a small dataset, achieving an average score of 3.12 on ELYZA-tasks-100 (Q5_K_M quant).
  - Downloads: 41
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - Rinna/nekomata-7b-gguf provides a GGUF quantized version of the 7B rinna/nekomata model optimized for lightweight inference using llama.cpp, recommending GGUF q4_K_M for 4-bit quantization.
  - Downloads: 41
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following, based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 41
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This repository provides a Japanese+English Sentence-BERT model (â€œsonoisa/sentence-bert-base-ja-en-mean-tokensâ€) pretrained on cl-tohoku/bert-base-japanese-whole-word-masking, demonstrating improved English STS benchmark accuracy with fugashi and ipadic dependencies.
  - Downloads: 40
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA-Small model, utilizing subword tokenization from Japanese Wikipedia with MeCab-ipadic-NEologd, for use with the `transformers` library.
  - Downloads: 39
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository outlines the terms of use for Fugaku-LLM, a large language model developed through a collaborative effort utilizing the Fugaku supercomputer, permitting commercial and non-commercial use, modification, and redistribution.
  - Downloads: 38
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B is a GGUF conversion of a 7B Japanese language model built by combining ChatNTQ-ja-7b-v1.0 with a modified WizardLM-2-7b, aiming to enhance Japanese performance.
  - Downloads: 38
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporationâ€™s japanese-large-lm-3.6b-instruction-sft model, intended for use with llama.cpp, but subject to potential incompatibility with future updates.
  - Downloads: 38
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - This repository provides a Japanese GPT-2 model pretrained on Japanese Wikipedia and CC-100, suitable for text generation or fine-tuning after word segmentation with Juman++.
  - Downloads: 38
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - This repository provides a fine-tuned Japanese language model, based on studio-ousia/luke-japanese-large, for automatic defamation detection with labels identifying threatening, insulting, or reputation-damaging speech.
  - Downloads: 37
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following, based on Stability AIâ€™s Japanese Stable LM Base Gamma and requiring Transformers 4.34.0+.
  - Downloads: 37
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - This repository distributes ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and built upon megagonlabs/transformers-ud-japanese-electra-base, offering enhanced Japanese natural language processing via GiNZA v5.
  - Downloads: 37
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - ArrowNeo-AME-4x3B-v0.1 is a 4x3B Mixture-of-Experts (MoE) model built upon sarashina-2.2-instruct-v0.1, specializing in coding, prompt following, and multi-turn conversations for AI virtual YouTubers (â€œAItuberâ€) with a focus on lightweight performance using Mergekit-MoE.
  - Downloads: 37
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - This repository provides a Japanese-StableLM-Base-Alpha-7B model fine-tuned to emulate the speech patterns of Reimu Hakurei from *Touhou Project*, enabling conversational interactions using a specific prompt format and utilizing 4-bit quantization.
  - Downloads: 36
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - This repository provides a T5-base model fine-tuned on the XLSum dataset for Japanese summarization, achieving specific Rouge scores detailed in the description, with further training and usage details pending completion.
  - Downloads: 36
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer modelâ€”an encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 36
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - This repository details a 1.3B parameter Japanese GPT-2 model ("dolly-japanese-gpt-1b") fine-tuned with RLHF on datasets like â€œdatabricks-dolly-15k-jaâ€ and â€œoasst1-89k-jaâ€ for interactive AI, requiring 7GB VRAM/RAM, though recent updates show decreased QA accuracy.
  - Downloads: 36
- [yasu-oh/Llama-3-Swallow-Infused-R1776-70B](https://huggingface.co/yasu-oh/Llama-3-Swallow-Infused-R1776-70B)
  - Llama-3-Swallow-Infused-R1776-70B is a 70B parameter language model merging reasoning from r1-1776-distill-llama-70b with instruction-following from the Swallow model for improved English and Japanese performance.
  - Downloads: 36
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - This repository offers a MIT-licensed, GGUF quantized version of CyberAgentâ€™s DeepSeek-R1-Distill-Qwen-32B-Japanese large language model.
  - Downloads: 36
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - This repository provides a Japanese ELECTRA-base model, pretrained and finetuned on disaster tweets for information triage, licensed under CC BY-SA 4.0.
  - Downloads: 35
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - This repository provides a large Japanese BART model pre-trained on Japanese Wikipedia for conditional text generation tasks, utilizing the `transformers` library and requiring word segmentation with Juman++.
  - Downloads: 35
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - DeBERTa-small-japanese-aozora is a pre-trained DeBERTa V2 model based on the Aozora Bunko text corpus, suitable for fine-tuning on Japanese NLP tasks like POS-tagging and dependency parsing.
  - Downloads: 34
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia for accurate part-of-speech tagging and dependency parsing using Universal Dependencies and long-unit-words.
  - Downloads: 34
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese language model pretrained on Wikipedia and Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword approach.
  - Downloads: 33
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a large 890GB corpus, requiring fine-tuning for specific tasks and acknowledging potential biases in generated text.
  - Downloads: 33
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100B Japanese-focused large language model pre-trained on 1.5T tokens and instruction-tuned with synthetic data generated by Qwen2.5.
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - This repository details a fine-tuned Japanese GPT-2 model for generating *ES* (employment application essays) using over 20,000 examples from successful applicants, built upon rinna's japanese-pretrained-models and accessible via a web app.
  - Downloads: 31
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Beta 70B language model, offering various parameter configurations.
  - Downloads: 31
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - TohokuNLP's BERT-alpha 500M is a Japanese BERT model based on the Llama architecture, enabling long sequence input (up to 8,192 tokens) as an encoder-type language model.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a 370m parameter Japanese chat language model built on the Mamba state-space architecture for efficient, linear-time sequence modeling.
  - Downloads: 31
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM provides open-source, locally deployable Japanese-to-Chinese translation modelsâ€”fine-tuned on general and ACGN-style dataâ€”for light novels and galgames, released under a non-commercial license.
  - Downloads: 30
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts LINE Corporation's 1.7B parameter Japanese language model, quantized to 8-bit with 1GB activation order, and fine-tuned for instruction following.
  - Downloads: 30
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - This repository provides a 9GB LoRA fine-tune of OpenAIâ€™s Whisper Large V2, specifically for improved Japanese speech transcription, with a focus on comedy content, and is currently undergoing testing.
  - Downloads: 30
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This repository provides a Japanese-specialized, reasoning-enhanced version of the phi-4 model, built using the open-r1 methodology inspired by Deepseek-R1's distillation techniques, and capable of flexible English integration.
  - Downloads: 30
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pre-trained on Japanese Wikipedia, requiring Juman++ word segmentation for text generation or fine-tuning tasks.
  - Downloads: 29
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - This repository provides a Japanese pre-trained ALBERT model ("ken11/albert-base-japanese-v1-with-japanese-tokenizer") designed for easy fine-tuning on various Japanese NLP tasks using a BertJapaneseTokenizer.
  - Downloads: 29
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - This repository provides a Japanese 1B GPT model fine-tuned to mask Personally Identifiable Information (PII) â€“ including names, birthdays, phone numbers, addresses, and IDs â€“ within Japanese text.
  - Downloads: 29
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Instruct Gamma 7B, optimized for efficiency using hardware from Massed Compute and supported by a16z grant.
  - Downloads: 29
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - This repository provides a GGUF-formatted version of the Matsuo Lab's Weblab-10B model, compatible with `llama.cpp` examples, utilizing a development branch for faster updates.
  - Downloads: 29
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-Of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 28
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - Sarashina2.2-3b-RP-v0.1 is a 3B parameter model fine-tuned for role-playing, utilizing a system prompt to define character and scenario within a fantasy setting, and available in GGUF format for use with tools like Ollama.
  - Downloads: 28
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 27
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - JMedRoBERTa-base-manbyo-wordpiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, licensed under CC BY-NC-SA 4.0.
  - Downloads: 27
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Instruct Beta 7B, optimized for efficiency using hardware from Massed Compute and supported by a16z grant.
  - Downloads: 27
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - This repository provides a merged, work-in-progress Japanese language model based on Mixtral-8x7B-Instruct-v0.1, enhanced with vocabulary expansion and shared as an intermediate evaluation model by ABEJA.
  - Downloads: 27
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - This repository provides an early-access, Japanese-extended version of the Mixtral-8x7B-Instruct-v0.1 language model, evaluated and released by ABEJA, built upon the Metagton-LM framework.
  - Downloads: 27
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Vaporetto + Unigram segmentation, requiring a downloaded dictionary file for proper loading.
  - Downloads: 27
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - This repository provides a Rust implementation of Tohoku University's large Japanese BERT model, enabling its use in Rust projects with instructions for cloning, project setup, and basic usage via the `rust-bert` crate.
  - Downloads: 27
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - This repository provides a Japanese BERT large model pre-trained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved masked language modeling.
  - Downloads: 27
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - This repository provides a 4-bit quantized MLX version of the DeepSeek-R1-Distill-Qwen-32B-Japanese language model, enabling efficient inference using the `mlx-lm` library.
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - This repository hosts llm-jp-3-980m-instruct2, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡žä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 27
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - This repository provides a 1.5 billion parameter Japanese GPT2 model, pretrained on Japanese Wikipedia and CC-100, for text generation and fine-tuningâ€”requiring Juman++ word segmentation.
  - Downloads: 26
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Sentence Text Similarity (JSTS) using the JGLUE dataset, achieving a Pearson correlation of 0.8971.
  - Downloads: 26
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - Fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding model, based on bert-base-japanese-v3 and trained on limited data for tasks like similarity, entailment, and retrieval using datasets including JSTS, JSNLI, and MMARCO.
  - Downloads: 26
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF quantized versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model for use with llama.cpp, a Japanese language model designed as a helpful assistant.
  - Downloads: 26
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa-v2-large Japanese model fine-tuned for universal dependency parsing and part-of-speech tagging with the goeswith subword tokenizer, requiring the fugashi library for use.
  - Downloads: 26
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - This repository provides a Japanese T5 language model fine-tuned on a 100GB corpus of Japanese text data for text generation tasks, built upon the `sonoisa/t5-base-japanese-v1.1` base model.
  - Downloads: 26
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - This repository hosts rinna's 1.3B-parameter Japanese GPT model, utilizing T5Tokenizer for text processing and supporting CUDA acceleration.
  - Downloads: 26
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal POS-tagging and dependency parsing on Japanese Wikipedia, extending the `bert-large-japanese-char-extended` model with long-unit-word tagging.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUFã¯Japanese-LLaMA-3-8B-Instruct-v2ã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 26
- [tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF](https://huggingface.co/tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF)
  - This repository provides GGUF quantized files for the ELYZA-japanese-Llama-2-13b-instruct model, optimized via TensorBlock and compatible with llama.cpp.
  - Downloads: 25
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer, 64-dimensional architecture.
  - Downloads: 25
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - This repository provides a pre-trained Japanese GPT2 model (â€œskytnt/gpt2-japanese-lyric-smallâ€) for generating Japanese lyrics, with example code and a demo available at lyric.fab.moe.
  - Downloads: 25
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 25
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese and English generation with an extended 128k context window and improved long-context memory.
  - Downloads: 25
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This repository provides a JAX/Flax-based transformer language model (0.1B parameters) trained on Japanese text, built upon the Flax lm1b example, and includes benchmark scores & FlaxAutoModel support.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and WordPiece, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer (Juman++ + WordPiece) requiring a downloaded dictionary file for usage with transformers and pyknp.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - This repository provides an uncensored (abliterated) version of vecteus v1, a high-performance Japanese large language model specializing in novel text generation and offering improved freedom in text creation while disclaiming responsibility for generated content.
  - Downloads: 25
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository provides the 32B parameter DeepSeek-R1-Distill-Qwen Japanese language model converted to MLX format for use with the `mlx-lm` library.
  - Downloads: 25
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Asagi-2B is a large-scale Japanese Vision & Language Model trained on a diverse, synthetically-generated Japanese dataset using open-source LLMs like CALM3 and Phi3.5-vision.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒžãƒ¼ã‚¸ãªã©ã‚’ç”¨ã„ä½œæˆã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - This project fine-tunes a Japanese GPT-2 model specifically for generating application essays (ES) for IT industry job seekers, building upon the rinnakk/japanese-pretrained-models codebase.
  - Downloads: 24
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - This repository provides a fine-tuned Japanese GPT-2 model, trained on 140,000 ES (resume/CV) examples, and accessible via the web app http://www.eswrite.com, built upon rinnakk/japanese-pretrained-models.
  - Downloads: 24
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 3.6B-parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following.
  - Downloads: 24
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - This repository provides the Tanuki-Zero base model, built upon llm-jp/llm-jp-13b-v1.0 and trained with a 15k sample of the Jaster instruction dataset.
  - Downloads: 24
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained, bilingual Japanese-English language model adapting Llama-2-7b with 42 billion tokens from Cultura-X, achieving state-of-the-art results in perplexity and translation.
  - Downloads: 24
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora Bunko texts for universal part-of-speech tagging and dependency parsing of long-unit words.
  - Downloads: 24
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - KoichiYasuoka/bert-large-japanese-char-extended is a pre-trained Japanese BERT model based on bert-large-japanese-char, enhanced with extended character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest â™»
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - ClassCat/gpt2-base-japanese-v2 is a Japanese GPT-2 base model trained on Japanese Wikipedia and web crawl data, utilizing a 60,000-token BPE tokenizer for text generation with transformers v4.19.2.
  - Downloads: 23
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B parameter Japanese GPT2 model finetuned by jweb (based on rinna's work) in both PyTorch and Rust formats, requiring T5Tokenizer for use.
  - Downloads: 23
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - This repository provides a Japanese BERT base model and tokenizer trained on the June 2021 Japanese Wikipedia dataset for natural language processing tasks.
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja is a Japanese language LayoutLM model pretrained for token classification tasks and finetuned from cl-tohoku/bert-base-japanese-v2, released under CC BY-SA 3.0.
  - Downloads: 23
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - This repository provides a 4-bit AutoAWQ quantized version of the phi-4 language model, calibrated with both Japanese and English data for improved performance.
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - This repository provides a RoBERTa-large model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction-following tasks.
  - Downloads: 22
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - This repository details hyperparameter optimization using Optuna for a Japanese sentiment analysis model (based on cl-tohoku/bert-base-japanese-whole-word-masking and llm-book/wrime-sentiment) trained with AdamW, achieving optimal results with a cosine learning rate schedule, learning rate of 3.9e-5, batch size of 128, weight decay of 5.2e-5, and 100 epochs with early stopping.
  - Downloads: 22
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - This repository provides a Japanese Natural Language Inference (NLI) model, built on bert-base-japanese-v3 and trained with SentenceTransformersâ€™ Cross-Encoder on the JSNLI dataset, outputting entailment scores for sentence pairs.
  - Downloads: 22
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - This repository provides a Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, a large language model, with instructions and code for usage via Hugging Face Transformers.
  - Downloads: 22
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - This repository provides a DPO-finetuned version of Microsoft's Phi-3-mini-4k-instruct model, trained on the llm-jp/hh-rlhf-12k-ja dataset for improved Japanese instruction following.
  - Downloads: 22
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - This repository provides a merged 7B Japanese chat modelâ€”umievo-itr004-7bâ€”created from Aratako/Antler-7B-RP-v2, NTQAI/chatntq-ja-7b-v1.0, and TFMC/Japanese-Starling-ChatV-7B using a linear merge with bfloat16 precision and int8 parameter masking.
  - Downloads: 22
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Base Beta 70B, offering an efficient and accurate large language model, supported by a16z and hardware from Massed Compute.
  - Downloads: 22
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AIâ€™s Japanese StableLM Instruct Gamma 7B language model, offering various parameter permutations for optimized performance.
  - Downloads: 22
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - This repository provides a Japanese BERT-base model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 22
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - This repository provides a Japanese pre-trained MobileBERT modelâ€”a faster alternative to BERTâ€”built for use with the `transformers` library and leveraging the Tohoku University BERT tokenizer.
  - Downloads: 22
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - KoichiYasuokaâ€™s roberta-large-japanese-aozora is a pre-trained RoBERTa language model for Japanese, using the Japanese-LUW-Tokenizer and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
  - LLM-jp-3.1-8x13b-instruct4 is an improved, instruction-following large language model developed by NII, building on the LLM-jp-3 series with mid-training instruction pre-training.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - jp-ModernBert-base-preview is a 150M parameter Japanese language model, trained on 300B tokens of Japanese data (fineweb2), featuring an 8192 context length and utilizing BertJapaneseTokenizer for efficient inference, potentially with FlashAttention.
  - Downloads: 22
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, featuring 12 layers, 64 hidden dimensions, and 1 attention head.
  - Downloads: 21
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository provides a Japanese GPT-2 distillation model trained using rinna/japanese-gpt2-medium as the teacher, achieving a perplexity of around 40 on Wikipedia data, and utilizing Hugging Face Transformers with modified training code.
  - Downloads: 21
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - Nagisa-BERT is a Japanese BERT model and tokenizer, available via Transformers, for masked language modelingâ€”requiring Python 3.7+ on Linux or macOS and installable via pip.
  - Downloads: 21
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - This repository provides a RoBERTa-large Japanese model pretrained for universal dependency parsing and part-of-speech tagging, built upon roberta-large-japanese and utilizing the `goeswith` subword tokenizer, requiring `fugashi` for use.
  - Downloads: 21
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - This repository provides a base model card template and details a small, pretrained Japanese/English T5 text-to-text transformer model, requiring further information regarding its developer, type, language specifics, and license.
  - Downloads: 21
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository provides a Japanese language model, â€œluke-japanese-wordpiece-baseâ€, based on Japanese BERT and pre-trained on July 2023â€™s Japanese Wikipedia, enhanced to handle unknown entities and utilizing WordPiece tokenization.
  - Downloads: 21
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model built with the Heron library, enabling image-based conversation using a pretrained 7B parameter model.
  - Downloads: 21
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model fine-tuned from Llama-2-7b using direct preference optimization on the Cultura-X dataset, built by SambaNova Systems.
  - Downloads: 21
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
  - This repository provides a 4-bit quantized ExLlamaV2 model of augmxnt/shisa-base-7b-v1, a 7B+8B token Japanese language model extending Mistral 7B with data from MADLAD-400 and English sources, utilizing a 120k token tokenizer.
  - Downloads: 21
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - This repository provides a Japanese instruction-tuned Llama 2 model (based on if001/llama2_ja_small) fine-tuned with instruction data, offering example code for usage with Transformers.
  - Downloads: 21
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - This repository provides a Japanese BERT-base model (â€œVaporetto + BPEâ€) and instructions for loading its tokenizer using a downloaded dictionary file.
  - Downloads: 21
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Unigram, requiring a downloaded dictionary file for setup.
  - Downloads: 21
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This repository provides a Japanese BERT-base model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 21
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - This repository provides a DeBERTa-large V2 model pre-trained on Japanese é’ç©ºæ–‡åº« texts using BertJapaneseTokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on the Japanese Common Voice 8.0 dataset, achieving a Word Error Rate of 1.0132 and trained with a learning rate of 7.5e-05 and batch size of 32.
  - Downloads: 20
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 7B model, offering various parameter options for efficient performance.
  - Downloads: 20
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - This repository provides a Japanese SPLADE model (aken12/splade-japanese) fine-tuned on the mMARCO dataset, built upon tohoku-nlp/bert-base-japanese-v2, and includes example code for query encoding.
  - Downloads: 20
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - This repository provides Japanese-finetuned versions of Meta-Llama-3-8B-Instruct, compatible with both transformers and the original llama3 codebase, trained on a 49k conversational dataset with an 8192 context length.
  - Downloads: 20
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - This repository provides a fully fine-tuned Phi-3-mini-4k-instruct model (ryota39/Phi-3-mini-4k-instruct-dpo) trained on the llm-jp/hh-rlhf-12k-ja dataset for improved Japanese instruction following.
  - Downloads: 20
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Vecteus Ninja is a Mistral-7B-based large language model fine-tuned for high-quality Japanese and English generation with enhanced long-context memory capabilities, developed with support from the LocalAI hackathon.
  - Downloads: 20
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - This repository provides a LoRA-fine-tuned language model (Ninja-v1-NSFW) for roleplaying, based on Aratako/Ninja-v1-RP, utilizing the Vicuna chat template and trained on Japanese datasets like Rosebleu and LimaRP.
  - Downloads: 20
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This repository provides a QLoRA-finetuned Llama-2-13b-chat-hf model, trained on a Japanese/Chinese dataset of chat and non-chat samples, with recommended generation parameters and testing script.
  - Downloads: 20
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - This repository provides a 1.3B parameter Japanese GPT modelâ€”alpaca-guanaco-japanese-gpt-1bâ€”trained on alpaca_ja and GuanacoDataset, requiring 7GB VRAM or RAM for conversational AI applications.
  - Downloads: 20
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - DeBERTa-v2 pre-trained on Japanese Wikipedia and Aozora Bunko texts provides a foundation for fine-tuning various Japanese NLP tasks like POS-tagging and dependency parsing.
  - Downloads: 20
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 20
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) modelâ€”pre-trained on 8K vocabulary from large-scale web and Wikipedia corporaâ€”along with training code.
  - Downloads: 20
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - This repository provides a Japanese error detection and correction model, a fine-tuned mt5-base trained on 20,000 text pairs, formatted for text-to-text tasks with a "correction: " prefix.
  - Downloads: 20
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - This repository provides a transformer-based Japanese-to-Hebrew translation model (jpn-heb) trained on OPUS data, utilizing SentencePiece preprocessing and including evaluation results on standard test sets like BLEU and chr-F.
  - Downloads: 20
- [Aratako/MistralPrism-24B](https://huggingface.co/Aratako/MistralPrism-24B)
  - This repository hosts a merged and enhanced 24B parameter roleplay model based on Mistral-Small-3.1-24B-Instruct-2503, optimized for use with specific chat templates and system prompts defining character and dialogue context.
  - Downloads: 20
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - This repository hosts JP-ModernBERT-Large, a 396M parameter Japanese language model trained on ~100B tokens with an 8192 context length, leveraging the BertJapaneseTokenizer and optimized for FlashAttention.
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - This repository provides a GGUF version of the Llama-3.3-Swallow-70B-Instruct-v0.4 language model, fine-tuned with the imatrix Japanese LLM dataset.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - This repository provides a ctranslate2-compatible dataset converted from the AIBunCho Japanese novel GPT-J 6B model, featuring 8-bit quantization which may affect accuracyâ€”quantitative data is currently unavailable.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 20
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 19
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - This repository provides a fine-tuned RoBERTa-base Japanese model for zero-shot text classification on the JSNLI dataset, achieving 93.28% accuracy with pre-processed Juman++ tokenized input.
  - Downloads: 19
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - This repository provides a Japanese RoBERTa-large model pre-trained on Japanese Wikipedia & CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 19
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - This repository provides a Japanese chat model, a variant of ebisuke/liz-nojaloli-ja, fine-tuned from abeja/gpt-neox-japanese-2.7b for personal study, and utilizes a specific input format for conversational turns.
  - Downloads: 19
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Instruct Beta 70B, optimized for efficient performance with support from a16z and Massed Compute.
  - Downloads: 19
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-instruct-gamma-7b using Slerp merging across the first 32 layers, based on Mistral-7B as the base model.
  - Downloads: 19
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)
  - This repository provides a Japanese-adapted version of the BLOOM-7B language model, fine-tuned with LAPT and randomness, and accessible via PEFT and Transformers libraries for causal language modeling.
  - Downloads: 19
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - This repository provides a Japanese-tuned version of Meta-Llama-3-8B-Instruct, finetuned on a 49k conversation dataset using h2o-llmstudio with an 8k context length, and usable with the transformers library.
  - Downloads: 19
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - This repository hosts Tokara-0.5B-v0.1, a chat-enhanced 0.5 billion parameter language modelâ€”fine-tuned from Qwen1.5-0.5B with Japanese/English dataâ€”capable of multi-turn conversations and benchmarked on tasks like reasoning and roleplay, though repetition may require penalty adjustments.
  - Downloads: 19
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - This repository provides a Japanese-pretrained version of the 275.86M parameter Mixtral model, demonstrated with code for text generation using the Hugging Face Transformers library.
  - Downloads: 19
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Base Beta 70B language model, offering various parameter permutations for optimized performance.
  - Downloads: 19
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 19
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - This repository provides a Japanese BERT-base model & tokenizer utilizing MeCab and WordPiece for efficient Japanese text processing within the Transformers library.
  - Downloads: 19
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small, fast, Japanese-language LLaMA-based model pretrained entirely on Japanese text, producing surprisingly coherentâ€”though not always helpfulâ€”responses despite its limited size (24GB VRAM).
  - Downloads: 19
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-large model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword tokenizer.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 19
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This repository provides a Japanese sentence-T5 model, pre-trained using sonoisa/t5-base-japanese, requiring `sentencepiece` for inference and achieving accuracy comparable to sonoisa/sentence-bert-base-ja-mean-tokens.
  - Downloads: 19
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Zenz-v1 is a 90M parameter Japanese language model based on GPT-2, finetuned for Kana-Kanji conversion and designed for use with the Zenzai neural conversion system, licensed under CC-BY-SA 4.0.
  - Downloads: 19
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - This repository provides a fine-tuned Japanese BERT model for multi-class classification of grammar points, trained on dictionary data and LLM-augmented synthetics, for use in language learning tools.
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - Sarashina2.2-3Bx8-moe is a Japanese Mixture of Experts language model, built upon sbintuitions/sarashina2.2-3b-instruct-v0.1 with mergekit-moe, offering diverse and high-quality responses through the integration of eight specialized models.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - This repository provides a 4-bit quantized, Japanese-language fine-tune of the Llama-2-Chat 70B model, trained on the Alpaca-JA dataset, requiring adherence to the Meta LLaMA license.
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance ã®GGUFç‰ˆ Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - KoichiYasuoka/bert-base-japanese-char-extended is a Japanese BERT model pre-trained on Wikipedia, featuring extended character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 18
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - This repository provides a pretrained ESPnet2 text-to-speech (TTS) model, jsut_vits_accent_with_pause, trained on the JSUT dataset and originally shared on Zenodo.
  - Downloads: 18
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text discrimination, leveraging the architecture detailed in the original ELECTRA paper.
  - Downloads: 18
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - This repository provides a pretrained FastText word embedding model for Japanese, along with a Google Colaboratory example demonstrating its use with Transformers and MeCab for feature extraction.
  - Downloads: 18
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - This repository provides a fine-tuned T5-base-Japanese model for Japanese title generation from input text, pretrained on a 100GB Japanese corpus including Wikipedia and OSCAR data.
  - Downloads: 18
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 1.7B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following.
  - Downloads: 18
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporationâ€™s repository hosts a 1.7B parameter, 4-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 18
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and Retrieval-Augmented Generation (RAG) with Japanese context, and quantized using both AutoGPTQ and AutoAWQ for potentially GPT-3.5-level performance.
  - Downloads: 18
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - This repository reproduces a 7B-parameter Japanese language model, fine-tuned for instruction following using translated Databricks Dolly-15k and other datasets, based on Japanese Stable LM Base Gamma 7B and trained with the notus codebase.
  - Downloads: 18
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - This repository provides a DeBERTa-v3-base Japanese model, pretrained on LLM-jp, for universal dependency parsing and part-of-speech tagging using the goeswith subword approach.
  - Downloads: 18
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - This repository provides a GPTQ 4-bit quantized version of the C3TR-Adapter model for efficient English-Japanese and Japanese-English translation, runnable on free Colab (with improved quality on paid versions).
  - Downloads: 18
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built upon a base model trained on 42 billion Japanese tokens from Cultura-X.
  - Downloads: 18
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - This repository provides a pretrained DeBERTa V2 small model for Japanese masked language modeling, leveraging code from retarfi/language-pretraining and utilizing a specific tokenizer.
  - Downloads: 18
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese BERT base model, â€œbert-base-japanese-ssuwâ€, optimized for super short unit words and requiring full-width character input with prior segmentation (e.g., using KyTea).
  - Downloads: 18
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - This repository provides a Japanese-language model, fine-tuned from Googleâ€™s mt5-base, for summarizing patent claims specifically within the pharmaceutical domain.
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for real-time Japanese speech-to-text translation, trained on a large corpus of Japanese speech datasets like Common Voice and CSS10.
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specializing in Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a fine-tuned Mistral-7B LLM boasting a 128k context window, high-quality Japanese & English generation, and improved long-context memory, even for NSFW content.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is a 90M parameter Japanese GPT-2 language model finetuned for high-performance, context-aware Kana-to-Kanji conversion, built upon ku-nlp/gpt2-small-japanese-char and licensed under CC-BY-SA 4.0.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - This repository provides a fine-tuned Japanese LLM for Whisper, specifically trained on Dominion card terminology to improve speech-to-text accuracy for the board game, including challenging vocabulary and phonetic variations.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - This repository provides a fork of LINE's DistilBERT model, pre-trained on a large Japanese web text corpus, with updated tokenizer code compatible with transformers>=4.34.
  - Downloads: 17
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - This repository provides a dynamically generated model card detailing key informationâ€”including developer, funding, license, and languageâ€”for a ðŸ¤— Transformers model hosted on the Hub.
  - Downloads: 17
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - This repository hosts a Japanese-tuned version of Metaâ€™s Llama 3 8B Instruct model, enhanced with ChatVector and QLoRA, achieving an average score of 3.32 on the ELYZA-tasks-100 benchmark.
  - Downloads: 17
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository provides HuBERT-base model weights trained on JTubeSpeech for speech recognition tasks, functioning as an encoder to embed audio into latent variablesâ€”not a speech *generation* model.
  - Downloads: 17
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - Rinna/nekomata-14b-instruction-gguf provides a GGUF quantized version of the 14B instruction-tuned Nekomata model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 17
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - Rinna/nekomata-14b-gguf provides a GGUF quantized version of the rinna/nekomata-14b model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 17
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE is a suite of Japanese decoder-only language models by CyberAgent, Inc., fine-tuned with LoRA using PEFT, PyTorch, and Transformers.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for operation.
  - Downloads: 17
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - This repository provides an inference model cloned from rinna's "japanese-gpt-1b", fine-tuned on the "databricks-dolly-15k-ja" dataset, and details the environment (Windows 10, RTX4070, Python 3.9.6) used for its creation and inference.
  - Downloads: 17
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - This repository provides a fine-tuned DistilHuBERT speech recognition model for Japanese, trained on a 50k-step corpus including JVS, Tsukuyomi-Chan, and custom ITA datasets, subject to the JVS corpus terms of use.
  - Downloads: 17
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - ModernBERT-large-japanese-aozora is a pre-trained Japanese language model, trained on the Aozora text corpus, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants, with Hugging Face format checkpoints.
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B is a 7-billion parameter Japanese language model, fine-tuned from Japanese Stable LM Base Gamma 7B, with some ACG (anime, manga, visual novel) knowledge for experimental fanfiction generation.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - This repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-of-Speech tagging and dependency parsing of character-level long-unit words.
  - Downloads: 16
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 16
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts a 3.6B parameter, 8-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction-following tasks.
  - Downloads: 16
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA is a Japanese vision-language model built via the Chat Vector method, combining weights from llava-v1.5-7b, Llama-2-7b-hf, and ELYZA-japanese-Llama-2-7b to enable conversation about images.
  - Downloads: 16
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - This repository provides a fine-tuned Japanese language modelâ€”based on japanese-novel-gpt-j-6b and utilizing 4-bit quantizationâ€”allowing users to converse with the Touhou Project character, Marisa Kirisame, as a secondary creation.
  - Downloads: 16
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - This repository provides a Japanese BERT-base model and instructions for loading its associated Nothing + BPE tokenizer using a dictionary file.
  - Downloads: 16
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository provides a medium-sized, right-to-left (reversed) Japanese GPT-2 model utilizing a BERT-like tokenizer and requiring PyTorch, Fugashi, and Hugging Face Transformers for implementation.
  - Downloads: 16
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised Chinese-Japanese masked language model pretraining framework leveraging the Unihan database and a coarse-to-fine approach to exploit shared character morphology between the two languages.
  - Downloads: 16
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - This repository provides a Japanese RoBERTa-base model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 16
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 16
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B is a 1 billion parameter language model pre-trained on 100 billion tokens, utilizing a Differential Transformer with Differential Attention applied to the Llama architecture for improved focus and reduced noise, and optimized for efficiency with patch-level training and a faster optimizer.
  - Downloads: 16
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B is a 12B parameter language model merged from ForgottenMaid, Francois-PE-V2-Huali, and Ohashi-NeMo using the Model Stock merge method with Orihime-12B as the base.
  - Downloads: 16
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - This repository provides a Japanese BERT base model pretrained with character-level tokenization and whole word masking, offering a streamlined alternative to `cl-tohoku/bert-base-japanese-char-v2` by removing the dependency on external tools like Fugashi or UniDic Lite.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese language model pretrained on the Aozora corpus for universal dependency parsing and part-of-speech tagging, utilizing the goeswith subword tokenizer.
  - Downloads: 16
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - This repository details hyperparameter optimizationâ€”using Optunaâ€”for a Japanese sentiment analysis model (cl-tohoku/bert-base-japanese) trained on multilingual data (tyqiangz/multilingual-sentiments) with AdamW, achieving optimal settings of cosine learning rate schedule (2.82e-05), gradient accumulation of 1, and weight decay of 0.00017.
  - Downloads: 16
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - This repository details Japanese sentiment analysis experiments using a BERT base model, the WRIME dataset, Adafactor optimization, and hyperparameter tuning with Optuna across various learning rates, batch sizes, and weight decay values.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, featuring 12 layers, 256 hidden dimensions, and 4 attention heads.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - DeBERTa-large-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora text corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - This repository provides a 1.3B parameter NLLB-200 model fine-tuned for Japanese-to-English translation of the â€œAscendance of a Bookwormâ€ web novel.
  - Downloads: 15
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing Japanese RoBERTa and UD_Japanese-GSDLUW resources.
  - Downloads: 15
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This repository provides a BART large language model for Japanese, converted from Kyoto Universityâ€™s original release and compatible with the `transformers` library using `BartJapaneseTokenizer`.
  - Downloads: 15
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants like llm-jp-13b-instruct-lora, alongside pre-trained checkpoints.
  - Downloads: 15
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-7B language model, available on Hugging Face.
  - Downloads: 15
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - This repository hosts llm-jp-1.3b-v1.0-aya, a 1.3 billion parameter Japanese language model fine-tuned on Cohere's aya dataset, achieving an average kcoopermiller/llm-jp-eval score of 0.0698.
  - Downloads: 15
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built upon a base model trained on 42 billion Japanese tokens from Cultura-X.
  - Downloads: 15
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - This repository provides a Japanese-adapted version of Mistral-7B, fine-tuned with LAPT and CLP+ techniques, and readily usable with PEFT and Transformers libraries.
  - Downloads: 15
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base is a 3 billion parameter language model pre-trained on Japanese and English data, utilizing the RetNet architecture and a retention mechanism for research purposes, released under the MIT license.
  - Downloads: 15
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B is a Japanese causal language model fine-tuned with LAPT and random data, available via PEFT and Transformers for use with 8-bit loading and GPU acceleration.
  - Downloads: 15
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 15
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - This repository provides a commercially usable, Japanese-speaking AI model (based on Gemma-2B) fine-tuned for conversational ability, utilizing transformers and PEFT within a Google Colab environment.
  - Downloads: 15
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and Stability AIâ€™s japanese-stablelm-base-gamma-7b using Slerp merging, specifically targeting self-attention layers, to create a multilingual model.
  - Downloads: 15
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - This repository provides GGUF conversions of LINE Corporation's Japanese large language model (3.6B parameters) for use with llama.cpp, noting potential incompatibility with future llama.cpp updates.
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - This repository provides a Japanese BERT-base tokenizer trained with Nothing + Unigram, requiring a downloaded dictionary file for proper loading and use.
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Byte Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 15
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This repository provides a fine-tuned cyberagent/open-calm-7b language model, trained with H2O LLM Studio on a Japanese quiz dataset, and ready for text generation using the transformers pipeline with specified library versions.
  - Downloads: 15
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B is a pre-trained Japanese language modelâ€”similar to GPT2/GPT3â€”trained on a large Japanese corpus and usable via the `transformers` pipeline for text generation.
  - Downloads: 15
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - This repository provides a Japanese ELECTRA model, pretrained on 200M sentences and tokenized with SudachiTra/WordPiece, for use with the `transformers` library.
  - Downloads: 15
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specialized for Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 15
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
  - Shirayukihime-12B is a 12B parameter language model merged using the TIES method with Mistral-Nemo as the base, incorporating shisa-v2-mistral-nemo-12b and Himeyuri-v0.1-12B, and utilizing bfloat16 precision.
  - Downloads: 15
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - This model merges differences between Mixtral-8x7B-Instruct and Mixtral-8x7B-v0.1 onto Swallow-MX-8x7b-NVE-v0.1, improving Japanese naturalness and offering top-tier performance for local LLMs with a 32k context size.
  - Downloads: 15
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - This repository provides a fine-tuned Wav2Vec2 model (facebook/wav2vec2-large-xlsr-53) for Japanese accent recognition, achieving a 15.82% Word Error Rate on 16kHz audio input.
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - This repository hosts a Japanese language model (llm-jp-3-3.7b-instruct) fine-tuned for long-form text generation using the Japanese-LongWriter-3k dataset, trained with a learning rate of 1e-05 and specific batch sizes.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Rinna's Llama 3 Youko 70B is a continually pre-trained version of Meta-Llama-3-70Bâ€”using 5B Japanese/English tokensâ€”that enhances performance on Japanese language tasks, with 8B parameter versions available in HF and GPTQ formats.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [ayousanz/modernbert-vtuber-finetuned](https://huggingface.co/ayousanz/modernbert-vtuber-finetuned)
  - This repository provides a finetuned ModernBERT-Ja-130M model for binary classification (VTuber/non-VTuber) based on YouTube channel text data like titles and descriptions.
  - Downloads: 14
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa(V2) model pre-trained on Japanese Wikipedia and é’ç©ºæ–‡åº« for dependency parsing and question answering, optimized for long-unit-word head-detection using the [MASK] token for disambiguation.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa large Japanese model pretrained on the Aozora corpus for universal dependencies tasks like POS-tagging and dependency parsing, utilizing the goeswith subword tokenizer.
  - Downloads: 14
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - This repository provides a Japanese RoBERTa-base model pretrained for universal dependency parsing and POS-tagging, utilizing the goeswith subword approach and requiring the fugashi library.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 14
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
  - This repository provides a Japanese language model based on Mistral-7B, fine-tuned with LAPT and heuristics (untied), and readily usable with PEFT and Transformers libraries.
  - Downloads: 14
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - This repository provides a BERT model, fine-tuned on Japanese novel data, for classifying text (titles & summaries) into genres, built upon the cl-tohoku/bert-base-japanese-char-v3 base model.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Unigram, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Unigram, requiring a downloaded dictionary file for proper loading and processing.
  - Downloads: 14
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model featuring GEGLU activation andâ€”for optimal fine-tuningâ€”recommends enabling dropout.
  - Downloads: 14
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This repository details a Japanese GPT-1B-based model fine-tuned for extractive question answering and answer refinement within the gpt-index (v0.2.5) framework, utilizing specific prompt templates for context-based responses.
  - Downloads: 14
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - This repository provides a RoBERTa-base Japanese model, pretrained on 200M sentences with extended input length (1282 max positions) requiring Juman++ pre-tokenization and SentencePiece tokenization.
  - Downloads: 14
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling.
  - Downloads: 14
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - This repository provides ja_ginza_electra, a spaCy v3 package distributing an ELECTRA model pretrained on Japanese mC4 data and finetuned on UD_Japanese, requiring SudachiTra for tokenization.
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository provides a fine-tuned Japanese reward model, based on modernbert-ja-310m, designed to predict user evaluation scores for novel text, primarily for reinforcement learning applications in text generation.
  - Downloads: 14
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - This repository provides a Japanese-enhanced, fine-tuned version of Mistral-Nemo (v0.2) for role-playing, utilizing a larger, multilingual dataset and optimized for Japanese output with a recommended temperature of 0.3.
  - Downloads: 14
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - This repository shares a merged Stable Diffusion model (â€œKokuwaâ€) built upon KiwiMix and other sources, specializing in slightly quirky and deformed character generation with potential seed-based variation, and acknowledging the original model creators.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-large Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, built upon existing char-level and UD Japanese resources.
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository provides a Japanese-finetuned version of the unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit model, optimized for generating responses and thought processes in Japanese with a helpful assistant persona.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated)
  - This repository provides a bfloat16 merged language model combining shisa-ai/shisa-v2-mistral-nemo-12b with the nbeerbower/Mistral-Nemo-12B-abliterated-LORA, offering a ready-to-deploy or fine-tune solution with included LoRA merging code.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA model (electra-base-japanese-discriminator) tokenized with SudachiTra/WordPiece, trained on 200M Japanese sentences and usable with the Hugging Face Transformers library.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - DeBERTa-large-japanese-wikipedia is a pre-trained Japanese language model trained on Wikipedia and Aozora Bunko texts, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built upon a base model trained on 42 billion Japanese tokens from Cultura-X.
  - Downloads: 13
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - This repository details swallow-hermes-st-v1, a Japanese language model created by merging English language model vectors (chat and narrative) using an evolutionary strategy to generate bedtime stories, aiming for a more natural and less rigidly positive tone than models like GPT-4.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 13
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository provides a quantized (GPTQ) version of ELYZA-japanese-CodeLlama-7b-instruct, fine-tuned with a 1k Japanese calibration dataset including Wikipedia and ELYZA-tasks-100, for improved performance.
  - Downloads: 13
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - REV-Mix is an anime and realistic style diffusion model for Stable Diffusion, inspired by DateMix & RDtMix, with recommended settings like DDIM/DPM++ SDE Karras sampling, 40+ steps, and optional embeddings for enhanced quality.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - This repository provides a Japanese BERT-base model (â€œVaporetto + WordPieceâ€) and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 13
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model featuring GEGLU activation and no pre-training dropout, optimized for fine-tuning with dropout re-enabled.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa-v2-based Japanese natural language processing model, pretrained for part-of-speech tagging and dependency parsing using the goeswith subword tokenizer, and requiring the fugashi library.
  - Downloads: 13
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-base Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, leveraging UD_Japanese-GSDLUW and designed to handle ambiguous words with [MASK] tokens.
  - Downloads: 13
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This repository provides a 6.8 billion parameter Japanese pre-trained language model based on EleutherAI's Mesh Transformer JAX, utilizing T5Tokenizer and SentencePiece for Japanese tokenization.
  - Downloads: 13
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - KoichiYasuoka/roberta-base-japanese-aozora is a pre-trained RoBERTa model for Japanese, trained on the Aozora Bunko corpus and tokenized with Japanese-LUW, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset, offering a non-transformer alternative for sequence modeling.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-V2 model for Japanese natural language processing, utilizing the BertJapaneseTokenizer and trained on the Aozora Bunko corpus for tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT datasets, requiring 16kHz sampled input audio.
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja is a fine-tuned, Japanese-language speech-to-text model based on OpenAI's Whisper Small, trained on Common Voice 17.0 with specific hyperparameters like a 1e-05 learning rate and AdamW optimizer.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa(V2) model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependency parsing and part-of-speech tagging, built with goeswith subword tokenization.
  - Downloads: 13
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - Isekai-BERT-v1 is a Japanese BERT-base model fine-tuned on an undisclosed dataset with a reported loss of 1.9164, using a learning rate of 0.0005 and specific batch sizes/seed.
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1ã‚’ huggingface/text-embeddings-inferenceã§å‹•ã‹ã™ãŸã‚ã® fork ã§ã™ã€‚
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause â™»
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - This repository provides a Japanese ELECTRA base model pretrained on Japanese Wikipedia data, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - This repository provides a DeBERTa-large Japanese model, pre-trained on Aozora texts, for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 12
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly improved accuracy, particularly in qualitative evaluations, and requiring SentencePiece for inference.
  - Downloads: 12
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-13B language model, available on Hugging Face.
  - Downloads: 12
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - This model generates Japanese novels, fine-tuned via QLoRA on 216 highly-rated web novels, Aozora Bunko texts, and Wikipedia, and is steerable using genre, keywords, and prompt instructions, with knowledge up to 2021.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following and built upon Japanese Stable LM Base Gamma, requiring Transformers 4.34.0 or newer.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0 or newer.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing the Nothing + WordPiece algorithm, requiring a downloaded dictionary file for proper loading and use.
  - Downloads: 12
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - This repository provides a Japanese chat model, â€œliz-nojaloli-ja,â€ fine-tuned from rinna/japanese-gpt-neox-3.6b, designed for personal use and iteratively updated with a specific dataset for conversational responses formatted with speaker attribution.
  - Downloads: 12
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - Bert-base-sudachitra-v11 is a Japanese language model based on SudachiTra, modified with surface-form nouns and adjusted vocabulary for improved tokenization.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-based model pre-trained on Japanese text (Wikipedia & Aozora Bunko) for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-based question-answering model, pretrained on Japanese literary text (é’ç©ºæ–‡åº«) for dependency parsing and optimized for handling ambiguous words using masked input.
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora texts for Universal POS-tagging and dependency parsing of long-unit words.
  - Downloads: 12
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - This repository details fine-tuning of the japanese-large-lm-3.6b model with WikiBook and Sakura dataset for improved Japanese language instruction following, specifically targeting middle and high school curriculum.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 model, currently offering the Q4_K_M quantization.
  - Downloads: 12
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - Zenz v2.5 is a Japanese GPT-2-based conditional language model specifically designed for kana-to-kanji conversion, offered in small, medium, and large sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is a full, instruction-following language model based on Meta-Llama-3-8B-Instruct, developed and tested on ConoHa VPS with NVIDIA H100 GPUs.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - This repository provides a Japanese language model exhibiting inconsistent persona (shifting gender & unstable personality) but consistently cheerful behavior, intended for experimentation rather than merging, with specific generation parameters for conversational output.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - ModernBERT-base-japanese-char is a pre-trained Japanese language model based on BERT, trained on Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æž)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€text-embeddings-inference (TEI) ã§ã€mecab / unidic ãªã©ã‚’ç”¨ã„ãŸæ—¥æœ¬èªžTokenizerã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€dummy ã® tokenizer.json ã‚’ç”¨ã„ã¦ç„¡ç†ã‚„ã‚Šå‹•ã‹ã™ æ–¹æ³•ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa(V2) model pre-trained on Japanese Wikipedia and Aozora Bunko for universal part-of-speech tagging and dependency parsing of long-unit words.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on Japanese Wikipedia and a literary text corpus, specifically for dependency parsing and handling ambiguity with masked words.
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and RAG tasks with contextual learning, and quantized using both GPTQ and AutoAWQ to achieve performance exceeding GPT-3.5.
  - Downloads: 11
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 large language model, currently offering the Q4_K_M quantization.
  - Downloads: 11
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - This repository hosts a merged language modelâ€”built with mergekit and based on Aratako/Ninja-v1-RP-WIPâ€”optimized for roleplaying through task vector addition and model stock merging, utilizing the Vicuna chat template and requiring `eos_token` for multi-turn conversations.
  - Downloads: 11
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 11
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - This repository provides a 130.78M parameter Llama 2 model trained on Japanese text, utilizing the `transformers` library and a specified training script for causal language modeling.
  - Downloads: 11
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - This repository provides a 417.12M parameter Llama 2 model trained on Japanese text, utilizing the `if001/sentencepiece_ja` tokenizer and demonstrated with a sample prompt and generation code.
  - Downloads: 11
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository provides a Japanese BERT-large model, fine-tuned on the JGLUE/JCommonsenseQA dataset, for performing CommonsenseQA tasks.
  - Downloads: 11
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - This repository provides a XLM-RoBERTa-base model fine-tuned on a Japanese mMARCO dataset using ANCE, with a checkpoint at 50k steps optimized for MRR@100 performance, and includes a link to the dataset preparation script.
  - Downloads: 11
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 is a finetuned Japanese GPT-2 model, trained on the ATOMIC dataset using causal language modeling, and readily usable for text generation with the `transformers` pipeline.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-large Japanese question-answering model, pretrained on Aozora Bunko for dependency parsing and optimized for handling ambiguous words with `[MASK]` tokens.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - DeBERTa-base-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 11
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU is an experimental Japanese-Ainu machine translation model, fine-tuned from T5, with example translations provided and built upon the â€œsonoisa/t5-base-japaneseâ€ pre-trained model.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - This repository provides a Japanese natural language processing pipeline featuring a BERT-based transformer, parser, and NER components, utilizing the UD_Japanese-GSD dataset and licensed under CC BY-SA 4.0.
  - Downloads: 11
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - This repository provides a pre-trained Japanese ELECTRA-small model, leveraging subword tokenization from Japanese Wikipedia with MeCab-ipadic-NEologd for enhanced performance.
  - Downloads: 11
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese RoBERTa base model optimized for super short unit word (SSUW) text, requiring full-width conversion and SSUW segmentation as preprocessing steps.
  - Downloads: 11
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 11
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository provides the DeepSeek-R1-Distill-Qwen-32B-Japanese language model converted to MLX format for use with the `mlx-lm` library.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository provides a fine-tuned modernbert-ja-130m reward model for evaluating the quality of Japanese novels, intended for use with reinforcement learning in generative novel models, and predicts user ratings via regression while acknowledging potential biases beyond text quality.
  - Downloads: 11
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - This repository provides a Japanese BART modelâ€”converted from Kyoto Universityâ€™s originalâ€”for sequence-to-sequence tasks using the BartJapaneseTokenizer and the Hugging Face Transformers library.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 11
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - This repository provides a pretrained Japanese ELECTRA model (ShinoBU) tokenized with SudachiTra/WordPiece, trained on 200M sentences and readily usable with the Hugging Face Transformers library.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset for conversational ability.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - This repository provides a pretrained VITS prosody model for Japanese text-to-speech (TTS) trained with ESPnet2 on the JSUT dataset.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent â™»
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chatã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - æ¦‚è¦ GLM-4-9B-Chatã‚’ã€æ—¥æœ¬èªžã®Wikiãƒ‡ãƒ¼ã‚¿ã‚’é¸å®šã—ã€è¿½åŠ å­¦ç¿’ã—ãŸæ—¥æœ¬èªžã«éžå¸¸ã«å¼·ã„ã‚¹ã‚³ã‚¢ã‚’å‡ºã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
### Syntactic Text Processing
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 3,690
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM-Small is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., readily usable via Hugging Face Transformers.
  - Downloads: 3,298
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-Japanese-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,004
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - This repository provides the BracingEvoMix_v2 model, licensed under CreativeML Open RAIL-M with additional authorship by sazyou_roukaku, and clarifies that users are solely responsible for generated content adhering to the license's restrictions (specifically restriction A prohibiting criminal or specialized uses).
  - Downloads: 2,873
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B is a suite of 7B-parameter decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., and accessible via Hugging Face Transformers.
  - Downloads: 2,531
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow is a continually pre-trained language model, based on Llama 2 and enhanced with Japanese data, offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,518
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - This repository provides GGUF-quantized versions of the Aratako/Qwen3-30B-A3B-ERP-v0.1 large language model, released under the MIT license.
  - Downloads: 2,241
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - This repository provides a GGUF formatted version of the Ninja-v1-NSFW-128k language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 2,171
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 2,031
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B is a suite of 3 billion-parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and readily usable with Hugging Face Transformers.
  - Downloads: 1,969
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,945
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM is a suite of decoder-only language models pre-trained by CyberAgent on Japanese datasets for text generation tasks.
  - Downloads: 1,657
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 1,572
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-8B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,470
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 is a Japanese-enhanced language model continuously pre-trained from Mixtral-8x7B-Instruct-v0.1, utilizing the same tokenizer and excelling in Japanese performance.
  - Downloads: 1,378
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,314
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,290
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,273
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,271
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium is a suite of decoder-only language models pre-trained by CyberAgent on Japanese datasets, readily usable with the `transformers` library for text generation.
  - Downloads: 1,238
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 1,233
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,214
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,184
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - This model, licensed under CreativeML Open RAIL++-M, provides image generation with recommended settings (DPM++ 2M SDE karras, 30-40 steps, 1152x896 resolution) but strictly prohibits generating violent, sexually explicit, or exploitative content, especially involving minors or real individuals without consent.
  - Downloads: 1,124
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - This repository provides GGUF format conversions of the Ninja-v1-NSFW language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for local inference.
  - Downloads: 873
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KillerWhaleã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - This repository provides GGUF formatted versions of rinna's llama-3-youko-8b and other models, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 578
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository provides GGUF format conversions of the llm-jp-3-7.2b-instruct3 Japanese language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp (excluding custom chat templates/`-cvn`).
  - Downloads: 548
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Jpã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 460
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 451
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - This repository provides a Stanza NLP model for Japanese, offering tools for linguistic analysis including syntax and entity recognition, automatically generated via Hugging Face.
  - Downloads: 438
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - This repository provides statically quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model in GGUF format, offering various quantization levels (e.g., Q2_K) for efficient inference.
  - Downloads: 396
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-128k language model, trained on the imatrix dataset, for use with llama.cpp and focused on Japanese novel generation.
  - Downloads: 394
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - This repository presents evaluation results for several Japanese information retrieval models (including splade variants) on MIRACL and JQaRA datasets, reporting metrics like nDCG, Recall, and MRR.
  - Downloads: 376
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªžæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - Wanabi-24B is a Japanese large language model fine-tuned for novel writing assistance, built on Mistral-Small-24B, and currently available in GGUF format with ongoing development and increasing training data in each version (currently v0.3).
  - Downloads: 348
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Commonã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 312
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores archived, experimental, and â€œlametta-styleâ€ SD1.5 merge materials (v1745 + littleMonsters_anime) designed to create heavily deformed and simplified outputs when merged with other models, licensed under the lametta terms.
  - Downloads: 255
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Large-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 249
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - This repository provides a faster, CTranslate2-converted Japanese speech recognition model based on OpenAIâ€™s Whisper-large-v2, optimized for use with the `faster-whisper` library.
  - Downloads: 220
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - This repository provides a GGUF-formatted conversion of the matsuo-lab weblab-10b-instruction-sft model, runnable with llama.cpp, utilizing a modified llama.cpp branch for faster development.
  - Downloads: 214
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - Wanabi-24B is a preview of a 24B Japanese language model fine-tuned for novel writing assistanceâ€”specializing in idea generation, plot development, and coherent continuationâ€”based on Mistral-Small-24B and currently available in GGUF format.
  - Downloads: 197
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - This repository provides GGUF-formatted versions of RakutenAI-7B, a base language model, compatible with llama.cpp for inference and experimentation.
  - Downloads: 193
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-A-v1-7B base model, leveraging models like Shisa, Gamma, and Mistral 7B, for use with llama.cpp.
  - Downloads: 186
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - Shisa-base-7b-v1 is a 7B language model fine-tuned from Mistral 7B with 8B Japanese tokensâ€”sourced from MADLAD-400 and English dataâ€”and an expanded 120k tokenizer for improved Japanese language efficiency (~2.3 characters/token).
  - Downloads: 173
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - This repository provides GGUF-formatted versions of the shisa-7b-v1 language model, demonstrated with examples using llama.cpp for tasks like Japanese prompting and translation.
  - Downloads: 165
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-breadcrumbsã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIæ§˜ã® Llama-3.1-8B-EZO-1.1-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Oumuamua-7b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-v1-7B base model, a merge of models like Shisa Gamma, WizardMath, and Abel, designed for use with llama.cpp.
  - Downloads: 99
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/Ninja-v1-RP-expressive model, requiring users to check the original model for licensing details.
  - Downloads: 93
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides statically quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, offering various quantizations (Q2_K, Q3_K_S) for different size/quality trade-offs.
  - Downloads: 91
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Ninja-v1-RP language model, requiring users to check the original model for licensing details.
  - Downloads: 86
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 81
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on chatntq-ja-7b-v1.0, originally derived from Mistral-7B-v0.1, with details and GGUF versions available.
  - Downloads: 70
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - This repository provides a research-focused, 5000-step fine-tuned version of OpenAIâ€™s whisper-large-v2 model for Japanese speech recognition, trained and evaluated on the CommonVoice v11 dataset, achieving a 0.7449 WER.
  - Downloads: 69
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository releases a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS, incompatible with the original tokenizer and currently in beta.
  - Downloads: 69
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - This repository provides a GGUF-formatted, 7B parameter version of the Deepreneur Blue Lizard model, intended for use with llama.cpp and licensed under the Llama 2 license.
  - Downloads: 65
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 64
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 48
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps speech recognition model to the CTranslate2 format for faster inference using CTranslate2 or projects like faster-whisper.
  - Downloads: 45
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha is a vision-language model generating Japanese descriptions from images and optional text inputs, built on Llama and utilizing transformers for instruction following.
  - Downloads: 42
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - This repository provides static quantized versions of the Japanese-Llama-3-8B-Instruct-v2 model in GGUF format, offering various quantizations (like Q2_K) for different size/performance trade-offs.
  - Downloads: 41
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - This repository provides GGUF-formatted conversions of the Tora-7B-v0.2 model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt text data.
  - Downloads: 39
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - This repository provides a model for generating article bodies from titles, as detailed in the linked Qiita article.
  - Downloads: 36
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIæ§˜ã® Llama-3-EZO-8b-Common-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - This repository provides a GGUF-formatted version of the Tanuki-ZeRo base language model, compatible with llama.cpp for tasks like natural language processing.
  - Downloads: 30
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository provides a Japanese novel quality assessment Reward Model, fine-tuned from sbintuitions/sarashina2.1-1b for use in tasks like reinforcement learning for text generation, predicting user ratings to indirectly evaluate text quality while acknowledging potential biases.
  - Downloads: 30
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - This repository provides a GGUF converted version of ChatNTQ-JA-7b-v1.0, a Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b (based on Mistral 7B).
  - Downloads: 25
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - This repository implements reinforcement learning to generate more character-consistent conversational responses.
  - Downloads: 24
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - Suzume-POC is a commercially usable, Japanese-adapted base model derived from Googleâ€™s Gemma-2B, designed for resource-constrained devices despite potential instruction-tuning challenges.
  - Downloads: 24
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 22
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - This repository provides GGUF-formatted conversions of the Tora-7B-v0.1 language model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt text data.
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAEã®å†…è‡“ã¯ãªã„ãžï¼ã¨è¨€ã‚ã›ãªã„ãžï¼ï¼ï¼ï¼
  - Downloads: 21
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - This repository provides a Japanese language model example utilizing AutoTokenizer, AutoModelForCausalLM, and Unifine format for in-context and instruction learning with formatted text input/output.
  - Downloads: 20
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - This repository provides Mixtral-8x7B-v0.1-japanese, a Japanese language model built upon Mixtral-8x7B-v0.1 with extended vocabulary and continued pre-training, as detailed in ABEJAâ€™s tech blog.
  - Downloads: 20
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: æ—¥æœ¬èªžã§è³ªå•ã™ã‚‹ã¨ã€æ—¥æœ¬èªžã§å›žç­”ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚
  - Downloads: 20
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - This repository implements Named Entity Recognition (NER) using the modernBERT Japanese language model ("sbintuitions/modernbert-ja-130m") with a custom tokenizer and defined label set for entities like person, organization, location, and product names.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - This repository showcases a Japanese text generation example using the Phos 7B model, demonstrating a narrative exchange with a pleading entity seeking aid before its demise.
  - Downloads: 17
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - Tokara-0.5B-v0.1 is a Japanese-enhanced language model, fine-tuned from Qwen1.5-0.5B on 5B Japanese-English tokens to improve stable Japanese output, despite slightly lower benchmark scores.
  - Downloads: 16
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - This repository implements a reasoning model focused on famous quotes ("åè¨€") for tasks like inference and understanding.
  - Downloads: 15
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on Mistral-7B-v0.1 and chatntq-ja-7b-v1.0, with available 6-bit quantized and GGUF versions.
  - Downloads: 15
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - This repository provides an LLava-compatible model built upon Vecteus, incorporating EvoVLM-JP-v1-7B for enhanced expressiveness.
  - Downloads: 15
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B is a 7 billion parameter Japanese language model, fine-tuned with LAPT and heuristics, and available via PEFT and Transformers for efficient 8-bit loading and GPU use.
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - This repository hosts a merged language modelâ€”based on OpenBioLLM-8B and Llama-3-youko-8b-instruct-chatvectorâ€”aiming to improve Japanese-language biomedical knowledge, though it may exhibit hallucinations and weakened safety filters.
  - Downloads: 14
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - This repository offers a merged language modelâ€”based on Aratako/Ninja-v1-RP and derived from Elizezen/Antler-7Bâ€”enhanced for expressive roleplaying using the Vicuna chat template and requiring an `eos_token` for multi-turn conversations.
  - Downloads: 14
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - This repository provides a Japanese language Bloom model with 10,000 vocabulary size, 12 layers, and 8 attention heads.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - YACIS-ELECTRA-Small is a pretrained Japanese language model based on ELECTRA-Small, trained on a 5.6 billion-word blog corpus using MeCab and WordPiece tokenization.
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - This repository modifies the tokenizer and chat template of tokyotech-llm/Swallow-MS-7b-instruct-v0.1, potentially altering system prompt behavior for a Japanese assistant.
  - Downloads: 13
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - This repository provides a streaming-trained, dummy Japanese tokenizer based on the snow_simplified_japanese_corpus, usable with Hugging Face Transformers for basic Japanese sentence tokenization and further customization via the `tokenizer.py` script.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - This repository provides a Japanese SentencePiece tokenizerâ€”with a 52000 vocabularyâ€”specifically trained for creative writing tasks with AI Novelistâ€™s SuperTrin and Damsel 20B models.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - â—†ArcanaMix äºŒæ¬¡å…ƒã‚¤ãƒ©ã‚¹ãƒˆã‚’ä¸­å¿ƒã«ã€ã‹ã‚ã„ã„ã‚¤ãƒ©ã‚¹ãƒˆãŒå‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€‚
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - Sarashina2.2-3Bx4-moe is a 12B parameter Mixture of Experts (MoE) model created by merging one base model with three â€œsbintuitions/sarashina2.2-3b-instruct-v0.1â€ expert models to enhance performance and generate high-quality responses.
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - This model is a QLoRA-finetuned version of sbintuitions/sarashina2.2-3b-instruct-v0.1, enhanced with Japanese-Pythonic-FunctionCall and Kendamarron/jimba-instruction-all to enable calling Python functions via a specific system tool format.
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This model fine-tunes SakanaAI/TinySwallow-1.5B-Instruct with reinforcement learning to automatically generate numerous, concise, bullet-point slidesâ€”each with a maximum of 15 charactersâ€”in the style of the Takahashi Method for impactful presentations.
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - DataPilotâ€™s Arrival-32B-Instruct-v0.4 is a Japanese-enhanced, instruction-tuned language model finetuned from Abejaâ€™s Qwen2.5-32B base (using a negative chat vector due to limited access) and incorporating DeepSeek-R1-Distill-Qwen-32B-Japanese.
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6Bæ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­å¤§æ¨¡åž‹ï¼Œæœ¬é¡¹ç›®ä¸ºChatGLM3-6BåŠ å…¥æ—¥æ–‡èƒ½åŠ›ã€‚
  - Downloads: 12
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository provides details and links to the AfterRealXL_beta2 modelâ€”licensed under CreativeML Open RAIL++-Mâ€”along with merged checkpoints, clarifying usage restrictions and disclaiming liability for generated content.
  - Downloads: 11
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares experimental, lightly-adjusted merged models derived from `lametta_v1921` and other sources like `vorpal_v1` and `snowpearAnime_v10`, intended as shareable "leftovers" rather than polished releases.
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details â€»å¥½å¥‡å¿ƒã‹ã‚‰ç”Ÿã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2ã®ãƒžã‚¤ãƒŠãƒ¼ãƒã‚§ãƒ³ã‚¸ç‰ˆã§ã™ã€‚
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is an English-to-Japanese translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via the Hugging Face `transformers` pipeline.
  - Downloads: 75,732
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT is a Japanese-to-English translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via Hugging Face pipelines and evaluated on the Tatoeba dataset.
  - Downloads: 56,699
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - Shisa-gamma-7b-v1 is a Japanese language model fine-tuned from Stable LM Base Gamma 7B, showcasing strong performance based on JA MT-Bench results and built upon the Shisa 7B model.
  - Downloads: 21,360
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªžã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 10,197
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source, multilingual (including Chinese, English, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 5,634
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - Ultralytics YOLOv11 is a state-of-the-art, fast and flexible model for object detection, tracking, segmentation, classification, and pose estimation, building on previous YOLO versions.
  - Downloads: 5,011
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 4,219
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX language model, trained on a large corpus including Japanese CC-100, C4, and The Pile, based on EleutherAIâ€™s GPT-NeoX architecture.
  - Downloads: 3,097
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-8B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,805
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a 2B parameter small language model delivering near-7B model translation quality for Japanese-English and English-Japanese tasks with a compact ~2GB file size.
  - Downloads: 2,738
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - This repository provides a GGUF-formatted version of the Aya-23-8B language model, trained with data from TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its use with llama.cpp.
  - Downloads: 2,719
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL is a 260M parameter translator modelâ€”finetuned from sbintuitions/modernbert-ja-130m and Dart v3â€”that converts Japanese and English into Danbooru tags, licensed under Apache-2.0.
  - Downloads: 1,934
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - This repository provides a Japanese-to-Korean translation model leveraging BERT for Japanese encoding and KOGPT2 for Korean decoding, with a Hugging Face Space demo and dependencies including PyTorch and Transformers.
  - Downloads: 1,718
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,653
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FinguAI-Chat-v1 is a language model designed to improve English, Korean, and Japanese proficiency through a finance, investment, and legal curriculum with a global market focus.
  - Downloads: 1,588
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - This repository provides a 4-bit quantized version of the Gemma 3B-IT model, optimized with a Japanese-rich imatrix for use with the latest llama.cpp, enabling image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 1,587
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 886
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - This repository provides GGUF-formatted conversions of the multilingual Suzume-Llama-3-8B model, built with data from TFMC/imatrix-dataset-for-japanese-llm, alongside links to other related Lightblue and mmnga language models.
  - Downloads: 876
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained on 2.5T tokens by OrionStarAI, with associated demos, benchmarks, and technical documentation.
  - Downloads: 863
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 667
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-inclusive, 4-bit quantized version of the Gemma 3B model, optimized for use with the latest llama.cpp and capable of processing images via `llama-mtmd-cli` and the `mmproj.gguf` file.
  - Downloads: 596
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT-BT-ja-en is an openly licensed Japanese-to-English translation model, fine-tuned from ElanMT-base-ja-en using back-translated Wikipedia and CC0/BY/SA data, avoiding web-crawled or machine-translated corpora.
  - Downloads: 577
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - This repository provides GGUF versions of the Mistral-nemo-ja-rp-v0.2 Japanese language model, referencing the original model for details.
  - Downloads: 486
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªžã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 479
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin-inst-merge language model, trained with data from TFMC/imatrix-dataset-for-japanese-llm and licensed under Tongyi-Qianwen.
  - Downloads: 440
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - This repository provides GGUF-formatted conversions of the lightblue-suzume-llama-3-8B Japanese language model, built using imatrix data, alongside links to other related lightblue/mmnga models.
  - Downloads: 440
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleæ§˜ã® google/gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 333
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [shisa-ai/shisa-v2-llama3.1-405b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b)
  - Shisa V2 is a family of improved, bilingual (Japanese/English) chat models by Shisa.AI, focusing on enhanced Japanese language performance with robust English capabilities through increased Japanese pre-training and tokenizer efficiency.
  - Downloads: 242
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi-mini-12B-GGUF is a Japanese large language model fine-tuned for novel writing assistance, offering comparable functionality to wanabi-24B in a more accessible, quantized GGUF format for consumer GPUs with 8GB+ VRAM, and trained on a high-quality, focused dataset.
  - Downloads: 230
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, optimized for Japanese language performance with a custom tokenizer and 8B additional Japanese tokens during pre-training.
  - Downloads: 222
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - This model is based on CreativeML Open RAIL-M, adding â€œsazyou_roukakuâ€ as an additional author, and usage is subject to the original licenseâ€”specifically excluding use case A (criminal/specialized applications)â€”with no liability assumed by the author.
  - Downloads: 218
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 215
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - QuinceMix â€œDefactaâ€ is a merged Stable Diffusion model excelling in backgrounds and effects, optimized for settings like DDIM/DPM++ SDE Karras samplers, 20-30 steps, and utilizing "EasyNegative" with recommended CFG scales of 5-8 and denoise strengths of 0.4-0.7.
  - Downloads: 194
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin Japanese language model, utilizing TFMC/imatrix data and licensed under Tongyi-Qianwen, compatible with llama.cpp.
  - Downloads: 160
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository provides GGUF conversions of rinna's japanese-gpt-neox-3.6b-instruction-ppo model, intended for use with llama.cpp, but potentially incompatible after official GPT-Neox support is added.
  - Downloads: 158
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - This repository provides a VAE-integrated model, SakuraMixSeries, prioritizing both background and character quality, licensed under a modified CreativeML OpenRAIL-M license permitting commercial use, modification, and sharingâ€”even resaleâ€”without attribution.
  - Downloads: 146
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased training data and tokenizer efficiency.
  - Downloads: 137
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the Karasu-Mixtral-8x22B-v0.1 language model, along with other related lightblue and mmnga models, trained on the TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 112
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - This repository details llm-jp-clip-vit-large-patch14, a 467M parameter Japanese CLIP model trained on a translated ReLAION-5B dataset, providing tools for zero-shot image classification using `open_clip_torch`.
  - Downloads: 111
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 is a family of bilingual (Japanese/English) general-purpose chat models from Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 105
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT-BT-en-ja is an English-to-Japanese translation model built by ELAN MITSUA, fine-tuned from a base model trained solely on openly licensed and Wikipedia back-translated data, avoiding web-crawled or machine-translated corpora.
  - Downloads: 102
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for enhanced Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 96
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T corpus of Chinese, English, and Japanese data, with available demos, benchmarks, and technical documentation.
  - Downloads: 92
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - This repository provides a 1.3B parameter NLLB model fine-tuned for Japanese to English translation of light novels, capable of processing up to 512 tokens.
  - Downloads: 90
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source multilingual large language model series by OrionStarAI, trained on a 2.5T corpus with support for Chinese, English, Japanese, and Korean, offering demos, benchmarks, and a tech report.
  - Downloads: 86
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 85
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 82
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance through increased training data and optimized tokenization.
  - Downloads: 74
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medLLM is a trilingual (English, Japanese, Chinese) biomedical large language model built on Llama-3-8B, trained via continued pre-training and supervised fine-tuning for enhanced domain expertise and multilingual capabilities.
  - Downloads: 70
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - ã€ŒLLM-jp-3 172Bã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172Bã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 66
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 64
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 63
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 59
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instructã‚’CoTãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸreasoningãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 54
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - This repository provides a T5-fine-tuned machine translation model (â€œfriendly_JAâ€) that simplifies Japanese for English speakers by prioritizing Latin/English-derived *katakana* over traditional Sino-Japanese vocabulary.
  - Downloads: 47
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 45
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and a tech report.
  - Downloads: 44
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 44
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source multilingual large language model series from OrionStarAI, trained on 2.5T tokens of diverse languages including Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 42
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - This repository hosts a Japanese large language model, built upon Llama2-13b and fine-tuned on *okashi* (pun/gag) data, featuring a 45,046-token vocabulary expanded for Japanese language processing with support from AWS and utilizing their Trainium instances.
  - Downloads: 41
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - This repository details llm-jp-clip-vit-base-patch16, a 248M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset using OpenCLIP for zero-shot image classification.
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 36
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 35
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - This repository provides a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model with an extended 8192 context length, fine-tuned using RoPE and built upon EleutherAIâ€™s GPT-NeoX architecture.
  - Downloads: 35
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling is a 7B multilingual language model continually pretrained on Korean, English, Chinese, Japanese, and a 500-language corpus, based on Google's Gemma-7B.
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a DPO-trained, Japanese-language version of Qwen/Qwen2.5-7B-Instruct, improving conversational performance and recommended for general AI chat applications.
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix æ¦‚è¦ / Overview Yaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 32
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and technical documentation.
  - Downloads: 28
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This repository provides a fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en, trained on the bsd_ja_en dataset.
  - Downloads: 27
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - This repository provides the Japanese-Alpaca-2-13B language model in GGUF format, sourced from Hugging Face.
  - Downloads: 26
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B is a LLaMA 2-based Japanese-English bilingual LLM trained with the LEIA technique to improve cross-lingual transfer and achieve enhanced performance on Japanese question answering benchmarks.
  - Downloads: 24
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides a transformer-aligned Japanese-to-Malay machine translation modelâ€”jpn-msaâ€”trained on OPUS data with SentencePiece preprocessing and requiring a language ID token for input.
  - Downloads: 23
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source, multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 23
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a Marian-NMT translation model leveraging transformers and sentencepiece to translate from German, English, Spanish, French, Italian, Russian, and Ukrainian to Japanese, trained with ParaCrawl data.
  - Downloads: 23
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 is a 70B language model currently exhibiting performance issuesâ€”potentially due to bugs related to repetition penalty and temperatureâ€”resulting in lower benchmark scores than its predecessor, Swallow.
  - Downloads: 22
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - This repository provides a Japanese-input compatible version of the SDXL 1.0 base model, fine-tuned using a novel approach on the text encoder (OpenCLIP-ViT/G, CLIP-ViT/L) with Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 22
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for superior Japanese language performance.
  - Downloads: 21
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for improved Japanese language performance.
  - Downloads: 21
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - This repository details the karakuri-MS-01 model.
  - Downloads: 21
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository provides a quantized (.exl2) merged language model (Qwen-14B family) specifically for translating Japanese game scripts into fluent Chinese, utilizing provided character and historical context within prompts.
  - Downloads: 20
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - This project provides a Japanese-to-English machine translation model fine-tuned for Weiss Schwarz (WS) trading card text, accessible via a Gradio app and runnable locally via a cloned Hugging Face Spaces repository.
  - Downloads: 20
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - This repository provides a LoRA-fine-tuned 8B Llama 3 Youko model for English-to-Japanese translation, achieving a COMET score of 0.9126 and BLEU of 35.2 on Flores200.
  - Downloads: 20
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - This repository provides a fine-tuned MPT-7B base model (Jumtra/mpt-7b-base) achieving 47% accuracy on a 100QA dataset, requiring `trust_remote_code=True` due to its custom architecture and incorporating training efficiencies like FlashAttention.
  - Downloads: 20
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 20
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on 2.5T tokens of diverse text including Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 18
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - This repository provides a Japanese mT5-based doc2query model to enhance document retrieval by generating synonym-rich queries for improved BM25 indexing and lexical search relevance.
  - Downloads: 18
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - This repository provides a Japanese language model built upon Llama-2-13B, enhanced with Mixture-of-Experts (MoE) using mergekit, and based on the elyza/ELYZA-japanese-Llama-2-13b and elyza/ELYZA-japanese-Llama-2-13b-instruct models, licensed under Llama 2 Community License.
  - Downloads: 17
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is a Japanese-English bilingual LLM built on LLaMA 2, enhanced with the LEIA training technique to improve cross-lingual transfer and performance on Japanese question-answering tasks.
  - Downloads: 17
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - This repository provides Swallow-MoE-2x13B-v0.1, a merged Mixture-of-Experts model built upon the Llama-2 based Swallow-13b-instruct-hf and Superswallow-13b-v0.2, inheriting both Llama 2 and potentially AI2 ImpACT licenses.
  - Downloads: 17
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 16
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B provides both a foundational 7B parameter language model and a LoRA-adapted version for Japanese language processing.
  - Downloads: 16
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - This repository provides a work-in-progress Japanese-to-English translation model built on tinyllama, designed for long-context inputs (500-1000 tokens) and requiring deterministic inference settings.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - This repository provides a 2x7B Mixture-of-Experts (MoE) Japanese language model built by merging and fine-tuning the instruction-tuned and base versions of elyza/ELYZA-japanese-Llama-2-7b-fast, inheriting the Llama 2 license.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository provides a 2x7B Mixture-of-Experts (MoE) model built by merging and instruction-tuning the ELYZA-japanese-Llama-2-7b and ELYZA-japanese-Llama-2-7b-instruct models, licensed under Llama 2 Community License.
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, optimized for strong Japanese performance via synthetic data and a more efficient tokenizer, with additional pre-training on 8B Japanese tokens.
  - Downloads: 15
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba is a multilingual NLI/text classification model based on XLM-RoBERTa, served via TensorFlow, and trained on diverse public and private datasets including GLUE, CLUE, and JGLUE.
  - Downloads: 14
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - This repository details the karakuri-midrose-mg model.
  - Downloads: 14
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - This repository provides a Japanese-adapted refiner model for Stable Diffusion XL 1.0, achieved by fine-tuning the OpenCLIP-ViT/G or CLIP-ViT/L text encoder using Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 14
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - ByT5-small-ain-jpn-mt is a pretrained and fine-tuned machine translation model for Ainu-to-Japanese translation, built using Googleâ€™s ByT5-small architecture and web-crawled bilingual data.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime ã¯æ—¥æœ¬èªžã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - This repository provides full and LoRA versions of the Japanese-Alpaca-2-13B instruction-following model, built upon the Japanese-LLaMA-2-13B base model.
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹HODACHI/Llama-3.1-70B-EZO-1.1-itã®ggufç‰ˆã§ã™ã€‚
  - Downloads: 12
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - This repository provides base and instruction-tuned LLaMA 2 models (13B parameters) for Japanese language processing, including both full models and LoRA adaptations.
  - Downloads: 11
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - This repository provides a compiled version of the Watashiha-Llama-2-13B-Ogiri-sft model optimized to run on AWS inf2 instances using Neuron, requiring an inf2.xlarge instance with at least 256GB storage and the Deep Learning AMI Neuron PyTorch 1.13.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool utilizing a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga content, handling diverse layouts, fonts, and image qualities.
  - Downloads: 400,252
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - This repository provides a GGUF-formatted, Japanese-focused quantization of the DeepSeek-V3 model, specifically reconfigured with carefully selected MoE experts for improved performance on Japanese examples, and designed for use with llama.cpp.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - Rinnaâ€™s Japanese CLIP model, `rinna/japanese-clip-vit-b-16`, enables contrastive language-image pre-training for Japanese text and images, installable via pip and utilizing PyTorch.
  - Downloads: 27,061
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - This repository provides a Japanese CLIP model, trained on 1 billion image-text pairs, for zero-shot image classification and multimodal retrieval tasks.
  - Downloads: 10,444
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet-tdt_ctc-0.6b-ja is a 600M parameter Japanese Automatic Speech Recognition (ASR) model developed by NVIDIA NeMo, utilizing a Hybrid FastConformer TDT-CTC architecture to transcribe speech with punctuation.
  - Downloads: 8,829
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - This repository provides a 619M parameter, subword-based RNN-T ASR modelâ€”built with a Longformer-enhanced Conformer architectureâ€”for efficient, long-form Japanese audio transcription using the ReazonSpeech v2.0 corpus.
  - Downloads: 7,274
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is a Japanese ASR model building on kotoba-tech/kotoba-whisper-v2.0, enhanced with integrated punctuation and postprocessing pipelines developed collaboratively by Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,956
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the Umievo-itr012-Gleipnir-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp.
  - Downloads: 2,518
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B is a Japanese and English vision-language model built on the NVILA-Lite architecture, utilizing Qwen2.5-1.5B-Instruct and a paligemma-siglip vision encoder, with installation requiring specific versions of transformers, accelerate, opencv-python, and torch.
  - Downloads: 2,184
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing Qwen2.5-14B-Instruct and a paligemma-siglip vision encoder, with installation requiring specific versions of `transformers`, `accelerate`, and `opencv-python`.
  - Downloads: 1,593
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is a Japanese ASR model building on kotoba-tech/kotoba-whisper-v1.0, enhanced with integrated postprocessing pipelines for automatic punctuation.
  - Downloads: 1,297
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a multilingual speech foundation model offering ASR, LID, SER, and AED capabilities, with available models on Modelscope & Hugging Face and online demos.
  - Downloads: 1,240
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - Karamaru is a conversational AI model by Sakana AI fine-tuned on historical Edo-period Japanese textâ€”including both human-transcribed and AI-OCR-converted sourcesâ€”to generate responses in that style.
  - Downloads: 1,058
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - This repository provides a Japanese CLIP model for encoding text and images, enabling tasks like similarity calculation, embedding, and multimodal search, with accompanying tutorials and a demo using illustrations.
  - Downloads: 799
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - This repository provides a GGUF-formatted version of the DataPilot-ArrowPro-7B-KUJIRA language model, trained with TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 610
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - This repository provides a Japanese Contrastive Language-Image Pretrained (CLIP) modelâ€”built on ViT-B/32 and RoBERTa-baseâ€”for tasks like zero-shot image classification and text-image retrieval.
  - Downloads: 554
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the ELYZA-Shortcut-1.0-Qwen-7B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 490
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B is a Japanese and English vision language model built on the NVILA-Lite architecture using Qwen2.5-0.5B-Instruct and a paligemma-siglip vision encoder, requiring transformers 4.45.0, accelerate, and opencv-python.
  - Downloads: 460
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-performing 8B Japanese Vision Language Model, built on Sarashina2-7B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 439
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - This repository provides a GGUF format conversion of the DataPilot-ArrowPro-7B-RobinHood language model, trained on the TFMC/imatrix-dataset-for-japanese-llm dataset and usable with llama.cpp.
  - Downloads: 419
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - This repository provides a fine-tuned OpenAI Whisper-large-v3 model for Japanese speech recognition, trained on Common Voice 16.1 with 4000 steps, achieving a 0.4057 loss and reported WER on an evaluation set.
  - Downloads: 378
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a high-performing Japanese large vision language model, built on Sarashina2-13B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 316
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - This repository provides a GGUF-formatted version of the cyberagent's Mistral-Nemo-Japanese-Instruct-2408 model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 312
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - This repository provides a GGUF-formatted version of the stockmark-100b language model, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm and runnable with llama.cpp.
  - Downloads: 301
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B is a text-to-speech model based on sarashina2.2â€‘0.5bâ€‘instructâ€‘v0.1 and XCodec2, offering fine-grained control over voice characteristics via prompt engineering, similar to Parler-TTS, and built on a Llama base.
  - Downloads: 300
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - This repository provides a Donut base model fine-tuned on a synthetic dataset of visual novel images for improved text recognition in that style, with example usage in a Colab notebook.
  - Downloads: 276
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - This repository provides a Japanese text-to-speech modelâ€”SpeechT5 fine-tuned on the JVS dataset of 100 speakersâ€”using 16-dimensional speaker embeddings for voice independence.
  - Downloads: 266
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B is a Japanese & English vision language model built on the NVILA-Lite architecture using Qwen2.5-32B-Instruct and a siglip2 vision encoder, requiring specific transformer & library versions for setup.
  - Downloads: 181
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - NABLA-VL is a Japanese vision-language model by NABLAS that processes images, multiple images, and videos to understand and generate text for multimodal tasks.
  - Downloads: 174
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - This repository provides a GGUF-formatted version of the QwQ-32B language model, utilizing TFMC/imatrix-dataset-for-japanese-llm data and compatible with llama.cpp for inference.
  - Downloads: 156
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a multilingual text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio, with a demo available at Fish Audio.
  - Downloads: 151
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 æ˜Žç¤ºçš„ãªè¨±è«¾ã‚’å¾—ãŸã‚ªãƒ—ãƒˆã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªž/è‹±èªžãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«CLIP (Contrastive Language-Image Pre-training)ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 149
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input audio.
  - Downloads: 148
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - This repository provides a GGUF version of the Ocuteus model, optimized for use with Koboldcpp, and recommends lower resolution images due to token limits, with a suggested context size of 16384.
  - Downloads: 130
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - This repository provides a fine-tuned Japanese Hubert-base ASR model, trained on common_voice_11_0, specifically for predicting Hiragana with reported WER results.
  - Downloads: 76
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - This repository provides GGUF-formatted, K-quantized language models derived from Local-Novel-LLM, enhanced with iMatrix using the TFMC c4_en_ja_imatrix.txt text dataset.
  - Downloads: 74
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - This repository provides optical character recognition (OCR) specifically tailored for Japanese text, particularly within Japanese manga.
  - Downloads: 71
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT corpora, requiring 16kHz sampled audio input.
  - Downloads: 55
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B Japanese text-to-speech model based on end-to-end transformers, offering fluent generation and one-shot voice cloning, building upon the metavoice framework.
  - Downloads: 53
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - This repository provides GGUF-formatted and K-quantized versions of the Japanese-Chat-Umievo-itr004-7b language model, utilizing iMatrix with the c4_en_ja_imatrix.txt text for improved performance.
  - Downloads: 50
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool utilizing a Vision Encoder Decoder framework, optimized for accurately extracting text from manga, including vertical text, furigana, and varied font styles, while also functioning as a general Japanese OCR solution.
  - Downloads: 49
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - This repository provides GGUF conversions of the ArrowPro-7B-KUJIRA model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 45
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a Japanese vision-language model built by fine-tuning llm-jp/llm-jp-1.3b with the LLaVA method on datasets like LLaVA-CC3M and Japanese Visual Genome for image-based conversation.
  - Downloads: 45
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - This repository provides a fine-tuned XLSR-53 model for Japanese two-speaker speech diarization, specifically trained on CallHome phone call data using Wav2Vec2.
  - Downloads: 43
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository provides a CTranslate2-formatted version of the vumichien/whisper-large-v2-jp speech recognition model, enabling faster transcription via CTranslate2 and projects like faster-whisper.
  - Downloads: 42
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max is a 1.3B parameter Japanese vision-language model built on LLaVA architecture, utilizing a ConvNeXt Large vision encoder and trained on a custom Japanese dataset with 1280x1280 resolution and 1024 token context length.
  - Downloads: 40
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR is a Japanese text recognition system built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga textâ€”including vertical writing, furigana, and varied fontsâ€”but also usable for general printed Japanese.
  - Downloads: 39
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - This repository provides a fine-tuned wav2vec2-base model for Japanese automatic speech recognition (ASR) that predicts Hiragana, achieving a Word Error Rate (WER) of 1.0 after 1000 training steps on the common_voice_11_0 dataset.
  - Downloads: 39
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository converts the whisper-large-v2-mix-jp speech recognition model to the CTranslate2 format for faster and efficient transcription, compatible with tools like faster-whisper.
  - Downloads: 38
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - This repository details a Japanese language Stable Diffusion model, trained to generate PokÃ©mon images from text prompts, utilizing the diffusers library and licensed under CreativeML OpenRAIL-M.
  - Downloads: 36
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - This repository provides a 150M parameter text-to-speech (TTS) model, Canary-TTS-150M, built on llm-jp/llm-jp-3-150m-instruct3 and XCodec2, enabling fine-grained voice control via prompt-based adjustments like Parler-TTS.
  - Downloads: 36
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - Rinnaâ€™s Japanese data2vec Audio Base model is a 12-layer transformer trained on 19,000 hours of Japanese audio, replicating the original data2vec architecture and training process.
  - Downloads: 29
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer is a fine-tuned Whisper-large-v3 model for Japanese speech recognition that transcribes audio, detects non-speech sounds, and adds punctuation, requiring specific post-processing for optimal performance.
  - Downloads: 28
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for Japanese audio transcription directly into Hiragana, achieving a 0.2227 Character Error Rate on the Common Voice dataset after converting all text to Hiragana via pykakasi and tokenizing with fugashi.
  - Downloads: 27
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech recognition, achieving 9.34% CER on Common Voice Japanese data with 16kHz audio input and continuous sentence output.
  - Downloads: 26
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªžã«å¯¾å¿œã—ã¦ã„ã‚‹Llama-3ãƒ™ãƒ¼ã‚¹ã®ï¼”ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒžãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 25
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on ~60 hours of combined Japanese speech datasetsâ€”Common Voice, JUST, JSSS, and CSS10â€” achieving benchmark WER results.
  - Downloads: 24
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - This repository provides a Japanese CLIP model (ViT-H/14) pretrained for multimodal tasks like zero-shot image classification, mapping Japanese text and images into a shared embedding space under a CC BY-NC-SA 4.0 license.
  - Downloads: 23
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - ReazonSpeech-espnet-next provides cutting-edge, freely-available Japanese Automatic Speech Recognition (ASR) models and datasets developed by the ReazonSpeech team, encouraging community feedback and collaboration.
  - Downloads: 23
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - This repository provides a fine-tuned Whisper Large V3 model for accurate Japanese speech transcription to Katakana with pitch accent annotation, trained on Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 23
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned wav2vec2-xls-r-300m model for Japanese speech recognition, trained on the Mozilla Common Voice 8.0 dataset with Kanji-to-Hiragana conversion, and evaluated using Character Error Rate (CER).
  - Downloads: 21
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - This repository provides a Japanese CLIP model (ViT-H/14) enabling zero-shot image classification and multimodal tasks by aligning Japanese text and images in a shared embedding space, licensed under CC BY-NC-SA 4.0.
  - Downloads: 21
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO is a Japanese-specific Stable Diffusion model fine-tuned for generating photorealistic images from text prompts, built upon ðŸ¤— Diffusers.
  - Downloads: 19
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - This repository provides a Japanese-only Text-to-Speech (TTS) model, â€œAmitaro,â€ finetuned from Plachtaa's VITS using free voice data and trained for 600 epochs by Lycoris52.
  - Downloads: 18
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - This repository provides a Japanese VL-T5 model pretrained on a Japanese corpus for unifying vision-and-language tasks via text generation.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 â™»
  - Downloads: 17
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the Heron library, capable of image-based conversation and utilizing LlamaTokenizer.
  - Downloads: 16
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - This repository provides an ESPnet-based Japanese automatic speech recognition (ASR) model trained on the 15,000-hour ReazonSpeech corpus, requiring 16kHz sampled audio input.
  - Downloads: 16
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech model for a specific language, trained on datasets like Common Voice, and requiring 16kHz sampled input.
  - Downloads: 15
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM Base 7B is a vision-language model, trained with the Heron library, enabling conversational image understanding and demonstrated through a provided demo with code examples.
  - Downloads: 15
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 â™»
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 â™»
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa æ¦‚è¦ tokyotech-llm/Swallow-7b-hfã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ä»¥ä¸‹ã®4ãƒ¢ãƒ‡ãƒ«ã‚’gate_mode=randomã§MoEã—ã€ãã®å¾ŒLISAã¨ã„ã†æ‰‹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This repository provides a Style Bert VITS2 voice clone capable of English, Japanese, and Chinese text-to-speech generation, featuring a young, neutral voice suitable for diverse applications like virtual YouTubers.
  - Downloads: 14
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the Heron library, capable of image-based conversation and utilizing Llama tokenizer.
  - Downloads: 14
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - This repository offers a fine-tuned OpenAI Whisper tiny model for Japanese speech recognition, achieving a 225.23 WER on the Common Voice dataset with a learning rate of 1e-05.
  - Downloads: 14
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - This repository provides a VITS-TTS Japanese voice model finetuned on Sakura Miko's voice data, intended for non-commercial, secondary creative works as per Cover Corporation's guidelines.
  - Downloads: 14
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - This repository provides a free, commercially-usable voice generation modelâ€”a â€œcoolâ€ version of RikkaBotanâ€”specializing in childlike, gentle speech ideal for reading text, with alternate versions for emotionality, English, ASMR, and Chinese language output.
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool utilizing a Vision Encoder Decoder framework, specifically designed for high-quality OCR of mangaâ€”handling vertical/horizontal text, furigana, diverse fonts, and low-quality images.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave â™»
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - â– endlessMixã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦ æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Defactaã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸéšŽå±¤ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, enabling joint embedding of Japanese text and images under a CC BY-NC-SA 4.0 license.
  - Downloads: 12
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository provides a free, commercially-usable voice modelâ€”an ASMR version of RikkaBotanâ€”specializing in gentle, childlike voices, with variations for emotional, English, Chinese, and logical speech styles.
  - Downloads: 11
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a Japanese-specific, fine-tuned version of OpenAI's Whisper-tiny model, trained on the Common Voice dataset, for real-time speech recognition with a WER of 301.625840.
  - Downloads: 11
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA is a fine-tuned speech-to-text model for Japanese, trained on the Common Voice 11.0 dataset and achieving a CER of 17.7261 on the SVJ Japanese dataset.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave â™»
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTæ§˜ã® AXCXEPT/EZO-gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a 100GB corpus of Japanese text, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in the training data.
  - Downloads: 9,357
- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and evaluation models for improved translation performance.
  - Downloads: 6,003
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.3-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 4,835
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS-mini-v1, featuring a custom tokenizer and offering high-quality voice synthesis.
  - Downloads: 2,838
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - This repository hosts the v2 release of chilled_remix and reversemix models, licensed under CreativeML Open RAIL-M with added authorship by sazyou_roukaku, and clarifies that users are responsible for generated content adhering to the licenseâ€™s restrictions.
  - Downloads: 2,397
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitionsâ€™ sarashina2.2-0.5B-instruct-v0.1 is a Japanese autoregressive language model evaluated on Japanese & English tasks, achieving competitive performance against other models like Qwen and RakutenAI.
  - Downloads: 2,280
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - This repository provides a licensed AI model (BraV6, XXMix_9, Soda Mix) with usage restrictions prohibiting violent, sexually explicit, or exploitative contentâ€”especially involving minorsâ€”and requiring consent for depictions of real people.
  - Downloads: 1,826
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the ABEJA-Qwen2.5-7b-Japanese-v0.1 large language model, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 1,123
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) v1.1 model pretrained on a 100GB corpus of Japanese text data, requiring fine-tuning for specific tasks and acknowledging potential biases present in the training data.
  - Downloads: 1,113
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-ELYZA-JP-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-35Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 993
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 848
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen2.5-bakeneko-32b-instruct-v2 language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for Japanese language tasks.
  - Downloads: 841
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-2-2b-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 788
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/gemma-3-4b-it-RP-v0.1 model, inheriting the Gemma Terms of Use and Prohibited Use Policy.
  - Downloads: 782
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠåˆ†ã»ã©ãŒæ—¥æœ¬èªžãªã®ã§magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªžã«ã¯å¼·ã„ã¯ãšï¼Ÿ
  - Downloads: 727
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - This repository provides a GGUF-formatted version of the Qwen3-30B-A3B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is designed for use with llama.cpp.
  - Downloads: 678
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - This repository provides a fine-tuned mt5-small model for Japanese summarization, specifically trained on BBC news articles to generate summaries from provided news stories.
  - Downloads: 677
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - This repository provides a GGUF quantized version of Qwen/Qwen2.5-3B-Instruct, optimized with a Japanese-focused importance matrix (iMatrix) to enable accurate summarization of extremely long texts exceeding 32K tokens.
  - Downloads: 655
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - This repository provides a GGUF-formatted version of Microsoft's Phi-3-medium-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 645
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the cogito-v1-preview-Qwen-32B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 571
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - This repository details a Japanese question generation model fine-tuned on a cleaned, machine-translated SQuAD 1.1 dataset, using a Japanese T5 model to generate questions from given answers and contexts.
  - Downloads: 549
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-70b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 546
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual provides faster, distilled Whisper models for Japanese & English speech recognition and translation, built upon OpenAI's large-v3 and developed by Asahi Ushio & Kotoba Technologies.
  - Downloads: 527
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹datagemma-rag-27b-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 424
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a Japanese/English instruction-tuning dataset (Sarashina2.2) built from fineweb-edu and fineweb-2, aiming for higher accuracy than existing imatrix datasets, and is designed for use with tools like Ollama using Q5_K_M quantization.
  - Downloads: 404
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaæ§˜ã® rinna/gemma-2-baku-2b-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 296
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) modelâ€”a fine-tuned rinna/japanese-hubert-largeâ€”trained on reazonspeech and common_voice data, outputting only Hiragana.
  - Downloads: 261
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - This repository provides a GGUF-formatted conversion of the QwQ-32B-Preview language model, utilizing TFMC/imatrix-dataset-for-japanese-llm data and intended for use with llama.cpp.
  - Downloads: 244
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository provides GGUF quantized versions of a VNTL LLaMA 3 8B QLoRA merge, featuring a new chat mode optimized for Japanese grammar and including translation prompts/examples.
  - Downloads: 241
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-mini-instruct model for use with tools like llama.cpp and text-generation-webui.
  - Downloads: 202
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Qwen2.5-72B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 188
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality voice synthesis with a custom, incompatible tokenizer, currently in beta.
  - Downloads: 182
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 170
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a GGUF format conversion of the sarashina2.2-3b-instruct-v0.1 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 133
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Humanities-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 133
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - This repository provides a HubERT-based Japanese Automatic Speech Recognition (ASR) model, fine-tuned on the uniTKU dataset to predict Hiragana with reported WER as low as 0.337.
  - Downloads: 132
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the ABEJA-Qwen2.5-32b-Japanese-v0.1 large language model, utilizing the imatrix dataset and compatible with llama.cpp for inference.
  - Downloads: 132
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªžåŒ»ç™‚LLM MedLlama3-JP ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama3ã®ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸï¼”ç¨®é¡žã®LLMã‹ã‚‰æˆã‚‹ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 122
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - This repository provides a model for automatically generating titles from article text, as detailed in the linked Qiita post.
  - Downloads: 91
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - This repository provides a T5 model pretrained on a balanced 500GB English-Japanese corpus, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 85
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and post-trained models alongside a pair-wise evaluation model.
  - Downloads: 75
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Vecteus-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 68
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - This repository provides a Japanese ByT5 modelâ€”a tokenizer-free Text-to-Text Transfer Transformerâ€”pretrained on a 100GB corpus including Wikipedia, OSCAR, and CC-100, requiring fine-tuning for specific tasks and mindful use due to potential biases in generated text.
  - Downloads: 60
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/c4ai-command-r-v01-japanese-instruct model, requiring users to check the original model for licensing details.
  - Downloads: 60
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 is a Japanese T5 model finetuned on the ATOMIC dataset for text-to-text generation, offering a pipeline for predictable, reproducible results.
  - Downloads: 60
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository provides GGUF quantizations of the VNTL Gemma 2 27B model, enhanced with a Japanese grammar-focused "chat mode" and translation examples.
  - Downloads: 59
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository offers static, quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model in GGUF format, with various quantization levels and size options detailed for usage.
  - Downloads: 56
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Kage-v0.1-2x7B is a merged 2x7B Japanese text generation model, enhanced with Ninja-v1 and utilizing the Vicuna prompt format for improved performance.
  - Downloads: 50
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and evaluation models for improved translation performance.
  - Downloads: 50
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - This repository provides a GGUF-formatted version of the Sarashina 2.1-1B-sft Japanese language model, trained with imatrix data and usable with llama.cpp for tasks like recipe generation.
  - Downloads: 47
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - Gendec is a machine learning framework for gender detection from Japanese names (provided in romaji) based on the ISDA'23 paper, offering male/female labeling functionality.
  - Downloads: 44
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime ã¯æ—¥æœ¬èªžã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 40
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - This repository showcases EZO2.5-gemma-3-12b-it, a Japanese language model trained with a novel â€œEZOâ€ techniqueâ€”mixing GRPO/PPO conceptsâ€”to improve performance on benchmarks like Japanese MT Bench and Elyza Tasks100 with limited resources.
  - Downloads: 34
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7Bã‚’ä¼šè©±ã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 30
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - This repository details a modified CreativeML OpenRAIL-M license permitting commercial use, including selling generated images, commercial services, and model merges, even with differing permissionsâ€”examples provided.
  - Downloads: 27
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - Tokara-0.5B-v0.1 is a Japanese instruction-tuned language modelâ€”based on Qwen1.5-0.5B and further trained on 5B Japanese/English tokensâ€”capable of multi-turn conversations, though repetition may occur without penalty adjustments.
  - Downloads: 25
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints ã‚’ optimum ç”¨ã« ONNX ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - This repository details a Japanese audio transcription model, finetuned from distil-whisper/distil-large-v2, specifically for visual novel audio and integrated with WaifuModel/Assistant projects.
  - Downloads: 22
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a 6 billion parameter Japanese language model finetuned from EleutherAIâ€™s GPT-J 6B specifically for generating web novels, utilizing RoPE embeddings and a 50,400 GPT-2/3 vocabulary.
  - Downloads: 20
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - This repository provides an alpha version of a Japanese-language assistant AI, fine-tuned from calm2-7b-chat, designed to continue writing provided text, trained on ~150M novel tokens and compatible with TextGen-WebUI.
  - Downloads: 20
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository provides a fine-tuned version of MosaicMLâ€™s MPT-7B-instruct model, evaluated on the Jumtra/test_data_100QA dataset and requiring `trust_remote_code=True` due to its custom architecture and training optimizations like FlashAttention.
  - Downloads: 20
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - Saikyou Shield 30M is a lightweight (30M parameter) Japanese text classification model designed to classify *all* prompts as dangerous, including jailbreaks and prompt injections, for ultimate safetyâ€”released as an April Fool's joke.
  - Downloads: 20
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - This repository provides a fully instruction-tuned, 1.7 billion parameter Japanese language model based on line-corporation/japanese-large-lm-1.7b.
  - Downloads: 19
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - This repository provides a finetuned GPT-2 XL model (COMET-GPT2) for text generation, based on the ATOMIC dataset and utilizing causal language modeling.
  - Downloads: 18
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - This repository provides a 7B-parameter Japanese language model fine-tuned with Direct Preference Optimization (DPO) on machine-translated Ultrafeedback and hh-rlhf datasets, based on the STF Japanese Stable LM Instruct Gamma 7B.
  - Downloads: 18
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri is a Japanese, vision-language model fine-tuned with LLaVA on STAIR Captions and Japanese Visual Genome VQA data for image-based humor generation, utilizing a CLIP-ViT-B-32 vision encoder and licensed under LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 17
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository provides a 32B Japanese language modelâ€”merged from pre-trained models using mergekitâ€”optimized for code generation with parameters based on the FuseO1-Preview evaluation, demonstrated by a FizzBuzz example.
  - Downloads: 17
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on a machine-translated Ultrafeedback dataset, building upon the Japanese Stable LM Instruct Gamma 7B model.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 v2 is a finetuned, large-version GPT-2 model trained on the ATOMIC dataset using causal language modeling for text generation with reproducible results.
  - Downloads: 15
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 is a Japanese language model based on DeepSeek-V3, refined by selectively reconstructing each layer with the 64 most frequently used experts from its Mixture of Experts (MoE) architecture to improve stability and performance.
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - This model is licensed under a modified CreativeML OpenRAIL-M license permitting commercial use, image selling, and merging, even with altered permissions or for resale, without requiring creator credit.
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2ã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model, trained on 49,000 Guanaco chat samples, with improved performance in Chinese and Japanese, and includes a test script and recommended generation parameters.
  - Downloads: 12
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - This repository details the karakuri-midroze-CV model.
  - Downloads: 12
- [OmeletRice/japanese-wav2vec2-base-bf16-asmr](https://huggingface.co/OmeletRice/japanese-wav2vec2-base-bf16-asmr)
  - This repository provides a ðŸ¤— Transformers model, pre-trained with ASMR data on reazon-research/japanese-wav2vec2-base, for Japanese speech processing.
  - Downloads: 11
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - This repository provides a Japanese-language AI modelâ€”fine-tuned for a girl-like speaking styleâ€”with specific settings for merging (layer adjustments, scaling, skipping layers) and text generation (max length, sampling parameters) as demonstrated by a sample self-introduction.
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Natural Language Interfaces
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumerã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Reflection-Llama-3.1-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,323
- [dahara1/gemma-3-1b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-1b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-focused, 12B parameter Gemma model quantized using an imatrix for faster inference with llama.cpp, specifically the `gemma-3-12b-it-qat-q4_0-japanese-imatrix-Q4_0.gguf` file.
  - Downloads: 1,155
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-ArrowSE-8B-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 700
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with limitations in following instructions.
  - Downloads: 646
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the JSQuAD dataset, achieving an F1 score of 0.876 and an exact match (EM) of 0.758.
  - Downloads: 546
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotæ§˜ã® Llama3-ArrowSE-8B-v0.3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 449
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - This repository provides statically quantized versions of the DeepSeek-R1-Distill-Qwen-7B-Japanese model in GGUF format, with community requests accepted for additional imatrix quantizations not currently available.
  - Downloads: 375
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers statically quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model in GGUF format, with imatrix quants potentially available upon request, and references TheBloke's resources for usage guidance.
  - Downloads: 309
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmæ§˜ã® Llama-3-Swallow-8B-Instruct-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - This repository provides a question-answering model fine-tuned from luke-japanese-large-lite using the DDQA dataset, achieving 86.3% exact match accuracy.
  - Downloads: 118
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - This repository demonstrates learning to solve simple arithmetic problems using GRPO, featuring a specific prompt format with `<think>` and `<answer>` blocks for thought process and solution, and utilizes on-the-fly generated training data.
  - Downloads: 102
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - This repository provides GGUF quantized versions of the ArrowPro-7B-RobinHood model, further enhanced with iMatrix using the c4_en_ja_imatrix.txt text dataset.
  - Downloads: 91
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B ã®GGUFé‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 84
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned on the DDQA dataset for Question-Answering tasks, utilizing transformers, PyTorch, and SentencePiece.
  - Downloads: 66
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the DDQA dataset, achieving an exact match accuracy of 0.845933.
  - Downloads: 42
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository provides a Japanese DeBERTa-v2-tiny model fine-tuned on the DDQA dataset for Question-Answering tasks, compatible with transformers and PyTorch.
  - Downloads: 38
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - Karasu-LoRA-JP-QA-Chat is a LoRA-tuned Japanese language modelâ€”based on Karasu 1.1B and sake-qaâ€”optimized for question answering and RAG systems using a dedicated Q&A dataset.
  - Downloads: 33
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and further trained on Japanese conversational data to enable natural, real-time overlapping speech and turn-taking.
  - Downloads: 31
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 is a merged language modelâ€”optimized for helpful, harmless responses, role-playing (especially as a Japanese persona to avoid translation-like outputs), and improved multi-turn conversationâ€”with suggested temperature 0.1 and top_p 1.0.
  - Downloads: 30
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - This repository provides a small Japanese DialoGPT model, trained on dialogue extracted from Aozora Bunko public domain books, and offers a Hugging Face Spaces demo due to issues with the local demo.
  - Downloads: 27
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - Lightblueâ€™s QLoRA finetune specializes in Japanese closed-question answering, trained on SNOW TyDiQA, XLSUM, and a combined dataset of 13,167 samples, leveraging OpenOrcaâ€™s Open-OrcaxOpenChat-Preview2-13B.
  - Downloads: 26
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - This repository hosts Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, a Japanese language model based on Mixtral-8x7B, extending context length to 32K and improving instruction-following capabilities through strategic merging of English model differences.
  - Downloads: 25
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - This repository hosts a Japanese instruction-tuned language model (c4ai-command-r-v01) further refined with ichikara-instruction, trained on Runpod using LoRA with specific parameters and evaluated on jsquad and jcommonsenseqa datasets.
  - Downloads: 18
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - Animagineç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ãŸVAEå†…è”µãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 is a question-answering model built on japanese-stablelm-instruct-gamma-7b, designed to help users learn Japanese in English with optimized responses using a specific prompt format and Transformers 4.34.0+.
  - Downloads: 16
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - This repository provides a finetuned, MIT-licensed causal language model (based on cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) for Japanese conversational chat, acknowledging limitations due to its small training dataset.
  - Downloads: 16
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - This repository provides a Japanese instruction-tuned model based on TinyMixtral, trained on a dataset and available on Hugging Face.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - This repository provides a fine-tuned Qwen2.5-7B-Instruct language model capable of generating chain-of-thought reasoning from given question-answer pairs, using a specific input/output format with `<Query>`, `<Answer>`, and `<Thought>` tags.
  - Downloads: 13
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This repository provides a Japanese chatbot model finetuned on the Yuyuyui scenario corpus, using rinna/japanese-gpt2-medium to generate responses based on character-prefixed conversational context.
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ä¸Šè¨˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚¢ãƒ€ãƒ«ãƒˆç”¨èªžã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct ðŸš¨ This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using the llm-book/ner-wikipedia-dataset, as presented in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 83,302
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - This repository provides a Japanese Named Entity Recognition (NER) model built on BERT, capable of extracting eight entity typesâ€”person, organization, location, facility, product, event, political organization, and otherâ€”from Japanese text.
  - Downloads: 3,262
- [Tetsuo3003/ner-medical-japanese](https://huggingface.co/Tetsuo3003/ner-medical-japanese)
  - This repository provides a fine-tuned Japanese NER model, based on XLM-RoBERTa, specifically for extracting entities (names, organizations, locations, etc.) from medical conversations and documents to support tasks like data anonymization and analysis.
  - Downloads: 1,178
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository provides public access to files and content upon acceptance of specified conditions.
  - Downloads: 574
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from luke-japanese-base using a Wikipedia dataset, achieving a reported F1-score of 0.77 for organizational names.
  - Downloads: 553
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - This repository provides a fine-tuned Japanese BERT model (tohoku-nlp/bert-base-japanese-v3) for Named Entity Recognition (NER) using a Wikipedia-derived dataset from Stockmark Inc.
  - Downloads: 508
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This repository provides a Japanese medical named entity recognition model, with Python scripts for predicting entities (diseases, medications, etc.) within medical text using provided input files.
  - Downloads: 321
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This repository provides a Japanese medical named entity recognition model, trained on MedTxt-CR-JA, with a prediction script outputting XML-formatted tags and entity normalization.
  - Downloads: 277
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - Kurumi_flux_lora_v1.0 is a non-commercial LoRA model based on flux1-dev, optimized for realistic, beautiful girl images, with usage restrictions including prohibiting retraining and requiring model name attribution.
  - Downloads: 218
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune on an expanded VNTL dataset to enhance English translation accuracy and stability for Japanese visual novels, omitting chat mode functionality.
  - Downloads: 163
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - This repository provides weighted/imatrix quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, offering various quantization levels (including IQ-quants) for different size/performance trade-offs.
  - Downloads: 154
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - This repository details a binary classification model (ID 59362) trained with AutoNLP for Japanese sentiment analysis, achieving 95.27% accuracy, and accessible via a Hugging Face API endpoint.
  - Downloads: 153
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - This repository provides a Japanese BERT-large model fine-tuned on a Wikipedia dataset for Named Entity Recognition (NER) tasks, achieving an overall accuracy of 0.862.
  - Downloads: 136
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune, leveraging an expanded and rebuilt VNTL dataset to improve Japanese visual novel translation to English with enhanced accuracy and stability.
  - Downloads: 121
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository provides fastText classifiersâ€”trained on Wikipedia and LLM annotationsâ€”to assess the educational value of Japanese web pages under a CC BY-SA 4.0 license.
  - Downloads: 108
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - This repository provides a Japanese Named Entity Recognition (NER) modelâ€”built by fine-tuning `cl-tohoku/bert-base-japanese-v3` with a CRF layer on the `llm-book/ner-wikipedia-dataset`â€”as featured in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 79
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - This repository provides a Japanese language model, fine-tuned from `studio-ousia/luke-japanese-large-lite`, that scores short texts for sexual content on a 0-1 scale to aid content moderation.
  - Downloads: 65
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - This repository provides a fine-tuned luke-japanese-large model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.845.
  - Downloads: 44
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - This repository provides a Japanese text classifier, finetuned on a BERT model, to predict JLPT levels with precision, recall, and F1-scores ranging from 0.71 to 0.95 on the training data.
  - Downloads: 34
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - This repository provides a Fully Convolutional Neural Network (FCN) model trained for classifying images of 49 Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - `ja_core_news_lg` is a CPU-optimized spaCy 3.7+ pipeline for Japanese language processing, including tokenization, morphology, parsing, sentence segmentation, named entity recognition, and attribute rules, trained on UD Japanese GSD v2.8 data with 300-dimensional vectors.
  - Downloads: 24
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This repository provides a named entity recognition model, with associated files and a prediction script, for processing Japanese medical text and identifying entities like diseases, treatments, and time expressions.
  - Downloads: 22
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - WRIME is a dataset for weakly-supervised relation information extraction, providing a tab-separated file of sentences with relation labels.
  - Downloads: 21
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - This repository provides a model for automatically generating article titles from article content, as detailed in the linked Qiita post.
  - Downloads: 20
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This repository provides a baseline transformer model trained and evaluated on the awesome-japanese-nlp-classification-dataset, achieving 97% accuracy with precision, recall, and F1-scores detailed in the description.
  - Downloads: 17
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - `ja_core_news_md` is a CPU-optimized spaCy 3.7.0+ pipeline for Japanese language processing, featuring tokenization, morphological analysis, parsing, sentence splitting, named entity recognition, and attribute rules with 200-dimensional vectors trained on UD Japanese GSD v2.8.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from deberta-v2-base-japanese using a Wikipedia-based dataset.
  - Downloads: 16
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model, trained on the 49000 chat and 280000 non-chat Guanaco dataset, exhibiting improved Chinese and Japanese performance with provided testing and generation parameters.
  - Downloads: 16
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository provides a DeBERTa-v2-large-japanese model fine-tuned for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset.
  - Downloads: 14
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - This repository provides a binary classification model (ID 59363) trained with AutoNLP for Japanese sentiment analysis, achieving 97.4% recall and 97.2% precision, and accessible via a Hugging Face API.
  - Downloads: 14
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This repository provides a language model trained on 2022 Japanese parliamentary proceedings, built as a multi-GPU/node training example for the #ABCILLM hackathon, and utilizes data from the National Diet Library's API.
  - Downloads: 13
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This So-vits-svc 4.0 model creates natural-sounding, friendly female vocalsâ€”synthesized from the authorâ€™s voice and expanded with ElevenLabsâ€”and includes necessary checkpoints & notebooks for inference/training, acknowledging potential audio artifacts and imperfect pronunciation.
  - Downloads: 13
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - `ja_core_news_trf` is a spaCy 3.7.x Japanese language model featuring a transformer pipeline (cl-tohoku/bert-base-japanese-char-v2) for NLP tasks like morphological analysis, parsing, and named entity recognition.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Aratako/Japanese-Novel-Reward-TinySwallow-1.5B is a Japanese novel quality assessment reward model, fine-tuned from SakanaAI/TinySwallow-1.5B for applications like reinforcement learning of text generation, predicting user evaluation as a regression task while acknowledging potential biases.
  - Downloads: 11
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - This repository provides a 4-bit quantized Llama-2-70b-chat model fine-tuned on the Japanese instruction dataset â€œizumi-lab/llm-japanese-datasetâ€ for enhanced performance in Japanese language tasks.
  - Downloads: 11
### Reasoning
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-QwQ32b-Reasoning-Japanese-v1.0 language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 1,112
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - This repository provides a Japanese-specific fine-tune of the DeepSeek-R1 model, addressing its inconsistency in generating Japanese text and improving performance for Japanese language tasks.
  - Downloads: 1,067
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹mathstral-7B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,067
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 is a Japanese reasoning model built upon ABEJA-Qwen2.5-32b-Japanese-v0.1 by merging Qwen/QwQ-32Bâ€™s chat vectors and further training, utilizing `<think>` tags to prompt a reasoning process before output.
  - Downloads: 779
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B is a Japanese reasoning model built on Qwen/Qwen2.5-32B-Instruct and improved via imitation learning with Monte Carlo Tree Search-generated, long Chain of Thought data.
  - Downloads: 564
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - This repository provides a GGUF formatted version of Microsoft's Phi-4-reasoning-plus language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm for data and compatible with llama.cpp.
  - Downloads: 529
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - This repository provides a GGUF-formatted version of YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1, a Japanese language model for mathematical tasks, built using the TFMC/imatrix-dataset dataset and compatible with llama.cpp.
  - Downloads: 273
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is an anime-style AI model merging Stable Diffusion and Wifu Diffusion fine-tunes, with LoRA enhancements from Nijijourney and personal creations, prioritizing transparency through disclosed datasets and aiming for clear lines, cute girls, and moderate NSFW/tag support.
  - Downloads: 192
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - This repository details Polyglot-math-4x7b-24b, a multilingual Mixture of Experts model merging Chinese and Japanese language capabilities, fine-tuned on GSM8k with a 20GB VRAM footprint and providing inference code examples.
  - Downloads: 136
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository provides a fine-tuned version of luke-japanese-large for the JCommonsenseQA task, achieving a high accuracy of 83.82% on commonsense question answering.
  - Downloads: 99
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference (NLI) modelâ€”trained on JGLUE-JNLI & JSICK using SentenceTransformersâ€”that classifies sentence pairs as contradiction, entailment, or neutral.
  - Downloads: 93
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Natural Language Inference (JNLI) using the JGLUE dataset, achieving 89.77% accuracy.
  - Downloads: 50
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-Japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 20
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - This model merges reasoning capabilities from DeepSeek-R1-Distill-Llama-8B into the Japanese Llama-3.1-Swallow-8B-v0.2, enhancing its reasoning performance.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman for morphological analysis.
  - Downloads: 16
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a Japanese finetuned COMET model for causal language modeling, trained on the Japanese TimeATOMIC dataset and detailed in a LREC-COLING2024 paper, utilizing Juman++ and SentencePiece for text processing.
  - Downloads: 14
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository provides a DeBERTa-v2-base-Japanese model fine-tuned on the JGLUE/JCommonsenseQA dataset for commonsense question answering tasks.
  - Downloads: 11
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - This repository provides a highly accurate (80.07%) Japanese language model, fine-tuned from luke-japanese-base using the JGLUE JCommonsenseQA dataset, for multiple-choice commonsense question answering.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ About this model.
  - Downloads: 816
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository provides a private demonstration project.
  - Downloads: 320
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 320
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 232
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model with a CreativeML OpenRAIL-M license, granting usage rights while prohibiting illegal/harmful outputs and establishing user accountability for generated content.
  - Downloads: 88
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 83
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - This repository provides a Japanese language ELECTRA Small model finetuned for cyberbullying detection using data from both BBS comments and Twitter, built upon a YACIS corpus pretrained foundation.
  - Downloads: 79
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - This commercially usable Stable Diffusion model, licensed under CreativeML Open RAIL++-M, generates images with hires support but prohibits depictions of violence, explicit or exploitative content, and unauthorized likenesses of real people; use #tsubaki_mix when sharing publicly.
  - Downloads: 39
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 30
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Twitter/twhin-bert-large model for classifying Japanese social media comments into offensive, gray-area, and non-offensive categories, achieving a macro-averaged F1-score of 64.8% and accuracy of 66.1%.
  - Downloads: 23
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged modelâ€”similar to HimawariMixâ€”focused on strong backgrounds and detail, tuned with ideas from "Riga," and includes a standard VAE, but prohibits commercial use, resale, or malicious/unpermitted distribution.
  - Downloads: 22
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before use.
  - Downloads: 20
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA Base model finetuned for cyberbullying detection using a combined dataset of online comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 18
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - This repository provides instruction-tuned language models for Japanese (llm-jp).
  - Downloads: 14
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese language model, based on studio-ousia/luke-japanese-large-lite, for classifying online comment offensiveness with 64.0% macro-averaged F-score and 65.0% accuracy.
  - Downloads: 13
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 13
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese BERT model (twhin-bert-base) for classifying the offensiveness of social media comments, achieving macro-averaged F1-score of 64.7% and accuracy of 65.6% on a manually labeled dataset.
  - Downloads: 12
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Sentiment Analysis
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model trained on the chABSA dataset, achieving 1.0 accuracy and F1 score.
  - Downloads: 7,699
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This repository provides a Japanese emotion analysis model, fine-tuned from Luke-japanese-large-lite using the wrime dataset, to detect eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, and trustâ€”within text.
  - Downloads: 1,488
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - This repository provides a Japanese BERT Base model finetuned for both sentiment analysis and automatic irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 1,154
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model finetuned from scratch using the Japanese Sentiment Polarity Dictionary dataset and based on the jarvisx17/japanese-sentiment-analysis pretrained model.
  - Downloads: 860
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - This repository provides a Japanese BERT model finetuned for sentiment analysis of Twitter data using the JTS1k dataset, classifying tweets as negative, neutral, or positive.
  - Downloads: 587
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This repository provides a Japanese BERT-based model fine-tuned for detecting and classifying ten distinct emotions â€“ amaze, anger, dislike, excite, fear, joy, like, relief, sad, and shame â€“ from text.
  - Downloads: 562
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - This repository provides a GGUF-formatted conversion of the ummiyuki-Japanese-Chat-Umievo-itr001-7b language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 317
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - This repository provides a Japanese BERT Base model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 60
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - This repository provides a fine-tuned calm-2-7b-chat model using the Tsukuyomi corpus for conversational AI, subject to the specified Tsukuyomi character and AI development plan licenses.
  - Downloads: 41
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - This repository provides a sentiment analysis model trained on Japanese stock comments to classify opinions as bullish or bearish, assisting investors and analysts.
  - Downloads: 25
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - This repository provides a Japanese ELECTRA-based model finetuned for irony detection using a dataset of ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 22
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese is a finetuned language model for detecting irony and sarcasm in Japanese tweets, licensed under CC BY-SA 4.0.
  - Downloads: 15
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - This repository provides a Japanese BERT model pretrained on a Twitter corpus, optimized for social media tasks like sentiment analysis and defamation detection, and used as a base for further finetuned models.
  - Downloads: 13
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository provides a Japanese language ELECTRA model finetuned for irony detection using ironic tweet data, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - This repository provides a free, commercially usable voice modelâ€”a â€œsweetâ€ version of RikkaBotanâ€”specializing in gentle, childlike speech, with variations available for cooler tones, English, ASMR whispers, and Chinese.
  - Downloads: 13
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - This repository provides a Japanese BERT-based model, trained on Financial PhraseBank, for analyzing the sentiment of financial news into positive, negative, or neutral categories.
  - Downloads: 12
### Responsible NLP
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªžç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªžãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a Japanese speech-to-text model fine-tuned on a large anime voice acting dataset, excelling in that domain but also offering unique performance on other audio, and **should be used without an initial prompt** to avoid hallucinations.
  - Downloads: 2,343
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - This model is a personally-merged, stylized checkpoint focused on generating low-ratio, young female characters with preserved eye highlights, requiring careful age adjustment and potentially limited compatibility with LoRAs, best used with DPM++ 2M Karras sampler and potentially adjusted Hires steps.
  - Downloads: 447
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - This repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime models (with specified ratios) for Stable Diffusion, noting potential over-saturation when merged with realistic models and compatibility issues with SD 2.1 768, alongside download commands.
  - Downloads: 400
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - Suzume_mix_v1.0 is a non-commercial, merged Stable Diffusion model based on flux1-dev, designed to soften facial features and intended for personal use with attribution when sharing generated images.
  - Downloads: 149
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - YaguruMagiku 0.6, a merged model based on AbyssOrangeMix2, aims to generate consistent black-haired ponytail characters, potentially including NAI leak elements, and is best used with four simultaneous image generations and a VAE for improved color.
  - Downloads: 34
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - This repository hosts Llama-3-Umievo-Shizuko-sqlcoder-2x8B, a Mixture of Experts (MoE) language model created with MergeKit, combining Japanese language and SQL generation capabilities through fine-tuning on SQL datasets.
  - Downloads: 24
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - Sarashina 2.2-3B-instruct, a Japanese language model, is built upon the sbintuitions base model and trained with the imatrix dataset.
  - Downloads: 23
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 merges DreamShaper 3.3 with WaifuDiffusion/Stable Diffusion VAEs to improve color vibrancy and expressive range, creating highly realistic and beautiful imagesâ€”though it may contain NAI/Insta-style leaks and is usable in Colab.
  - Downloads: 21
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - ã‚·ã‚µãƒ èªžã«ã‚ˆã‚‹èª¬æ˜Ž ã‚¢ã‚¤ãƒŒèªžã¨æ—¥æœ¬èªžã®åŒæ–¹å‘æ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - This repository details a Stable Diffusion model mergeâ€”MoeDiffusion and HassanBlend/VMix03â€”focused on generating black-haired ponytailed characters, potentially exhibiting some instruction-following issues, and is intended for SFW content with Colab WebUI compatibility.
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªžç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªžãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 13
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 11
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performing, CPU-runnable Japanese text embedding model optimized for semantic similarity and retrieval tasks, achieving state-of-the-art results on benchmarks like MIRACL.
  - Downloads: 15,538
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - This repository distributes LoRA models created by Hotaru Jujo, released under dual MIT/CreativeML Open RAIL-M licenses with no usage restrictions but social media credit appreciated.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT v1 is a Japanese document retrieval model based on ColBERT, achieving near-multilingual performance on standard benchmarks despite out-of-domain evaluation data.
  - Downloads: 607
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - This repository provides a series of small, fast Japanese re-ranking models (v2) with varying sizes and speeds, offering trade-offs between performance (up to 0.8661 score) and inference time (from 2.1s to 312.2s on GPU).
  - Downloads: 429
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - This repository provides a series of fast and very small Japanese reranker models (v2) with varying layer counts and hidden sizes, benchmarked for speed and score.
  - Downloads: 105
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - This repository provides a passage encoder for the BPR document retrieval model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of â€œIntroduction to Large Language Models.â€
  - Downloads: 53
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - This repository provides a finetuned BERT model and code for extracting key entities (location, type, season, ingredient) from Japanese cooking-related questions to generate search keywords.
  - Downloads: 30
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã§tohoku-nlp/bert-base-japanese-v3ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã§pkshatech/GLuCoSE-base-jaã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 is a 7 billion parameter Japanese language model fine-tuned for instruction following, achieving a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 17
## ðŸ§  Datasets

This list is sorted by downloads as of June 10, 2025.
543 datasets are listed.

### Information Extraction & Text Mining
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset provides news articles from livedoor News, used in the book *Introduction to Large Language Models*, under a Creative Commons Attribution-NoDerivs 2.1 Japan license for named entity recognition (NER) tasks.
  - Downloads: 7,706
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - FineWeb2 Edu Japanese is a high-quality, 89.3B token dataset of 120 million educational Japanese texts, with provided subsets for sampling and shorter text analysis.
  - Downloads: 2,365
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a multilingual sentence dataset enabling translation between customizable language pairs (specified by language codes) accessible via the `load_dataset` function, with a default version of v2021-07-22.
  - Downloads: 2,215
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - This repository provides a cleaned Japanese news corpus of approximately 612M tokens from Common Crawl (July-October 2024) extracted using Uzushio, configured with `pipeline_03a.conf`, and compatible with the llm-jp/llm-jp-13b-v1.0 tokenizer.
  - Downloads: 1,782
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - WRIME is a dataset for emotional intensity estimation, featuring both self-reported and reader-annotated intensity levels of social media posts from 50 writers.
  - Downloads: 979
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a machine learning-ready dataset derived from Aozora Bunko, a Japanese public-domain ebook library, with code available for reproduction.
  - Downloads: 422
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository provides a sharded, Japanese-language subset of the CC100 dataset in Parquet format.
  - Downloads: 309
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - JSICK is a Japanese natural language inference and semantic textual similarity dataset, manually translated from English SICK, designed for researching multilingual compositional inference and model stress-testing.
  - Downloads: 282
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository provides version 2.0 of a Japanese named entity recognition dataset created by Stockmark, used in the book *Introduction to Large Language Models*, and licensed under CC-BY-SA 3.0.
  - Downloads: 254
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA is a Hugging Face mirror of the AWS Open Data Registry dataset containing Japanese captions and images for multimodal AI research.
  - Downloads: 233
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This repository provides a filtered Japanese summarization dataset (XL-Sum) processed with PaLM 2 filters to reduce 15-gram overlap, containing 4215 training, 758 validation, and 766 test examples.
  - Downloads: 191
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - This dataset extracts September and October news articles from the CC-news-2024-July-October-cleaned corpus, adjusted to approximately 1000 tokens for efficient training with the llm-jp/llm-jp-3-13b tokenizer, assuming a 1024 output token limit.
  - Downloads: 176
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - This dataset provides Japanese named entity recognition (NER) training data extracted from Wikipedia, licensed under CC-BY-SA 3.0 and developed by Stockmark Inc.
  - Downloads: 169
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - This Japanese dataset provides three-line summaries and indexing of thousands of mycological taxonomy papers from the Daikinrin website, curated by Atsushi Nakajima.
  - Downloads: 167
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - This dataset provides images for evaluating Japanese image classification models across four tasks, including 101 types of Japanese food, licensed under CC-BY-4.0.
  - Downloads: 164
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - This dataset classifies GitHub repository descriptions as relevant (1) or not relevant (0) to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing, sourced from "awesome-japanese-nlp-resources".
  - Downloads: 157
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - This repository provides a Parquet dataset of Japanese Wikipedia data extracted on January 1, 2023, generated using the `datasets` library.
  - Downloads: 131
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - This dataset provides 5,000 annotated Japanese tweets (Feb-Jun 2022) for detecting online defamation, labeling both the target of abuse and the type of abusive content.
  - Downloads: 127
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This dataset provides named entity recognition (NER) labels for Wikinews articles in Japanese, featuring 8 entity types and licensed under CC BY 2.5, specifically for use with the book *Introduction to Large Language Models*.
  - Downloads: 126
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - This dataset provides Japanese language instruction-tuning data based on the japanese-alpaca-lora project, requiring further documentation.
  - Downloads: 125
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - This Japanese dataset compiles manually extracted diagnostic character comparisonsâ€”similarities and differencesâ€”between fungal species, sourced from a summarized index of thousands of mycological taxonomy papers maintained on the Daikinrin website.
  - Downloads: 124
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository provides the Japanese subset of the wiki40b dataset, formatted as three parquet files generated using the `datasets` library.
  - Downloads: 118
- [OmniAICreator/Japanese-Wikipedia-202506](https://huggingface.co/datasets/OmniAICreator/Japanese-Wikipedia-202506)
  - This dataset provides a snapshot of the Japanese Wikipedia articles current as of June 1, 2025.
  - Downloads: 111
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - This dataset contains cleaned Japanese news articles from September and October 2024, preprocessed with date prefixes and adjusted to ~1000 tokens for efficient continued pre-training with the llm-jp/llm-jp-3-13b tokenizer, assuming an output length of 1024 tokens.
  - Downloads: 109
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This dataset provides 8.75K comprehensive records of Japanese laws â€“ including number, title, effective date, and full text â€“ sourced from e-Gov and deduplicated as of August 1, 2023.
  - Downloads: 104
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - This repository provides a JSON dataset of anime metadata with cross-references to popular anime platforms like MAL, AniList, and Kitsu, based on the Manami Project's offline database.
  - Downloads: 97
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR is a 209K-instance dataset of narrative text extracted from manga, combining annotations from Manga109 and a manga onomatopoeia dataset to support OCR research.
  - Downloads: 87
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
  - This repository analyzes a 400,000-entry Japanese Wikipedia dataset, counting 221,115 total tokens, including 204,661 nouns and 16,454 verbs.
  - Downloads: 81
- [clnine/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-nikkei225)
  - This dataset provides a working sample of records extracted from the January 2023 Japanese Wikipedia dataset focusing on articles within the â€œCategory:Nikkei 225â€ (æ—¥çµŒå¹³å‡æ ªä¾¡).
  - Downloads: 81
- [clnine/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-financial-terms)
  - This dataset is a working sample of Parquet files extracted from the range3/wikipedia-ja-20230101 Japanese Wikipedia dataset, specifically containing records related to articles within the "Category:æŠ•è³‡" (Investment) category.
  - Downloads: 81
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository provides a voice tag dataset for Nene Kusakabe (CV: Machico) from *Project Sekai: Colorful Stage! feat. Hatsune Miku*, with plans for future completion and standardization, and a linked QQ group for a full character dataset.
  - Downloads: 78
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
  - This dataset provides high-quality Japanese-English text data, sourced from Blue Sky Library, Japanese/English Wikipedia, and Project Gutenberg, for distilling knowledge into large language models using the llama-imatrix method.
  - Downloads: 78
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench is a 102-question benchmark dataset of 21 Japanese images, categorized by conversation, detail, and complexity, designed to evaluate Vision-Language Models across seven subcategories like anime, culture, and landscape.
  - Downloads: 78
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - This repository provides a dataset subject to a LICENSE agreement that users must review and agree to before use.
  - Downloads: 73
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a list of 100 common Japanese stopwords derived from the CC-100 and Wikipedia datasets, intended for use with the nagisa text analysis library.
  - Downloads: 72
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - This dataset provides a filtered, approximately 10 billion Japanese token corpus from Common Crawl, processed to remove sensitive personal information (PII) using rule-based and machine learning methods, licensed under CC terms.
  - Downloads: 72
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - This Japanese dataset provides multi-label annotations of NLP research fields for GitHub repositories, utilizing repository content like descriptions, READMEs, and images for training a multi-label classification model.
  - Downloads: 69
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - This repository provides a cleaned, UTF-8 encoded dataset of over 7,000 human-generated Japanese subtitles from OpenSubtitles, including text, timing data, metadata, and a version formatted for Open Assistant tasks.
  - Downloads: 57
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - This dataset provides anime song lyrics in Parquet format for research and enthusiast use, offering a structured collection of lyrical content.
  - Downloads: 57
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - This dataset provides 53,640 annotated Japanese tweets (Jan-Jun 2020) related to COVID-19, intended for text classification tasks and requiring users to retrieve original tweets via the Twitter API.
  - Downloads: 56
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - This dataset, zenz-v2.5, comprises 190M pairs of contextualized katakana input and kanji output for training conditional language models for Japanese kana-to-kanji conversion, with trained models (zenz-v2.5-medium, -small, -xsmall) and an evaluation benchmark (AJIMEE-Bench) also available.
  - Downloads: 50
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - J-NER is a Japanese Named Entity Recognition dataset containing 1,570 examples of 157 entity typesâ€”sourced from Wikipedia and designed for LLM trainingâ€”with 5 positive and 5 negative examples per entity.
  - Downloads: 50
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - This repository provides scripts to download, parse, and preprocess the publicly available en-ja-alignæ—¥è‹±å¯¾è¨³æ–‡ dataset (Uchiyama et al., 2003) for English-Japanese parallel text, without redistributing the data itself.
  - Downloads: 49
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - This dataset provides 55.4 billion Llama-4 tokens from over 23 million Japanese web novel records for machine-learning research, requiring a use-case explanation for access.
  - Downloads: 49
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains excerpts from 2024 Japanese Securities Reports (æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸) filed via EDINET, providing company information like name, code, financial period, and JCN (corporate number) with source URLs included.
  - Downloads: 47
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - This dataset provides a collection of anime quotes with associated characters, sourced from Anime Motivation, for use in analysis, research, or personal enjoyment.
  - Downloads: 44
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-following dataset containing 6,259 manually annotated input-output pairs for tasks like question answering, sourced from CohereForAI/aya_dataset.
  - Downloads: 44
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - This repository provides a HuggingFace-compatible version of the Kyoto University Japanese Wikipedia Input Error Dataset (v2) licensed under CC-BY-SA 3.0, originally published by the Language Media Research Lab.
  - Downloads: 41
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - Giellm is a Japanese general information extraction dataset built from the Livedoor News corpus and used for training a large language model leveraging mutual reinforcement learning, as detailed in the linked arXiv paper.
  - Downloads: 40
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - This dataset provides raw, Unicode-normalized Japanese text from fineweb-2-edu-japanese, cleaned with noise inference (threshold 0.7, length >= 4) and marked with `noise_spans` (start/end positions), licensed under ODC-By.
  - Downloads: 37
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a Japanese language benchmark evaluating LLM long-context capabilities across extractive QA and abstractive summarization tasks, utilizing real web documents and GPT/Claude-generated question-answer pairs.
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 37
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Jibiki.fr provides a collaborative, large-coverage French-Japanese dictionary and aligned bilingual corpus built from multiple sources, currently containing over 154,000 Japanese-French entries.
  - Downloads: 36
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - JParaCrawl is a large, publicly available English-Japanese parallel corpus built by web crawling and automatic alignment, accessible via the `datasets` library.
  - Downloads: 35
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides a JSONL dataset of validated linguistic minimal pairs for Japanese, used for benchmarking natural language processing models, as detailed in Someya and Oseki (2023).
  - Downloads: 34
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is a JSONL dataset of English and Japanese text scraped from the Touhou Wiki, designed for unsupervised text generation model training and potentially text classification.
  - Downloads: 32
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 32
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - This repository provides the English/Japanese dataset utilized for training the shisa-7b-v1 language model, with further details available in that modelâ€™s documentation.
  - Downloads: 31
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - This repository provides a work-in-progress Japanese-English dataset sourced from Ruby/Rails documentationâ€”including synthetically generated dataâ€”for machine translation or similar tasks, readily loadable with `load_dataset`.
  - Downloads: 30
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - This repository expands the Japanese Cosmopedia dataset to 100k entries (from 20k) with contributions from kunishou, including translated text generation prompts, available on Hugging Face.
  - Downloads: 29
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - This Japanese RLHF dataset reformats reward modeling into a binary classification task (chosen/rejected) using synthetically generated text from Phi-3-medium, offering a resource for training reward models with moderate quality Japanese text.
  - Downloads: 29
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This synthetic question-answering dataset, built from the sakura_japanese_dataset, was generated using Nurture-intelligence/Gemma-2-108B-DPO-v0.1 with Pro-type inference, and is subject to both the original dataset's license and Gemma Terms of Use.
  - Downloads: 29
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - This repository provides a dataset of furigana annotations extracted from bibliographic data sourced from the National Diet Library, available for download as a ZIP file.
  - Downloads: 28
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - This repository provides the dataset and code for the SLG framework, a sentence-to-label generation approach for multi-task Japanese sentence classification and named entity recognition, detailed in the linked paper and arXiv preprint.
  - Downloads: 28
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This AutoTrain dataset, generated for the â€œtam_jpâ€ project, provides Japanese (ja) language data instances with â€œcontextâ€ fields for training machine learning models.
  - Downloads: 28
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - This repository provides a clustered datasetâ€”derived from Japanese customs advance ruling dataâ€”for training and evaluating embedding models, featuring item classifications (â€œgeneral namesâ€) with labels representing HS code sections, split into train/test sets while maintaining label proportions.
  - Downloads: 27
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This repository provides a dataset of extracted sections from Japanese Securities Reports (æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸) submitted to EDINET from 2014-2022, including company information, document details, and reporting periods.
  - Downloads: 26
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - This dataset provides English-Japanese paragraph pairs with labels indicating whether they describe the same or different entities, extending the original PubChem & Wikipedia classification task to a multilingual setting.
  - Downloads: 26
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - This repository provides a work sample of parquet files extracted from the range3/wikipedia-ja-20230101 Japanese Wikipedia dataset, specifically containing records related to articles within the "Category:æŠ•è³‡" (Investment) category.
  - Downloads: 25
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences is a Japanese Wikipedia sentence dataset containing article and section titles alongside cleaned text, generated from Wikipedia dumps under CC BY-SA 4.0 and GFDL licenses.
  - Downloads: 24
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - This project automatically generates question-answer pairs from Japanese Wikipedia articles using a 5-bit GGUF version of Mixtral 8x22b, leveraging the TSUBAME4.0 supercomputer, but requires filtering due to potential hallucinations in the responses.
  - Downloads: 24
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository provides a Japanese subset of the NTX dataset converted to the Aya instruction format, licensed under CC-BY-SA 4.0, and linked to the full instruction-formatted dataset and associated research.
  - Downloads: 20
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - This dataset contains approximately 7,000 rows of disease-symptom relationships extracted from Japanese medical journal articles (2003-2023), including disease names, related findings, supporting text, and article metadata, formatted as a JSON-Lines file.
  - Downloads: 20
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M is a corpus of 3.5 million unique Japanese light novel character names from syosetu.com, designed for culturally aware NLP applications like NER and name generation.
  - Downloads: 19
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - This repository provides a working sample of records extracted from the January 2023 Japanese Wikipedia dataset (range3/wikipedia-ja-20230101) specifically focusing on articles within the "Category:Nikkei 225" (æ—¥çµŒå¹³å‡æ ªä¾¡).
  - Downloads: 19
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - This repository provides a human-translated, passage and sentence-level English-Japanese parallel dataset created for machine learning use, prioritizing a permissive license by minimizing reliance on restricted translation tools and leveraging openly licensed LLMs like CALM3 and Qwen.
  - Downloads: 19
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - This repository provides a JSONL version of the Dolly-15k-ja dataset, formatted for use with the SFTTrainer, and licensed under CC BY SA 3.0.
  - Downloads: 18
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - This repository provides a dataset of cooking recipe search queries with labeled entities (area, type, season, ingredients) alongside code for language model fine-tuning and application development utilizing the dataset.
  - Downloads: 17
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - This repository provides a clustered datasetâ€”derived from Japan's PMDA websiteâ€”for training and evaluating embedding models, featuring text data (â€œgeneric nameâ€ & definition) and corresponding â€œclassification codesâ€ as labels, split into train/test sets with preserved label proportions.
  - Downloads: 17
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - This dataset contains crawled data from the â€œKawaryu Toshu Marusenâ€ Japanese haiku/senryu website, including prompts and over 5,000 responses formatted for text-to-text tasks, intended for use within the YANS hackathon.
  - Downloads: 17
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - This dataset contains 221 award-winning haiku from the Itoen Shinhaiku Grand Prix, including Japanese and English versions, author/judge comments, and image URLs, structured with metadata like contest details and language.
  - Downloads: 17
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset provides 600 Japanese sentences from Wikipedia articles related to thoroughbred racehorses, annotated with nine named entity typesâ€”including a dedicated â€œracehorse nameâ€ tagâ€”for named entity recognition, acknowledging potential data imperfections due to Wikipedia/DBpedia link rot.
  - Downloads: 14
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - This dataset provides a 5% subset of Japanese text (sourced from CulturaX) selected using DSIR to closely resemble modern Japanese literature from XLSum and Aozora Bunko, demonstrating improved quality.
  - Downloads: 13
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully is a publicly available, commercially usable dataset in Japanese and other languages designed to improve LLM safety, but its use is restricted to safety enhancement and redistribution is prohibited, although derivative data creation is allowed with proper attribution and adherence to the terms.
  - Downloads: 12
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’UVRã‚’ä½¿ç”¨ã—ã¦BGMã‚„ãƒŽã‚¤ã‚ºé™¤åŽ»ã—ãŸã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tagsâ€”covering characters, artists, and moreâ€”suitable for training image classification and multi-label tagging models.
  - Downloads: 5,592
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - MOMIJI is a large (56M documents, 249M images, 110B characters) Japanese image-text dataset extracted from Common Crawl for training vision-language models.
  - Downloads: 2,409
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST is a dataset of 70,000 handwritten Japanese characters for image classification tasks.
  - Downloads: 1,914
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 is a 292,637-clip audio-text dataset from visual novels created to improve automatic speech recognition accuracy, and is distinct from its V1 counterpart.
  - Downloads: 1,895
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - This dataset provides thousands of Japanese anime/visual novel audio clips and transcriptions to improve automatic speech recognition accuracy, particularly for challenging dialogue common in this genre.
  - Downloads: 1,784
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - ReazonSpeech is a large, free 35,000+ hour Japanese speech dataset in FLAC format intended for Automatic Speech Recognition (ASR) research, subject to Japanese copyright law (Article 30-4).
  - Downloads: 670
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark designed to rigorously evaluate Large Multimodal Model (LMM) performance, addressing cultural biases present in existing benchmarks through expert-created, culture-agnostic questions.
  - Downloads: 521
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - This repository provides a CC-0 licensed dataset of AI-generated anime images with corresponding Japanese captions (including translations of Dense Captions from Phi-3 Vision) for ethically-sourced machine learning.
  - Downloads: 473
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 provides a dataset of over 240,000 curated animation clipsâ€”primarily Japanese animeâ€”along with accompanying blog posts, addressing the need for animation-focused video data for AI and research.
  - Downloads: 338
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This repository provides a transcribed voice dataset totaling 77 characters from the game *Umamusume*, featuring audio from characters like Sweep, Tokai Teio, and others, with a total duration exceeding 10,000 seconds.
  - Downloads: 321
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - This dataset provides diverse, high-quality images of Japanâ€”spanning landscapes, culture, and daily lifeâ€”captured in the 2020s for AI training.
  - Downloads: 240
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - This dataset provides roughly 39 million characters of high-quality Japanese text from 1,924 research papers (including NLP2024) and 360 journal articles under CC-BY licenses, suitable for pre-training language models and RAG applications.
  - Downloads: 224
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset provides a clarified version of the Japanese-Heron-Bench, offering image, context, and question data for evaluating vision-language models in Japanese.
  - Downloads: 221
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample Japanese Visual Genome VQA dataset used to evaluate EvoVLM-JP-v1-7B and available under a Creative Commons license.
  - Downloads: 195
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This project creates a publicly available dataset of voice data from Vtuber Sakura Miko (hololive) for use in speech recognition and other applications, adhering to hololiveâ€™s secondary creation guidelines and acknowledging Cover Corporationâ€™s copyright.
  - Downloads: 188
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - This repository provides a Japanese audio dataset of synthetic human-machine conversations simulating call center interactions, licensed under CC BY-NC 4.0 and curated by AIxBlock.
  - Downloads: 174
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - This repository provides Japanese Automatic Speech Recognition (ASR) transcriptions generated using Whisper, sourced from the Reazon Speech dataset, excluding the original audio files.
  - Downloads: 173
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a 1.2 million+ image MIT-licensed anime illustration dataset sourced from diverse materials, offering high quality with a focus on image diversity despite relatively few tags per image.
  - Downloads: 161
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tags (averaging 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 158
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - This repository provides a comprehensive 86,000+ sample Japanese voice datasetâ€”combining StoryTTS, genshin-voice, and japanese-anime-speechâ€”for speech recognition, text-to-speech, and machine learning applications.
  - Downloads: 133
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - This repository provides translated speech instructionsâ€”sourced from the Common Voice dataset across 120 languagesâ€”into English, Arabic, Japanese, Mandarin, and French, ideal for fine-tuning Speech LLMs.
  - Downloads: 126
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - This dataset contains 2735 WAV audio clips extracted from the game *Project Sekai*'s character Emu Otori, intended for research use with So-vits-svc 4.0, and licensed under CC-BY-NC 4.0 with copyright retained by SEGA and voice actors.
  - Downloads: 123
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - This corpus provides 120 participants' Japanese telephone conversations recorded in the United States, offering audio data for speech research and requiring proper citation per TalkBank rules.
  - Downloads: 116
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja is a synthetic Japanese multi-turn dialogue dataset created by translating DailyTalk with Gemma-3-27B and synthesizing speech using Zyphra/Zonos-v0.1-transformer, featuring five distinct speaker voices.
  - Downloads: 89
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - This dataset provides a furigana-annotated speech corpus derived from Aozora Bunko and SAPIE audio daisy data, containing 3,361,443 processed and cleaned entries with kanji.
  - Downloads: 80
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - This repository provides the Japanese Sakura Corpus, comprising audio data from 31 participants in Japan, requiring citation and adherence to TalkBank usage rules as detailed at https://ca.talkbank.org/access/Sakura.html.
  - Downloads: 76
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - This dataset enhances Aratako's synthetic Japanese roleplay data (20k entries, based on DeepSeek-V3-0324) with added system messages and formatting, released under the MIT license.
  - Downloads: 76
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This synthetic dataset, built using images from ThePioneer/japanese-photos, was generated with Qwen2-VL-7B-Instruct and Qwen2.5-32B-Instruct-AWQ models.
  - Downloads: 74
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from Project Sekai for research using so-vits-svc 4.0, licensed under CC-BY-NC 4.0 with copyright remaining with SEGA and voice actors.
  - Downloads: 64
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA is a synthetic VQA dataset of 41,895 samples generated from 8,379 Manga109 images using GPT-4o, licensed under CC BY 4.0 and subject to OpenAI terms.
  - Downloads: 59
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - This repository provides a split dataset derived from the joujiboi/japanese-anime-speech-v2 project, likely for easier access or specific training/testing purposes.
  - Downloads: 57
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This dataset contains crawled data from the HomeMate Kawara Awardâ€™s â€œPhoto Senryuâ€ competition, including 435 image prompts and 1767 corresponding senryu poems, intended for use within the YANS hackathon.
  - Downloads: 56
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay dataset (created with DeepSeek-R1-0528) by adding system messages and formatting, released under the MIT license.
  - Downloads: 54
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - This dataset provides English and Japanese captions generated by BLIP for PokÃ©mon images from the FastGAN dataset, used for training PokÃ©mon text-to-image models.
  - Downloads: 53
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - This repository provides a Japanese translation of the HelpSteer dataset, designed for experimenting with and aligning Large Language Models using NVIDIAâ€™s SteerLM technique and NeMo Aligner.
  - Downloads: 47
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS scores) for the Common Voice Corpus 17.0, detailing data saved in a JSON file and providing counts of audio files exceeding specific MOS values, with analysis powered by resources from AiHUB.
  - Downloads: 44
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - This dataset provides approximately 1000 Japanese roleplay instructions created by applying the Magpie method to Nvidiaâ€™s Nemotron-4-340B-Instruct, built with DeepInfra, and may contain low-quality records due to minimal post-filtering.
  - Downloads: 43
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This repository provides a 66.4-hour Japanese voice dataset of 30,800 recordings from Fate/Grand Order characters, filtered to single voice actors, for ASR/ASV model training and evaluation.
  - Downloads: 41
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This repository provides a Japanese text corpus generated by Phi-3 from randomly sampled data, utilizing resources including the OpenMathInstruct-1-1.8m-ja code and computational support from the TSUBAME4.0 supercomputer.
  - Downloads: 40
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - This dataset filters high-quality recordsâ€”totaling 5,475 with Apache 2.0, CC-BY-SA-3.0, and MIT licensesâ€”from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja, specifically for JGLUE benchmarks (JcommonsenseQA, MARC-ja, JSQuAD).
  - Downloads: 40
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS scores and transcriptions) for reazon-research/reazonspeech-v2, saved as a JSON file and visualized with histograms, utilizing computational resources provided by AiHUB.
  - Downloads: 39
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - This repository provides a CC0-licensed dataset of Japanese places intended for training text-to-image or other machine learning models without copyright concerns.
  - Downloads: 37
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This dataset facilitates evaluation of large language models on three Japanese *okashi* (humorous response) generation tasksâ€”text-to-text, image-to-text, and text-image-to-textâ€”using image and text prompts.
  - Downloads: 36
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - This repository provides a Hugging Face Datasets version of the Tanaka Corpus, preprocessed into a dataset of Japanese-English sentence pairs with IDs.
  - Downloads: 32
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - This dataset pairs images of PDF pages (resized to 896px, 700px, or 588px) with OCR-processed text (using NDLOCR, potentially containing "ã€“" for failed reads) and question-answer pairs generated by Qwen/Qwen2.5-14B-Instruct to train image retrieval models.
  - Downloads: 32
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - This repository provides a dataset of emotion analysis results for Japanese music videos, featuring predicted moods, valence, and arousal scores in JSONL format generated using Music2Emotion.
  - Downloads: 31
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus provides a dataset of 96kHz/16bit Japanese speech recordings by a virtual character ("Lux"), including both raw and cleaned audio files with corresponding transcriptions and metadata under a CC BY 4.0 license.
  - Downloads: 30
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage is a Japanese dataset derived from JDocQAâ€™s test split, providing questions answered with single 200dpi PNG images sourced from PDF documents, and including question/answer text, IDs, answer types, and original PDF filepaths.
  - Downloads: 30
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - This repository provides the LLaVA JP Instruct 108K dataset, a Japanese instruction-tuned dataset created from Japanese Visual Genome VQA and docci_ja data, licensed under Apache 2.0.
  - Downloads: 29
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This dataset contains crawled data from Japanese *senryu* (short poem) websitesâ€”Homemate and Marusenryuâ€”for image-to-text and text-to-text tasks, featuring 70 images/prompts and 140/60 corresponding responses, filtered to include only top-rated entries.
  - Downloads: 29
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - This repository provides data and instructions to reproduce the FrÃ©chet Video Distance (FVD) evaluation for the AIdeaLab VideoJP video generation model.
  - Downloads: 29
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - This repository provides hard negative examples mined from the Japanese MS MARCO dataset, processed with normalization, filtering, and collection selection, and used to train and compare a SPLADE information retrieval model against mMARCO.
  - Downloads: 28
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - This dataset provides 6315 converted 1024x1024 PNG images of Kanji characters from KanjiVG, paired with their textual definitions, readily loadable via the `datasets` library.
  - Downloads: 26
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - This repository provides a 32,000-instruction Japanese dataset, Rakuten-Alpaca-Data-32K, automatically generated using RakutenAI-7B-chat based on the Stanford Alpaca methodology, and acknowledges contributions from seed_tasks_japanese.jsonl, recommending data filtering due to quality concerns.
  - Downloads: 25
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - This dataset provides diverse, high-quality images and videos of Japanâ€”spanning landscapes, culture, and daily lifeâ€”captured primarily in the 2020s for AI training.
  - Downloads: 24
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Japanese LLaVA Instruct 150K is a Japanese-translated version of the LLaVA Visual Instruct 150K dataset, enabling visual instruction tuning for Japanese language models under a CC BY-NC-4.0 license.
  - Downloads: 23
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - This repository provides a large-scale Japanese speech dataset for VOICEVOX, comprising 445,793 .wav files (577 hours of audio) built from the ITA, Tsukuyomi-chan, and ROHAN corpora.
  - Downloads: 23
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This dataset provides 330k Japanese web text examples (train/test) with noise spans identified by an LLM (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) in a strict JSON format for tasks like cleaning web-scraped text.
  - Downloads: 20
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This dataset contains Japanese humorous response data from the Bokete website, sourced from CLoT-Oogiri-Go, encompassing text-to-text, image-to-text, and text-image-to-text tasks with a total of 100 examples for evaluation and leaderboard participation.
  - Downloads: 20
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This dataset contains 100 Japanese *senryu* (short poem) examplesâ€”70 image-to-text and 30 text-to-textâ€”crawled from photo and posting sites, designed for evaluating generative models via a leaderboard and human evaluation.
  - Downloads: 19
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - This dataset provides Japanese audio and transcriptions of veterinary medicine terminologyâ€”including drugs, diseases, and symptomsâ€”for training speech recognition or natural language processing models.
  - Downloads: 18
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - This repository provides a clustered dataset of Japanese business descriptions from EDINET, labeled with industry codes, for training and evaluating embedding models, split into train/test sets with balanced label representation.
  - Downloads: 18
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository provides a dataset of voice embeddings for Japanese members of parliament, created using speechbrain/spkrec-ecapa-voxceleb, intended for tasks like speaker separation and speech analysis of parliamentary proceedings.
  - Downloads: 17
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - This dataset provides CC0 licensed images of Japanese scenery for training text-to-image models and other applications without copyright concerns.
  - Downloads: 16
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella is a Japanese a cappella vocal ensemble corpus featuring scores and isolated audio tracks of six-part arrangements of out-of-copyright children's songs.
  - Downloads: 15
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - This repository provides a cleaned and filtered version of the Aozora Bunko Furigana Speech Corpusâ€”containing 2,536,041 entriesâ€”improving upon the original dataset's Whisper-generated transcriptions through sanity checks and kanji reading validation for training speech models like FLFL.
  - Downloads: 14
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - This repository provides a clustered dataset of 6,127 Japanese legal documents (from e-Gov XML data, specifically "Heisei" and "Reiwa" eras) with titles, provisions, and labels based on legal classification, split into train/test sets for embedding model learning and evaluation.
  - Downloads: 13
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - This dataset provides quiz data licensed under CC-BY-SA-4.0, extracted from the JAQKET dataset originally used in the AI King competition, and is accessible via the `datasets` library.
  - Downloads: 12
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - This dataset provides multilingual Amazon product reviews (English, Japanese, German, French, Chinese, Spanish) collected between 2015-2019 for text classification, but is currently defunct and inaccessible.
  - Downloads: 2,187
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - This repository ranks Large Language Models (LLMs) by their English translation quality of Japanese Visual Novels, offering a comparison to established translation tools with preliminary, evolving results.
  - Downloads: 854
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - This repository provides a Japanese translation of the research-safe English subset of the ReLAION-5B dataset, created using the Gemma-2-9b-it LLM and the vLLM inference library via the text2dataset tool.
  - Downloads: 469
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªžã‹ã‚‰è‹±èªžã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 365
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - This repository provides Japanese queries from the MMarco dataset, paired with hard negatives retrieved using both multilingual e5 and BM25 models for information retrieval research.
  - Downloads: 317
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - JHumanEval is a manually post-edited, Japanese translation of the HumanEval code-generation benchmark, designed to evaluate the performance of Japanese LLMs even with imperfect documentation.
  - Downloads: 289
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - This dataset provides Japanese-to-English translations of the kaken subset from the llm-jp-corpus-v3, generated using Qwen/Qwen2.5-32B-Instruct, and is released as an open, parallel corpus under a CC-BY 4.0 license.
  - Downloads: 189
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS is a 30-hour paired dataset of 10,000 Japanese speech samples and Simplified Chinese translations for training and evaluating speech translation and multilingual speech understanding models.
  - Downloads: 181
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - This repository provides the filtered training and development datasets from JSNLI v1.1, used in the book *Introduction to Large Language Models*, licensed under CC BY-SA 4.0.
  - Downloads: 174
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - This dataset provides a cleaned Japanese corpus extracted from OSCAR-2301, containing 94M Japanese words (181.2GB) after processing with corpus-cleaner, excluding several problematic metadata files.
  - Downloads: 160
- [DataPilot/Zero_SFT_Ja_v3.5](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3.5)
  - Zero_SFT_Ja_v3.5 is a 108,000-example Japanese instruction-following dataset built with the BARE method, utilizing models like Sarashina2-70B and Qwen3-235B-A22B for question/answer generation and multilingual-E5-large/Phi-4 for filtering, output in JSON Lines format.
  - Downloads: 136
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository hosts oasst2-33k-ja, a Japanese instruction tuning dataset translated from an English oasst2 subset using DeepL, built upon kunishou/oasst2-135k-ja by LLM-jp.
  - Downloads: 123
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLMã®ãŸã‚ã®æ—¥æœ¬èªžã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸ å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€ æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªžå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›žå¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
  - Downloads: 117
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja is a 12,000-entry Japanese human preference dataset, translated from hh-rlhf using DeepL, comprising random samples from its harmless, helpful, and rejection-sampled training splits.
  - Downloads: 116
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - This repository provides a Japanese translation of the MS MARCO dataset, created using google/madlad400-3b-mt and structured like the original, though translation quality is limited and itâ€™s recommended to compare with higher-quality multilingual datasets like mMARCO.
  - Downloads: 106
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - This repository provides a hand-crafted Japanese dataset ("liz-nojaloli-ja") for preparing data for Reinforcement Learning from Human Feedback (RLHF), potentially referencing code from Qiita.
  - Downloads: 106
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This repository provides a Japanese-English parallel text dataset for translation, licensed under Apache 2.0 with potential source-specific restrictions, and includes metadata indicating source and fanfiction status.
  - Downloads: 98
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - This repository provides a Japanese translation of the GSM8k reasoning dataset, along with extracted answers, using a quantized language model and addressing potential Japanese data inconsistencies, exemplified by a multi-step reasoning problem and its solution.
  - Downloads: 97
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This repository provides a sentence-aligned Japanese web novel and English fan translation dataset for document translation, including metadata like series titles and alignment scores sourced from NovelUpdates.
  - Downloads: 97
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - This repository provides a deduplicated and randomized dataset of English-Japanese sentence pairs sourced from Tatoeba, designed for machine translation or language learning.
  - Downloads: 93
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl provides a Japanese information retrieval dataset, converted to the BeIR format for compatibility with the mteb benchmark.
  - Downloads: 66
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Japanese LLaVA Pretrain is a Japanese-translated version of the original LLaVA dataset, enabling similar multilingual visual language modeling research and requiring adherence to the CC-3M and BLIP licenses.
  - Downloads: 65
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - This dataset provides the Japanese-English parallel corpus extracted from the Asian Language Treebank (ALT) project, sourced from the Hugging Face `alt` dataset, and cited in Riza et al. (2016).
  - Downloads: 61
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset provides corrections and translations for the Japanese portion of the multilingual LLAVA-Bench-in-the-wild benchmark, building upon the original liuhaotian/llava-bench-in-the-wild data.
  - Downloads: 59
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - LLM-jp's Synthetic-JP-EN-Coding-Dataset is an instruction tuning datasetâ€”a subset of Aratako's 801k datasetâ€”created through a Japanese collaborative project for English-Japanese coding tasks.
  - Downloads: 58
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - This repository provides a 3.3 million row Vietnamese-Japanese parallel corpus for machine translation and natural language processing tasks.
  - Downloads: 57
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA is a 30-hour dataset of ~10,000 Japanese audio-Chinese text pairs from movies/TV shows, designed for speech translation and multilingual speech understanding research.
  - Downloads: 56
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP is a Japanese anime knowledge benchmark dataset comprising 100 quiz questionsâ€”20 per animeâ€”from five popular series, designed to evaluate LLM understanding in Japanese.
  - Downloads: 55
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - This repository provides a free 850,000 sentence sample of a larger paid English-Japanese parallel corpus, covering diverse topics and suitable for machine translation and text analysis.
  - Downloads: 54
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - This repository provides a Japanese translation of a cleaned "bluemoon-fandom-1-1-rp" dataset, utilizing the command-r-08-2024 model via the openrouter API for faster, resource-efficient, and uncensored NSFW translation.
  - Downloads: 53
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual dataset for form understanding, providing human-labeled key-value pairs in seven languages to benchmark performance on form analysis tasks.
  - Downloads: 50
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - This repository provides a quality-filtered subset of 1 million rows from the JParaCrawl v3 English-Japanese parallel corpus, addressing issues with alignment and completeness in the original dataset.
  - Downloads: 49
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - This repository provides a 20,000-record Japanese-English translation dataset generated using the Magpie method applied to Nvidia's Nemotron-4-340B-Instruct, alongside the code used for its creation, noting the dataset may contain low-quality records due to minimal post-filtering.
  - Downloads: 48
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This dataset is a Japanese RLHF instruction-tuning collection derived from kunishou/hh-rlhf-49k-ja, excluding examples filtered for negative guidance translation.
  - Downloads: 46
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a manually created, high-quality Japanese dataset of 100 chain-of-thought examples, available in both connected and separated CoT/output formats, with the connected version offering more natural transitions.
  - Downloads: 42
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - LIMA-JA is a Japanese translation and refined version of Metaâ€™s LIMA dataset, accessible via the `datasets` library for training and evaluating language models.
  - Downloads: 41
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers a Japanese translation of the MBPP dataset, created using LLM-jp and DeepL, with contributions from Han, Otake, Ozaki, and Miyao.
  - Downloads: 40
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This repository provides a Japanese translation of the ViQuAE dataset, focusing on translating the original answers into Japanese while leaving the answers themselves untranslated.
  - Downloads: 38
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - JSynFlow is a ~10,000-entry Japanese flowchart VQA dataset generated using Meta's Llama 3.1 405B, containing Mermaid-formatted diagrams and QA pairs describing job tasks across various professions, intended for research purposes.
  - Downloads: 38
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - This repository provides a corrected Japanese translation of MT-Bench, utilizing and building upon Stability AI's Japanese MT-Bench for certain questions.
  - Downloads: 37
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This repository provides the Japanese language portion of the Guanaco dataset, similar to the alpaca-guanaco-japanese-gpt-1b dataset.
  - Downloads: 36
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - This repository provides a long-text instruction dataset built from the Aozora Bunko corpus, designed to challenge question answering models with difficult, unfiltered long-form content under a CC BY 4.0 license.
  - Downloads: 35
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - This dataset provides a Japanese translation of the FED dataset using the Google Cloud Translate API v2, with potential inconsistencies in some dimensions due to machine translation and annotation differences.
  - Downloads: 33
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - This repository provides a sample of a 9.83 million sentence pair Chinese-Japanese parallel corpus in text format, covering diverse fields and suitable for machine translation and text analysis.
  - Downloads: 28
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This repository provides a Korean, Chinese, and Japanese translation dataset built from OpenOrca, matched by ID and refined using embedding similarity to select the best translation for each language (DeepL for Korean, Google Translate for Chinese, and original OpenOrca for Japanese).
  - Downloads: 28
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This repository provides a JSON conversion of the NilanE/ParallelFiction-Ja_En-100k dataset, formatted for use in text-generation-webui model training for document translation tasks involving Japanese web novels and English fan translations.
  - Downloads: 28
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - This repository provides the Japanese translation of the SciQ dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed under CC-BY-NC 3.0.
  - Downloads: 26
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - This repository provides paired Japanese and Vietnamese sentences for machine translation and language learning.
  - Downloads: 25
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - This repository provides a parallel English-Japanese corpus extracted from Wikidata for machine translation, formatted as a JSONL file optimized for Hugging Face transformers training, and filtered based on word count to improve translation quality.
  - Downloads: 24
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - This repository provides a Japanese translation of the MMLU dataset, created using GPT-3.5-turbo, for use in MultilingualSIFT research.
  - Downloads: 23
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - This repository provides a Python function (`prompt`) for rigorously evaluating the accuracy of Japanese-to-English translations based on completeness, accuracy, grammar, and overall quality.
  - Downloads: 22
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository provides 1k responses generated using DeepSeek-R1-Distill-Llama-8B on the aya-ja-evol-instruct-calm3-dpo-masked dataset with 8-bit quantization, noting potential accuracy loss and formatting issues for research or preprocessing purposes.
  - Downloads: 22
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - This repository provides a Faiss index and sentence embeddings of Japanese Wikipedia paragraphs generated using the intfloat/multilingual-e5-base model for efficient similarity search.
  - Downloads: 22
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - This repository provides a ~2.46M row Japanese instruction-tuning dataset in Aya format, converted from v1.0.0 and licensed under CC-BY-SA 4.0.
  - Downloads: 21
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - This repository provides the Tanaka-corpus, a publicly available Japanese-English parallel text dataset compiled by Professor Tanaka and used for machine translation research.
  - Downloads: 21
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - This dataset provides summaries for long texts sourced from the cleaned Aozora Bunko corpus, licensed under CC BY 4.0.
  - Downloads: 21
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - This repository provides a sample of a 101,702-entry Japanese pronunciation dictionary created by linguists, intended for research and development of Japanese Automatic Speech Recognition (ASR) technology, with a link to the full paid dataset.
  - Downloads: 19
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - OPUS-MIT-5M is a large, balanced multilingual image translation dataset of 5 million sentence pairs sampled from the OPUS corpus, designed for robust cross-lingual image-text understanding.
  - Downloads: 19
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - This repository provides Japanese-English translations licensed under CC-BY 4.0.
  - Downloads: 18
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - This dataset provides Japanese-English translation pairs from the TALPCo corpus in Hugging Face format, with whitespace from Japanese tokenization removed, and licensed under CC-BY 4.0.
  - Downloads: 16
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - This dataset is a 50k-sample English subset extracted from the larger 801k Synthetic-JP-EN-Coding-Dataset, requiring users to consult the original datasetâ€™s documentation for details and caveats.
  - Downloads: 15
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - This repository provides a question-answering dataset generated from the Japanese wiki40b dataset.
  - Downloads: 14
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - This repository provides the Japanese translation of the PIQA dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed identically to the original PIQA.
  - Downloads: 11
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This repository provides a Japanese-English sentence-aligned web novel dataset, converted to Alpaca format and chunked to 4096 tokens for use with the augmxnt/shisa-base-7b-v1 model.
  - Downloads: 11
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository provides the TaCo dataset used for research on improving cross-lingual transfer in low-resource language LLMs via translation-assisted Chain-of-Thought prompting, requiring citation if used.
  - Downloads: 11
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - This repository provides a 380,000-sentence sample of a larger, paid Japanese-English parallel corpus suitable for machine translation, text analysis, and NLU tasks, excluding sensitive content.
  - Downloads: 11
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides the TaCo dataset for cross-lingual instruction tuning, featuring paired non-English instructions/inputs and their English/native language responses, as detailed in the linked research paper.
  - Downloads: 11
### Semantic Text Processing
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a Japanese biomedical LLM benchmark dataset with a provided evaluation framework (med-eval) designed for assessing performance and encouraging contributions.
  - Downloads: 2,426
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - JGLUE is a Japanese natural language understanding benchmark dataset, built from scratch by Yahoo Japan and Waseda University, designed to evaluate and advance NLU research in Japanese.
  - Downloads: 1,726
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a Japanese text embedding benchmark comprising 24 datasets across 6 tasks for evaluating model performance on Japanese language understanding.
  - Downloads: 1,181
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - This repository provides Japanese/English synthetic conversation datasets derived from LMSYS-Chat-1M, used for post-training Llama-3.1-Swallow and Gemma-2 models.
  - Downloads: 1,111
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - This repository provides Japanese Wikipedia embeddings and a FAISS index for RAG applications, including demos, conversion scripts, and evaluations of various Japanese embeddings (including OpenAI's `text-embedding-3-small`) for search and Q&A tasks, under a mixed CC-BY-SA-4.0 and OpenAI license.
  - Downloads: 865
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - This repository provides a Japanese instruction-following (chat) dataset for fine-tuning LLMs, particularly for improving chat response capabilities, with updates to remove data subject to licensing changes and address data quality issues like blank outputs and missing entries.
  - Downloads: 629
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - This repository merges and extracts lines of 256 characters or less from the neody/c4-ja-cleaned, neody/cc100-ja-cleaned, and neody/oscar-ja-cleaned Japanese datasets.
  - Downloads: 225
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - This dataset provides simple training data for a "Zunda Mon" character LLM, including JSONL formats for LLM-jp and ChatGPT, created from publicly available and officially sourced information, and requiring careful review of the license for all uses.
  - Downloads: 208
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - This repository provides a 200GB subset of the multilingual FineWeb2-HQ dataset, specifically containing Japanese text data split into 14 JSONL chunks for easier handling.
  - Downloads: 197
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - This repository provides a runnable ggml-Japanese-GPT2 model for Windows, requiring downloaded bin and SentencePiece model files and executable command-line usage, with a note about a currently broken xsmall model file.
  - Downloads: 187
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - This repository provides a Japanese chat datasetâ€”derived from izumi-lab/llm-japanese-datasetâ€”designed for fine-tuning large language models, particularly for instruction-following and chat tasks using methods like LoRA.
  - Downloads: 163
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - This repository provides a cleaned dataset of 5.1M Japanese sentences with context, suitable for training unsupervised semantic similarity models.
  - Downloads: 127
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - This repository provides the Japanese Wikipedia sentences dataset used in the book â€œIntroduction to Large Language Models,â€ sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 117
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, built upon TinyStories and designed for easy NLP data filtering.
  - Downloads: 94
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - This repository provides a processed version of the ShareGPT52K dataset, converted to Markdown, language-labeled, and enhanced with CJK whitespace handling and Traditional/Simplified Chinese conversion tools.
  - Downloads: 70
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - llm-jp-instructions is a manually created Japanese instruction dataset, available via the `datasets` library with train, development, and test splits for v1.0.
  - Downloads: 67
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository hosts databricks-dolly-15k-ja, a Japanese translation of the dolly-15k instruction tuning dataset created collaboratively by LLM-jp.
  - Downloads: 65
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„19800ä»¶ã®æ—¥æœ¬èªžãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åŽéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 63
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - This repository provides a continuously updated, GPT-4 generated Japanese question-answering dataset for fine-tuning open-source, non-English language models, created with minimal verification beyond semantic similarity filtering.
  - Downloads: 60
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - This repository provides a Japanese fake news dataset converted for use with Hugging Face datasets, including text content, labels indicating authenticity (real, partial GPT-2, or full GPT-2), and character counts for true/fake portions.
  - Downloads: 59
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This dataset provides passage embeddingsâ€”generated using llm-book/bert-base-japanese-v3-bpr-passage-encoderâ€”for the AI-King competition passages from llm-book/aio-passages, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 49
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - This repository provides document-level merged versions of the cc100/cc100-ja Japanese text dataset, originally line-separated, while maintaining the original license.
  - Downloads: 48
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - This dataset consists of bullet-point lists generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and DeepSeek-R1-Distill-Qwen-32B-Japanese models, licensed under CC-BY-SA 4.0.
  - Downloads: 47
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - This dataset provides 39.6k formatted Japanese roleplay conversations generated with gpt-4o-mini, including system messages, licensed under CC-BY-NC-SA 4.0, and prohibits use for developing competing OpenAI models.
  - Downloads: 46
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - This repository provides a Japanese Wikipedia paragraphs dataset used in the book â€œIntroduction to Large Language Models,â€ sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 46
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - This dataset provides 190,854 Japanese preference labels generated using five open-source models (including Tanuki and Qwen) and judged by Qwen/Qwen2.5-72B, focusing on high-quality instruction following and addressing potential positional bias.
  - Downloads: 44
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - Shisa-base-7b-v1 is pre-trained on this dataset comprising a 90/10 Japanese/English mix of MADLAD-400 tokens sampled using DSIR.
  - Downloads: 44
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - This repository provides a 69k Japanese-English coding dialogue dataset generated using models like Nemotron, Phi-3, Mixtral, and Calm3 via the Magpie method, including the generation code and noting potential data quality issues due to minimal post-filtering.
  - Downloads: 43
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG is a 20,000-record benchmark for evaluating the Japanese language Retrieval-Augmented Generation (RAG) performance of Large Language Models, utilizing (Context, Question, GroundtruthAnswer) data.
  - Downloads: 42
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - This dataset, created by the University of Tokyo's Matsuo-Iwasawa Lab LLM course, comprises human-authored inputs and outputs from two language models (watashiha-gpt-6b and Watashiha-Llama-2-13B-Ogiri-sft) for supervised fine-tuning (SFT) exercises, intended for educational and research use.
  - Downloads: 41
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - This dataset records PokÃ©mon VGC Regulation F team selection data collected from YouTube battle streams, including data from the author (trainer_id 13), and was used for a presentation at the Remote PokÃ©mon Society in May 2024.
  - Downloads: 40
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets with a 16kHz sampling rate.
  - Downloads: 37
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - This dataset provides cleaned Japanese example sentences demonstrating various grammatical patternsâ€”including politeness, negation, desire, progressive aspect, and moreâ€”generated using the calm3-22b language model.
  - Downloads: 36
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - This dataset provides a ShareGPT-formatted, Japanese language version of the OpenAssistant/oasst2 135k dataset, optimized for multi-turn conversation fine-tuning with significant computational resource requirements.
  - Downloads: 35
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - This repository provides a Japanese version of the Databricks Dolly 15k dataset, enhanced with BERTScore-calculated translation quality scores to identify and address issues like inconsistencies and inaccuracies in the machine translation.
  - Downloads: 34
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - This dataset contains conversational data generated using GPT-3.5-Turbo based on the Japanese Wikipedia (July 2023) dataset, and is **not** licensed for commercial use.
  - Downloads: 32
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - DataPilot converted the Japanese instruction-following dataset, ichikara-instruction-003, into the widely-used ShareGPT format for improved LLM fine-tuning, particularly for conversational models, providing each question-answer pair as a separate conversation in JSON Lines format.
  - Downloads: 31
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - YokaiEval is a Japanese dataset of 810 multiple-choice questions designed to evaluate Large Language Models' knowledge of Japanese folklore, specifically concerning *yokai* (supernatural creatures) across six knowledge categories.
  - Downloads: 31
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - Kanalizer is a dataset repository for a library that predicts pronunciations from English words, with code available at VOICEVOX/kanalizer and trained models at VOICEVOX/kanalizer-model.
  - Downloads: 30
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 30
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, and built as an improvement upon TinyStories for NLP tasks.
  - Downloads: 30
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One designed to enhance realism, increase image complexity, and simplify anime-style illustration generation within the stable-diffusion-webui.
  - Downloads: 29
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-example Japanese coding dialogue dataset generated using the Magpie technique applied to NVIDIAâ€™s Nemotron-4-340B-Instruct, along with the code used for its creationâ€”note that the dataset contains unfiltered, potentially low-quality records.
  - Downloads: 28
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - This dataset comprises human-written (OSCAR) and LLM-generated (GPT-3.5 Turbo) Japanese text for evaluating the performance of LLMs in detecting machine-generated text.
  - Downloads: 28
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - This repository provides manually created variations of tasksâ€”presenting conversion candidates like IME and correcting bracket matchingâ€”developed to address weaknesses in a model created by @pokutuna for the 2024 University of Tokyo Matsuo Lab Deep Learning competition.
  - Downloads: 26
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - This repository provides a model fine-tuned on both the kenkensz9/1242tw2 dataset and a custom collection of personality-driven tweets, and evaluates generated tweets with a subjective 1-10 point score reflecting quality and personality.
  - Downloads: 25
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small is a Japanese synthetic dataset of high-quality prompts and AI outputs automatically generated using the Mistral Small 3.1 24B Instruct model, formatted as JSONL for supervised fine-tuning.
  - Downloads: 24
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - This repository provides a Japanese translation of the HelpSteer2 dataset, used for training and aligning large language models like Nemotron-4-430B-Reward within the NVIDIA SteerLM framework.
  - Downloads: 24
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - This repository provides an experimental dataset of 50 queries and corresponding responsesâ€”generated with ChatGPT-4o across five perspectives (excluding direct patent attorney referrals)â€”along with evaluation points, supplemented by manually created answers from public patent databases for 10 excluded queries.
  - Downloads: 23
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - This dataset is a Japanese role-playing learning resourceâ€”a translation of the Bluemoon_Top50MB dataset using the karakuri-lm-8x7b-chat-v0.1-awq LLM via DeepInfra, with translations truncated to 8000 tokens and some records removed due to LLM repetition or incompletenessâ€”but note that length/token count columns are inaccurate.
  - Downloads: 21
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - This repository provides human-generated responses to the ELYZA-tasks-100 benchmark for Japanese LLM evaluation, achieving an average GPT-4o score of 3.69 and outperforming Claude 3.5 Sonnet with similar automated scoring.
  - Downloads: 20
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository provides a 60k Japanese synthetic instruction dataset, created using Self-Instruct with Qwen2.5-72B, built upon and expanding the Aratako/Magpie-Tanuki dataset, and licensed under Apache 2.0 with Qwen License considerations.
  - Downloads: 19
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This repository provides a Japanese translation of Databricks' Dolly project, licensed under CC BY-SA 3.0, utilizing and building upon data from sources like Wikipedia under the same license.
  - Downloads: 18
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - This repository provides a filtered and modified Japanese/Chinese parallel dataset from WikiMatrix v1, processed with regex filtering, semantic similarity scoring (LaBSE, threshold 0.6), and Traditional to Simplified Chinese conversion using `zhconv`.
  - Downloads: 18
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - This repository provides a 26.5k Japanese instruction dataset created by applying Magpie and Evol-Instruct techniques to team-hatakeyama-phase2/Tanuki-8x8B data, clustered using Mini Batch K-Means, and licensed under Apache 2.0 with Qwen restrictions.
  - Downloads: 17
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - This dataset provides educational scores (0-4) for Japanese text from the fineweb-2 dataset, created using the Deepseek API and static embeddings to assess educational value, comprising approximately 280k training and 30k testing examples.
  - Downloads: 16
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese temporal inference datasetâ€”including templates, training, and test dataâ€”designed to evaluate the generalization capacity of language models on temporal reasoning.
  - Downloads: 15
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - This dataset provides Japanese example sentences created with calm3-22b, covering diverse grammatical patterns including politeness, negation, desire, progress, and various sentence structures for requests, permissions, obligations, and expressions of opinion/intention.
  - Downloads: 15
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - This dataset converts the Open-Platypus-Japanese-masked dataset into OpenAI messages format for use in language model training and evaluation.
  - Downloads: 14
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset provides detailed, natural language overviews of VTubersâ€”including their character, activities, collaborations, and styleâ€”collected using GPT-4o Search Preview with a cost of $27.04 for 36,276 tokens.
  - Downloads: 13
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - This repository mirrors the non-Wikipedia portion of the LLM-jp Corpus v3, excluding content licensed under CC-BY-SA.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªžãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - This project experiments with using the Magpie method to extract prompts with the rinna/llama-3-youko-8b language model, based on a translated prompt from the Magpie research paper.
  - Downloads: 11
### Natural Language Interfaces
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda provides 40 Japanese-language questionsâ€”covering history, society, government, and geographyâ€”to benchmark and rank the performance of Japanese large language models.
  - Downloads: 496
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset designed to evaluate the effectiveness of Retrieval-Augmented Generation (RAG) for improving LLM accuracy by leveraging external knowledge retrieval.
  - Downloads: 449
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a Japanese dialogue corpus of approximately 14,000 conversations with speaker personas and personality traits, intended for research with ethical considerations regarding privacy and impersonation.
  - Downloads: 352
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696 question-answer pair dataset, sourced from Japanese Wikipedia, designed for Japanese machine reading comprehension and achieving strong results with BERT-Japanese fine-tuning.
  - Downloads: 339
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - This repository provides the Japanese question-answering benchmark dataset used for evaluating large language models, specifically within the book *Large Language Models II* and utilizing the llm-jp-eval framework, under the Apache 2.0 license.
  - Downloads: 279
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This repository provides the Japanese subset of the LAION-2B-multi-research-safe dataset, a large-scale multilingual image-text dataset.
  - Downloads: 257
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - JAQKET is a Japanese open-domain question answering dataset, built from Wikipedia articles, featuring multiple-choice questions designed to advance QA/machine reading research in Japan.
  - Downloads: 198
- [UEC-InabaLab/KokoroChat](https://huggingface.co/datasets/UEC-InabaLab/KokoroChat)
  - KokoroChat is a large, human-collected Japanese dialogue dataset featuring role-played psychological counseling sessions with counselor feedback, designed for research in empathetic response generation and mental health language modeling.
  - Downloads: 189
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - AIxBlock provides a quality-assured dataset of 500k Japanese sentences for applications like speech data generation and Natural Language Processing.
  - Downloads: 133
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository provides a Japanese dialogue summarization dataset translated from dialogsum and CSDS, offering resources for training and evaluating summarization models in Japanese.
  - Downloads: 96
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIçŽ‹ (AIO) is a Japanese quiz datasetâ€”specifically, the Version 2.0 validation setâ€”containing questions with manually annotated, valid answers and metadata like timestamps and competition details.
  - Downloads: 95
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a 4K+ question-answer dataset with commentaries from 13 years of Japanese National Pharmacist License Exams, including recent image data and a performance leaderboard for models like GPT-4o and MedGemma.
  - Downloads: 90
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - This repository provides a multi-turn Q&A dataset automatically generated using Mixtral-8x22B with data sourced from oasst2, Databricks-Dolly, Minnade, and CyberAgentâ€™s chatbot arena, adhering to respective licenses (Apache 2.0, CC-BY-SA-3.0, CC0, CC-BY-4.0).
  - Downloads: 89
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k)
  - This dataset contains 10,000 synthetic Japanese roleplay conversations, each around 20 turns long, with detailed metadata including genre, tags, and character/setting information formatted for model training via system messages.
  - Downloads: 81
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The Business Scene Dialogue (BSD) dataset is a Japanese-English parallel corpus of written business conversations created through scenario writing and translation, offering data for dialogue systems and machine translation.
  - Downloads: 79
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval benchmark dataset of 5,000 questions and 500,000 web page titles/summaries, designed to evaluate systems answering natural language queries using diverse web content sourced from Hatena Bookmark.
  - Downloads: 77
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - This dataset contains 20,000 synthetic Japanese roleplaying conversations (10-20 turns each) generated with DeepSeek-V3-0324, annotated with genre, tags, and detailed character/setting information for building and customizing dialogue systems.
  - Downloads: 70
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a challenging Japanese visual question answering benchmark dataset built on a food image classification dataset, featuring multiple-choice questions designed to test vision-language models.
  - Downloads: 66
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - This repository provides a 100k Japanese question-answering dataset with answer candidates generated & evaluated using Qwen models (0.6B & 14B) derived from hotchpotch/japanese-qa-reasoning-100k and kuotient/orca-math-word-problems-193k-korean.
  - Downloads: 65
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - JEMHopQA is a Japanese multi-hop question answering dataset with explainable derivation steps, focusing on compositional and comparison questions sourced from Wikipedia, licensed under CC BY-SA 4.0.
  - Downloads: 64
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - LLM-jp Chatbot Arena Conversations Dataset provides ~1,000 pairwise human-preference conversationsâ€”primarily in Japaneseâ€”collected from head-to-head LLM evaluations, including model names, transcripts, votes, and metadata.
  - Downloads: 61
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA is a benchmark dataset of 526 manually created question-answer pairs designed to evaluate manga understanding using images from Manga109.
  - Downloads: 60
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository provides the Kyoto University-created Japanese Vicuna QA Benchmark, an 80-question dataset across 10 categories for evaluating Japanese LLM responses without references, licensed under Apache 2.0.
  - Downloads: 56
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - This dataset provides 11,808 multi-turn conversational instructions about Japanese photos, generated using GPT-4o via Azure OpenAI, based on images from the Japanese Photos dataset.
  - Downloads: 56
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - This repository provides a Japanese translation of the Open-Orca dataset, currently containing approximately 20% of the full dataset, and is available for commercial use.
  - Downloads: 53
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - MC4 is a massively multilingual dataset of cleaned Common Crawl data, offering over 100 languages for training and evaluating multilingual models.
  - Downloads: 51
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1ã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ä¸€éƒ¨ã¨ã€OpenAIã«ç”Ÿæˆã•ã›ãŸæ–‡ç« ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€tohoku-nlp/bert-base-japanese-whole-word-masking ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ãŸæ–‡ç« ã‚’æ–‡è„ˆãŒæˆã‚Šç«‹ã¤å½¢ã§åˆæˆã—ã€æ–°ãŸãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚‚ã®ã€‚
  - Downloads: 49
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This repository provides a 3,000-conversation dataset generated from Japanese Wikipedia using llama2Pro8B, licensed for commercial use, with potential for unrefined dialogue due to automated screening.
  - Downloads: 48
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a 4,246-dialogue Japanese dataset for multi-domain task-oriented dialogue research, featuring Wizard-of-Oz collection, six domains, and annotations for dialogue state tracking and goal-oriented conversations.
  - Downloads: 47
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository provides a Japanese translation of the everyday-conversations-llama3.1-2k dataset, formatted as topic-based user-assistant dialogue pairs translated with DeepL and licensed under Apache 2.0.
  - Downloads: 43
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench is a Japanese multimodal QA benchmark featuring geometry problems with contextual descriptions, questions, images, and exact/textual answers for evaluating AI reasoning.
  - Downloads: 43
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - This dataset is a Japanese translation of the â€œdatabricks-dolly-15kâ€ dataset, modified with â€œã«ã‚ƒã‚“ï¼â€ appended to each sentence using ArrowPro-7B-KUJIRA, primarily for experimentation with sentence endings rather than performance improvement.
  - Downloads: 42
- [nguyenthanhasia/japanese-bar-exam-qa](https://huggingface.co/datasets/nguyenthanhasia/japanese-bar-exam-qa)
  - This dataset provides 2,846 True/False question-answer pairs from the Japanese Bar Examination (2015-2024) covering Criminal, Constitutional, and Civil Law for binary classification tasks.
  - Downloads: 42
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - This dataset contains Japanese conversations extracted from public-domain books in Aozora Bunko using a heuristic approach to identify utterances within quotation marks, with code provided for reproduction.
  - Downloads: 42
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - This repository provides a commercially usable, multi-turn conversation dataset created from Japanese Wikipedia using the Orion14B-Chat model, governed by a specific community license and generated with significant computational resources.
  - Downloads: 39
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset provides 60,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using Llama2Pro8B, automatically screened but potentially containing unusual dialogue.
  - Downloads: 38
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - This repository provides a multi-turn Q&A dataset automatically generated using Calm3-22b from open data sources like oasst2, databricks-dolly, and minnade, utilizing resources including Tokyo Tech's TSUBAME4.0 supercomputer.
  - Downloads: 38
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - This repository provides a Japanese question-answer dataset derived from Stack Overflow, featuring processed question/answer pairs with markdown formatting, code block preservation, and image URL replacement, available in both detailed and simplified subsets.
  - Downloads: 37
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This repository provides a growing, manually created dataset designed for training Japanese chatbots.
  - Downloads: 37
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - AI-generated Turkish and Japanese subtitles, created with Gemini 2.0 Flash, are provided for chatbot training purposes, acknowledging potential errors and unsuitability for translation AI.
  - Downloads: 36
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - This dataset contains approximately 39,600 synthetic Japanese roleplaying conversations generated with gpt-4o-mini, each with 5-10 turns and detailed metadata including genre, tags, and character/scene settings, formatted for easy model training.
  - Downloads: 35
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset comprises over 80,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using LLaMA-Pro-8B, with automated screening but potential for unusual dialogues.
  - Downloads: 33
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - This repository provides Tengentoppa, a large Japanese instruction-following dataset created by merging 16 diverse datasets in JSON format for supervised fine-tuning of language models.
  - Downloads: 33
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository provides the 1.56B token Japanese text datasetâ€”sourced from publicly available datasets like accommodation dialog and movie recommendation logsâ€”used for pre-training the AKU-d_ms-0.5B-chat-v0.1 model, with each dataset's license and processing scripts detailed within.
  - Downloads: 33
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - This repository provides a filtered and original Japanese dialogue corpus extracted from role-playing forum conversations, excluding threads with single or very short contributions.
  - Downloads: 33
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This repository provides a 3,000-conversation dataset generated from Japanese Wikipedia using llama2Pro8B, licensed for commercial use, and automatically screened with potential for some unrefined dialogue.
  - Downloads: 33
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - This dataset removes prompts matching those from the chatbot-arena-ja-calm2-7b-chat dataset.
  - Downloads: 32
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - This dataset consists of high-quality, 96k-context Magpie-Tanuki data re-generated with Qwen2.5-72B-Instruct, licensed under Apache 2.0 but subject to Qwen License restrictions for model training.
  - Downloads: 30
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese instruction tuning dataset created by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, alongside the dataset generation code, noting the absence of post-filtering and potential for lower-quality records.
  - Downloads: 30
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - This dataset is a manually cleaned, Japanese translation of the WikiHowNFQA dataset.
  - Downloads: 29
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - This repository provides a commercially-usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of the Orion-14B Models Community License and acknowledging computing resources from ABCI.
  - Downloads: 27
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - This dataset contains approximately 1,000 synthetic Japanese roleplay dialogues, each with 10 turns, generated using Nvidia's Nemotron-4-340B-Instruct and Magpie, potentially including lower-quality records and a tendency to prematurely end longer conversations.
  - Downloads: 27
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - This repository provides a Japanese multi-turn conversation dataset, generated with Qarasu14B from Wikipedia data for non-commercial use and compatible with Axolotl, utilizing the Tsuginosuke AI Super Computer.
  - Downloads: 26
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - This repository provides Japanese chatbot conversations generated using the Swallow-MX-8x7b model, based on single-turn prompts from the Chatbot Arena Conversations JA dataset (CC-BY 4.0) and translated with a Facebook translation model.
  - Downloads: 26
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 is a 200-question Japanese dataset derived from shunk031/JDocQA, featuring PDF questions converted to single 200dpi images and limited to 50 examples per question type for faster processing.
  - Downloads: 26
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - This dataset provides question-answer pairs derived from the Japanese Stack Exchange data dump, processed with markdown formatting, base64 image replacement, and includes IDs for questions, answers, accepted, and popular responses in both a detailed and simplified format.
  - Downloads: 25
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - This dataset is a Japanese translation of the WikiHowNFQA dataset, providing question-answer pairs based on WikiHow articles.
  - Downloads: 25
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This repository provides a sample of the Nexdata Japanese Conversational Speech dataset, featuring roughly 1000 speakers engaged in natural, manually-transcribed dialogues on diverse topics recorded via mobile phone.
  - Downloads: 24
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - This repository provides a filtered subset of the Aozora Bunko clean dataset, specifically including texts written in modern Japanese orthography (æ–°å­—æ–°ä»®å).
  - Downloads: 24
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - This repository provides a Japanese-only subset of the OpenAssistant Conversations Dataset, formatted as human-assistant message pairs, potentially lacking full conversational context per row.
  - Downloads: 22
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - This dataset consists of human-checked and corrected instructions for open-source LLMs, with outputs generated using Swallow-MX, though the outputs haven't been verified for accuracy and were created during the LOCAL AI HACKATHON #000.
  - Downloads: 20
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW is a synthetic dataset for benchmarking Japanese language model roleplaying abilities, featuring key attributes like genre, setting, and character information, and licensed for broad use with consideration for its Claude 3.5 Sonnet-generated origin.
  - Downloads: 15
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - This dataset provides Japanese example sentences created using the `if001/elementray_m calm3-22b` model, covering various grammatical patterns like affirmation, negation, politeness, desire, and more.
  - Downloads: 12
### Text Generation
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - This repository provides a clustered Japanese text corpus of approximately 10,000 entries, created from cleaned web corpora like mc4-ja, for information analysis purposes, with files primarily in Parquet format and accessible via Git LFS.
  - Downloads: 831
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese benchmark evaluating large language model performance via multiple-choice questions, encompassing both translated MMLU tasks and uniquely Japanese cultural content.
  - Downloads: 523
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard evaluates the performance of Japanese Retrieval-Augmented Generation (RAG) solutions across five industry domains, providing datasets and a comprehensive assessment of Parser, Retrieval, and Generation componentsâ€”a currently unique resource for Japanese RAG evaluation.
  - Downloads: 419
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual (English, Spanish, Japanese, Russian) NL-to-code benchmark comprising 945 samples and 1,707 human-written test cases for evaluating code generation models.
  - Downloads: 199
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ567077ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This repository provides a Japanese dataset from the Bokete humor site, containing image and text-based tasksâ€”image captioning, text-to-text generation, and fill-in-the-blankâ€”derived from the CLoT-Oogiri-Go dataset for creative text generation research.
  - Downloads: 142
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - This repository extracts text data of 256 characters or less from the neody/oscar-ja-cleaned dataset.
  - Downloads: 114
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - This repository provides a templated dataset of ~40 Japanese downstream tasksâ€”with up to 20,000 samples per task, split between 0-shot and few-shot learningâ€”intended for high-quality, non-machine-translated instruction tuning of LLMs.
  - Downloads: 95
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15/T23 is a 50,000-sentence Japanese corpus with aligned original, simplified, and English translations, designed for text simplification and Japanese-English translation using a 2,000-word vocabulary.
  - Downloads: 84
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct is a 5.2K instruction dataset for code-related tasks â€“ including code generation, behavior verification, and bug fixing â€“ built from commercially-licensed and permissively-obtained programming learning content, primarily in Japanese (translated from English).
  - Downloads: 71
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - This Japanese preference dataset, built upon llm-jp/magpie-sft-v1.0, uses Aratako/Llama-Gemma-2-27b-SFT-trial1 to generate responses judged against Qwen/Qwen2.5-32B-Instruct outputs by google/gemma-2-27b-it, and is subject to META LLAMA 3.1, Gemma Terms of Use, and Qwen licensing (requiring attribution if used for training).
  - Downloads: 64
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - This repository provides a dataset of approximately 3000 Japanese childrenâ€™s stories, synthetically generated with GPT-4o-mini using simplified vocabulary, based on the methodology detailed in a linked research paper.
  - Downloads: 63
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - This 801k dataset provides synthetic, instruction-tuned code data in both Japanese (173k) and English (627k) generated using models like Nemotron, Phi-3, and Mixtral, and enhanced with Evol-Instruct techniques and metadata.
  - Downloads: 54
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - This dataset contains question-answer pairs automatically generated from Japanese Wikipedia using DeepSeek-R1-Distill-Qwen-32B-Japanese, and is licensed under CC-BY-SA 4.0.
  - Downloads: 53
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This dataset from YANS-official contains Japanese humorous response dataâ€”text, image, and text-image pairsâ€”crawled from Bokete, a popular online comedy site, and used for text-to-text, image-to-text, and fill-in-the-blank tasks, originating from the CLoT-Oogiri-Go dataset.
  - Downloads: 49
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - This repository provides automatically generated Japanese Q&A data sourced from both team-created content and Common Crawl, utilizing Mixtral-8x22B for generation with a focus on low textual dependency and requiring potential cleaning due to unnatural phrasing.
  - Downloads: 40
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - This repository provides a prototype dataset, generated by Deepseek-V3-0324, designed to improve adherence to constrained system prompts, with code and data licensed under MIT.
  - Downloads: 40
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - This dataset converts a subset of the LLM-JP-Corpus v3 into Hugging Face format, adding article titles sourced from original URLs while maintaining a CC-BY 4.0 license.
  - Downloads: 38
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - This Japanese dataset provides 10K-100K question-answer pairs labeled with "evil" and "justice" â€“ generated using magnum-v4-12b â€“ for both classification and generation tasks, released under the Apache-2.0 license.
  - Downloads: 38
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset provides Japanese translations of English quotes from the original Hugging Face dataset, generated using the llm-jp/llm-jp-3.7b model and licensed under CC BY 4.0.
  - Downloads: 38
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªžæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
  - Downloads: 37
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends the CommonCatalog CC-BY dataset with English and Japanese captions generated by Phi-3 models, enabling commercial use under the CC BY license and efficient streaming access via a CSV file.
  - Downloads: 36
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This dataset is a Japanese translation of the Open_o1_sft_Pro dataset using Qwen2.5-14B-Instruct, formatted with user/assistant conversations, thoughts, and IDs, and licensed according to the original dataset.
  - Downloads: 35
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset (BCP-47 ja-JP) intended for advancing research in multimodal ad text generation models.
  - Downloads: 35
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository provides a Japanese preference dataset for SimPO training, created by scoring and ranking responses generated from Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter1 using Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, and licensed under META LLAMA 3.1 and Gemma Terms of Use.
  - Downloads: 34
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - This repository provides automatically generated Q&A data sourced from Common Crawl using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, employing random text excerpts to reduce reliance on original content, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 33
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - This dataset converts level 2 filtered data from the llm-jp-corpus-v3 (Japanese language model corpus) to Hugging Face format, adding article titles sourced from original URLs, and is licensed under CC-BY 4.0.
  - Downloads: 33
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - This dataset provides 3,907 three-line summaries of Livedoor News articles, formatted with prompts for Llama v2 and recommending the addition of "[R_START]" and "[R_END]" as special tokens for training.
  - Downloads: 31
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - This repository provides a Japanese Reinforcement Learning from Human Feedback (RLHF) dataset, reformatted as a classification task with chosen/rejected sentence labels (1/0), derived from the open_preference_v0.1 dataset and acknowledging potential quality issues due to synthetic and machine-translated text.
  - Downloads: 31
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - This repository presents a JSON dataset (reazonspeech-all-wada-snr.json) containing SNR values and transcriptions from analysis of the reazon-research/reazonspeech-v2[all] dataset, including a count of 1,208,360 entries with SNR values over 100, and acknowledges AiHUB for providing computational resources.
  - Downloads: 29
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This dataset of question data, generated using Qwen/Qwen2.5-32B-Instruct via ollama, is licensed under Apache 2.0 *only for the questions* and is not recommended for instruction fine-tuning due to the differently licensed responses.
  - Downloads: 29
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset provides Japanese instruction data, translated using KUJIRA, focused on investment topics, particularly Berkshire Hathaway and Warren Buffett, originally from glaive-aiâ€™s in-foxhound.
  - Downloads: 28
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - This repository provides a Japanese dataset generated by ELYZA-japanese-Llama-2 for evaluating AI text detection and self-instruct methods, sourced from the GPT-4-Self-Instruct-Japanese dataset and licensed per the base model.
  - Downloads: 27
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - This repository provides Japanese translations of the aimo-validation-aime dataset, generated using ChatGPT-4o, with a unique 0-30 index differing from the standard AIME24 format.
  - Downloads: 26
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This repository provides a Japanese translation of the nlvr (natural language visual reasoning) dataset, originally developed by lil-lab for multimodal reasoning tasks.
  - Downloads: 24
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - This repository provides a Japanese dataset generated by Qwen/Qwen1.5-14B for evaluating AI text detection and self-instruct methods, sourced from GPT-4-Self-Instruct-Japanese instructions.
  - Downloads: 23
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese synthetic dataset of high-quality prompts and AI outputs generated using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 23
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset combines null-instruct-ja and DeepSeek-v2.5 Q4, generated using ollama and 7 A5000 GPUs in 2 hours and 7 minutes, and is licensed under the DeepSeek license.
  - Downloads: 21
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - This repository provides a Japanese-English glossary of generative AI terminology, intended to improve translation quality with models like GPT-4, though accuracy isn't guaranteed.
  - Downloads: 20
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This dataset facilitates evaluating large language models on *okashi* (humorous response) generation via two tasksâ€”text-to-text and image-to-textâ€”using humorous prompts and corresponding image inputs.
  - Downloads: 20
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository provides a Japanese preference dataset for Reinforcement Learning from Human Feedback (RLHF), created by scoring and ranking multiple responses generated with Llama-Gemma and Qwen models based on instruction data, and is subject to Meta Llama 3.1, Gemma, and Qwen licenses.
  - Downloads: 17
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - This repository provides 1k Japanese responses generated using DeepSeek-R1-Distill-Qwen-32B based on the aya-ja-evol-instruct-calm3-dpo-masked dataset, noting potential accuracy loss from 8-bit quantization and issues with `<think>` token generation, intended for reference or preprocessing only.
  - Downloads: 13
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - This repository provides a synthetic instruction dataset, â€œjimba-wiki-instruction-calm3â€, generated using the CALM3-22B-Chat model and Japanese Wikipedia data, designed for potentially lower hallucination but requiring caution due to unfiltered, raw outputs.
  - Downloads: 11
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository provides a dataset of all *Keitai Oogiri* comedy prompts and responsesâ€”scraped from a Hatenablog archiveâ€”including IDs, episode numbers, prompts, and lists of responses.
  - Downloads: 11
### Syntactic Text Processing
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - This dataset provides human-annotated preferences (with intensity scores from -3 to 3) for paired question-response sets in Japanese, covering diverse topics like general knowledge, history, medicine, coding, and creative writing, while noting limited overall and per-genre variation.
  - Downloads: 240
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully is a publicly available, commercially usable dataset in Japanese and other languages designed to improve LLM safety, with restrictions against using it to *bypass* safety measures and a requirement to acknowledge its use in any derived datasets, while acknowledging it contains potentially harmful content.
  - Downloads: 231
- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
  - This dataset facilitates author identification modeling by providing cleaned paragraphs from 17 Japanese authors, focusing on stylistic features rather than specific works, with controlled data qualityâ€”avoiding label bias, duplicate content, and ensuring paragraph length between 100-400 characters for training and validation.
  - Downloads: 197
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This repository integrates data from erai-raws and MyAnimeList for 2056 anime, providing IDs and RSS feeds across platforms like AniDB, Kitsu, and LiveChart, with publication details for over 500 titles.
  - Downloads: 189
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - This dataset contains 2139 human-evaluated question-answer pairs and ratings from LLMChat, a system used to benchmark 13 language models â€“ including Tanuki and Llama-3 â€“ through pairwise comparisons between August 19-25, 2024.
  - Downloads: 172
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - This repository provides Japanese datasets transformed for easy SentenceTransformers training, particularly for contrastive learning, derived and filtered from multiple sources using rerank scores to create (anchor, positive/negative) triplets.
  - Downloads: 168
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - This repository provides Japanese text data extracted from PDF documents sourced from CommonCrawl.
  - Downloads: 123
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - This repository provides a text dataset extracted from the January 1, 2024 Japanese Wikipedia HTML dump, offering paragraph-structured text without unnecessary markup, and including data & scripts for various NLP tasks.
  - Downloads: 79
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)
  - This dataset provides high-quality, human-verified synthetic data for fine-tuning reasoning models, featuring question-answer pairs with explicit &lt;think&gt;...&lt;/think&gt; tagged thought processes for improved inference.
  - Downloads: 72
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - This repository provides a multilingual dataset released under the MIT license.
  - Downloads: 71
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 69
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - This repository provides a dataset of approximately 150,000 paired Danbooru and Japanese tags, filtered using fastText and the Calm3 LLM to improve accuracy and ensure at least one associated Japanese translation exists.
  - Downloads: 62
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - This dataset provides approximately 2800 high-quality (beauty score 87+) images of a generated virtual girlfriend (versions 2.1 & 2.6) designed to mitigate portrait rights issues, with over 1000 images scoring 90+ for beautyâ€”a notably large collection for training AI models.
  - Downloads: 62
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - This dataset annotates the Aratako/Magpie-Tanuki-8B-97k datasetâ€”created by applying the Magpie method to Tanuki-8Bâ€”with difficulty, quality, and category labels using cyberagent/calm3-22b-chat to assess instruction complexity.
  - Downloads: 57
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - Alpaca-jp-python is a Japanese synthetic dataset, curated by HachiML under the Apache 2.0 license, generated using the Stanford Alpaca method and Mistral AI's Mixtral-8x22B-Instruct-v0.1, and accessible via the `datasets` library.
  - Downloads: 51
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - This repository provides a validated dataset of furigana (reading aids) derived from the National Diet Library's bibliographic data, with 5064 inconsistencies corrected.
  - Downloads: 45
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k is a 97,269-record Japanese dialogue dataset created by applying the Magpie method to weblab-GENIAC/Tanuki-8B-dpo-v1.0, potentially containing low-quality entries due to a lack of post-filtering.
  - Downloads: 43
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - This repository provides a CBZ-format manga dataset from Nhentai, with metadata, for research in areas like image and text analysis, specifically focusing on adult content (NSFW) and requiring legal compliance.
  - Downloads: 43
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - This dataset provides furigana annotations derived from Aozora Bunko and SAPIE braille data, with validation correcting 307 mismatches found in the original text.
  - Downloads: 36
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja is a 525k instruction-tuning dataset created by automatically translating the open-source, high-quality ApolloCorpus multilingual medical dataset into Japanese, with a note of potential translation errors for cautious use in medical LLM applications.
  - Downloads: 35
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This dataset, â€œTrait Circus,â€ contains automatically extracted and aggregated fungal trait dataâ€”currently for casual use onlyâ€”derived from natural language processing of fungal descriptions, with potential inaccuracies due to the automated extraction process.
  - Downloads: 34
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - This repository provides a modified parsing and chunking method for Wikipedia data, built upon and pre-processed with a fork of singletongue/wikipedia-utils, using data crawled between December 5-8, 2023.
  - Downloads: 33
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - This repository likely provides resources or a project related to the popular Japanese character "Chiikawa" (ã¡ã„ã‹ã‚), stemming from the original "hachiwari/ã¯ã¡ã‚ã‚Œ" source.
  - Downloads: 30
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - ã‚‹ã‚Šã®ã‚¹ãƒ†ãƒƒã‚«ãƒ¼ is a fun collection of stickers featuring the character Ruri.
  - Downloads: 30
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - This repository provides a Japanese question-answer dataset of approximately 1,300 pairs manually created for Databricks, sourced from official blogs, FAQs, and Qitta articles by Databricks employees.
  - Downloads: 29
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - This repository provides a prepared dataset of 50,000 Delite posts by "t_w" optimized for embedding learning, available for research use under Japanese law prohibiting redistribution.
  - Downloads: 28
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - This repository provides the first 300k entries of a Japanese image-caption dataset, built upon alfredplpl/commoncatalog-cc-by-ja with images resized to within 512px.
  - Downloads: 28
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSECãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ likely provides resources and information related to the Japan System Engineering Consortium (JSEC).
  - Downloads: 28
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - This repository scrapes metadata (excluding full text) from the Dengeki Bunko novecomi website.
  - Downloads: 27
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Japanese LLaVA v1.5 Instruct 620K is a 620K Japanese translation of the original LLaVA v1.5 Visual Instruct dataset, intended for visual instruction tuning in Japanese language applications under a CC BY-NC-4.0 license.
  - Downloads: 26
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - This repository provides a 280-image dataset of female illustrations generated with NijiJourney v5, intended for LoRA model training and transparency, including some copyrighted characters, with accompanying tags and a warning against misuse.
  - Downloads: 25
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - This repository provides a 3-turn, multi-turn instruction dataset generated with Qwen/Qwen2.5-32B-Instruct-AWQ, containing mixed English and Chinese data based on Aratako/Magpie-Tanuki-8B-annotated-96k instructions, requiring potential filtering.
  - Downloads: 24
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - This repository provides a refined dataset of 50,000 Delite posts by data creator t_w, corrected and restructured for embedding learning, with usage limited to training and prohibiting redistribution under Japanese law.
  - Downloads: 23
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - This dataset contains freely reusable quiz questions sourced from Quiz Forest as of August 5, 2024, suitable for RAG, document search, and other applications, with a permissive license allowing commercial use and redistribution while respecting Quiz Forest and related parties.
  - Downloads: 22
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - This repository provides voice data for Chiaki Nanami from the *Danganronpa* series.
  - Downloads: 22
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - This repository provides automatically generated question-answer pairs created using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF from various openly licensed (CC-BY-SA-3.0, Apache-2.0) data sources, with modifications and random text excerpts to reduce similarity to the originals â€“ cleaning is recommended.
  - Downloads: 21
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data for unspecified purposes.
  - Downloads: 19
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended solely for research purposes and acknowledging support from IPA (Information-technology Promotion Agency).
  - Downloads: 18
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - This repository provides a cleaned dataset of 950 Japanese mathematical questions and solutions from MetaMathQA, translated using RekaAI/reka-flash-3, with formatting issues removedâ€”however, logical correctness of the solutions remains unchecked.
  - Downloads: 17
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthetic dataset created using the Evol-Instruction method and mistralai's Mixtral-8x22B-Instruct-v0.1, built from translated Stanford Alpaca seed tasks and licensed under Apache 2.0.
  - Downloads: 14
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - âš 
  - Downloads: 13
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, formatted as JSON for training and evaluating large language models, and licensed under Apache-2.0.
  - Downloads: 11
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - This repository provides a 26,728-dataset subset of annotated Japanese instruction-following dataâ€”filtered for high quality and easy-to-medium difficultyâ€”specifically for fine-tuning small Japanese chat LLMs (excluding coding) using the Qwen-2.5-turbo model, covering information seeking, reasoning, planning, and editing tasks.
  - Downloads: 11
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - This dataset contains freely reusable quiz questions sourced from Quiz Works as of August 4-5, 2024, suitable for RAG and document retrieval system development.
  - Downloads: 11
### Responsible NLP
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 906
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, offered for research purposes with acknowledgement to IPA (Information-technology Promotion Agency), and requires separate copyright permissions for non-research use.
  - Downloads: 776
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to Japan's IPA for resource support.
  - Downloads: 766
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset comprises manually extracted FAQs from Japanese government websites, licensed under CC-BY-4.0, for use in instruction tuning of large language models.
  - Downloads: 419
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository provides the Japanese Web Corpus 2010, licensed for research purposes only, with automatically added punctuation via morphological analysis, including conversion scripts and tools.
  - Downloads: 290
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - This dataset contains question-answer pairs generated by an LLM, based on paraphrased text from Japanese Wikipedia, and is released under the CC-BY-SA 4.0 license.
  - Downloads: 206
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - This repository provides text regenerated with Phi-3 based on randomly sampled data from sources like Wikibooks, Wikipedia, Cosmopedia, and legal case data (tens of GB in Parquet format), potentially requiring Git LFS for full access and utilizing supercomputing resources for processing.
  - Downloads: 130
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - This repository provides a deduplicated and preprocessed query-passage dataset (mqa) with cleaned text and NFKC normalization, where passage IDs correspond to indices within the `collection` file, adhering to the original dataset's license.
  - Downloads: 105
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This dataset provides filtered Japanese Wikipedia typo dataâ€”specifically kanji conversion errorsâ€”split into pre- and post-text segments for use with Hugging Face models.
  - Downloads: 96
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - This repository provides a large (tens of GB) corpus of Japanese text regenerated with Phi-3 and automatically translated to English, sourced from Wikibooks, Wikipedia, and code, potentially requiring Git LFS for full download due to its size.
  - Downloads: 76
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - This dataset contains Japanese translations of English Wikipedia text generated by DeepSeek-R1-Distill-Qwen-32B, including input/raw output and licensed under CC-BY-SA 4.0.
  - Downloads: 57
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - This dataset converts the oasst1-89k-ja Japanese dataset into a ShareGPT-formatted, multi-turn conversation format suitable for fine-tuning large language models, requiring significant computational resources.
  - Downloads: 35
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst2 dataset, automatically translated with DeepL and formatted for instruction/output-based fine-tuning of language models, with provided code for conversion.
  - Downloads: 34
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - This repository provides the Japanese prompts from the GuanacoDataset, identified and extracted using language detection.
  - Downloads: 32
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - This repository provides automatically generated multi-turn dialogue data created with Calm3-22B-chat, based on randomly extracted text from the Aozora Bunko library, specifically using a cleaned version of "I Am a Cat."
  - Downloads: 30
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This repository provides a Japanese-only subset of the Open Assistant dataset, sourced from the timdettmers/openassistant-guanaco base dataset.
  - Downloads: 29
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - This repository provides automatically generated Japanese question-answer pairs for retrieval-augmented generation (RAG) training, sourced from Wikibooks, Wikipedia, and legal case data, intended for pre-training rather than instruction-tuning, with some computations utilizing the TSUBAME4.0 supercomputer.
  - Downloads: 28
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - This repository provides data based on Japan Post's May 9, 2024, English/Chinese translations of item descriptions and HS codes for international mailing, as detailed on their website.
  - Downloads: 25
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset comprises 1243 carefully selected tweets (May 2022 - May 2024) notable for articulating complex ideas or possessing a unique worldview, intended for fine-tuning language modelsâ€”particularly for character development and tweet generation with fixed system prompts and inputs.
  - Downloads: 25
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - This repository provides released models and datasets under a license agreement prohibiting commercial use, disclaiming warranty, and requiring adherence to legal compliance by both users and any shared parties.
  - Downloads: 25
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - This dataset contains Japanese sentences generated by Phi-3 based on ConceptNet 5.7 triples, aiming to express the relationship between subject, relation, and object.
  - Downloads: 24
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - This repository provides a low-quality dataset of 1002 examples for enabling Python function calling in chat LLMs, generated with Qwen2.5 and Phi-4, and containing potential issues with missing/Chinese tools and repetitive/low-quality responses.
  - Downloads: 22
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty or liability, and ensure legal compliance in usage and distribution.
  - Downloads: 19
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - This repository provides a deduplicated and preprocessed version of the mmarco query-passage dataset, with IDs referencing indices within the `collection` subset for direct data access, adhering to the original dataset's license.
  - Downloads: 16
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - This dataset provides 20,000 automatically generated Japanese instruction-following examples, including optional Chain-of-Thought reasoning, created with the LLM-JP 3.13B Instruct model and formatted as JSON for LLM training and evaluation.
  - Downloads: 15
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset contains cleaned lines from the anime "My Next Life as a Villainess," featuring dialogue primarily from the character Lay and responses from Claire, with no usage responsibility assumed by the creator.
  - Downloads: 15
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This repository provides a 16,000-sample Japanese instruction-following dataset, automatically generated using Qwen2.5, containing instructions, reasoning steps, initial responses, and refined answers in JSONL format for LLM training and evaluation.
  - Downloads: 14
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 14
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - This repository provides publicly released models and datasets under a license agreement prohibiting enjoyment of expressed ideas/feelings and disclaiming any warranty or liability for their use, requiring legal compliance from both users and any shared recipients.
  - Downloads: 14
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty, and ensure legal compliance in their use and distribution.
  - Downloads: 13
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - This repository provides publicly released models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty, and ensure legal compliance in their use and distribution.
  - Downloads: 12
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset provides question-answer pairs about the Touhou Project's Tokama Club characters for use in training chatbots, question answering systems, and machine learning models.
  - Downloads: 11
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - This repository provides publicly available models and datasets with a usage license requiring adherence to legal compliance, prohibiting enjoyment of expressed ideas/emotions, and disclaiming any warranty or liability from the publisher.
  - Downloads: 11
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository provides a Japanese-only dataset extracted from Common Crawl using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 11
### Reasoning
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a synthetic, assured-correctness Japanese mathematical datasetâ€”created by translating English PRM800K & GSM8K problemsâ€”designed for chain-of-thought reasoning with large language models.
  - Downloads: 183
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using the Qwen3-235B-A22B model, formatted as JSONL for training and reasoning tasks.
  - Downloads: 147
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset is a small, commercially-usable, high-quality Japanese dataset for commonsense reasoning and mathematical problem solving, built upon existing datasets and licensed under DbCL v1.0.
  - Downloads: 123
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - JSNLI is a Japanese natural language inference (NLI) dataset, a translation of the SNLI benchmark, formatted as TSV with morphologically analyzed text using JUMAN++.
  - Downloads: 123
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - This dataset, abc-multiple-choice, is a Japanese multiple-choice question answering dataset based on questions from the â€œabcâ€ quiz competition, intended for research purposes only and detailed in the NLP2024 workshop paper.
  - Downloads: 122
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a human-crafted Japanese dataset of multi-turn conversations and passages for training and evaluating logical reasoning capabilities, demonstrated with Qwen2.5-7B models on Japanese MT-Bench.
  - Downloads: 116
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - JSQuAD is a Japanese reading comprehension dataset, based on SQuAD 1.1 and utilizing the 20211101 Japanese Wikipedia dump, released under a Creative Commons Attribution Share Alike 4.0 license.
  - Downloads: 102
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - This repository provides a commercially-usable, 1.8 million instruction-tuning datasetâ€”a Japanese translation of OpenMathInstruct-1â€”focused on mathematics, generated using Mixtral-8x7B and based on GSM8K & MATH benchmarks, and licensed under NVIDIAâ€™s commercially-permissive terms.
  - Downloads: 92
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - This dataset contains 17k synthetic Japanese math instruction-following records generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, filtered for consistency between responses from two distinct system promptsâ€”one for logical assistance and another for Python-based problem-solving.
  - Downloads: 76
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - JCommonsenseQA is a Japanese multiple-choice question answering dataset, based on CommonsenseQA and ConceptNet, designed to evaluate commonsense reasoning abilities, and is licensed under CC BY-SA 4.0.
  - Downloads: 59
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - æ—¥æœ¬èªžæŒ‡ç¤ºãƒ»æŽ¨è«–ãƒ»å›žç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªžç‰ˆã®æŒ‡ç¤ºãƒ»æŽ¨è«–ãƒ»å›žç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 59
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - This repository offers a 50k-subset of the NuminaMath CoT dataset, enhanced for Japanese language reasoning by prompting LLMs to iteratively refine their multistep solutions.
  - Downloads: 52
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100kã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 51
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - This repository provides a 100k Japanese question answering and reasoning dataset, evaluated using Qwen/Qwen3-14B, and acknowledges support from the TPU Research Cloud program.
  - Downloads: 44
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - This repository provides a 100k question-answering dataset in Japanese, generated using Gemini Pro 2.5, licensed under Open Data Commons Attribution.
  - Downloads: 42
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - This dataset provides Japanese question-answer and keyword-text pairs generated with DeepSeek-R1 from the fineweb2-edu-japanese dataset, including reasoning traces, and is licensed under ODC-By.
  - Downloads: 40
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - OpenO1-SFT (Japanese Translation) is a 77,312-sample dataset of translated Chain of Thought reasoning examples, derived from the original OpenO1-SFT and intended for fine-tuning language models.
  - Downloads: 37
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - This repository provides a high-quality, 1800-example Japanese instruction-following, reasoning, and answer dataset generated using the Qwen/Qwen2.5-32B-Instruct model, based on SkunkworksAI/reasoning-0.01.
  - Downloads: 37
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE (JNLI) is a Japanese Natural Language Inference dataset for evaluating relationships â€“ entailment, contradiction, or neutral â€“ between premise and hypothesis sentences.
  - Downloads: 36
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suiteâ€”based on and extending the FraCaS datasetâ€”for evaluating recognizing textual entailment through premise-hypothesis pairs labeled with entailment, neutral, or contradiction judgements.
  - Downloads: 34
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - This repository provides a synthetic instruction dataset generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, filtered to include only responses matching those from two distinct system prompts focused on logical/mathematical assistance and Python-based code execution.
  - Downloads: 32
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - This repository provides a 125,000-sample Japanese instruction-following dataset automatically generated using Qwen2.5-32B-instruct, featuring diverse, multi-persona instructions and Chain-of-Thought reasoning, in JSONL format under the Apache-2.0 license.
  - Downloads: 32
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository provides a Japanese multi-turn conversation dataset, synthesized from cosmopedia, focusing on reasoning, knowledge, and interactive dialogue with high information density.
  - Downloads: 31
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - JCQ is a Japanese NLP dataset of 700 questions (100 per task) designed to evaluate creativity, inspired by the Torrance Test of Creative Thinking and published in an NLP2025 research paper, covering tasks like unusual uses, consequences, and hypothetical situations.
  - Downloads: 29
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - This dataset filters the magpie-reasoning-llama-nemotron-70b-100k dataset to exclude examples lacking refinement labels and converts the results into OpenAI message format.
  - Downloads: 28
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench is a Japanese reasoning benchmark utilizing challenging 2023 Kyoto University math entrance exam questions to evaluate the problem-solving skills of Large Language Models (LLMs).
  - Downloads: 26
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - This repository provides a 100k-sample Japanese translation of the NuminaMath CoT dataset, featuring 860k math problems with Chain of Thought solutions, translated using the Gemma-2-27b-it language model.
  - Downloads: 21
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa is a 210-question, multi-turn Japanese language benchmark evaluating reasoning across seven diverse domainsâ€”math, writing, coding, understanding, grammar, culture, and general logicâ€”with 30 questions per category.
  - Downloads: 18
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - This dataset comprises 200 simplified instruction-following examples derived from the Kendamarron/jimba-instuction-1k-beta dataset, created to replicate the "Wizard LM" In-depth evolving process as a result of collaboration during the LOCAL AI HACKATHON #000.
  - Downloads: 12
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja ã®question_jaã‚’ã‚‚ã¨ã«phi-3-mediumã«ã‚ˆã‚Šãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžã‚’ç”¨ã„ãªã„å½¢å¼ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
### Responsible & Trustworthy NLP
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - This repository releases a 42k Japanese-English parallel dataset, part of Swallow-Magpie-Ultra-v0.1, for instruction tuning Llama-3.1-Swallow models.
  - Downloads: 113
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec is a machine learning framework and dataset for gender detection from Japanese names, detailed in a research paper accepted at ISDA'23, intended solely for research use.
  - Downloads: 104
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - JaNLI is a Japanese adversarial Natural Language Inference (NLI) dataset, modeled after HANS, designed to test model understanding of Japanese linguistics and identify vulnerabilities.
  - Downloads: 100
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA is a 1402-question Japanese red teaming dataset designed to evaluate LLM vulnerability to generating harmful responses through adversarial questioning, containing potentially offensive content.
  - Downloads: 53
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset provides a Japanese language dataset for evaluating and mitigating toxicity in large language models, linked to further details at the provided URL.
  - Downloads: 42
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - This repository provides a dataset of Japanese medical national exam questions (NMLE, 110th-117th exams) for model evaluation, RAG, and overviewing exam content, licensed under CC-BY-NC-ND 4.0 (non-commercial use only).
  - Downloads: 29
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository provides a dataset detailing game states with unit informationâ€”including IDs, classes, locations, and team affiliationsâ€”across various timestamps for strategic game analysis.
  - Downloads: 17
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - This repository contains pairwise comparison data evaluating responses from two LLMChat models using various other models, created to verify the consistency between human and automated (open LLM) evaluations, and licensed under the team-hatakeyama-phase2/LLMChat termsâ€”note potential restrictions on using outputs for model training.
  - Downloads: 16
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - This dataset provides a 10,341-hour sample of unsupervised Japanese speech across 28 diverse domains, designed for improving AI model performance while upholding strict data privacy and legal standards.
  - Downloads: 13
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - This repository provides access to code and instructions for a dataset designed to prevent unintentional inclusion in large language model training.
  - Downloads: 11
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - Cauldron-JA is a Japanese translation of the Idefics2 fine-tuning dataset, The Cauldronâ€”a collection of 50 vision-language datasetsâ€”excluding OCR, coding, and graph-related data due to translation limitations.
  - Downloads: 48,999
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This repository provides a Japanese translation of the databricks-dolly-15k dataset, licensed under CC-BY-SA-3.0.
  - Downloads: 640
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst1 dataset, flagging unsuccessful translations ("ng_translation": 1) where original and translated texts match, with recent manual corrections to code-related translation errors.
  - Downloads: 109
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository provides a collection of ~40 high-quality, natively Japanese datasets for downstream tasks and LLM instruction finetuning, categorized by task type.
  - Downloads: 81
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN offers deduplicated Japanese translations of radiology reports paired with chest CT volumes from the CT-RATE dataset, supporting development of Japanese medical AI models.
  - Downloads: 57
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset provides 69,000 Japanese translations of the databricks-dolly-15k dataset, licensed under CC BY SA 3.0, for ja-en translation tasks.
  - Downloads: 42
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - This repository provides a Japanese-Korean paired text dataset from Helsinki-NLP/Tatoeba-Challenge for training translation models, excluding commercial use.
  - Downloads: 33
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - Hoshikuzu/Japanese-Law-Translation is a 260k sentence parallel corpus of Japanese laws sourced from japaneselawtranslation.go.jp, designed for machine translation tasks.
  - Downloads: 28
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka is a dataset of Japanese â€œkaidanâ€ (ghost stories) linked to the Hyakumonogatari ritual, offering a resource for exploring Japanese folklore and supernatural tales.
  - Downloads: 20
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated Japanese dataset derived from ultrachat_200k, containing 6,537 training and 995 test samples, with IDs corresponding to the original dataset.
  - Downloads: 13
### Information Retrieval
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - This dataset provides reranking scores from five multilingual and Japanese models (BGE, GTE, RURI, RURIv3, ja-ce) applied to existing Japanese search and QA datasets containing positive and negative example documents for each query.
  - Downloads: 509
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - This repository provides AutoWikiQA, the largest freely available Japanese question-answering dataset generated using Swallow-MX from Wikipedia text, featuring diverse question-answer pairs suitable for QA model training and RAG development.
  - Downloads: 205
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - This dataset provides QA pairs for training document retrieval models, specifically used in the book â€œIntroduction to Large Language Models,â€ sourced from cl-tohoku/quiz-datasets with a mix of licensing including CC BY-SA and GFDL for quizzes and passages.
  - Downloads: 69
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
  - JCrRAG is a 20,000-record Japanese benchmark evaluating Retrieval-Augmented Generation systems' ability to accurately process fixed, provided context for question answering across diverse categories and complexity levels.
  - Downloads: 45
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - This dataset provides Japanese question-answering pairs with corresponding Wikipedia article retrievals performed by human workers, detailing data collection and structure for research purposes.
  - Downloads: 43
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset provides automatically translated Japanese versions of records 20k-100k from the cosmopedia-100k dataset, excluding entries with translation errors, and will be merged with and eventually removed from the 0-20k translation results.
  - Downloads: 40
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This repository provides the passage dataset used in the â€œAI Kingâ€ competition featured in the book *Introduction to Large Language Models*, sourced from cl-tohoku/quiz-datasets and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 36
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - This repository provides 1 million synthetic, long-context, multi-turn chat entriesâ€”derived from rewritten web text using large language modelsâ€”for continued pre-training and research on data/internet culture.
  - Downloads: 29
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - Jawiki-20220404-c400 is a Japanese Wikipedia passage dataset (â‰¤400 characters) used for question answering baselines, including those for the AIçŽ‹ competition.
  - Downloads: 19
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - This dataset provides JSONL-formatted metadata for YouTube channels, labeled as either VTuber (1) or non-VTuber (0), to facilitate text classification model training and evaluation, primarily in Japanese with potential multilingual content, and licensed under MIT.
  - Downloads: 14
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - This dataset archives 11 years of historical comments from the now-discontinued "Niconico Realtime Commentary" service, preserving a valuable record of user engagement before its 2020 transition and API shutdown.
  - Downloads: 889,918
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100 is a 100-example Japanese dataset for evaluating instruction-tuned models, featuring complex tasks requiring helpful, nuanced responses and annotated for reliable evaluation across summarization, reasoning, and creative generation.
  - Downloads: 2,782
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - This dataset provides a binary positive/negative sentiment classification version of the WRIME Japanese sentiment analysis dataset, intended for use with the â€œLarge Language Model Introductionâ€ bookâ€™s sample code.
  - Downloads: 501
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
  - This repository generates and filters Japanese sentences using the phi4 model, covering 53 grammatical patterns with a 2364-word vocabulary, focusing on common constructions like polite forms, requests, and opinions.
  - Downloads: 240
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - This repository provides the Sayoko TTS corpus, an 81-year-old female Japanese speech dataset with both noisy (wav_noise) and noise-reduced (wav) audio files, along with phoneme labels, downloadable via Google Drive or Hugging Face Hub.
  - Downloads: 123
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - This dataset adapts the databricks-dolly-15k instruction-following dataset to mimic the emotionless, indifferent speech patterns of the character Yuki Nagato from "The Melancholy of Haruhi Suzumiya" by modifying polite Japanese sentence endings.
  - Downloads: 26
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - This repository hosts a Japanese instruction dataset created by manually checking and correcting the output of the calm2-7b-chat model, detailed in the linked Zenn article.
  - Downloads: 24
### Linguistics & Cognitive NLP
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - NewsQ is a free, Japanese question-answering benchmark for current events, distributed via Hugging Face with access granted upon agreement to terms and provision of user information for verification and communication.
  - Downloads: 44
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - This repository provides FuguMT-translated and manually corrected English text from the â€œchosenâ€ dataset within the helpful-base portion of the anthropics/hh-rlhf project, improving translation quality.
  - Downloads: 28
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - NewsQ is a free, Japanese question-answering benchmark for current events, distributed via Hugging Face under a terms of use agreement requiring applicant information and consent for data handling.
  - Downloads: 21
