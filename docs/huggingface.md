# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
[![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
[![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

A curated list of resources dedicated to Python libraries, llms, dictionaries, and corpora of NLP for Japanese
This page lists Japanese NLP-specific models and datasets available on Hugging Face. Currently, it includes 159 models and 86 datasets.

_Updated on Jan 27, 2026_

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Ranking](#Ranking)
   * [Models](#models-ranking)
   * [Datasets](#datasets-ranking)
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [sentence-similarity](#sentence-similarity)
   * [feature-extraction](#feature-extraction)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [translation](#translation)
   * [token-classification](#token-classification)
   * [text-ranking](#text-ranking)
   * [text-classification](#text-classification)
   * [image-to-text](#image-to-text)
   * [image-text-to-text](#image-text-to-text)
   * [audio-to-audio](#audio-to-audio)
   * [text-to-speech](#text-to-speech)
   * [text-to-audio](#text-to-audio)
   * [others](#others)
 * [Datasets](#Datasets)

## Ranking

### Models-ranking

| # | Model | Downloads | Likes | Category |
|---|-------|-----------|-------|----------|
| 1 | [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) | ğŸ“¥ 4M | â­ 52 | automatic-speech-recognition |
| 2 | [llm-jp-3-3.7b-instruct](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct) | ğŸ“¥ 994k | â­ 12 | text-generation |
| 3 | [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) | ğŸ“¥ 500k | â­ 73 | fill-mask |
| 4 | [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) | ğŸ“¥ 319k | â­ 11 | sentence-similarity |
| 5 | [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) | ğŸ“¥ 318k | â­ 63 | sentence-similarity |
| 6 | [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | ğŸ“¥ 217k | â­ 163 | image-to-text |
| 7 | [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) | ğŸ“¥ 211k | â­ 20 | text-generation |
| 8 | [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) | ğŸ“¥ 187k | â­ 13 | feature-extraction |
| 9 | [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) | ğŸ“¥ 133k | â­ 10 | translation |
| 10 | [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) | ğŸ“¥ 126k | â­ 15 | text-generation |
| 11 | [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) | ğŸ“¥ 97k | â­ 10 | others |
| 12 | [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) | ğŸ“¥ 95k | â­ 8 | fill-mask |
| 13 | [ruri-small](https://huggingface.co/cl-nagoya/ruri-small) | ğŸ“¥ 78k | â­ 9 | sentence-similarity |
| 14 | [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) | ğŸ“¥ 68k | â­ 13 | others |
| 15 | [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) | ğŸ“¥ 62k | â­ 59 | others |
| 16 | [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) | ğŸ“¥ 54k | â­ 35 | text-generation |
| 17 | [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) | ğŸ“¥ 54k | â­ 3 | fill-mask |
| 18 | [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) | ğŸ“¥ 53k | â­ 142 | text-generation |
| 19 | [SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF](https://huggingface.co/hiratagoh/SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF) | ğŸ“¥ 50k | â­ 1 | text-generation |
| 20 | [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) | ğŸ“¥ 45k | â­ 51 | feature-extraction |

### Datasets-ranking

| # | Dataset | Downloads | Likes |
|---|---------|-----------|-------|
| 1 | [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) | ğŸ“¥ 1M | â­ 19 |
| 2 | [emb](https://huggingface.co/datasets/hpprc/emb) | ğŸ“¥ 7k | â­ 13 |
| 3 | [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) | ğŸ“¥ 4k | â­ 10 |
| 4 | [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) | ğŸ“¥ 3k | â­ 18 |
| 5 | [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) | ğŸ“¥ 3k | â­ 7 |
| 6 | [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) | ğŸ“¥ 3k | â­ 99 |
| 7 | [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) | ğŸ“¥ 3k | â­ 3 |
| 8 | [Japanese-Eroge-Voice-V2](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice-V2) | ğŸ“¥ 2k | â­ 13 |
| 9 | [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) | ğŸ“¥ 2k | â­ 31 |
| 10 | [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) | ğŸ“¥ 2k | â­ 96 |
| 11 | [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) | ğŸ“¥ 2k | â­ 8 |
| 12 | [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) | ğŸ“¥ 2k | â­ 46 |
| 13 | [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) | ğŸ“¥ 1k | â­ 142 |
| 14 | [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) | ğŸ“¥ 1k | â­ 128 |
| 15 | [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) | ğŸ“¥ 1k | â­ 8 |
| 16 | [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) | ğŸ“¥ 1k | â­ 107 |
| 17 | [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) | ğŸ“¥ 1k | â­ 4 |
| 18 | [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) | ğŸ“¥ 1k | â­ 24 |
| 19 | [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) | ğŸ“¥ 922 | â­ 31 |
| 20 | [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) | ğŸ“¥ 901 | â­ 2 |

## Models
### text-generation
 * [llm-jp-3-3.7b-instruct](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct) - ğŸ“¥ 994k / â­ 12 / Japanese and multilingual language models (1.8â€¯bn to 13â€¯bn parameters, base and instruct variants) released by NIIâ€™s Large Language Models R&D Center, packaged for Huggingâ€¯Face Transformers and pretrained on Japanese Wikipedia, Common Crawl, WARP/PDF/HTML, Kaken, English Wikipedia, and Dolma datasets.
 * [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - ğŸ“¥ 211k / â­ 20 / OpenCALM is a suite of decoderâ€‘only Japanese transformer language models (160â€¯Mâ€“6.8â€¯B params) released by CyberAgent, Inc. under CCâ€‘BYâ€‘SAâ€¯4.0, trained on Japanese Wikipedia and Common Crawl, and usable via Huggingâ€¯Faceâ€™s torchâ€‘transformers.
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - ğŸ“¥ 126k / â­ 15 / A 12â€‘layer, 768â€‘hidden Japanese GPTâ€‘NeoX model trained on CCâ€‘100, C4, and Wikipedia, compatible with Huggingface, with an optional toy prefixâ€‘tuning weight that forces each sentence to end with a smilingâ€‘face emoji.
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - ğŸ“¥ 54k / â­ 35 / TinySwallowâ€‘1.5B is a compact Japanese instructionâ€‘following language model by Sakana AI and the Swallow Team that uses TAID distillation from Qwen2.5â€‘32Bâ€‘Instruct, is further preâ€‘trained on Japanese text, and is released under ApacheÂ 2.0 for research use only.
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - ğŸ“¥ 53k / â­ 142 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced, 8â€‘billionâ€‘parameter Llamaâ€¯3 model by ELYZA, fineâ€‘tuned for Japanese on Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct.
 * [SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF](https://huggingface.co/hiratagoh/SIP-jmed-llm-3-8x13b-AC-32k-instruct-GGUF) - ğŸ“¥ 50k / â­ 1 / Quantized and GGUFâ€‘converted version of the nonâ€‘commercial, SIPâ€‘Phaseâ€¯3 Japanese medical LLM SIPâ€‘jmedâ€‘llmâ€‘3â€‘8x13bâ€‘ACâ€‘32kâ€‘instruct, featuring iMatrix generation from TFMCâ€™s dataset and licensed for academic research only under CCâ€¯BYâ€‘NCâ€‘SAâ€¯4.0 with attribution.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - ğŸ“¥ 19k / â­ 18 / Llamaâ€¯3.1â€¯Swallow is a set of 8â€‘B and 70â€‘B models that continue preâ€‘training Metaâ€™s Llamaâ€¯3.1 to boost Japanese language performance, then instructionâ€‘fineâ€‘tune on synthetic Japanese dataâ€”providing multiple released variants with improved conversational behavior comparable to gemmaâ€‘3â€‘27bâ€‘it.
 * [LFM2.5-1.2B-JP-GGUF](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP-GGUF) - ğŸ“¥ 15k / â­ 27 / LFM2.5â€‘1.2Bâ€‘JP is a 1.2â€¯Bâ€‘parameter Japanese text generation model built on the LFM2.5 hybrid architecture, optimized for generation and completion tasks, hosted on Hugging Face and runnable via llama.cpp.
 * [shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - ğŸ“¥ 14k / â­ 18 / Fineâ€‘tuned the Japanese Stableâ€¯LM Base Gammaâ€¯7B with Shisaâ€¯7B data, achieving strong results on the JAâ€¯MTâ€‘Bench.
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - ğŸ“¥ 12k / â­ 58 / A 2.7â€‘Bâ€‘parameter Japanese GPTâ€‘NeoX model trained on Japanese CCâ€‘100 and OSCAR by ABEJAâ€¯Inc, usable via Hugging Face Transformers pipelines or PyTorch and released under the MIT license.
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - ğŸ“¥ 10k / â­ 4 / Japaneseâ€‘optimized 8â€‘billionâ€‘parameter Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B, built on Metaâ€‘Llamaâ€‘3â€‘Instruct with extra preâ€‘training and instruction tuning, is offered in GGUF and AWQ quantized forms for vLLM or OpenAIâ€‘compatible inference.
 * [Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 9k / â­ 21 / Llama3â€¯Swallow is a Japaneseâ€‘enhanced Meta Llamaâ€¯3 family released Julyâ€¯1â€¯2024, offering 8B and 70B variants in Instruct and chat forms fineâ€‘tuned with SFT and Chatâ€¯Vector on Megatronâ€‘LM, and benchmarked on key Japanese NLP tasks.
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - ğŸ“¥ 9k / â­ 84 / Rinnaâ€™s 24â€‘layer, 1024â€‘hidden Japanese GPTâ€‘2â€‘medium model, trained on CCâ€‘100 and Wikipedia with SentencePiece tokenization, is available in the rinna/japaneseâ€‘pretrainedâ€‘models repo (MITâ€‘licensed, released Aprilâ€¯7â€¯2021, updated 25â€¯Augâ€¯2021).
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - ğŸ“¥ 8k / â­ 4 / Experimental Japanese model created by extracting differences between lightblue/suzumeâ€‘llamaâ€‘3â€‘8Bâ€‘japanese and Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct using a chatâ€‘vector approach, upsampled and applied to Metaâ€‘Llamaâ€‘3â€‘70Bâ€‘Instruct, showing little change and planning future scaling.
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - ğŸ“¥ 7k / â­ 25 / rinnaâ€™s Japanese GPTâ€‘2 small is a 12â€‘layer, 768â€‘hidden transformer trained on Japanese CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released under MIT onâ€¯Augâ€¯25â€¯2021 (Huggingâ€¯Face: rinna/japaneseâ€‘gpt2â€‘small, see https://arxiv.org/abs/2404.01657).
 * [LFM2.5-1.2B-JP](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP) - ğŸ“¥ 6k / â­ 133 / LFM2.5â€‘1.2Bâ€‘JP is a Japaneseâ€‘optimized chat model that outperforms LFM2 on Japanese knowledge and instructionâ€‘following, supports fineâ€‘tuning with LoRA, inference via Transformers, vLLM, and llama.cpp, and achieves 50.7 JMMLU, 58.1 Mâ€‘IFEval, and 56.0 GSM8K scores.
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ğŸ“¥ 6k / â­ 74 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter extension of Metaâ€™s Llamaâ€‘2 model, preâ€‘trained on Japanese data with instruct and fast variants, and usable through Huggingâ€¯Face Transformers.
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - ğŸ“¥ 6k / â­ 14 / Provides the 1.8â€¯Bâ€‘parameter llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4 Japanese instructionâ€‘tuned model from NII, compatible with HuggingÂ Face Transformers and Torchâ€¯â‰¥â€¯2.3.0, including pretrained and fineâ€‘tuned checkpoints and usage examples.
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - ğŸ“¥ 5k / â­ 205 / OpenCALM is a Japanese decoderâ€‘only transformer languageâ€‘model suite from CyberAgent, Inc. featuring versions ranging from 160â€¯M to 6.8â€¯B parameters preâ€‘trained on Wikipedia and Common Crawl, available via the Transformers library under a CCâ€¯BYâ€‘SAâ€¯4.0 license.
 * [Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408) - ğŸ“¥ 5k / â­ 48 / A Japanese continuallyâ€‘preâ€‘trained Mistralâ€‘Nemo model (Mistralâ€‘Nemoâ€‘Japaneseâ€‘Instructâ€‘2408) built on mistralai/Mistralâ€‘Nemoâ€‘Instructâ€‘2407, usable via transformers with device mapping and ChatML prompts, released under Apacheâ€‘2.0 by Ryosuke Ishigami.
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - ğŸ“¥ 4k / â­ 44 / TokyoTechâ€‘LLM offers the Swallowâ€¯Llamaâ€¯2 familyâ€”Japaneseâ€‘enhanced, superâ€‘visedâ€‘fineâ€‘tuned and noâ€‘vocabularyâ€‘expansion variants for 7â€¯B, 13â€¯B andâ€¯70â€¯B models, with recent releases includingâ€¯Swallowâ€‘7bâ€‘instructâ€‘v0.1 andâ€¯Swallowâ€‘70bâ€‘NVEâ€‘hf.
 * [Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4) - ğŸ“¥ 4k / â­ 4 / Llamaâ€¯3.3â€¯Swallow is a 70â€‘billionâ€‘parameter model that augments Metaâ€™s Llamaâ€¯3.3 for stronger Japanese capabilities while keeping English performance, built through continual preâ€‘training and instructionâ€‘fineâ€‘tuning, with multiple Instruct variants released since Decemberâ€¯2024 and available on HuggingFace.
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - ğŸ“¥ 3k / â­ 23 / Llamaâ€¯3.1â€¯Swallow is a Japaneseâ€‘enhanced series of 8B/70B Llamaâ€¯3.1 models, trained via continual preâ€‘training and Japaneseâ€‘specific instruction fineâ€‘tuning, with the latest 8Bâ€‘Instructâ€‘v0.3 setting stateâ€‘ofâ€‘theâ€‘art results on Japanese MTâ€‘Bench.
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - ğŸ“¥ 3k / â­ 17 / LLMâ€‘jpâ€‘3.1â€‘13bâ€‘instruct4 is a 13â€‘B, instructionâ€‘preâ€‘trained Japanese language model developed by NIIâ€™s R&D Center, released as a Huggingâ€‘Face Transformers checkpoint with a UNIGRAMâ€‘byteâ€‘fallback tokenizer.
 * [DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese) - ğŸ“¥ 2k / â­ 95 / A Japaneseâ€‘finetuned version of DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14B, featuring usage examples, a specified prompt format, MIT licensing, and authored by Ryosuke Ishigami.
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - ğŸ“¥ 2k / â­ 21 / youriâ€‘7b is a 32â€‘layer, 4096â€‘hidden transformer built from Llama2â€‘7b, continually preâ€‘trained on ~40â€¯B Japanese tokens (CCâ€‘100, C4, OSCAR, Pile, Wikipedia) and released Octâ€¯31â€¯2023, achieving competitive scores on AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA and Winogrande.
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - ğŸ“¥ 2k / â­ 34 / Offers an autoregressive Japanese language model (sarashina2.2â€‘3Bâ€‘instructâ€‘v0.1) from SBâ€¯Intuitions, benchmarked against other models, with example usage scripts and a note that safety training is limited.
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - ğŸ“¥ 2k / â­ 57 / TinySwallowâ€‘1.5Bâ€‘Instruct is a 1.5â€¯B Japanese instructionâ€‘tuned autoregressive language model distilled with TAID from Qwen2.5â€‘32Bâ€‘Instruct, intended for research use only.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ğŸ“¥ 2k / â­ 81 / Japaneseâ€‘enhanced Llamaâ€‘2â€‘7B from ELYZA, preâ€‘trained for extended Japaneseâ€‘language capability with standard, instruct, and fast variants, detailed usage examples, developer credits, and licensed under Metaâ€™s Llamaâ€‘2 Community License.
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - ğŸ“¥ 2k / â­ 11 / llmâ€‘jpâ€‘3.1â€‘1.8b is a 1.8â€¯bâ€‘parameter Japanese LLM from NIIâ€™s Large Language Models R&D Center, distributed as a Huggingâ€¯Face checkpoint (torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, flashâ€‘attnâ€¯â‰¥â€¯2.5) with full model specs, tokenizer, and preâ€‘training details in the repo.
 * [llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b) - ğŸ“¥ 2k / â­ 10 / Huggingâ€¯Faceâ€‘compatible Japanese transformer LLMs (1.8b,â€¯3.7b,â€¯13b and their instruct/beta variants) built with torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40.1, accelerate, flashâ€‘attn, and pretrained on mixed Japaneseâ€‘English corpora such as Wikipedia, Commonâ€¯Crawl, and Dolma.
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - ğŸ“¥ 2k / â­ 15 / A collection of Japanese largeâ€‘language models (1.8â€¯b to 172â€¯bâ€¯beta1, with instruct variants) from NIIâ€™s R&D Center, packaged in Huggingâ€¯Face Transformers format and pretrained on a mix of Japanese, English, and Web corpora totalling >1â€¯trillion tokens, requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [gemma-2-2b-jpn-it-GGUF](https://huggingface.co/bartowski/gemma-2-2b-jpn-it-GGUF) - ğŸ“¥ 2k / â­ 2 / Llama.cppâ€‘based imatrixâ€‘quantized weights for the gemmaâ€‘2â€‘2bâ€‘jpnâ€‘it model (f16, Q8_0, Q6_K, Q5_K, Q4_K, Q3_K, Q4_0 variants) compiled with releaseâ€¯b3972, optimized for lowâ€‘memory ARM inference (SVE/i8mm) and usable in LMâ€¯Studio with a specific prompt format.
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - ğŸ“¥ 2k / â­ 10 / Repository provides GGUFâ€‘formatted, quantised model files for Stability AIâ€™s Japanese StableLMâ€¯Instructâ€¯Gammaâ€¯7B, created with Massedâ€¯Compute hardware and part of TheBlokeâ€™s a16zâ€‘funded LLM work.
 * [llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct) - ğŸ“¥ 2k / â­ 31 / Japanese largeâ€‘language models (llmâ€‘jpâ€‘3â€‘13bâ€‘instruct and its variants) from NII, built on Huggingâ€¯Face Transformers with 2.1â€¯T parameters, a unigram byteâ€‘fallback tokenizer, and trained on mixed Japanese/English corpora, partially funded by GENIAC.
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ğŸ“¥ 2k / â­ 96 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a Japaneseâ€‘optimized extension of Metaâ€™s Llamaâ€‘2â€¯7â€¯B, offering instruct and fast variants with 6.27â€“6.37â€¯B parameters that can be accessed via the Huggingâ€‘Face Transformers library.
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - ğŸ“¥ 1k / â­ 26 / Japaneseâ€‘StableLMâ€‘Instructâ€‘Betaâ€‘70B is a 70â€‘billionâ€‘parameter Japanese decoderâ€‘only Llama2â€‘based language model fineâ€‘tuned on Dollyâ€‘15k, Anthropic HH, and other public data, available as a 7â€‘billionâ€‘parameter variant and released under the Llama2 Community License.
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - ğŸ“¥ 1k / â­ 15 / LLMâ€‘jp offers 13â€¯B and 1.3â€¯B transformer language models, including multiple instructionâ€‘tuned variants, built with Megatronâ€‘DeepSpeed and the Huggingâ€¯Face Transformers ecosystem.
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - ğŸ“¥ 1k / â­ 20 / OpenCALM is a family of Japanese decoderâ€‘only Transformer language models (160â€¯Mâ€“6.8â€¯B parameters) from CyberAgent, trained on Japanese Wikipedia and Common Crawl and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - ğŸ“¥ 1k / â­ 41 / Largeâ€‘language models from LLMâ€‘jp â€“ 13B and 1.3B Japaneseâ€‘English transformers with multiple instruction and LoRA variants, preâ€‘trained via Megatronâ€‘DeepSpeed and released in Huggingâ€¯Face format (torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34).
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - ğŸ“¥ 1k / â­ 17 / Japaneseâ€‘StableLMâ€‘Baseâ€‘Betaâ€‘70B is a 70â€‘Bâ€‘parameter Llamaâ€‘2â€‘derived decoderâ€‘only language model fineâ€‘tuned on diverse Japanese data, offering smaller 7â€¯B versions, an instructionâ€‘following variant, and a faster inference release, all licensed under the Llama2 Community License.
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - ğŸ“¥ 1k / â­ 13 / This repository hosts SBâ€¯Intuitionsâ€™ 1â€¯Bâ€‘parameter autoregressive Japanese instruction model sarashina2.2â€‘1bâ€‘instructâ€‘v0.1, benchmarked against other Japaneseâ€‘BERTs on Japanese and English MT and instruction tasks, with a torchâ€‘transformer usage snippet and a warning of limited safety training.
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - ğŸ“¥ 1k / â­ 53 / Japanese Stableâ€¯LMâ€¯Instructâ€¯Gammaâ€¯7B is a 7â€‘Bâ€‘parameter decoderâ€‘only Japanese language model fineâ€‘tuned on instruction datasets, built on the Base Gammaâ€¯7B, requires Transformersâ€¯4.34+, is Apacheâ€¯2.0â€‘licensed, and is developed by Stabilityâ€¯AI.
 * [Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 1k / â­ 17 / Llamaâ€¯3.1â€¯Swallow is a set of 8B and 70B Japaneseâ€‘enhanced language models derived from Metaâ€™s Llamaâ€¯3.1 through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning, released in Octâ€¯2024 and hosted on swallowâ€‘llm.github.io.
 * [Wanabi-Novelist-24B-GGUF](https://huggingface.co/kawaimasa/Wanabi-Novelist-24B-GGUF) - ğŸ“¥ 1k / â­ 2 / Repository contains a GGUFâ€‘quantized Wanabiâ€‘Novelistâ€‘24B Japanese novelâ€‘writing LLM optimized for Projectâ€¯Wannabe/koboldcpp, offers experimental imatrix quantization, but lacks general instruction capability and requires the latest Projectâ€¯Wannabe update.
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 8 / Hosts LLMâ€‘jpâ€™s 13â€¯B and 1.3â€¯B Japaneseâ€‘English instruction and pretrained models, offered in several variant checkpoints for Huggingâ€¯Face Transformers with torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, accelerateâ€¯0.23 and DeepSpeed support.
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - ğŸ“¥ 1k / â­ 25 / A 7â€‘Bâ€‘parameter, autoregressive, decoderâ€‘only Japanese model based on Mistralâ€‘7Bâ€‘v0.1, released by Stabilityâ€¯AI under Apacheâ€¯2.0 for highâ€‘performance Japanese language and downstream tasks and requiring Transformersâ€¯4.34.0+.
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - ğŸ“¥ 1k / â­ 15 / Repositories of 13â€‘B and 1.3â€‘Bâ€‘parameter LLMâ€‘jp instructionâ€‘fineâ€‘tuned modelsâ€”including LoRA variantsâ€”packaged in Huggingâ€¯Face Transformers format and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, and accelerateâ€¯0.23, trained on ~50â€¯k mixed Japanese/English/code examples with Megatronâ€‘DeepSpeed and PEFT.
 * [karasu-1.1B](https://huggingface.co/lightblue/karasu-1.1B) - ğŸ“¥ 1k / â­ 7 / Pretrained TinyLlama in Japanese (â‰ˆ50â€¯k steps) built on ~3â€¯B OSCAR/mC4 tokens, usable via HuggingFace Transformers or VLLM, created by Peterâ€¯Devine, Shoâ€¯Higuchi, Yuukiâ€¯Yamanaka, Atomâ€¯Sonoda, Shunichiâ€¯Taniguchi, Tomiokaâ€¯Wataru, and Renjuâ€¯Aoki.
 * [llm-jp-13b-instruct-full-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 4 / LLMâ€‘jp supplies instructionâ€‘style and pretrained 13B/1.3B Transformer models in Hugging Face and DeepSpeed formats, trained on 50â€¯k+ mixed Japanese/English/sourceâ€‘code data and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34 and accelerateâ€¯0.23.
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ğŸ“¥ 1k / â­ 23 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter Japanese extension of Metaâ€™s Llamaâ€‘2â€‘7B, further preâ€‘trained for Japanese language tasks and offered in base, instruct, fast, and fastâ€‘instruct variants, maintained by the ELYZA team under the Llamaâ€¯2 Community License.
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - ğŸ“¥ 1k / â­ 37 / Offers the Swallowâ€¯Llamaâ€‘2 family of Japaneseâ€‘English LLMsâ€”from 7B,â€¯13B,â€¯andâ€¯70B models with instruct, NVE, and preview variantsâ€”tuned via supervised fineâ€‘tuning, available through Megatronâ€‘LM with a tokenizer, and benchmarked on core Japanese tasks.
 * [RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct) - ğŸ“¥ 1k / â­ 49 / RakutenAIâ€‘7Bâ€‘instruct, built on Mistralâ€‘7Bâ€‘v0.1, is a Japaneseâ€‘focused LLM that tops Japanese benchmark scores while matching English performance, and a chatâ€‘tuned version is also released.
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - ğŸ“¥ 1k / â­ 13 / An autoGPTQâ€‘quantized 10â€‘Bâ€‘parameter Japaneseâ€‘centric multilingual GPTâ€‘NeoX model (weblabâ€‘10bâ€‘instructionâ€‘sftâ€‘GPTQ) that shrinks the 21.42â€¯GB original to a faster, GPUâ€‘required version, with a 6.03â€¯GB gguf alternative for CPU via llama.cpp, and can be run locally with textâ€‘generationâ€‘webui (~16â€¯tokens/s on an RTXâ€¯3060) or interactively in Colab.
 * [Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - ğŸ“¥ 1k / â­ 17 / The TokyoTechâ€‘LLM repository provides the Swallowâ€¯Llamaâ€‘2 family of LLaMAâ€‘2 models, augmented with Japanese data, covering 7B, 13B, and 70B variants that include instructionâ€‘tuned, NVEâ€‘tuned, and a 7B Plus version released since Decemberâ€¯2023.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - ğŸ“¥ 1k / â­ 3 / Provides a 4â€‘bit, 4.11â€¯GB quantized version of Metaâ€™s Llamaâ€‘2â€¯7B (ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7bâ€‘fastâ€‘instruct) that cuts memory but degrades instructionâ€‘following, requires a GPU and autoGPTQ, and includes references to alternate AWQ, llama.cpp, and gguf quantizations and benchmark results.
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - ğŸ“¥ 1k / â­ 11 / OpenCALM is a decoderâ€‘only Japanese transformer family from CyberAgent (160â€¯Mâ€¯â€“â€¯6.8â€¯B parameters, from openâ€‘calmâ€‘small to openâ€‘calmâ€‘7b), trained on Japanese Wikipedia and Commonâ€‘Crawl, licensed CCâ€¯BYâ€‘SAâ€¯4.0 and usable through Hugging Face transformers.
 * [shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - ğŸ“¥ 1k / â­ 30 / Shisaâ€¯7B is a Japaneseâ€‘focused language model built on Mistralâ€¯7B, trained with curated airoboros, ultrafeedback, and synthetic ENâ€‘JA data, and includes code for preprocessing, translation, fineâ€‘tuning, and evaluation alongside future research documentation.
 * [shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - ğŸ“¥ 1k / â­ 16 / shisaâ€‘baseâ€‘7bâ€‘v1 augments Mistralâ€¯7B with 8â€¯B Japanese tokens from MADLADâ€‘400, trainedâ€¯inâ€¯2,400â€¯A100â€‘40 GPUâ€‘hours, and achieves classâ€‘leading Japanese benchmark performance, outperforming comparable 7â€‘B Japaneseâ€‘tuned models such as Japanese Stableâ€¯LM, ELYZA, and Youri.

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - ğŸ“¥ 500k / â­ 73 / Japanese BERTâ€‘base pretrained on 2019 Japanese Wikipedia with IPAâ€‘dictionary and wholeâ€‘word masking, 12â€‘layer 768â€‘dim, 32,000â€‘vocab, 512â€‘token sequences, 1â€¯M steps, available at clâ€‘tohoku/bertâ€‘japanese under CCâ€‘BYâ€‘SA.
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - ğŸ“¥ 95k / â­ 8 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden, 12 heads) pretrained on ~17â€¯M sentences from Japanese Wikipedia (2.6â€¯GB) using MeCab IPA wordâ€‘level tokenization followed by character tokenization into a 4000â€‘word vocabulary, with training code atâ€¯clâ€‘tohoku/bertâ€‘japanese and released under CCâ€¯BYâ€‘SAâ€¯3.0.
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - ğŸ“¥ 54k / â­ 3 / Japanese RoBERTaâ€‘base model pretrained on ~10â€¯M Japanese medical abstracts and 1.4â€¯M body texts from JST, tokenized with a 30â€¯kâ€‘token SentencePiece, released under CCâ€¯BYâ€‘4.0 and usable via Hugging Face pipelines.
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - ğŸ“¥ 42k / â­ 38 / A BERT base model pretrained on ~17â€¯M Japanese Wikipedia sentences (2.6â€¯GB) that tokenizes with the IPA dictionary and WordPiece, has 12 layers/768â€‘dim hidden states/12 heads, a 32â€¯000â€‘token vocabulary, was trained for 1â€¯M steps on Cloud TPUs and is released under CCâ€‘BYâ€‘SAâ€¯3.0.
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - ğŸ“¥ 33k / â­ 6 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) trained on 30â€¯M Wikipedia sentences (~4â€¯GB) with Unidicâ€¯2.1.2 wordâ€‘level tokenization followed by characterâ€‘level tokenization and wholeâ€‘word masking, using 512â€‘token sequences, 256 batches, and 1â€¯M training steps.
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - ğŸ“¥ 32k / â­ 48 / LINE DistilBERT Japanese is a 66â€‘millionâ€‘parameter DistilBERT model preâ€‘trained on 131â€¯GB of Japanese web text using an inâ€‘house BERTâ€‘base teacher, evaluated on JGLUE, tokenized with MeCabâ€¯Unidic and SentencePiece, and released under the Apacheâ€¯2.0 license.
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - ğŸ“¥ 22k / â­ 19 / ModernBERTâ€‘Jaâ€‘310M is a Japaneseâ€‘language BERT variant that blends localâ€‘global attention with RoPE, trained on 4.09â€¯T tokens of Japanese/English text, supports a 102â€¯400â€‘word vocabulary, 8â€¯192â€‘token sequences, and is optimized for Flashâ€¯Attentionâ€¯2.
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - ğŸ“¥ 22k / â­ 8 / Japanese DeBERTaâ€¯V2 large model trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with characterâ€‘level sentencepiece tokenization and wholeâ€‘word masking, ready for downstream fineâ€‘tuning through Huggingâ€¯Face Transformers.
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - ğŸ“¥ 11k / â­ 39 / Japaneseâ€‘Robertaâ€‘Base is a pretrained maskedâ€‘language model from rinna Co.,â€¯Ltd., with guidelines for proper loading, token preprocessing, positionâ€‘id handling, and usage examples emphasizing the need for a leading `[CLS]` token and consistent tokenization.
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - ğŸ“¥ 10k / â­ 6 / ModernBERTâ€‘Jaâ€‘70M is a lightweight Japanese BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯T mixedâ€‘language tokens (vocabâ€¯102â€¯400, maxâ€¯8â€¯192 tokens), supports Flashâ€¯Attentionâ€¯2, and comes in several sizes from 30â€¯M to 310â€¯M parameters.
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - ğŸ“¥ 7k / â­ 30 / Japanese DeBERTaâ€¯V2 base model pretrained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100 and OSCAR data using Juman++ segmentation and SentencePiece tokenization, trained for three weeks on eight NVIDIA A100 GPUs and ready for fineâ€‘tuning.
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - ğŸ“¥ 6k / â­ 27 / Japanese BERTâ€‘base (12â€¯layers, 768 hidden, 12 heads) pretrained on 4â€¯GB of Japanese Wikipedia (â‰ˆ30â€¯M sentences) with Unidicâ€¯2.1.2 wordâ€‘level tokenization, WordPiece subâ€‘tokenization, and wholeâ€‘word masking.
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - ğŸ“¥ 4k / â­ 45 / A 132â€‘millionâ€‘parameter Japanese ModernBERT model that blends localâ€‘global and RoPE attention, trained on 4.39â€¯T tokens (Japanese/English) with a 102â€‘kâ€‘size vocab, 8,192â€‘token max length, and optimized for Flashâ€¯Attentionâ€¯2.
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - ğŸ“¥ 3k / â­ 2 / Japanese DeBERTaâ€¯V2 tiny, pretrained on ~171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR corpora, requires Juman++ word segmentation, was trained in 33â€¯h on 8 NVIDIAâ€¯A100 GPUs, and can be fineâ€‘tuned for downstream tasks.
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - ğŸ“¥ 2k / â­ 4 / DeBERTaV2â€¯base trained on Japanese corpora (CCâ€‘100, mC4, OSCAR2301, Wikipedia, Wikinews) with FPâ€‘16 fineâ€‘tuning for NLU tasks (JSTS, JNLI, JCommonsenseQA), released under CCâ€¯BYâ€‘SAâ€¯4.0 and funded by Japanese research grants.
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - ğŸ“¥ 2k / â­ 9 / Japanese BERTâ€‘large (24 layers, 1024â€‘hidden size, 16 heads, 32â€¯K vocab) pretrained on 30â€¯M Japanese Wikipedia sentences with Unidicâ€‘2.1.2 wordâ€‘level tokenization, WordPiece subwords, and wholeâ€‘word masking over 1â€¯M steps.
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - ğŸ“¥ 2k / â­ 5 / ModernBERTâ€‘Jaâ€‘30M is a Japaneseâ€‘language BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯TB of Japanese/English text, supports 8,192â€‘token sequences, comes in sizes from 30â€¯M to 130â€¯M parameters, and works best with Flash Attentionâ€¯2.
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - ğŸ“¥ 1k / â­ 8 / Japanese RoBERTaâ€‘base model pretrained on Japanese Wikipedia and CCâ€‘100, using Juman++â€‘based SentencePiece tokenization, fineâ€‘tunable via Hugging Face, trained over 700k steps on 8â€¯A100 GPUs with mixedâ€‘precision.
 * [roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char) - ğŸ“¥ 1k / â­ 1 / robertaâ€‘smallâ€‘japaneseâ€‘aozoraâ€‘char is a Japanese RoBERTa model designed for downstream NLP tasks such as POSâ€‘tagging and dependency parsing, usable through the Transformers library.

### sentence-similarity
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - ğŸ“¥ 319k / â­ 11 / Japanese general text embedding models (Ruriâ€‘v3, 30â€‘310â€¯M parameters, 8192â€‘token max, high JMTEB scores) are offered with Sentenceâ€‘Transformers usage examples and benchmark comparisons to other Japanese embeddings.
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - ğŸ“¥ 318k / â­ 63 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token inputs, a 100Kâ€‘token vocabulary, FlashAttentionâ€‘accelerated inference, and multiple size variants for fast sentenceâ€‘transformer usage.
 * [ruri-small](https://huggingface.co/cl-nagoya/ruri-small) - ğŸ“¥ 78k / â­ 9 / Includes Ruriâ€¯v3 Japanese text embeddings (30â€¯Mâ€“310â€¯M parameters, 8192â€‘token limit, JMTEB 74.5â€“77.2), instructions for Sentence Transformers using â€œã‚¯ã‚¨ãƒª:â€ or â€œæ–‡ç« :â€ prefixes, and benchmark results for several Japanese models such as Sup/Unsup SimCSE, GLuCoSE, and LaBSE.
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - ğŸ“¥ 28k / â­ 34 / GLuCoSE is a Japanese sentenceâ€‘embedding model built on LUKE that outputs 768â€‘dim meanâ€‘pooled vectors (up to 512 tokens) trained on web and NLI/search data, achievingâ€¯0.864 Spearman andâ€¯0.818 Pearson on similarity benchmarks.
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - ğŸ“¥ 28k / â­ 44 / PLaMoâ€‘Embeddingâ€‘1B is a Japanese textâ€‘embedding model from Preferred Networks that converts Japanese text into vectors for information retrieval, classification, and clustering, shows strong performance on the JMTEB benchmark, and is freely available under an Apacheâ€¯v2.0 license.
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - ğŸ“¥ 22k / â­ 6 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese text embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192 tokens, a 100â€¯kâ€‘token vocabulary, FlashAttention acceleration, and multiple sizes from 37â€¯M to 315â€¯M parameters.
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - ğŸ“¥ 17k / â­ 21 / GLuCoSEâ€¯v2 is a CPUâ€‘friendly Japanese textâ€‘embedding model, fineâ€‘tuned by distillation and multiâ€‘stage contrastive learning, that delivers superior semanticâ€‘similarity and retrieval performanceâ€”outperforming comparableâ€‘size models on MIRACL and related benchmarks.
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - ğŸ“¥ 10k / â­ 16 / JaColBERTv2 is a Japaneseâ€‘only ColBERTâ€‘based retrieval model trained with knowledge distillation on MMarco (31 negatives per positive, 250k steps, batchâ€¯32) that currently outperforms multilingualâ€‘e5â€‘large, BGEâ€‘M3, and JaColBERT, with a full evaluation pending.
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - ğŸ“¥ 8k / â­ 36 / sbert-jsnliâ€‘lukeâ€‘japaneseâ€‘baseâ€‘lite is a 768â€‘dimensional sentenceâ€‘transformer built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite, trained one epoch on shunk031/jsnli, and includes examples for clustering, semantic search, and both Sentenceâ€‘Transformers and HuggingFace usage.
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - ğŸ“¥ 5k / â­ 2 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8192â€‘token sequences, a 100Kâ€‘token vocabulary, FlashAttention, and released in sizes from 30â€¯M to 310â€¯M parameters for use with sentenceâ€‘transformers.
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - ğŸ“¥ 3k / â­ 44 / A collection of releaseâ€‘ready Ruri v3 Japanese text embedding models (30mâ€“310m), complete with SentenceTransformer usage tips, query/passage prefixes, and JMTEB benchmark results showing how they compare to other Japanese and multilingual embeddings.
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - ğŸ“¥ 3k / â­ 1 / Ruriâ€¯v3 delivers highâ€‘performance Japanese text embeddings up to 8192 tokens, a 100kâ€‘token vocabulary, FlashAttention support, and multiple model sizes (30â€¯mâ€“310â€¯m) for efficient inference and fineâ€‘tuning via sentenceâ€‘transformers.
 * [ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2) - ğŸ“¥ 1k / â­ 10 / Japanese generalâ€‘text embeddings repository Ruri offers v3 models ranging from 30â€¯M to 310â€¯M parameters with JMTEB scores, shows how to load them with sentence_transformers (using â€œã‚¯ã‚¨ãƒª: â€ / â€œæ–‡ç« : â€ prefixes), and provides benchmark results comparing several Japanese embedding models.

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - ğŸ“¥ 187k / â­ 13 / Japanese CLOOBâ€‘VIT-B-16, a Visionâ€‘Language model based on vitâ€‘baseâ€‘patch16â€‘224 trained on translated CC12M captions and released by rinna Co., Ltd. on Mayâ€¯12â€¯2022 under Apacheâ€¯2.0.
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - ğŸ“¥ 45k / â­ 51 / A Japanese Sentenceâ€‘BERT v2, fineâ€‘tuned on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘wholeâ€‘wordâ€‘masking with MultipleNegativesRankingLoss, boosts accuracy by ~1.5â€“2â€¯% over v1 and is released asâ€¯sonoisa/sentenceâ€‘bertâ€‘baseâ€‘jaâ€‘meanâ€‘tokensâ€‘v2.
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - ğŸ“¥ 33k / â­ 19 / Sarashinaâ€‘Embeddingâ€‘v2â€‘1B is a 1,792â€‘dimensional Japanese sentence transformer trained with multiâ€‘stage contrastive learning that achieves stateâ€‘ofâ€‘theâ€‘art JMTEB scores, and can be used for semantic similarity, search, paraphrase mining, classification, and clustering via Sentenceâ€‘Transformers with optional instruction prefixes.
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - ğŸ“¥ 30k / â­ 22 / rinna/japanese-clipâ€‘vitâ€‘bâ€‘16 is an Apacheâ€‘2.0 licensed Japanese CLIP model based on ViTâ€‘B/16, trained on CC12M captions translated to Japanese and released on Mayâ€¯12â€¯2022.
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - ğŸ“¥ 23k / â­ 11 / Japanese Sentenceâ€‘BERT (v1) model for generating sentence embeddings, with an improved v2 available and sample usage via Huggingâ€¯Face Transformers and a custom `SentenceBertJapanese` class.
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - ğŸ“¥ 18k / â­ 29 / LY Corporationâ€™s clipâ€‘japaneseâ€‘base is a Japanese CLIP model trained on ~1â€¯B imageâ€‘text pairs, using an Eva02â€‘B transformer image encoder with a 12â€‘layer BERT text encoder, achieving R@1â€¯0.30 on STAIR, 0.89 accuracy on Recruit and 0.58 accuracy on ImageNetâ€‘1K, and supporting zeroâ€‘shot image classification and retrieval.
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - ğŸ“¥ 12k / â­ 3 / A Japanese BERTâ€‘base model fineâ€‘tuned with supervised SimCSE on JSNLI, exposed via Sentenceâ€‘Transformers or HuggingFace with CLS pooling, trained on 1â€¯M examples at 512â€‘batch size, 5â€¯Ã—â€¯10â»âµ learning rate, 5â€¯Ã—â€¯10â»âµ temperature, 64â€‘token limit, and BFloat16 precision.
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - ğŸ“¥ 9k / â­ 2 / ja_ginza_electra is a spaCyâ€¯v3 Python package offering a Japanese ELECTRA model fineâ€‘tuned on mC4 and UD_Japanese_BCCWJâ€¯r2.8 (based on megagonlabs/transformersâ€‘udâ€‘japaneseâ€‘electraâ€‘baseâ€‘discrimininator) with custom bunsetuâ€‘phrase detection, distributed under the MIT license.
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - ğŸ“¥ 4k / â­ 14 / Japanese Sentenceâ€‘LUKE model trained on the same dataset as Sentenceâ€‘BERT, outperforming or matching it, built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite and used via Huggingâ€¯Face Transformersâ€™ MLukeTokenizer and LukeModel.
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - ğŸ“¥ 2k / â­ 54 / A Japaneseâ€‘language T5 model, pretrained on ~100â€¯GB of Wikipedia and OSCAR data with SentencePiece tokenization, surpasses Googleâ€™s multilingual T5 on a newsâ€‘classification benchmark but needs fineâ€‘tuning and may yield biased outputs.
 * [clip-japanese-base-v2](https://huggingface.co/line-corporation/clip-japanese-base-v2) - ğŸ“¥ 2k / â­ 17 / Japanese CLIP model clipâ€‘japaneseâ€‘baseâ€‘v2, upgraded with ~2â€¯B imageâ€‘text pairs and distillation, pairs an Eva02â€‘B image encoder with a 12â€‘layer BERT text encoder to reach higher ImageNetâ€‘1k accuracy (0.708) than its predecessor.
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - ğŸ“¥ 1k / â­ 15 / Supâ€‘simcseâ€‘jaâ€‘large provides a supervised SimCSE fineâ€‘tuned Japanese BERTâ€‘large (clâ€‘tohoku/bertâ€‘largeâ€‘japaneseâ€‘v2) model with CLSâ€‘plusâ€‘MLP pooling, trained on ~1â€¯M JSNLI sentences (lrâ€¯5eâ€‘5, batchâ€¯512, tempâ€¯0.05, maxâ€¯64) and ready for use with Sentenceâ€‘Transformers or Huggingâ€¯Face Transformers.

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - ğŸ“¥ 4M / â­ 52 / Japanese wav2vecâ€‘2 XLSRâ€‘53 fineâ€‘tuned on Common Voiceâ€¯6.1, CSS10, and JSUT, requiring 16â€¯kHz audio and usable via HuggingSound or HuggingFace pipelines.
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - ğŸ“¥ 41k / â­ 89 / Kotobaâ€‘Whisperâ€‘v2.2 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated diarization and automatic punctuation via a HuggingFaceâ€‘Transformers pipeline, built in collaboration with Asahiâ€¯Ushio and Kotoba Technologies.
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - ğŸ“¥ 11k / â­ 4 / Fineâ€‘tuned wav2vec2â€‘base Japanese ASR model trained on Common Voiceâ€¯11.0 that predicts only Hiragana, built from rinna/japaneseâ€‘wav2vec2â€‘base with 20â€¯epochs at lrâ€¯1eâ€‘4.
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - ğŸ“¥ 9k / â­ 18 / Kotobaâ€‘Whisperâ€‘Bilingual v1.0 delivers 6.3Ã— faster distilled Whisper models for Japanese and English ASR plus bidirectional speechâ€‘toâ€‘text translation, built from OpenAIâ€™s Whisperâ€¯largeâ€‘v3 via knowledge distillation with crossâ€‘entropy and KLâ€‘divergence loss.
 * [japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh) - ğŸ“¥ 7k / â­ 2 / Japaneseâ€‘wav2vec2â€‘baseâ€‘rs35kh is a 96.7â€¯Mâ€‘parameter wav2vecâ€¯2.0 Base model fineâ€‘tuned on the ReazonSpeech v2.0 Japanese ASR corpus, reaching a 13.22â€¯% CER, deployable with Hugging Face transformers, and released under an ApacheÂ 2.0 license.
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - ğŸ“¥ 6k / â­ 35 / reazonspeech-nemo-v2 is a 619â€‘Mâ€‘parameter Japanese longâ€‘form ASR model built on an improved Fastâ€‘Conformer with Linearly Scalable Attention, trained on the ReazonSpeechâ€¯v2.0 corpus, offering multiâ€‘hour inference via a subword RNNâ€‘T decoder (3000â€‘token SentencePiece) and distributed under Apacheâ€¯2.0.
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - ğŸ“¥ 4k / â­ 117 / Anime Whisper is a lightweight Japanese ASR model fineâ€‘tuned on ~5,300â€¯h of animeâ€‘style dialogue that delivers low hallucination, rhythmâ€‘aligned punctuation and accurate transcription of nonâ€‘verbal sounds and NSFW content, and must be run without an initial prompt.
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - ğŸ“¥ 3k / â­ 42 / NVIDIA NeMoâ€™s 0.6â€¯Bâ€‘parameter Hybrid FastConformerâ€‘TDTâ€‘CTC ASR model transcribes Japanese speech with punctuation and is available for inference or fineâ€‘tuning within the NeMo framework.
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - ğŸ“¥ 3k / â­ 19 / Kotobaâ€‘Whisperâ€‘v2.1 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated punctuationâ€‘postprocessing pipelines that maintain comparable CER performance while enabling seamless, punctuationâ€‘aware transcription.
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - ğŸ“¥ 3k / â­ 84 / Kotobaâ€‘Whisperâ€¯v2.0 is a Japanese ASR model distilled from OpenAI Whisper largeâ€‘v3, trained on 7.2â€¯million ReazonSpeech clips, that runs 6.3Ã— faster while matching the teacherâ€™s CER/WER on inâ€‘domain tests and includes stableâ€‘ts/punctuation support and full training code on GitHub.
 * [kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - ğŸ“¥ 2k / â­ 58 / Kotobaâ€‘Whisperâ€¯v1.0, a Japanese ASR model distilled from OpenAIâ€™s Whisper largeâ€‘v3, delivers aâ€¯6.3Ã— speedâ€‘up with comparable accuracy, was trained on 1,253â€¯h of ReazonSpeech, and its code (plus a newer v2.0 variant on Huggingâ€¯Face) is publicly available.

### translation
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - ğŸ“¥ 133k / â­ 10 / A LLaMAâ€¯3 Youko qlora fineâ€‘tune built on a new VNTL dataset, optimized for accurate, literal translations of Japanese visual novels to English without chat mode, using the default LLaMAâ€¯3 prompt and recommending neutral sampling (temperatureâ€¯0, no repetition penalty).
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - ğŸ“¥ 36k / â­ 67 / Japaneseâ€‘toâ€‘English Transformerâ€‘Align MT model from the Opus corpus, using normalization and SentencePiece preprocessing, achieves 41.7 BLEU and 0.589 chrâ€‘F on the Tatoeba test set.
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - ğŸ“¥ 17k / â­ 28 / Fineâ€‘tuned, GGUFâ€‘quantized LFM2â€‘350M checkpoint for near realâ€‘time biâ€‘directional Japaneseâ€‘English translation of shortâ€‘toâ€‘medium text, usable via llama.cpp.
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - ğŸ“¥ 10k / â­ 14 / Englishâ€‘toâ€‘Japanese transformerâ€‘align MT model with 15.2â€¯BLEU, built on opus+btâ€‘2021â€‘04â€‘10 using normalizationâ€¯+â€¯SentencePiece, hosted on the Tatoeba Challenge.
 * [plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate) - ğŸ“¥ 2k / â­ 114 / PLaMo Translation Model is a largeâ€‘scale language model created by Preferred Networks for translation tasks, available in base, postâ€‘trained, and evaluation variants, released under the PLaMo community license and not instructionâ€‘tuned for chat or other downstream uses.
 * [sugoi-v4-ja-en-ctranslate2](https://huggingface.co/entai2965/sugoi-v4-ja-en-ctranslate2) - ğŸ“¥ 1k / â­ 1 / Sugoiâ€¯v4 is a Japaneseâ€‘toâ€‘English NMT model by MingShiba, downloadable from Huggingâ€¯Face and run with CTranslate2 for batch translation.

### token-classification
 * [MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - ğŸ“¥ 42k / â­ 4 / A Huggingâ€‘Faceâ€‘compatible NER model trained on the MedTxtâ€‘CRâ€‘JA Japanese medical dataset, accompanied by a predict script that normalizes entity outputs, produces XMLâ€‘tagged text, and uses an external `id_to_tags.pkl` to map label IDs to real tags.
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - ğŸ“¥ 9k / â­ 11 / Japanese NER using clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2 that extracts eight entity types (corporations, political/other organizations, facilities, products, events) viaâ€¯`BertForTokenClassification`, trained on the Stockmark Wikipedia dataset and installable with `transformers`, `unidic_lite`, and `fugashi` under a CCâ€¯BYâ€‘SAâ€¯3.0 license.
 * [MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA) - ğŸ“¥ 5k / â­ 12 / Japaneseâ€‘medicalâ€‘document NER model (download the five files, run **predict.py**â€¯â€“â€¯output tags for diseases, drugs, dates, etc.) used in the NAISTSOC 2022 NTCIRâ€‘16 Realâ€‘MedNLP task.
 * [bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - ğŸ“¥ 1k / â­ 10 / Fineâ€‘tuned Japanese BERTâ€‘Base for namedâ€‘entity recognition on a Wikipedia dataset, presented in Chapterâ€¯6 of the *Large Language Model Introduction* book and deployable via a Huggingâ€¯Face transformers pipeline (Apacheâ€¯2.0 licensed).
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - ğŸ“¥ 1k / â­ 25 / Fineâ€‘tuned XLMâ€‘RoBERTaâ€‘base on a Japanese NER corpus (tagsâ€¯PER,â€¯ORG,â€¯LOC,â€¯INS,â€¯PRD,â€¯EVT) using 5â€‘epoch Adam (lrâ€¯5eâ€‘5, batchâ€¯12) to reach a 0.0173 validation loss, released on Transformersâ€¯4.23.1 and PyTorchâ€¯1.12.1.

### text-ranking
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - ğŸ“¥ 16k / â­ 7 / Japanese CrossEncoder reranker models ranging from xsmall to large (plus BGE), evaluated on JQaRA, JaCWIR, MIRACL, and JSQuAD, with readyâ€‘toâ€‘use integration examples for sentence_transformers and HuggingFace.
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - ğŸ“¥ 9k / â­ 4 / Japaneseâ€‘trained CrossEncoder rerankers ranging from xsmall (384) to large (1024) plus a BGEâ€‘v2â€‘m3â€‘v1 model, with example code for fineâ€‘tuning, inference, and benchmark scores on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - ğŸ“¥ 9k / â­ 13 / Ruriâ€‘v3 Reranker is a robust Japanese text reranker built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token sequences, a 100kâ€‘token vocabulary, FlashAttention and a SentencePiece tokenizer, and it can be used via sentenceâ€‘transformers.
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - ğŸ“¥ 5k / â­ 6 / Fast, lightweight Japanese Rerankerâ€¯v2 models (tiny,â€¯xsmall,â€¯small,â€¯base) with benchmark scores and GPU speeds, usable via sentence_transformersâ€¯CrossEncoder and transformersâ€¯â‰¥â€¯v4.48 (optionally accelerated with flashâ€‘attn) and also available in ONNX/quantized forms for CPU/ARM.

### text-classification
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ğŸ“¥ 14k / â­ 43 / A Japanese LUKE model fineâ€‘tuned on the WRIME dataset that classifies which of eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, trustâ€”is expressed in a sentence.
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - ğŸ“¥ 13k / â­ 3 / Japanese BERT Base model fineâ€‘tuned on a 10â€‘label emotion blogâ€‘post dataset (~1,000 sentences) derived fromâ€¯tohokuâ€‘nlp/bert-base-japanese for accurate emotion detection and classification.
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - ğŸ“¥ 11k / â­ 15 / Japanese sentiment analysis model trained on the chABSA dataset, achieving lossâ€¯0.0001, accuracyâ€¯1.0, and F1â€¯1.0, built with Transformersâ€¯4.24.0 and PyTorchâ€¯1.12.1+cu113, optimized with Adam (learningâ€¯rateâ€¯2eâ€‘05, 10 epochs, batchâ€¯sizeâ€¯16) and evaluated via `model(**inputs)`.
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - ğŸ“¥ 4k / â­ 2 / A Japanese BERTâ€‘based model fineâ€‘tuned on the JGLUE JSTS dataset for semantic similarity scoringâ€”introduced in chapterâ€¯5 ofâ€¯â€œLarge Language Model Introductionâ€â€”with Colab notebooks, transformersâ€‘pipeline usage, and an Apacheâ€¯2.0 license.

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - ğŸ“¥ 217k / â­ 163 / Mangaâ€¯OCR is a Visionâ€¯Encoderâ€‘Decoder OCR tool that reads vertical and horizontal Japanese manga textâ€”including furiganaâ€”across diverse fonts and lowâ€‘quality images, with the source code freely available.
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - ğŸ“¥ 19k / â­ 3 / meikiocr supplies a Dâ€‘FINEâ€‘based, openâ€‘weight textâ€‘detection model for videoâ€‘games (v0.1 with MobileNetâ€‘v4 backbones, two resolution variants and a 64â€‘box limit) and experimental lowâ€‘latency tiny and small variants trained on Japanese videoâ€‘games and manga.
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - ğŸ“¥ 19k / â­ 4 / Meikiocrâ€™sâ€¯`meiki.text.recognition.v0`â€”a Dâ€‘FINE-based MobileNetV4 model fineâ€‘tuned on Japanese videoâ€‘game textâ€”delivers stateâ€‘ofâ€‘theâ€‘art accuracy and latency for horizontal text by detecting up to 48 characters from 960Ã—32 inputs, outputting each character with its bounding box and confidence score.

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - ğŸ“¥ 4k / â­ 118 / Fineâ€‘tuned from PaddleOCRâ€‘VL, PaddleOCRâ€‘VLâ€‘Forâ€‘Manga achieves 70â€¯% fullâ€‘sentence accuracy on Manga109â€‘s speechâ€‘bubble cropsâ€”over triple the 27â€¯% baselineâ€”using a multiâ€‘language dataset and includes training code and a developer guide.
 * [llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b) - ğŸ“¥ 2k / â­ 11 / LLMâ€‘jp VILA 14B is a 14â€‘Bâ€‘parameter visionâ€‘language model developed by Japanâ€™s National Institute of Informatics, intended for use with Pythonâ€¯3.10.12 through a cloneâ€‘installâ€‘run workflow that installs required libraries (including flashâ€‘attention) and accepts image inputs and text queries.
 * [Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B) - ğŸ“¥ 1k / â­ 14 / Heronâ€‘NVILAâ€‘Liteâ€‘15Bâ€‘hf is a Japaneseâ€‘centric visionâ€‘language model built on the NVILAâ€‘Lite architecture, featuring a PALIGEMMAâ€‘SIGLIP vision encoder, an MLPâ€‘downsample projector, and a Qwen2.5â€‘14Bâ€‘Instruct LLM, supporting Japanese and English with straightforward Hugging Face integration.

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - ğŸ“¥ 4k / â­ 13 / Animeâ€‘XCodec2â€‘44.1kHzâ€‘v2 upsamples 16â€¯kHz Japanese speech to 44.1â€¯kHz highâ€‘fidelity audio with a decoderâ€‘only RMSâ€‘loss fineâ€‘tune, keeping the encoder/codebook frozen and preserving identical speech tokens.

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - ğŸ“¥ 2k / â­ 25 / Animeâ€‘Llasaâ€‘3B is a Japanese TTS model built on HKUSTAudio/Llasaâ€‘3B, enhanced with more training data to boost expressiveness and stability, and licensed CCâ€‘BYâ€‘NCâ€‘4.0.

### text-to-audio
 * [japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate) - ğŸ“¥ 1k / â­ 23 / Japaneseâ€‘language TTS trained from Parlerâ€‘TTSâ€‘Largeâ€¯v1 delivers lightweight, highâ€‘quality speech in beta but can be unstable and has limited maleâ€‘voice fidelity, so the Mini version is recommended for reliability.

### others
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - ğŸ“¥ 97k / â­ 10 / Japaneseâ€‘language BERTâ€‘Base (12 layers, 768â€‘dim, 12 heads) pretrained with Unidicâ€‘based wordâ€‘level plus characterâ€‘level tokenization and wholeâ€‘word masking on CCâ€‘100 and 2023 Wikipedia, producing a 7,027â€‘token vocabulary.
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - ğŸ“¥ 68k / â­ 13 / Japaneseâ€‘BERTâ€‘Large trained on CCâ€‘100 and Wikipedia, using Unidicâ€‘lite wordâ€‘level tokenization with WordPiece subwords and wholeâ€‘word masking (24 layers, 1024â€‘dim hidden, 16 heads, 32k vocab), with pretraining code on clâ€‘tohoku/bertâ€‘japanese.
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - ğŸ“¥ 62k / â­ 59 / Japanese BERTâ€‘base (12 layers, 768â€‘dim hidden, 12 heads, 32â€¯k vocab) pretrained with wholeâ€‘word masking on CCâ€‘100 and 2023â€‘Jan Wikipedia, using Unidicâ€¯2.1.2 wordâ€‘level tokenization plus WordPiece, in 2â€¯M training steps.
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - ğŸ“¥ 14k / â­ 10 / A Japanese T5â€‘v1.1 model pretrained on â‰ˆ100â€¯GB of Wikipedia and OSCAR CCâ€‘100 data (mixed 10:1 for SentencePiece with byteâ€‘fallback), requiring fineâ€‘tuning for downstream tasks, includes transferâ€‘learning sample code, notes potential bias in outputs, and is licensed CCâ€‘BYâ€‘SAâ€¯4.0.
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - ğŸ“¥ 13k / â­ 17 / Japanese DeBERTaâ€¯V3â€¯base preâ€‘trained on 540â€¯B tokens from LLMâ€‘jpâ€¯v1.0, trained with a modified DeBERTaâ€¯V3 setup, uses a unigram byteâ€‘fallback tokenizer (no morphological analyzer), and is fineâ€‘tuned for JGLUE NLU tasks.
 * [Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5) - ğŸ“¥ 5k / â­ 9 / Llamaâ€¯3.1â€¯Swallowâ€¯v0.5 is an 8â€‘billionâ€‘parameter LLM that improves Metaâ€™s Llamaâ€¯3.1 on Japanese language and code/math reasoning while retaining English fluency, achieved through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning on synthetic Japanese data.
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - ğŸ“¥ 3k / â­ 71 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced 8â€‘B Llamaâ€¯3 model with GGUF (Q4_K_M) and AWQ quantization, ready to run via llama.cpp, LMâ€¯Studio, or an OpenAIâ€‘compatible API.
 * [llama-3-youko-8b-instruct-i1-GGUF](https://huggingface.co/mradermacher/llama-3-youko-8b-instruct-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / A collection of GGUFâ€‘quantized versions of rinna/llamaâ€‘3â€‘youkoâ€‘8bâ€‘instruct, listing their size/quality tradeâ€‘offs, with usage guidance, a comparison graph, and a FAQ/modelâ€‘request link.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - ğŸ“¥ 2k / â­ 38 / GGUFâ€‘formatted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen Japanese 32B model from cyberagent, built with the imatrix dataset and ready to run with llama.cpp.
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - ğŸ“¥ 2k / â­ 16 / Highâ€‘performance Japanese SPLADEâ€¯v2 enables sparseâ€‘vector conversion and inference through a WebUI demo, trains with YAST, offers YASEM embedding, and reports JMTEB benchmark results.
 * [Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf) - ğŸ“¥ 2k / â­ 3 / GGUFâ€‘formatted Qwen3â€‘30Bâ€‘A3B converted from TFMC/imatrixâ€‘datasetâ€‘forâ€‘Japaneseâ€‘LLM, ready to run with CUDAâ€‘enabled llama.cpp.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - ğŸ“¥ 1k / â­ 55 / Cyberagentâ€™s ggufâ€‘converted DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14Bâ€‘Japanese model (built from the TFMC imatrix dataset) is available under mmnga and can be run with CUDA support using llama.cpp.
 * [Tema_Q-R3.1-i1-GGUF](https://huggingface.co/mradermacher/Tema_Q-R3.1-i1-GGUF) - ğŸ“¥ 1k / â­ 1 / Offers a comprehensive list of weighted/imatrix GGUF quant versions for the Tema_Qâ€‘R3.1 model, detailing sizes, quality notes, download links, usage guidanceâ€”including GGUF file handlingâ€”and links to the model page, readmes, and FAQ.
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - ğŸ“¥ 1k / â­ 19 / A GGUFâ€‘format release of pfnetâ€™s plamoâ€‘2â€‘translate built from imatrix data based on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with instructions to compile and run it via llama.cpp on CUDAâ€‘enabled hardware.
 * [j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext) - ğŸ“¥ 1k / â­ 42 / Jâ€‘Moshi is a fullâ€‘duplex Japanese speechâ€‘dialogue system built on the 7Bâ€‘parameter Moshi model, providing pretrained weights, an interactive webâ€‘UI demo, and training code for realâ€‘time turnâ€‘taking conversation, and it requires a Linux GPU with at least 24â€¯GB VRAM.
 * [shisa-v2.1-qwen3-8b-UD-japanese-imatrix](https://huggingface.co/dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix) - ğŸ“¥ 1k / â­ 2 / A GGUFâ€‘quantized shisaâ€‘v2.1â€‘qwen3â€‘8b model built with Unsloth Dynamicâ€¯2.0, communityâ€‘patched Qwen3 settings to reduce malfunctions, a larger imatrix for stronger Japanese performance, and a 40K maximum context length.
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - ğŸ“¥ 1k / â­ 7 / A ggufâ€‘format conversion of Vecteusâ€‘v1 from Localâ€‘Novelâ€‘LLM, built using the imatrix dataset, that can be run with llama.cpp via `Vecteusâ€‘v1â€‘Q4_0.gguf` and lists other related models.

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - ğŸ“¥ 1M / â­ 19 / Aggregates overâ€¯150â€¯GB of NicoNico Liveâ€™s comment logs from 2009â€‘2024â€”including preâ€‘transition, postâ€‘transition, and realâ€‘time NXâ€‘Jikkyo capturesâ€”providing an API for easy retrieval of historical TVâ€‘broadcast discussions.
 * [emb](https://huggingface.co/datasets/hpprc/emb) - ğŸ“¥ 7k / â­ 13 / A catalog of Japanese and multilingual QA, NLI and paraphrase datasets, detailing each datasetâ€™s retrieval or QA tasks and its license (Apacheâ€¯2.0, CCâ€‘BYâ€‘SA/CCâ€‘BY, MIT, etc.).
 * [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) - ğŸ“¥ 4k / â­ 10 / Mirror of the Reazon Speechâ€¯v2 dataset on ğŸ¤—, licensedâ€¯CDLAâ€‘Sharingâ€‘1.0 and restricted to Japanese Copyright Actâ€¯Articleâ€¯30â€‘4, with 16â€¯kHz FLAC audio and accompanying metadata.
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - ğŸ“¥ 3k / â­ 18 / JMTEB is a Japanese textâ€‘embedding benchmark featuring 5 tasks (clustering, classification, STS, retrieval, reranking) and 28 datasets, offering a oneâ€‘line evaluation script and inviting community contributions.
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - ğŸ“¥ 3k / â­ 7 / JMedBench is a Japanese biomedical LLM benchmark comprising 20 datasets across five tasks (MCQA, NER, STS, etc.) sourced from MedMCQA, PubMedQA, MMLU, and others, each with its own license, and includes a note that translations may contain biases requiring human review.
 * [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ğŸ“¥ 3k / â­ 99 / A 100â€‘sample Japanese instructionâ€‘tuning evaluation dataset of annotated tasksâ€”ranging from summarization correction and math reasoning to translation, creative generation, and userâ€‘intent understandingâ€”designed for manual or automatic 5â€‘point rating of fineâ€‘tuned models.
 * [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) - ğŸ“¥ 3k / â­ 3 / Japanese Wikipedia sentences are transformed into various embeddings and a FAISS index, offering a Hugging Face Space demo, conversion scripts, and evaluations of search, Q&A, and OpenAIâ€¯textâ€‘embeddingâ€‘3â€‘small for RAG; embeddings are OpenAIâ€‘licensed, others CCâ€‘BYâ€‘SAâ€‘4.0.
 * [Japanese-Eroge-Voice-V2](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice-V2) - ğŸ“¥ 2k / â­ 13 / Japaneseâ€‘Erogeâ€‘Voiceâ€‘V2 offers 2,657â€¯hours of anonymized 1,033,142 eroge audioâ€“transcription pairs (mostly female, NSFW), MITâ€‘licensed for academic research.
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - ğŸ“¥ 2k / â­ 31 / Restructured reupload of the Galgame VisualNovel datasetâ€¯(OOPPEENN/56697375616C4E6F76656C5F44617461736574) for efficient Huggingâ€¯Faceâ€¯datasets loading, preserving all original audio/text and providing an extraction script with multiple gameâ€‘subset options.
 * [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) - ğŸ“¥ 2k / â­ 96 / Nemotronâ€‘Personasâ€‘Japan is an openâ€‘source, CCâ€¯BYâ€¯4.0 dataset of highâ€‘quality, synthetically generated Japanese personasâ€”incorporating name, gender, age, background, marital status, education, occupation and locationâ€”grounded in realâ€‘world demographic, geographic and personality distributions, engineered with probabilistic graphical models and GPTâ€‘OSSâ€‘120B to enhance diversity, reduce bias, prevent model collapse, assist sovereign AI development, and support commercial use.
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - ğŸ“¥ 2k / â­ 8 / Rakuda supplies 40 Japanese questionsâ€”openâ€‘ended for history, society, and government, and specific for geographyâ€”for benchmarking Japanese AI assistants, comparable to vicunaâ€‘eval, and can be loaded with `datasets.load_dataset`.
 * [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - ğŸ“¥ 2k / â­ 46 / Updated JGLUE dataset card and loading script for a Japanese NLP benchmark (created by Yahoo Japan and Waseda University) that covers text classification (MARCâ€‘ja, JCoLA), sentenceâ€‘pair classification (JNLI), and QA (JSQuAD, JCommonsenseQA), with releases linked on GitHub and Hugging Face.
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - ğŸ“¥ 1k / â­ 142 / Japanese Anime Speech Dataset offers 73,004 audioâ€‘text pairs (110â€¯hours total, evolving from V1 to V5) to enhance ASR models such as OpenAIâ€™s Whisper, available under an open license for all uses with credit appreciated.
 * [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) - ğŸ“¥ 1k / â­ 128 / Japanese Anime Speech Datasetâ€¯V2 delivers 292,637 cleaned audioâ€‘text pairsâ€”about 397.5â€¯h of SFW and 52.4â€¯h of NSFW contentâ€”in 128â€‘kbps MP3 files split by safety, designed specifically for training automatic speechâ€‘recognition models.
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - ğŸ“¥ 1k / â­ 8 / Cauldronâ€‘JA is a Japanese visionâ€‘language dataset of 44 subâ€‘datasets translated from The Cauldron using the DeepL API, available via HuggingFaceâ€™s datasets library and licensed identically to the original set, with prompts released under CCâ€‘BYâ€‘4.0.
 * [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) - ğŸ“¥ 1k / â­ 107 / ReazonSpeech is a free, FLACâ€‘encoded Japanese speech corpus with transcriptions, offered in five sizes from 8.5â€¯h to 35,000â€¯h, downloadable via Huggingâ€¯Face under the CDLAâ€‘Sharingâ€‘1.0 license and limited to use under Japan Copyright Act Articleâ€¯30â€‘4.
 * [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) - ğŸ“¥ 1k / â­ 4 / Provides a Japanese search/QA dataset with perâ€‘query scores computed by five multilingual/Japanese rerankers (e.g., BAAI/bgeâ€‘rerankerâ€‘v2â€‘m3, Alibabaâ€‘NLP/gteâ€‘multilingualâ€‘rerankerâ€‘base), including average scores for roughly 200 positive and negative example documents per query.
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - ğŸ“¥ 1k / â­ 24 / FineWeb2 Edu Japanese delivers ~120â€¯million highâ€‘quality educational Japanese texts (â‰ˆ89.3â€¯billion tokens) from FineWeb2, filtered by a DeepSeekâ€‘API classifier (scoreâ€¯â‰¥â€¯2.5), tokenized via ModernBERTâ€‘Jaâ€‘130M, and includes a smallâ€‘token subset (â‰¤512 tokens).
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - ğŸ“¥ 922 / â­ 31 / A 409â€‘hour Japanese eroge voice dataset processed with 2â€‘pass loudnorm (â€‘23â€¯LUFS, â€‘1â€¯dB peak, 11â€¯LRA), transcribed by litagin/anime-whisper, anonymized, stored as WebDataset (FLAC, JSON, TXT), largely featuring female voices with potential AI transcription errors, and MITâ€‘licensed for academic research.
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - ğŸ“¥ 901 / â­ 2 / Large compressed JSONâ€‘Lines dataset of anonymous 2ch.sc/2ch.net threads, including thread IDs, titles, board and region details, reply counts, and full post metadata (author, mail, date, content).
 * [JamC-QA](https://huggingface.co/datasets/sbintuitions/JamC-QA) - ğŸ“¥ 898 / â­ 4 / JamCâ€‘QA is a bilingual benchmark of multipleâ€‘choice questions spanning eight Japaneseâ€‘culture and knowledge categories, with leaderboard metrics comparing stateâ€‘ofâ€‘theâ€‘art models.
 * [llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - ğŸ“¥ 836 / â­ 142 / Japanese instructionâ€‘chat dataset for fineâ€‘tuning LLMs (e.g., with LoRA), 9â€¯M+ samples, recently updated to drop licensed Alpaca data, clean Wikipedia and ALT outputs, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - ğŸ“¥ 798 / â­ 21 / cc100-ja is a collection of the Japanese portion of the cc100 dataset, provided as sharded Parquet files.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - ğŸ“¥ 777 / â­ 87 / An automatically translated Japanese version of the databricksâ€‘dollyâ€‘15k dataset, licensed CCâ€‘BYâ€‘SAâ€‘3.0 and last updated on 2023â€‘05â€‘11.
 * [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) - ğŸ“¥ 704 / â­ 16 / Mirror of the Reazon Speechâ€¯v2 dataset: 3,674 denoised audio files processed with UVR on 8â€¯A800 GPUs over 10â€¯days by Stardustâ€‘minus, released under CDLAâ€‘Sharingâ€‘1.0 and without transcripts.
 * [llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - ğŸ“¥ 672 / â­ 32 / Japanese chatbot dataset stripped of English translation data from izumiâ€‘lab/llmâ€‘japaneseâ€‘dataset, offering 2.5â€¯million+ entries (v1.0.0) for fineâ€‘tuning Japanese LLMs on instructionâ€‘response tasks under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - ğŸ“¥ 653 / â­ 37 / A userâ€‘friendly, deduplicated CSV dataset of publicâ€‘domain Japanese texts from Aozoraâ€¯Bunko, processed with globisâ€‘org/aozorabunkoâ€‘extractor and cleaned for modernâ€‘Japanese machineâ€‘learning use.
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - ğŸ“¥ 644 / â­ 21 / A Japanese web collection of 56â€¯million documents, 110â€¯B characters and 249â€¯million images used to train large visionâ€‘language modelsâ€”offering a momiji_generator for data population, OBELICSâ€‘style visualization, and a sample model (Heronâ€‘NVILAâ€‘Lite).
 * [JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU) - ğŸ“¥ 617 / â­ 19 / JMMMU is a Japanese multimodal benchmark expanded over tenfold to 1,320 culturally diverse questions (720 cultureâ€‘agnostic, 600 cultureâ€‘specific) translated by native subject experts, now featuring a public leaderboard.
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - ğŸ“¥ 524 / â­ 31 / Syosetu711K is a Japanese dataset of ~711,700 novels scraped from å°èª¬å®¶ã«ãªã‚ã† on Marchâ€¯26â€‘27â€¯2023, providing full text and metadata (title, author, NCode, synopsis, etc.) for unsupervised text generation and classification tasks.
 * [paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa) - ğŸ“¥ 502 / â­ 2 / Dataset of LLMâ€‘generated queries and answers from paraphrases of Japanese Wikipedia text, built without using licenseâ€‘restricted models and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Galgame_Speech_SER_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_SER_16kHz) - ğŸ“¥ 456 / â­ 13 / A 104â€¯GB dataset of 3,746,131â€¯Galgame audio files (5,353â€¯h), adding LLMâ€‘generated emotion labels (possibly inaccurate) to the existing 16â€¯kHz ASR set, released under GPLâ€¯v3.0 with no commercial use allowed and requiring any trained models to be openâ€‘source.
 * [mc4-ja](https://huggingface.co/datasets/izumi-lab/mc4-ja) - ğŸ“¥ 441 / â­ 6 / Dataset card for the Japanese MC4 dataset (mc4-ja).
 * [EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench) - ğŸ“¥ 382 / â­ 10 / EDINETâ€‘Bench is a Japanese financial benchmark that evaluates LLMs on tasks such as accounting fraud detection, earnings forecasting, and industry prediction using ten years of EDINETâ€‘API disclosed reports, with construction and evaluation code provided and the dataset relicensed to PDLâ€¯1.0.
 * [callhome-ja-plus](https://huggingface.co/datasets/ayousanz/callhome-ja-plus) - ğŸ“¥ 380 / â­ 2 / Japanese Callhome speech files converted to WAV, accompanied by JSONâ€‘formatted metadata arrays and RTMM speaker label files for evaluation.
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - ğŸ“¥ 365 / â­ 4 / Reformatted Japanese subset of the Wiki40B dataset compiled by Mandy Guo, Zihang Dai, and Denny VrandeÄiÄ‡.
 * [sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese) - ğŸ“¥ 338 / â­ 5 / Converted Japanese datasets into SentenceTransformersâ€‘friendly columns, filtering examples by Rerank scores (â‰¥0.7 positive, â‰¤0.3 negative) from multiple HuggingFace sources to support contrastive learning while respecting the original licenses.
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - ğŸ“¥ 323 / â­ 39 / Galgame_Speech_ASR_16kHz is a 16â€¯kHz ASR dataset with 3.75â€¯million pairs (â‰ˆ5,354â€¯h), derived from Galgame_Dataset, released under GPLâ€¯v3.0 with commercial use prohibited and requiring any trained models to be openâ€‘source (citation optional).
 * [JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - ğŸ“¥ 321 / â­ 11 / JMMLU is a Japanese Massive Multitask Language Understanding Benchmark featuring 7,536 teacherâ€‘crafted questions across 56 subjects, including professional medicine, psychology, accounting, philosophy, and diverse highâ€‘school disciplines.
 * [xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - ğŸ“¥ 315 / â­ 6 / Japanese XLâ€‘Sum subset filtered via PaLMâ€‘2 15â€‘gram overlap, containing 4,215 training, 758 validation, and 766 test examples.
 * [RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA) - ğŸ“¥ 304 / â­ 33 / Allganize RAG Leaderboard publishes Japanese RAG performance data and automated endâ€‘toâ€‘end evaluation results across five industry domainsâ€”finance, telecom, manufacturing, public sector, and retailâ€”to help companies benchmark parser, retrieval and generation components where no comprehensive Japanese benchmark yet exists.
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - ğŸ“¥ 293 / â­ 2 / Huggingâ€¯Face mirror of the ABEJAâ€¯CCâ€‘JA dataset from AWS Openâ€¯Data, with details posted on ABEJAâ€™s tech blog.
 * [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) - ğŸ“¥ 275 / â­ 4 / Dataset of dialogues and lore from the Fate/Stayâ€¯Night character â€œEmiliaâ€, formatted for training and evaluating conversational language models.
 * [wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - ğŸ“¥ 265 / â­ 4 / Range3â€™sâ€¯wikipediaâ€‘jaâ€‘20230101 repository offers Parquet files containing only Japanese Wikipedia text, extracted from the full Wikipedia dataset and generated with Python code.
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - ğŸ“¥ 262 / â­ 5 / Japanese JaQuAD, a subset of QGâ€‘Bench, provides sentenceâ€‘ and paragraphâ€‘level data with highlighted answer tokens for training Japanese questionâ€‘generation models, evaluated by BLEU4, METEOR, ROUGEâ€‘L, BERTScore, and MoverScore.
 * [kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en) - ğŸ“¥ 260 / â­ 10 / A Japaneseâ€‘toâ€‘English parallel corpus translating the kaken subset of llmâ€‘jpâ€‘corpusâ€‘v3 with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, featuring custom translation columns and licensed under CCâ€‘BYâ€‘4.0.
 * [JGLUE](https://huggingface.co/datasets/llm-book/JGLUE) - ğŸ“¥ 257 / â­ 15 / Dataset card for the JGLUE dataset used in the book â€œLarge Language Model Introduction,â€ sourced from the original repo, with code licensed CCâ€¯BYâ€‘SAâ€¯4.0, data under the distributorâ€™s license, citing Kurihara & Kawahara (in Japanese), and built on Shunsuke Kitadaâ€™s repository.
 * [jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - ğŸ“¥ 257 / â­ 7 / JHumanEval is a handâ€‘translated Japanese version of the HumanEval benchmark, providing 164 Python programming problems with parallel English and Japanese comments to evaluate Japaneseâ€‘LLM code generation while preserving the original English errors.
 * [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) - ğŸ“¥ 252 / â­ 6 / Artificial voice dataset created with VOICEVOX from the ITA, Tsukuyomiâ€‘chan, and ROHAN corpora, containing 445,793 WAV files totaling 577â€¯hâ€¯51â€¯mâ€¯23â€¯s.
 * [JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - ğŸ“¥ 251 / â­ 19 / A Japanese QA dataset for evaluating Retrievalâ€‘Augmented Generation (RAG), built from JAQKET questions and Wikipedia passages with gold retrievalâ€‘relevance labels, released on HuggingFace and GitHub and scored primarily by nDCG@10.
 * [wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - ğŸ“¥ 244 / â­ 9 / A dataset card for llmâ€‘book/wrimeâ€‘sentiment offering a binary Japanese sentiment analysis set derived from WRIME, labeled as positive or negative based on Avg. Readers_Sentiment (with an option to include neutral cases), and intended as sample data for the book â€œIntroduction to Large Language Models.â€
 * [JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA) - ğŸ“¥ 239 / â­ 3 / Japanese Explainable Multiâ€‘hop Question Answering dataset featuring questions, answers, and stepâ€‘byâ€‘step derivations linking Wikipedia articles, with updated derivation formatting and multiple version releases.
 * [bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja) - ğŸ“¥ 225 / â­ 3 / BBHâ€‘ja provides a Japanese translation of the BIGâ€‘Bench Hard dataset, offering evaluation problems in JSONâ€‘L (input, correct target) and Chainâ€‘ofâ€‘Thought prompts in YAML (input, target), translated using the PLaMo model.
 * [jhle](https://huggingface.co/datasets/llm-jp/jhle) - ğŸ“¥ 209 / â­ 96 / Japaneseâ€‘translated Humanityâ€™s Last Exam dataset curated by LLMâ€‘jp, which omits image questions, samples five per raw_subject, is machineâ€‘translated and expertâ€‘reviewed, authored by Yujiâ€¯Tamakoshi,â€¯Koutaâ€¯Nakayama andâ€¯Yusukeâ€¯Miyao, and must never be used in training corpora.
 * [llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions) - ğŸ“¥ 207 / â­ 5 / llmâ€‘jpâ€‘instructions is a manually curated Japanese instruction dataset offering train, dev, and test splits for languageâ€‘model fineâ€‘tuning.
 * [JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - ğŸ“¥ 205 / â­ 11 / JaQuAD is a 2022 Japanese QA dataset of 39,696 SQuADâ€‘style extractive pairs from Wikipedia, totaling 73.2â€¯MB, that achieves 78.92â€¯% F1 (63.38â€¯% EM) when fineâ€‘tuned with BERTâ€‘Japanese.
 * [JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR) - ğŸ“¥ 205 / â­ 4 / JFWIR is a 64â€‘millionâ€‘pair Japanese IR dataset built from finewebâ€‘2â€‘edu web content, offering seven query types and hard negatives that raise benchmark scores on JQaRA, MIRACL(ja), jsquad and JaCWIR.
 * [livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ğŸ“¥ 188 / â­ 4 / Dataset card details the llm-book/ner-wikinews-dataset, a cleaned collection of livedoor News articles under CCâ€¯BYâ€‘NDâ€¯2.1â€¯JP used in the book *Introduction to Large Language Models* and supplied by LONWIIT.
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - ğŸ“¥ 178 / â­ 12 / Elite Voice Project compiles Hololive VTuber Sakuraâ€¯Mikoâ€™s audio from Twitch, Twitter and YouTube into a train/testâ€‘organized dataset for speechâ€‘recognition research, using Gitâ€‘LFS, licensed under Hololiveâ€™s fanâ€‘content rules and welcoming community contributions.
 * [Japanese-RAG-Generator-Benchmark](https://huggingface.co/datasets/neoai-inc/Japanese-RAG-Generator-Benchmark) - ğŸ“¥ 177 / â­ 4 / Japanese RAG Generator Benchmark (Jâ€‘RAGBench) supplies a multiâ€‘category QA datasetâ€”covering Integration, Reasoning, Logical, Table, and Abstentionâ€”designed to evaluate Japanese RAG generators, built with human effort and GPTâ€‘4.1, and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - ğŸ“¥ 174 / â­ 5 / JAQKET is a Japanese openâ€‘domain QA dataset derived from Wikipedia, offering versionâ€¯1.0 with multipleâ€‘choice quiz questions (13,061 training, 271 validation examples) and versionâ€¯2.0 with only question prompts requiring extracted answers (2,154 training, 1,164 validation), designed to facilitate research on QA systems.
 * [AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully) - ğŸ“¥ 172 / â­ 48 / AnswerCarefully Dataset supplies Japanese and multilingual data for commercial or nonâ€‘commercial LLM safety enhancement, prohibits any other useâ€”including safety circumventionâ€”allows derivative works with attribution, and carries a creator disclaimer of nonâ€‘liability for harms or service changes.
 * [gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset) - ğŸ“¥ 169 / â­ 2 / A dataset of 64,139 Japanese names labeled with biological genderâ€”presented in kanji, hiragana, and romajiâ€”whose 44.9â€¯k training, 6.41â€¯k validation, and 12.8â€¯k test split earned acceptance at ISDAâ€™23.
 * [WAON](https://huggingface.co/datasets/speed/WAON) - ğŸ“¥ 169 / â­ 2 / WAON is a largeâ€‘scale, highâ€‘quality Japanese imageâ€‘text pair dataset for visionâ€‘language models, built through size and SigLIPâ€‘score filtering and deduplication on URLs, captions, and perceptual hashes, and licensed under Apacheâ€¯2.0 with use limited to informational analysis under Japanese law.
 * [anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0) - ğŸ“¥ 159 / â­ 21 / AIâ€‘generated anime illustrations with English prompts and Phiâ€‘3 Visionâ€‘derived captions (English and Japanese) released into the public domain for free use.
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - ğŸ“¥ 159 / â­ 3 / AnimuSubtitleâ€‘JP hosts Japanese ASS/SSA subtitle datasets (data_ass, data_TS) that can be parsed with Pythonâ€™sâ€¯ass library or edited in Aegisub, and is released under an ODCâ€‘By license.
 * [auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - ğŸ“¥ 157 / â­ 24 / AutoWikiQA is Japanâ€™s largest free QA dataset (2â€¯,377â€¯,503 pairs) produced from Wikipedia text using Swallowâ€‘MX and vLLM, delivering diverse, templateâ€‘free questions and answers for knowledge injection and retrievalâ€‘augmented generation.
 * [vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard) - ğŸ“¥ 156 / â­ 40 / The VNTL leaderboard evaluates large language models on translating Japanese visual novels into English by averaging cosineâ€‘similarity scores over 256 samples, ranking preliminary results and benchmarking against tools such as Sugoi Translator, Google Translate, and Naver Papago.
 * [oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - ğŸ“¥ 143 / â­ 26 / Japanese-translated OpenAssistant/oasst1 data with failure flags, ~2,000 manually corrected code translation errors, a released chatâ€‘format subset (oasst1â€‘chatâ€‘44kâ€‘ja), and a script to convert entries into instructionâ€“output pairs for fineâ€‘tuning.
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - ğŸ“¥ 141 / â­ 6 / Deduplicated, NFKCâ€‘normalized mQA queryâ€“passage pairs, with pos_ids/neg_ids mapping to collection indices for direct retrieval via collection[pos_id], and licensed under the original datasetâ€™s terms.
 * [mc4-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/mc4-ja-filter-ja-normal) - ğŸ“¥ 137 / â­ 5 / Dataset card for the â€œmc4â€‘jaâ€‘filterâ€‘jaâ€‘normalâ€ dataset, with additional information pending.
 * [JaMARD](https://huggingface.co/datasets/elyza/JaMARD) - ğŸ“¥ 136 / â­ 10 / A highâ€‘quality synthetic Japanese math problem dataset with verified chainâ€‘ofâ€‘thought reasoning, built by translating PRM800K and GSM8K via Qwen2â€‘7Bâ€‘Instruct and filtering for correctness, available through the HuggingÂ Face datasets library.
 * [wrime](https://huggingface.co/datasets/shunk031/wrime) - ğŸ“¥ 128 / â­ 27 / The WRIME dataset is a Japanese collection of 42,200 posts annotated with Plutchikâ€™s eight emotions for the writer, three readers, and their averages, structured into 40â€¯kâ€‘train, 1.2â€¯kâ€‘validation, and 2â€¯kâ€‘test splits for sentimentâ€‘analysis tasks.
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - ğŸ“¥ 123 / â­ 8 / Umamusume voice transcriptions dataset listing 77 characters with their total audio duration (e.g., East Commerce 799â€¯s, East Imperial Emperor 1074â€¯s, â€¦).
 * [simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon) - ğŸ“¥ 120 / â­ 14 / A simple dataset of Zundamon character settingsâ€”compiled from online sources and admin dataâ€”for testing characterâ€‘LLMs, provided in zmnjp.jsonl and zmn.jsonl formats under a specified license.
 * [Hachi-Alpaca](https://huggingface.co/datasets/HachiML/Hachi-Alpaca) - ğŸ“¥ 120 / â­ 15 / Hachi-Alpaca delivers Japanese synthetic data derived from Stanford Alpaca, refined and verified by mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, used through Deepinfra, with â€œ_cleanedâ€ versions that have passed modelâ€‘based quality checks.
 * [MGSM_ja](https://huggingface.co/datasets/sbintuitions/MGSM_ja) - ğŸ“¥ 120 / â­ 2 / A reproducible scoring and Japaneseâ€‘only clone of the MGSM benchmarkâ€”a multilingual chainâ€‘ofâ€‘thought reasoning datasetâ€”released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - ğŸ“¥ 119 / â­ 16 / Dataset card for **japanese_alpaca_data**, built on masa3141â€™s Japaneseâ€‘Alpacaâ€‘LoRA work, with additional details in the referenced repository.
 * [livedoor-news-corpus](https://huggingface.co/datasets/shunk031/livedoor-news-corpus) - ğŸ“¥ 109 / â­ 7 / Japanese news articles from livedoor News under a CC BYâ€‘ND license are cleaned of HTML, delivered in 6,567 items split 80/10/10 for training, validation, and testing.
 * [JDocQA](https://huggingface.co/datasets/shunk031/JDocQA) - ğŸ“¥ 109 / â­ 10 / JDocQA is a Japanese PDFâ€‘based QA dataset comprising 5,504 documents and 11,600 questionâ€‘answer pairs that test yes/no, factoid, numerical, openâ€‘ended, and unanswerable comprehension using both visual and textual information.
 * [llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval) - ğŸ“¥ 109 / â­ 3 / Dataset card for the jaâ€‘vicunaâ€‘qaâ€‘benchmark used in the book â€œIntroduction to Largeâ€‘Scale LLMÂ IIâ€ and created by llmâ€‘jpâ€‘eval for crossâ€‘dataset Japanese LLM evaluation (ApacheÂ 2.0).
 * [nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - ğŸ“¥ 108 / â­ 2 / A MITâ€‘licensed list of the 100 most frequent Japanese words from the CCâ€‘100 Wikipedia dump, tuned for nagisaâ€™s tokenization and used for stopâ€‘word filtering in preprocessing and feature extraction.
 * [wikipedia-ja-20230720](https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720) - ğŸ“¥ 105 / â­ 13 / Dataset card for the 2023â€‘07â€‘20 release of the Japanese Wikipedia dataset.
 * [JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - ğŸ“¥ 105 / â­ 17 / JAâ€‘VGâ€‘VQAâ€‘500 is a 500â€‘sample subset of the Japanese Visual Genome VQA dataset, licensed CCâ€¯BYâ€¯4.0, used to benchmark EvoVLMâ€‘JPâ€‘v1â€‘7B.
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - ğŸ“¥ 103 / â­ 3 / Dataset of Japanese Boketeâ€‘site humor posts (from CLoTâ€‘Oogiriâ€‘Goâ€¯CVPRâ€¯2024) featuring three tasksâ€”textâ€‘toâ€‘text, imageâ€‘toâ€‘text, and textâ€‘imageâ€‘toâ€‘textâ€”with roughly 600â€¯examples, processed via GPTâ€‘4o OCR and HojiChar filtering.
 * [jsick](https://huggingface.co/datasets/hpprc/jsick) - ğŸ“¥ 101 / â­ 8 / JSICK is a Japanese NLI/STS dataset translated from SICK, offering a stress test that probes wordâ€‘order and caseâ€‘particle handling through multiple transformed sentenceâ€‘pair subsets to support research in multilingual compositional inference.
