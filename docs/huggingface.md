# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1490 models and 655 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## ğŸ“– Contents

Released [a tool ğŸ”](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search) for searching through a large number of repository information.

 * [Models](#models)

 * [Datasets](#datasets)


## ğŸ‰ The latest additions

**Models**
1 models have been added.

- [SIP-med-LLM/SIP-jmed-llm-3-8x13b-AC-32k-instruct](https://huggingface.co/SIP-med-LLM/SIP-jmed-llm-3-8x13b-AC-32k-instruct)


## Models

This list is sorted by downloads as of November 18, 2025.
1490 models are listed.

- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - rinna/japanese-cloob-vit-b-16
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - xlm-roberta-ner-japanese (Japanese caption : æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«)
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - RetrievaEmbedding-01: AMBER The AMBER (Adaptive Multitask Bilingual Embedding Representations) is a text embedding model trained by Retrieva, Inc.
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - BERT base Japanese (IPA dictionary, whole word masking enabled)
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - japanese-gpt-neox-small This repository provides a small-sized Japanese GPT-NeoX model.
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - Whatâ€™s this?
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - rinna/japanese-hubert-base Overview This is a Japanese HuBERT Base model trained by rinna Co.
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - BERT base Japanese (character tokenization)
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - rinna/japanese-clip-vit-b-16
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This is a Japanese sentence-BERT model.
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - DeepSeek-V3-slice-jp64 æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ DeepSeek-V3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã®ä¾‹æ–‡ã‚’å…ƒã«é »å‡ºã™ã‚‹ MoE (Mixture of Experts) ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®expertsã‚’å³é¸ã—ã¦å†æ§‹æˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®ggufç‰ˆã§ã™ã€‚
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese This is a DistilBERT model pre-trained on 131 GB of Japanese web text.
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - hotchpotch/japanese-reranker-xsmall-v2 ã¨ã¦ã‚‚å°ã•ãé€Ÿã„æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers fugashi sentencepiece unidic-lite Then you can load this model and run inference.
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-ner-wikipedia-dataset ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B æ—¥æœ¬èªç‰ˆã®README/Japanese README Model Overview PLaMo-Embedding-1B is a Japanese text embedding model developed by Preferred Networks, Inc.
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japanese æ—¥æœ¬èªã®README/Japanese README GLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This is a Japanese sentence-BERT model.
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - BERT base Japanese (IPA dictionary)
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - åæ¡è›ï¼ˆHotaru Jujoï¼‰ã®ä½œæˆã—ãŸLoRAã‚’é…å¸ƒã—ã¦ã„ã¾ã™ã€‚
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper (v2.0)
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B Model Description PLaMo 2 1B is a 1B model pre-trained on English and Japanese datasets, developed by Preferred Elements, Inc.
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - Model Card for Japanese DeBERTa V2 tiny Model description
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - clip-japanese-base This is a Japanese CLIP (Contrastive Language-Image Pre-training) model developed by LY Corporation.
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
  - llm-jp-3.1-1.8b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - japanese-gpt2-small This repository provides a small-sized Japanese GPT-2 model.
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTæ§˜ã® AXCXEPT/EZO-gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM-Small Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B æ—¥æœ¬èªã®README/Japanese README "Sarashina-Embedding-v1-1B" is a Japanese text embedding model, based on the 1.2B-parameter Japanese LLM "Sarashina2.1-1B".
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 Kotoba-Whisper-v2.2 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - alabnii/jmedroberta-base-sentencepiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - sbert-jsnli-luke-japanese-base-lite This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - japanese-sentiment-analysis This model was trained from scratch on the chABSA dataset.
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 tiny Model description This is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri: Japanese General Text Embeddings Usage Direct Usage (Sentence Transformers)
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - japanese-roberta-base This repository provides a base-sized Japanese RoBERTa model.
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - BERTã«ã‚ˆã‚‹æ—¥æœ¬èªå›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ« BertForTokenClassificationã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã®æ–‡ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ /
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - japanese-gpt-neox-3.6b Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - Sentence BERT base Japanese model This repository contains a Sentence BERT base model for Japanese.
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èª SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [LiquidAI/LFM2-350M-ENJP-MT](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT)
  - Playground Playground Playground Leap ï¼ˆè‹±èªã®å¾Œã«æ—¥æœ¬èªãŒç¶šãï¼‰ LFM2-350M-ENJP-MT Based on the LFM2-350M model, this checkpoint has been fine-tuned for near real-time bi-directional Japanese/English translation of short-to-medium inputs.
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - llm-jp-3-1.8b
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M This repository provides Japanese ModernBERT trained by SB Intuitions.
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - ä¸­æ–‡ | í•œêµ­ì–´ | æ—¥æœ¬èª | Ğ ÑƒÑÑĞºĞ¸Ğ¹ | Deutsch | FranÃ§ais | EspaÃ±ol | PortuguÃªs | TÃ¼rkÃ§e | Tiáº¿ng Viá»‡t | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ultralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - hotchpotch/japanese-reranker-cross-encoder-small-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - hotchpotch/japanese-reranker-cross-encoder-large-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - rinna/japanese-wav2vec2-base Overview This is a Japanese wav2vec 2.0 Base model trained by rinna Co.
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper ğŸ¤—ğŸ¤ğŸ“
- [NandemoGHS/Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B)
  - Anime-Llasa-3B Overview This is the Anime-Llasa-3B, a Text-to-Speech (TTS) model fine-tuned for Japanese.
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE Model description Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2 reazonspeech-nemo-v2 is an automatic speech recognition model trained on ReazonSpeech v2.0 corpus.
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - llm-jp-3-7.2b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.3-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - sbintuitions/sarashina2.2-3b-instruct-v0.1 Model Summary
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - japanese-gpt2-medium This repository provides a medium-sized Japanese GPT-2 model.
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
- [mmnga/K2-Think-gguf](https://huggingface.co/mmnga/K2-Think-gguf)
  - K2-Think-gguf LLM360ã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹K2-Thinkã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet TDT-CTC 0.6B (ja) | | parakeet-tdt_ctc-0.6b-ja is an ASR model that transcribes Japanese speech with Punctuations.
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹phi-4-deepseek-R1K-RL-EZOã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - nagisa_bert A BERT model for nagisa.
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - bilingual-gpt-neox-4b Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
- [reazon-research/japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh)
  - japanese-wav2vec2-base-rs35kh This model is a wav2vec 2.0 Base fine-tuned on the large-scale Japanese ASR corpus ReazonSpeech v2.0.
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹deepseek-r1-distill-qwen2.5-bakeneko-32bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [NandemoGHS/Anime-Llasa-3B-Captions](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-Captions)
  - Anime-Llasa-3B-Captions Overview This is Anime-Llasa-3B-Captions, a Text-to-Speech (TTS) model fine-tuned for Japanese, based on NandemoGHS/Anime-Llasa-3B.
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This is a Japanese sentence-LUKE model.
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
  - llm-jp-3.1-13b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - sbintuitions/sarashina2.2-0.5b-instruct-v0.1 Model Summary
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - BERT base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - rinna/youri-7b Overview We conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-Japanese-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B-GGUF Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - shisa-gamma-7b-v1 For more information see our main Shisa 7B model We applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-8B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [mmnga/plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf)
  - plamo-2-translate-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹plamo-2-translateã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instruct Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - umiyuki-Umievo-itr012-Gleipnir-7B-gguf umiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Umievo-itr012-Gleipnir-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - japanese-large-lm-3.6b
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - roberta-base-japanese-jsnli This model is a fine-tuned version of nlp-waseda/roberta-base-japanese on the JSNLI dataset.
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM-Large Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base ğŸ“„ Paper | ğŸ§‘â€
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - gpt-neox-japanese-2.7b
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 Kotoba-Whisper-v2.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯DeepSeek-R1-Distill-Qwen-14Bã‚’æ—¥æœ¬èªã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B-chat Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-32B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - llm-jp-3-1.8b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - DeBERTa V2 base Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual (v1.0)
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B (CALM2-7B)
- [webbigdata/VoiceCore](https://huggingface.co/webbigdata/VoiceCore)
  - News VoiceCoreãŒGENIACãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ(çµŒæ¸ˆç”£æ¥­çœã€NEDO)ã®å›½ç”£åŸºç›¤ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã«æ²è¼‰ã•ã‚Œã¾ã—ãŸã€‚
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - japanese-gpt-neox-3.6b-instruction-sft Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [Aratako/NemoAurora-RP-12B-GGUF](https://huggingface.co/Aratako/NemoAurora-RP-12B-GGUF)
  - NemoAurora-RP-12B-GGUF æ¦‚è¦ Aratako/NemoAurora-RP-12Bã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - æ—¥æœ¬èªã¯ã“ã¡ã‚‰ lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese Deepseek's R1 models are excellent, state-of-the-art reasoning models which have been trained to work bilingually, with English and Chinese.
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - ã€å‘ŠçŸ¥ã€‘chilled_remixåŠã³reversemixã¯2023å¹´5æœˆ21æ—¥ã«Versionå¤‰æ›´ã‚’è¡Œã„ã€v2ã¸ç§»è¡Œã„ãŸã—ã¾ã—ãŸã€‚
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fast Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - weblab-10b-instruction-sft-GPTQ Original model weblab-10b-instruction-sft which is a Japanese-centric multilingual GPT-NeoX model of 10 billion parameters created by matsuo-lab Takeshi Kojima.
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Tanuki-8Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„1.3Tãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸç´„8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - Model Card for Japanese DeBERTa V3 base Model description This is a Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0.
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - japanese-gpt-neox-3.6b-instruction-ppo Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - bert-finetuned-japanese-sentiment This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5)
  - Llama 3.1 Swallow v0.5 - Built with Llama Llama 3.1 Swallow v0.5 is a large language model (8B) that was built by continual pre-training on the Meta Llama 3.1 model.
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - japanese-large-lm-3.6b-instruction-sft
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - rinna/nekomata-14b Overview We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - hotchpotch/japanese-reranker-tiny-v2 ã¨ã¦ã‚‚å°ã•ãé€Ÿã„æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow - Built with Llama Llama 3.3 Swallow is a large language model (70B) that was built by continual pre-training on the Meta Llama 3.3 model.
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - Polyglot-math-4x7b-24b Polyglot-4x7b is a Mixture of Experts approach to a multilingual model.
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B (rinna/llama-3-youko-8b)
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B Model Description ELYZA-Thinking-1.0-Qwen-32B is a reasoning model trained by ELYZA, Inc.
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B Heron-NVILA-Lite-2B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - japanese-gpt-neox-3.6b-instruction-sft-v2 Overview
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - llm-jp-3-13b-instruct This repository provides large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - hotchpotch/japanese-reranker-cross-encoder-base-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - Qwen3-30B-A3B-ERP-v0.1-GGUF æ¦‚è¦ Aratako/Qwen3-30B-A3B-ERP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - japanese-large-lm-1.7b-instruction-sft This repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - google/gemma-3-12b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-12b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese..
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Finetuned Waseda RoBERTa to evaluate the generated answers on JTruthfulQA.
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - Model card for model ID
- [llm-jp/llm-jp-3.1-13b](https://huggingface.co/llm-jp/llm-jp-3.1-13b)
  - llm-jp-3.1-13b LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - sbintuitions/sarashina2.2-1b-instruct-v0.1 Model Summary
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - japanese-large-lm-1.7b This repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - rinna/nekomata-7b Overview We conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
- [Aratako/MistralPrism-24B-GGUF](https://huggingface.co/Aratako/MistralPrism-24B-GGUF)
  - MistralPrism-24B-GGUF æ¦‚è¦ Aratako/MistralPrism-24Bã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - Qwen3-8B-ERP-v0.1-GGUF æ¦‚è¦ Aratako/Qwen3-8B-ERP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - stockmark/gpt-neox-japanese-1.4b This repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta RoSEtta (RoFormer-based Sentence Encoder through Distillation) is a general Japanese text embedding model, excelling in retrieval tasks.
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - Model card for model ID
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow - Built with Llama Llama 3.3 Swallow is a large language model (70B) that was built by continual pre-training on the Meta Llama 3.3 model.
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - bilingual-gpt-neox-4b-instruction-ppo Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B Model Description
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct Stockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
- [sbintuitions/sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b)
  - Sarashina-Embedding-v2-1B æ—¥æœ¬èªã®README/Japanese README "Sarashina-Embedding-v2-1B" is a Japanese text embedding model, based on the Japanese LLM "Sarashina2.2-1B".
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - nlp-waseda/roberta-large-japanese-seq512 Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL++-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
- [llm-jp/llm-jp-3-440m](https://huggingface.co/llm-jp/llm-jp-3-440m)
  - llm-jp-3-440m LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - llm-jp-3-8x1.8b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-8x1.8b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B Model Description PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese open datasets, developed by Preferred Networks, Inc.
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF Original Model elyza/ELYZA-japanese-Llama-2-13b-fast-instruct Run with LlamaEdge LlamaEdge version: v0.2.8 and above Prompt template Prompt type: llama-2-chat Prompt string &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ user_msg_1 }}
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumerã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Reflection-Llama-3.1-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-14B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [tatsuyaaaaaaa/gpt-oss-20b-gguf](https://huggingface.co/tatsuyaaaaaaa/gpt-oss-20b-gguf)
  - OpenAIã®gpt-oss-20bã‚’ggufå¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-7B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - bert-base-japanese-v3-jnli ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(è‡ªç„¶è¨€èªæ¨è«–)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B-instruct Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B Heron-NVILA-Lite-1B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - Ninja-v1-NSFW-128k-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFW-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, with additional postprocessing stacks integrated as pipeline.
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M This repository provides Japanese ModernBERT trained by SB Intuitions.
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - japanese-gpt2-xsmall
- [Tetsuo3003/ner-medical-japanese](https://huggingface.co/Tetsuo3003/ner-medical-japanese)
  - ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª åŒ»ç™‚ä¼šè©± NER ãƒ¢ãƒ‡ãƒ« ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¥æœ¬èªã®åŒ»ç™‚ä¼šè©±æ–‡æ›¸ã«ç‰¹åŒ–ã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - Japanese SimCSE (BERT-base)
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - ã‹ã‚‰ã¾ã‚‹ Llama-3-Karamaru-v1 Karamaru is a conversational AI model developed by Sakana AI that responds in the style of Edo-period Japanese.
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - llm-jp-3-8x13b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-base Fine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - deberta-base-japanese-aozora-ud-head Model Description
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-Qwen2.5-7b-Japanese-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - BERT base Japanese (character tokenization, whole word masking enabled)
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-QwQ32b-Reasoning-Japanese-v1.0ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B sarashina2.2â€‘0.5bâ€‘instructâ€‘v0.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«å­¦ç¿’ã—ãŸTTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - Model Description llava-calm2-siglip is an experimental Vision Language Model that can answer questions in Japanese about images.
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - æ—¥æœ¬èªByT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - llm-jp-3-8x13b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-8x13b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 ABEJA-QwQ32b-Reasoning-Japanese-v1.0ã¯abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1(*)
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹mathstral-7B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - Model Card for Japanese character-level GPT-2 Small Model description This is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
  - llm-jp-3.1-8x13b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.2ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-ELYZA-JP-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - nlp-waseda/roberta-base-japanese Model description This is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - llm-jp-3-13b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-35Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - cyberagent-open-calm-7b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ï¼Ÿ
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B (rinna/qwq-bakeneko-32b)
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B "A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XL Model Description japanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
- [tatsuyaaaaaaa/LFM2-VL-3B-gguf](https://huggingface.co/tatsuyaaaaaaa/LFM2-VL-3B-gguf)
  - LiquidAIã®LFM2-VL-3Bã‚’ggufå¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - stockmark/stockmark-13b Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini Model Description RakutenAI-2.0-mini is a lightweight Japanese language model trained from scratch using a transformer architecture, designed for efficient performance in resource-constrained environments.
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - llm-jp-3-3.7b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1 static quants are available at https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - qwen2.5-bakeneko-32b-instruct-v2-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwen2.5-bakeneko-32b-instruct-v2ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1 shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T Base Model Description This is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - (ç®€ä½“ä¸­æ–‡|English|æ—¥æœ¬èª) Introduction github repo : https://github.com/FunAudioLLM/SenseVoice SenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED).
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Japanese-Instruct-2408ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ About this model.
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KillerWhaleã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-base-japanese-with-auto-jumanpp Model description
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-2-2b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - gemma-3-4b-it-RP-v0.1-GGUF æ¦‚è¦ Aratako/gemma-3-4b-it-RP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - Japanese to Korean translator Japanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - r1-1776-distill-llama-70b-gguf perplexity-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹r1-1776-distill-llama-70bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 ABEJA-Qwen2.5-7b-Japanese-v0.1ã¯Qwen/Qwen2.5-7B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®å­¦ç¿’ã‚’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
- [llm-jp/llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b)
  - llm-jp-3.1-1.8b LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 ABEJA-Qwen2.5-32b-Japanese-v0.1ã¯Qwen/Qwen2.5-32B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªä¸­å¿ƒã¨ã—ãŸç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF æ¦‚è¦ Aratako/calm3-22b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF This is quantized version of cyberagent/Mistral-Nemo-Japanese-Instruct-2408 created using llama.cpp Original Model Card Mistral-Nemo-Japanese-Instruct-2408 Model Description
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠåˆ†ã»ã©ãŒæ—¥æœ¬èªãªã®ã§magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªã«ã¯å¼·ã„ã¯ãšï¼Ÿ
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FINGU-AI/FinguAI-Chat-v1 Overview The FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - whisper-large-v3-japanese-4k-steps This model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL 2408 260M DanbotNL is translator that tranaslates from natural languages into Danbooru tags.
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Vecteus-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - bert-japanese_finetuned-sentiment-analysis This model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-ArrowSE-8B-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Japanese-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8b-Cosmopedia-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - hotchpotch/japanese-bge-reranker-v2-m3-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This is for (private) DEMO only.
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese License MIT License ğŸ‘‰ DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf ã“ã£ã¡ã®ãŒã„ã„ã‹ã‚‚ğŸ‘‰ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - Qwen3-30B-A3B-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-30B-A3Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - Japanese StableLM-3B-4E1T Instruct Model Description
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - oldï¼Ÿ
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
  - ABEJA-Qwen2.5-32b-Japanese-v1.0 ABEJA-Qwen2.5-32b-Japanese-v1.0ã¯Qwen/Qwen2.5-32B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªä¸­å¿ƒã¨ã—ãŸç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1ã«å¯¾ã—ã¦SFTã¨DPOã«ã‚ˆã‚‹äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
- [mmnga/Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf)
  - Qwen3-EZO-8B-beta-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-EZO-8B-betaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ about this model.
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - suzume_mix_v1.0ï¼ˆflux.1 ç³»ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼‰ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€flux1-dev ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€è¤‡æ•°ã®LoRAã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ–ãƒ¬ãƒ³ãƒ‰ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - bert-base-japanese-v3-unsup-simcse-jawiki ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬8ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [llm-jp/llm-jp-3-7.2b](https://huggingface.co/llm-jp/llm-jp-3-7.2b)
  - llm-jp-3-7.2b
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - Model Card for Japanese DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - calm3-22b-RP-v2-GGUF æ¦‚è¦ Aratako/calm3-22b-RP-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-multilingualã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-base - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-base StableLM
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf Googleã•ã‚“ã®gemma-2-2b-jpn-itã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - Llama-3-ELYZA-JP-8B-AWQ Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - cogito-v1-preview-qwen-32B-gguf deepcogitoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹cogito-v1-preview-qwen-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - llm-jp-3-980m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - wav2vec2-base-asr
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B Sarashina2-Vision-8B is a Japanese Large Vision Language Model trained by SB Intuitions.
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MS-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Aratako/Amaterasu-123B-GGUF](https://huggingface.co/Aratako/Amaterasu-123B-GGUF)
  - Amaterasu-123B-GGUF æ¦‚è¦ Aratako/Amaterasu-123Bã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - llm-jp-3-7.2b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-7.2b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-70b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - Japanese Parler-TTS Mini ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-mini-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplplã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Instruct-Jaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-70b-chat-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-13b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - qwq-bakeneko-32b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwq-bakeneko-32bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct Model Description PLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - Phi-4-reasoning-plus-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-4-reasoning-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLM-13B-instruct-gguf Fugaku-LLMã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Fugaku-LLM-13B-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - llm-jp-3-440m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-T2-2B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - ğŸ“‘ Paper | ğŸ¤—
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-Preferred-MedSwallow-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - sarashina2.2-3b-instruct-v0.1-gguf sbintuitionsã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.2-3b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - llm-jp-3-150m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Jpã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - Model Card for gemma-2-2b-jpn-it-translate-gguf gemma-2-2b-jpn-it-translate-ggufã¯ã€æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸSLMï¼ˆSmall Language Modelï¼‰ã§ã™ã€‚
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC Model Description PLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - ELYZA-Shortcut-1.0-Qwen-7B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-Shortcut-1.0-Qwen-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - ascktgcc/Mistral-nemo-ja-rp-v0.2ã®GGUFç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - llm-jp-3-980m LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - Swallow Education Classifier Japanese README Model summary This repository contains fastText classifiers for judging the educational value of Japanese web pages.
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA-japanese-CodeLlama-7b-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunChoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-novel-gpt-j-6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/karakuri-vl-32b-instruct-2507-gguf](https://huggingface.co/mmnga/karakuri-vl-32b-instruct-2507-gguf)
  - karakuri-vl-32b-instruct-2507-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-vl-32b-instruct-2507ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - llm-jp-3-13b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-13b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [stockmark/Stockmark-2-VL-100B-beta](https://huggingface.co/stockmark/Stockmark-2-VL-100B-beta)
  - Stockmark-2-VL-100B-beta Introduction Stockmark-2-VL-100B-beta is a 100-billion-parameter Japanese-specialized visual language model with Chain-of-Thought (CoT) reasoning for document reading comprehension.
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - mt5_summarize_japanese (Japanese caption : æ—¥æœ¬èªã®è¦ç´„ã®ãƒ¢ãƒ‡ãƒ«)
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 "A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
  - Gemma-2-Llama-Swallow-9b-it-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Gemma-2-Llama-Swallow-9b-it-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotæ§˜ã® Llama3-ArrowSE-8B-v0.3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Kotajiro/jmix-qwenimage-lora](https://huggingface.co/Kotajiro/jmix-qwenimage-lora)
  - Qwen-Imageç”¨ã®LoRAã§ã™ã€‚
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - RakutenAI-2.0-mini-instruct-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-2.0-mini-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository contains some GGUF quantizations of the merge of the VNTL LLaMA 3 8B qlora.
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - BERT Base Japanese for Irony
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èª SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Parakeet-Inc/furigana_whisper_small_jsut](https://huggingface.co/Parakeet-Inc/furigana_whisper_small_jsut)
  - æ¦‚è¦ æ—¥æœ¬èªã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€æ›¸è¨˜ç´ åˆ—ï¼ˆæ¼¢å­—ä»®åäº¤ã˜ã‚Šæ–‡ï¼‰ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å…¥ã‚Œã‚‹ã“ã¨ã§ã€æ›¸è¨˜ç´ åˆ—ã¨æ•´åˆæ€§ã®ã‚ã‚‹ãƒ¢ãƒ¼ãƒ©åˆ—ï¼ˆã‚«ã‚¿ã‚«ãƒŠåˆ—ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹datagemma-rag-27b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfin-inst-mergeã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - æ—¥æœ¬èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ« This is a CLIP text/image encoder model for Japanese.
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-instruct - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-instruct StableLM
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - Model Card for Japanese BART base Model description This is a Japanese BART base model pre-trained on Japanese Wikipedia.
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - recruit-jp/japanese-clip-vit-b-32-roberta-base Overview Developed by: Recruit Co.
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - llm-jp-3-150m LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - æ¦‚è¦ Imatrixã«ã¯neody/imatrix_datasetã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B Heron-NVILA-Lite-15B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - About static quants of https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1 weighted/imatrix quants are available at https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [John1604/John1604-strongperson-negotiate-japanese-gguf](https://huggingface.co/John1604/John1604-strongperson-negotiate-japanese-gguf)
  - Use the model in ollama First download and install ollama.
- [hotchpotch/japanese-reranker-base-v2](https://huggingface.co/hotchpotch/japanese-reranker-base-v2)
  - hotchpotch/japanese-reranker-base-v2 æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - Additional pretrained BERT base Japanese finance This is a BERT model pretrained on texts in the Japanese language.
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [mmnga/llm-jp-3-3.7b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-3.7b-instruct3-gguf)
  - llm-jp-3-3.7b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-3.7b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - About static quants of https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese weighted/imatrix quants seem not to be available (by me) at this time.
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Japanese-Chat-Umievo-itr001-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - Stanza model for Japanese (ja)
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M This repository provides Japanese ModernBERT trained by SB Intuitions.
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - karakuri-lm-32b-thinking-2501-exp-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-32b-thinking-2501-expã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - Qwen2.5 Bakeneko 32B Instruct V2 GGUF (rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - Moonlight-16B-A3B-Instruct-gguf moonshotaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Moonlight-16B-A3B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - Mistral-Nemo-Japanese-Instruct-2408 Model Description
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - llm-book/t5-base-long-livedoor-news-corpus ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬7ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹è¦ç´„ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleæ§˜ã® google/gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - recruit-jp/japanese-typo-detector-roberta-base ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ æ—¥æœ¬èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨å„æ–‡å­—ã”ã¨ã«èª¤å­—è„±å­—ã§ã‚ã‚‹ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¾ã™ å„ãƒ©ãƒ™ãƒ«ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ id label meaning 0 OK èª¤å­—ãªã— 1 deletion 1æ–‡å­—ã®æŠœã‘ 2 insertion_a ä½™åˆ†ãª1æ–‡å­—ã®æŒ¿å…¥ 3 insertion_b ç›´å‰ã®æ–‡å­—åˆ—ã¨ä¸€è‡´ã™ã‚‹ï¼’æ–‡å­—ä»¥ä¸Šã®ä½™åˆ†ãªæ–‡å­—ã®æŒ¿å…¥ 4 kanji-conversion_a åŒä¸€ã®èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰ 5 kanji-conversion_b è¿‘ã„èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰ 6 substitution 1æ–‡å­—ã®å…¥ã‚Œæ›¿ãˆ 7 transposition éš£æ¥ã™ã‚‹ï¼’æ–‡å­—é–“ã®è»¢ç½® 8 others ãã®ä»–ã®å…¥åŠ›èª¤ã‚Š èª¤ã‚Šç¨®é¡ã®è©³ç´°ã«ã¤ã„ã¦ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…ƒè«–æ–‡ã‚’ã”å‚ç…§ãã ã•ã„ æ—¥æœ¬èª Wikipedia ã®ç·¨é›†å±¥æ­´ã«åŸºã¥ã å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ ãã®ä»–ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯å½“ç¤¾ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„ èª¤å­—è„±å­—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Hubã«å…¬é–‹ã—ã¾ã—ãŸ (Re
- [mmnga/plamo-2-8b-gguf](https://huggingface.co/mmnga/plamo-2-8b-gguf)
  - plamo-2-8b-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹plamo-2-8bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Qwen3-4B-Instruct-2507-gguf](https://huggingface.co/mmnga/Qwen3-4B-Instruct-2507-gguf)
  - Qwen3-4B-Instruct-2507-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-4B-Instruct-2507ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [future-architect/Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B)
  - Llama 3.1 Future Code Ja Llama 3.1 Future Code Ja is a large language model with 8B parameters built on top of the Meta Llama 3.1 model.
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - llm-jp-3-3.7b
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf)
  - sarashina2.2-0.5b-instruct-v0.1-gguf sbintuitionsã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.2-0.5b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Commonã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - One more step before getting this model.
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - About static quants of https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese weighted/imatrix quants seem not to be available (by me) at this time.
- [prj-beatrice/japanese-hubert-base-phoneme-ctc-v3](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc-v3)
  - japanese-hubert-base-phoneme-ctc-v3 rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - Llama-3.1-70B-Japanese-Instruct-2407 Model Description This is a Japanese continually pre-trained model based on meta-llama/Meta-Llama-3.1-70B-Instruct.
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - WabiSabi-V1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹WabiSabi-V1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - wanabi-24B wanabi-24B ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmæ§˜ã® Llama-3-Swallow-8B-Instruct-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - hubert-large-asr
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - google/gemma-3-27b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-27b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese.https://huggingface.co/dahara1/imatrix-jpn-test).
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - gpt2-large-japanese This repository provides a large sized Japanese GPT-2 model.
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - Uploaded model Developed by: nappa0326 License: apache-2.0 Finetuned from model : elyza/Llama-3-ELYZA-JP-8B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama-3-ELYZA-JP-8Bã‚’ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - Japanese GPT2 Lyric Model Model description
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA-japanese-Llama-2-7b-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - DeepSeek-R1-Distill-Qwen-7B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [dahara1/gemma-3-270m_mitsuki_gguf](https://huggingface.co/dahara1/gemma-3-270m_mitsuki_gguf)
  - dahara1/gemma-3-270m_mitsuki_gguf éå¸¸ã«è»½é‡ãªSLMã€gemma-3-270mã‚’å¾®èª¿æ•´ã—ã€ãƒãƒ£ãƒƒãƒˆç”¨ã€é…ä¿¡ã®ãŠä¾›ç”¨ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»˜ã‘ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€æ›´ã«æ§˜ã€…ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§å‹•ãã‚ˆã†ã«ggufåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-1.7b-instruction-sft line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Tanuki-8x8Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„1.7Tãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸ8x8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç´„47Bã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç´„13Bï¼‰ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mradermacher/Japanese-Receipt-VL-3B-JSON-GGUF](https://huggingface.co/mradermacher/Japanese-Receipt-VL-3B-JSON-GGUF)
  - About static quants of https://huggingface.co/sabaridsnfuji/Japanese-Receipt-VL-3B-JSON For a convenient overview and download list, visit our model page for this model.
- [LiquidAI/LFM2-350M-PII-Extract-JP](https://huggingface.co/LiquidAI/LFM2-350M-PII-Extract-JP)
  - Playground Playground Playground Leap ï¼ˆæ—¥æœ¬èªã¯ã“ã¡ã‚‰ã‹ã‚‰ï¼‰ LFM2-350M-PII-Extract-JP Based on LFM2-350M, this checkpoint is designed to extract personally identifiable information (PII) from Japanese text and output it in JSON format.
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Large-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - rinna/japanese-hubert-large Overview This is a Japanese HuBERT Large model trained by rinna Co.
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - QwQ-32B-Preview-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹QwQ-32B-Previewã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - bert-base-japanese-wikipedia-ud-head Model Description
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - Japanese-Starling-ChatV-7B-GGUF GGUF conversion of "Japanese-Starling-ChatV-7B" "Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
- [dahara1/gemma-3-1b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-1b-it-qat-japanese-imatrix)
  - google/gemma-3-12b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-12b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese..
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - WRIME-fine-tuned BERT base Japanese This model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
  - llm-jp-3.1-13b-instruct4-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3.1-13b-instruct4ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [l0wgear/manga-ocr-2025-onnx](https://huggingface.co/l0wgear/manga-ocr-2025-onnx)
  - Manga OCR (ONNX)
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - japanese-stablelm-2-instruct-1_6b-gguf stabilityaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-stablelm-2-instruct-1_6bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªç‰ˆã¯ã¾ã ä½œæˆä¸­ã§ã™ã€‚
- [prj-beatrice/japanese-hubert-base-phoneme-ctc](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc)
  - japanese-hubert-base-phoneme-ctc rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - Finance Sentiment JA (base) Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
- [hotchpotch/japanese-reranker-small-v2](https://huggingface.co/hotchpotch/japanese-reranker-small-v2)
  - hotchpotch/japanese-reranker-small-v2 å°ã•ã‚ã§é€Ÿã„ã€æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
- [jzhang533/PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga)
  - PaddleOCR-VL-For-Manga Model Description PaddleOCR-VL-For-Manga is an OCR model enhanced for Japanese manga text recognition.
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - qwen2.5-bakeneko-32b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwen2.5-bakeneko-32b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - kurumi_flux_lora_v1.0ï¼ˆflux.1 ç³»çµ±ï¼‰ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€flux1.
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - RakutenAI-2.0-8x7B-instruct-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-2.0-8x7B-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - lightblue-suzume-llama-3-8B-japanese-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-RobinHoodã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja Model
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Honyaku-13bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [grapevine-AI/plamo-2-translate-gguf](https://huggingface.co/grapevine-AI/plamo-2-translate-gguf)
  - caution!
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT ElanMT-BT-ja-en is a Japanese to English translation model developed by ELAN MITSUA Project / Abstract Engine.
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - æœ¬ggufãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ about this gguf model gemma-2-2b-itã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹é‡è¦åº¦è¡Œåˆ—(iMatrix)ã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸggufç‰ˆã§ã™ã€‚
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - Atotti/RakutenAI-2.0-mini-instruct-gguf æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Rakuten/RakutenAI-2.0-mini-instruct ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€llama.cpp ã‚„ text-generation-webui ç­‰ã®ãƒ„ãƒ¼ãƒ«ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã« GGUF å½¢å¼ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation on MIRACL japanese These models don't train on the MIRACL training data.
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - wanabi-24B (preview) wanabi-24B ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã® ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ (preview) ã§ã™ã€‚
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-14Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - alabnii/jmedroberta-base-sentencepiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaæ§˜ã® rinna/gemma-2-baku-2b-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - RetrievaEmbedding-01: AMBER The AMBER (Adaptive Multitask Bilingual Embedding Representations) is a text embedding model trained by Retrieva, Inc.
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - â—†QuinceMix "Defacta"ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - Ninja-v1-NSFW-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFWã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Kotajiro/anzu-qwen-lora](https://huggingface.co/Kotajiro/anzu-qwen-lora)
  - Qwen-Imageç”¨ã®LoRAã§ã™ã€‚
- [tohoku-nlp/bybert-jp-v2-100m](https://huggingface.co/tohoku-nlp/bybert-jp-v2-100m)
  - (English part follows Japanese one.
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - Japanese Natural Language Inference Model
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - Model card for model ID
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Qwen2.5-72B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408 static quants are available at https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39ã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-4k-instruct-dpoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - bert-base-japanese-jsnli This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - Japanese Parler-TTS Mini (Î²ç‰ˆ) ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-mini-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - æ—¥æœ¬èªåŒ»ç™‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ« æ¦‚è¦ ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶å®¤ã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹MedTxt-CRã‚’ç”¨ã„ã¦ã€alabniiã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹RoBERTaã‚’fine-tuningã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B Heron-NVILA-Lite-33B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-8bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja has the following changes compared to Mistral-7B-v0.1.
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - MobileBERT æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çˆ†èª•ï¼ï¼
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B Sarashina2-Vision-14B is a Japanese Large Vision Language Model trained by SB Intuitions.
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - Model Card for NABLA-VL
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - rinna/japanese-gpt-neox-3.6b-instruction-ppo rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6b-instruction-ppoã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - Model Card For llm-jp-3-3.7b-instruct-gguf LLM-jpã•ã‚“ã®llm-jp-3-3.7b-instructã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - Wav2Vec2-Large-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - AXCXEPT-EZO-phi-4-v2_900-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-phi-4-v2_900ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - cyberagent-open-calm-3b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-3bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [prj-beatrice/japanese-hubert-base-phoneme-ctc-v2](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc-v2)
  - japanese-hubert-base-phoneme-ctc rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹stockmark-100bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - QwQ-32B-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹QwQ-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - shisa-7b-v1-gguf augmxntã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹shisa-7b-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - japanese-wav2vec2-large-rs35kh
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B static quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF Usage
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59362 Validation Metrics Loss: 0.13092292845249176 Accuracy: 0.9527127414314258 Precision: 0.9634070704982427 Recall: 0.9842171959602166 AUC: 0.9667289746092403 F1: 0.9737009564152002 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
- [turing-motors/Heron-NVILA-Lite-15B-hf](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B-hf)
  - Heron-NVILA-Lite-15B Heron-NVILA-Lite-15B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - rinna/japanese-gpt-neox-3.6b rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 æ˜ç¤ºçš„ãªè¨±è«¾ã‚’å¾—ãŸã‚ªãƒ—ãƒˆã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èª/è‹±èªãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«CLIP (Contrastive Language-Image Pre-training)ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - DeepSeek-R1-Distill-Qwen-1.5B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-1.5Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf rinnaã•ã‚“ã®gemma-2-baku-2b-itã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfinã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - SakuraMixSeries èƒŒæ™¯ã¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒ¼ã‚’ä¸¡ç«‹ã•ã›ãŸVAEå†…è”µå‹ãƒ¢ãƒ‡ãƒ« Model with built-in VAE for both background and character quality ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - albert-base-japanese-v1 æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ How to use ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ Fill-Mask ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«Sentencepieceã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ãã®ã¾ã¾ã§ã¯[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚ã¨ã«ä½™è¨ˆãªãƒˆãƒ¼ã‚¯ãƒ³ãŒæ··å…¥ã™ã‚‹å•é¡ŒãŒã‚ã‚‹ã®ã§ã€åˆ©ç”¨ã™ã‚‹éš›ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ for PyTorch from transformers import ( AlbertForMaskedLM, AlbertTokenizerFast ) import torch tokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese License MIT License ğŸ‘‰ DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf ã“ã£ã¡ã®ãŒã„ã„ã‹ã‚‚ğŸ‘‰ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KUJIRAã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - cyberagent-open-calm-1b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-1bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - Qwen2.5 Bakeneko 32B Instruct GGUF (rinna/qwen2.5-bakeneko-32b-instruct-gguf)
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - nlp-waseda/roberta-large-japanese Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹c4ai-command-r-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - t5-base-japanese-web (with Byte-fallback, 32K) Description megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - stockmark-gpt-neox-japanese-1.4b-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gpt-neox-japanese-1.4bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹phi-4-open-R1-Distill-EZOv1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - llm-jp-3-172b-instruct3
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Humanities-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-Qwen2.5-32b-Japanese-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Googleã®google/gemma-3-4b-itã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã„ã¾ã™ã€‚
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuisekiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri: Japanese General Text Embeddings âš 
- [r-g2-2024/Llama-3.1-70B-Instruct-multimodal-JP-Graph-v0.1](https://huggingface.co/r-g2-2024/Llama-3.1-70B-Instruct-multimodal-JP-Graph-v0.1)
  - Llama-3.1-70B-Instruct-multimodal-JP-Graph - Built with Llama Llama-3.1-70B-Instruct-multimodal-JP-Graph is a Japanese Large Vision Language Model.
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªåŒ»ç™‚LLM MedLlama3-JP ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama3ã®ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸï¼”ç¨®é¡ã®LLMã‹ã‚‰æˆã‚‹ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-7B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - llm-jp-3-7.2b-instruct This repository provides large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-breadcrumbsã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIæ§˜ã® EZO-Common-T2-2B-gemma-2-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - luke-japanese-large luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - luke-japanese-large-lite luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
- [cardiffnlp/tweet-topic-base-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-base-multilingual)
  - tweet-topic-base-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-base language model trained rained on ~198M multilingual tweets and finetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - Qwen2.5 Bakeneko 32B Instruct GPTQ int8 (rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - line-corporation/japanese-large-lm-1.7b line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
  - For usage instructions follow openai/whisper-large-v3-turbo Turbo finetune with japanese tokenizer.
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIæ§˜ã® Llama-3.1-8B-EZO-1.1-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - Model Card for llm-jp-clip-vit-large-patch14 Model Details Japanese CLIP model trained with OpenCLIP on relaion2B-en-research-safe-japanese-translation, a Japanese translation of the English subset of ReLAION-5B (https://huggingface.co/datasets/laion/relaion2B-en-research-safe),
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Oumuamua-7b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-small Fine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
- [SIP-med-LLM/SIP-jmed-llm-3-8x13b-AC-32k-instruct](https://huggingface.co/SIP-med-LLM/SIP-jmed-llm-3-8x13b-AC-32k-instruct)
  - SIP-med-LLM/SIP-jmed-llm-3-8x13b-AC-32k-instruct ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æˆ¦ç•¥çš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å‰µé€ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆSIPï¼‰ç¬¬ 3 æœŸèª²é¡Œã€Œçµ±åˆå‹ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«ãŠã‘ã‚‹ç”Ÿæˆ AI æ´»ç”¨ã€ãƒ†ãƒ¼ãƒï¼‘ã€Œå®‰å…¨æ€§ãƒ»ä¿¡é ¼æ€§ã‚’æŒã¤ã‚ªãƒ¼ãƒ—ãƒ³ãªåŒ»ç™‚ LLM ã®é–‹ç™ºãƒ»ç¤¾ä¼šå®Ÿè£…ã€ã«ãŠã„ã¦ç ”ç©¶é–‹ç™ºã•ã‚ŒãŸã€ç ”ç©¶ç”¨é€”é™å®šãƒ»å•†ç”¨åˆ©ç”¨ä¸å¯ã®åŒ»ç™‚ç‰¹åŒ–å‹ LLM ã§ã™ã€‚
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Model Details Model Description
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - Whisper Large V3 Japanese Phone Accent
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - DeepSeek-R1-Distill-Llama-8B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Llama-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - ç°¡å˜ãªç®—æ•°å•é¡Œã‚’è§£ã‘ã‚‹ã‚ˆã†ã« GRPO ã§å­¦ç¿’ã—ã¦ã¿ãŸã€‚
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - RoBERTa base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - bert-base-japanese-v3-jcommonsenseqa ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(å¤šè‚¢é¸æŠå¼è³ªå•å¿œç­”)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - Model Card For llm-jp-3-1.8b-instruct-gguf LLM-jpã•ã‚“ã®llm-jp-3-1.8b-instructã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [NandemoGHS/Anime-Speech-Japanese-Refiner-FP8-DYNAMIC](https://huggingface.co/NandemoGHS/Anime-Speech-Japanese-Refiner-FP8-DYNAMIC)
  - Anime-Speech-Japanese-Refiner-FP8-DYNAMIC
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - ELYZA-japanese-CodeLlama-7b-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-70b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - About static quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B weighted/imatrix quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [OmniAICreator/Galgame-Llasa-3B-v3](https://huggingface.co/OmniAICreator/Galgame-Llasa-3B-v3)
  - Galgame-Llasa-3B-v3 Overview This is the version 3 of the Galgame-Llasa-3B, a Text-to-Speech (TTS) model fine-tuned for Japanese.
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - GGUFç‰ˆã¯ã“ã¡ã‚‰ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF æ¦‚è¦ Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ æ—¥æœ¬èªã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãŸã‚magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªåŠ›ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã¯ãš Mistral-Nemoãƒ™ãƒ¼ã‚¹ãªã®ã§Temperatureã¯0.3ã‚’åŸºæº–ã«èª¿æ•´ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ system promptã«æ—¥æœ¬èªã§å‡ºåŠ›ã™ã‚‹æ—¨ã‚’è¨˜è¼‰ã™ã‚‹ã“ã¨ã§è‹±èªãŒæ··ã˜ã‚‹å•é¡Œã‚’æŠ‘åˆ¶ã§ãã¾ã™ v0.1ã‹ã‚‰ã®å¤‰æ›´ç‚¹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®system promptã«&lt;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨€èª&gt;ã§å‡ºåŠ›ã™ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ  ã‚¨ãƒãƒƒã‚¯ã‚’9å€ã«å¢—åŠ  ä½¿ç”¨ã•ã›ã¦ã„ãŸã ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ kalomaze/Opus_Instruct_25k Nopm/Opus_WritingStruct anthracite-org/kalo-opus-instruct-22k-no-refusal Aratako/Synthetic-Japanese-Roleplay-NSFW-Claude-3.5s-15.3k-formatted
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - ğŸ“‘ Paper | ğŸ¤—
- [nineninesix/kani-tts-370m-expo2025-osaka-ja](https://huggingface.co/nineninesix/kani-tts-370m-expo2025-osaka-ja)
  - KaniTTS EXPO2025 Osaka japanese A high-speed, high-fidelity Text-to-Speech model optimized for real-time conversational AI applications.
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL++-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-gguf matsuo-labã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹weblab-10b-instruction-sftã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Model Details Model Description This repository provides Asagi-8B, a large-scale Japanese Vision &amp; Language Model (VLM).
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - llm-jp-3-13b-instruct2 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - Feedback and support: TensorBlock's Twitter/X, Telegram Group and Discord server cyberagent/Mistral-Nemo-Japanese-Instruct-2408 - GGUF
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressiveã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B ã®GGUFé‡å­åŒ–ç‰ˆã§ã™ã€‚
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - 4-bit é‡å­åŒ–ç‰ˆ llm-jp-3-172b-instruct3 æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äººæƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€ŒNIIã€ï¼‰ãŒæä¾›ã™ã‚‹ã€Œllm-jp-3-172b-instruct3ã€(ä»¥ä¸‹ã€Œæœ¬ãƒ¢ãƒ‡ãƒ«ã€) ã‚’ 4-bit é‡å­åŒ–ã—ãŸæ´¾ç”Ÿãƒ¢ãƒ‡ãƒ« (ä»¥ä¸‹ã€Œæœ¬é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€) ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [mmnga/llm-jp-3.1-8x13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf)
  - llm-jp-3.1-8x13b-instruct4-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3.1-8x13b-instruct4ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - japanese-gpt-1b-PII-masking Model Description japanese-gpt-1b-PII-masking ã¯ã€ æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿1B GPTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ—¥æœ¬èªã®æ–‡ç« ã‹ã‚‰å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€MARC-ja(positive or negativeã®äºŒå€¤åˆ†é¡)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Atotti/miipher-2-HuBERT-HiFi-GAN-v0.1](https://huggingface.co/Atotti/miipher-2-HuBERT-HiFi-GAN-v0.1)
  - Speech Enhancement Model ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã¨è‹±èªã§å°è¦æ¨¡ã«å­¦ç¿’ã•ã‚ŒãŸéŸ³å£°å¾©å…ƒãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [buildertakuya/Berghof-Takuya7B](https://huggingface.co/buildertakuya/Berghof-Takuya7B)
  - Berghof Takuya 7B Model Description Elizezen/Berghof-NSFW-7Bã«å¯¾ã—ã€åŒäººæ‹“ä¹Ÿã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ãƒ‘ã‚­ã‚®ãƒ¡æ±šæŸ“ï¼ˆç¶™ç¶šäº‹å‰å­¦ç¿’ï¼‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
- [Aratako/sarashina2.2-3b-RP-v0.2-GGUF](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.2-GGUF)
  - sarashina2.2-3b-RP-v0.2-GGUF æ¦‚è¦ Aratako/sarashina2.2-3b-RP-v0.2ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - Model card for model ID
- [dahara1/gemma-3-4b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-4b-it-qat-japanese-imatrix)
  - google/gemma-3-4b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese Considering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - Japanese-Alpaca-2-13B-GGUF Japanese-Alpaca-2-13B-GGUFã¯Japanese-Alpaca-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - whisper-large-v2-japanese-5k-steps This model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
- [SiRoZaRuPa/japanese-HuBERT-base-VADLess-ASR-RSm](https://huggingface.co/SiRoZaRuPa/japanese-HuBERT-base-VADLess-ASR-RSm)
  - VAD-less Japanese ASR Model
- [ronantakizawa/sarashina2-7b-jreadability](https://huggingface.co/ronantakizawa/sarashina2-7b-jreadability)
  - Sarashina2-7B Difficulty-Balanced Japanese Text Generation with Difficulty Control Fine-tuned for difficulty-aware Japanese text generation with balanced learning and zero Simple text degradation ğŸ¯
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha Model Details Japanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - hubert-base-asr
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp Model description
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-v1-7B-gguf SakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - llm-jp-3-8x1.8b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [ouktlab/t5_g2p-jis-v2_corpus10-bccwj-wiki40b_std](https://huggingface.co/ouktlab/t5_g2p-jis-v2_corpus10-bccwj-wiki40b_std)
  - T5 G2P model ouktlab/t5_g2p-jis-v2_corpus10-bccwj-wiki40b_std
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [webbigdata/VoiceCore_gguf](https://huggingface.co/webbigdata/VoiceCore_gguf)
  - VoiceCore GGUF - æ¬¡ä¸–ä»£ æ—¥æœ¬èªVoice AI Agentç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆggufé‡å­åŒ–ç‰ˆï¼‰ webbigdata/VoiceCoreã¯AIãŒè‡ªç„¶ãªæ—¥æœ¬èªã‚’ç™ºå£°å¯èƒ½ã«ã™ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªVoice AI Agentãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer â™»
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medllm - Build with Llama3-8B ELAINE (EngLish-jApanese-chINesE)-
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteusã®GGUFç‰ˆã§ã™ã€‚
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - Whatâ€™s this?
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese GGUF Model Description
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Vecteus-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [NandemoGHS/Anime-Speech-Japanese-Captioner-FP8-DYNAMIC](https://huggingface.co/NandemoGHS/Anime-Speech-Japanese-Captioner-FP8-DYNAMIC)
  - Anime-Speech-Japanese-Captioner-FP8-DYNAMIC
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Japanese CLIP ViT-H/14 (Wider) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - luke-large-defamation-detection-japanese æ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºå™¨
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen1.5-110B-Chatã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - merge This is a merge of pre-trained language models created using mergekit.
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT Model The RetrievaBERT is the pre-trained Transformer Encoder using Megatron-LM.
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - ã€ŒLLM-jp-3 172Bã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172Bã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - XLNet-japanese Model description This model require Mecab and senetencepiece with XLNetTokenizer.
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct Model Description RakutenAI-2.0-8x7B-instruct is a fine-tuned variant of RakutenAI-2.0-8x7B, designed to push the boundaries of Japanese large language models (LLMs).
- [sabaridsnfuji/Japanese-Receipt-VL-3B-JSON](https://huggingface.co/sabaridsnfuji/Japanese-Receipt-VL-3B-JSON)
  - Japanese-Receipt-VL-3B-JSON Model Description Japanese-Receipt-VL-3B-JSON is a fine-tuned vision-language model based on Qwen2.5-VL-3B, specifically optimized for Japanese receipt OCR and structured data extraction.
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-medium-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - rinna/nekomata-7b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-7b-instruction.
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This is a ChatML model.
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-liteã®é‡ã¿ã®åå‰ã‚’XLMRobertaå½¢å¼ã«ç½®ãæ›ãˆã€XLMRobertaãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸç‰©ã§ã™ã€‚
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - c4ai-command-r-v01-japanese-instruct-GGUF æ¦‚è¦ Aratako/c4ai-command-r-v01-japanese-instructã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - BERT for Japanese Twitter
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF æ¦‚è¦ Aratako/Ninja-v1-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository contains some GGUF quantizations of the VNTL Gemma 2 27B model.
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
- [stockmark/Stockmark-2-100B-Instruct](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct)
  - Stockmark-2-100B-Instruct Model description Stockmark-2-100B-Instruct is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese.
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - NLLB 1.3B fine-tuned on Japanese to English Light Novel translation This model was fine-tuned on light and web novel for Japanese to English translation.
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - ã“ã¡ã‚‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã€civitaiã«ã¦å…ˆã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - Stockmark-2-100B-Instruct-beta-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Stockmark-2-100B-Instruct-betaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
- [shisa-ai/shisa-v2.1c-lfm2-350m](https://huggingface.co/shisa-ai/shisa-v2.1c-lfm2-350m)
  - Model Card for shisa-ai/shisa-v2.1c-lfm2-350m SOTA Japanese Shaberi Benchmarks @ &lt;0.5B and &lt;1B!
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 ja Finetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B (rinna/qwen2.5-bakeneko-32b)
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Assistance ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - Qwen3-8B-RP-v0.1 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ Qwen/Qwen3-8Bã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - About static quants of https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Model Details Model Description This repository provides Asagi-4B, a large-scale Japanese Vision &amp; Language Model (VLM).
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - Original Model Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - Canary-TTS-150M llm-jp/llm-jp-3-150m-instruct3 ã‚’ãƒ™ãƒ¼ã‚¹ã«å­¦ç¿’ã—ãŸTTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - calm3-22b-RP-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version ã¾ãŸã€ã“ã¡ã‚‰ã§æœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instructã‚’CoTãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸreasoningãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-KUJIRA ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset ) ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒãƒ™ãƒ«é¢¨ç”»åƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§naver-clova-ix/donut-baseã‚’è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 static quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF Usage
- [shozuru/t5-japanese-grammar-corrector](https://huggingface.co/shozuru/t5-japanese-grammar-corrector)
  - Model Card A Japanese language learning model designed to correct grammar mistakes in sentences.
- [EQUES/JPharmatron-7B](https://huggingface.co/EQUES/JPharmatron-7B)
  - JPharmatron-7B JPharmatron-7B is a 7B large language model designed for pharmaceutical applications and researches.
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-RobinHood ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1 æ—¥æœ¬èªç‰ˆã¯è¿‘æ—¥å…¬é–‹äºˆå®šã§ã™ï¼ˆæ—¥æœ¬èªã‚’å‹‰å¼·ä¸­ãªã®ã§ã€é–“é•ã„ã¯ã”å®¹èµ¦ãã ã•ã„ï¼
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [prj-beatrice/japanese-hubert-base-phoneme-ctc-v4](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc-v4)
  - japanese-hubert-base-phoneme-ctc-v4 rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - rinna/nekomata-14b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-14b-instruction.
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - ã¯ã˜ã‚ã« Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - æ›´æ–°æƒ…å ± æ—¥æœ¬èªæ©Ÿèƒ½ã¨instructãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã—ãŸver.2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2 ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€ Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - æ—¥æœ¬èªVL-T5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39æ§˜ã® Tora-7B-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - datasets: https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - bert-base-japanese-upos Model Description
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallow-70B-Instruct-v0.1 This is a merge of pre-trained language models created using mergekit.
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - sarashina2.1-1b-sft-gguf Aratakoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.1-1b-sftã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - roberta-small-japanese-aozora-char Model Description
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwm Model description This is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZA-japanese-Llama-2-13b-fast-instructã® GGUF å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ cyberagentã«ã‚ˆã‚‹deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã®æ—¥æœ¬èªè¿½åŠ å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã®AWQé‡å­åŒ–ç‰ˆã§ã™ã€‚
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - ğŸˆ FlexDreamHK FlexDreamHKã¯ãƒªãƒ¼ã‚¯ã•ã‚ŒãŸNovelAIãƒ¢ãƒ‡ãƒ«ã®å…¥ã£ã¦ã„ãªã„ã€ã‚ã‚‹ã„ã¯ãã®ãƒªã‚¹ã‚¯ã‚’å¯èƒ½ãªé™ã‚Šä½ãã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ Twitter/twhin-bert-large ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸ
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - Japanese transformer pipeline (bert-base).
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - yacis-electra-small
- [Alfaxad/LFM2-VL-1.6B-JP](https://huggingface.co/Alfaxad/LFM2-VL-1.6B-JP)
  - LFM2-VL-1.6B-jp (Japanese) Model Description LFM2-VL-1.6B-jp is a Japanese fine-tuned variant of LiquidAI/LFM2-VL-1.6B, optimized for Japanese vision-language tasks.
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-with-auto-jumanpp Model description
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B-GGUF GGUF conversion of "Japanese-WizardLM2-ChatV-7B" This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - bert-base-irony
- [pfnet/plamo-2.1-2b-cpt](https://huggingface.co/pfnet/plamo-2.1-2b-cpt)
  - PLaMo 2.1 2B Model Description PLaMo 2.1 2B is a model developed by Preferred Elements Inc.
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha Mixtral-8x7B-Instruct-v0.1-japanese-alphaã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸå­¦ç¿’é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - nlp-waseda/gpt2-small-japanese-wikipedia This model is Japanese GPT-2 pretrained on Japanese Wikipedia.
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "å½±" in Japanese or "Shadow" in English.
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯å¼·åŠ›ãªï¼”ã¤ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ã‚‰ã—ã•ã‚’ä»˜ä¸ã—ãŸé›‘è«‡å¿œç­”ã®ç”Ÿæˆ
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
- [shisa-ai/shisa-v2-llama3.1-405b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - About static quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF Usage
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - æ—¥æœ¬èªå‘ã‘ Llama 3 8B ã¯ã˜ã‚ã« ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯Llama 3ã‚’æ—¥æœ¬èªåŒ–ã—ã‚ˆã†ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-A-v1-7B-gguf SakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-A-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max This is a Japanese vision-language model based on LLaVA architecture with 1.3B parameters.
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - Details: https://spacy.io/models/ja#ja_core_news_lg Japanese pipeline optimized for CPU.
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - hubert-base-jtube This repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) CoolJapanDiffusion 2.1.1ã¨WaifuDiffusion 1.4 anime epoch2ã®ãƒãƒ¼ã‚¸ã€‚
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - æ¦‚è¦ vecteusã¯ã€é«˜æ€§èƒ½ãªæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - rinna/nekomata-7b-gguf Overview The model is the GGUF version of rinna/nekomata-7b.
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-large-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - We initialize SPLADE-japanese from tohoku-nlp/bert-base-japanese-v2.
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - Model Card for llm-jp-clip-vit-base-patch16 Model Details Japanese CLIP model trained with OpenCLIP on relaion2B-en-research-safe-japanese-translation, a Japanese translation of the English subset of ReLAION-5B (https://huggingface.co/datasets/laion/relaion2B-en-research-safe),
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
- [Alfaxad/LFM2-VL-450M-jp](https://huggingface.co/Alfaxad/LFM2-VL-450M-jp)
  - LFM2-VL-450M-jp (Japanese) Model Description LFM2-VL-450M-jp is a Japanese fine-tuned variant of LiquidAI/LFM2-VL-450M, optimized for Japanese vision-language tasks.
- [Aratako/Japanese-Novel-Reward-310m-v2](https://huggingface.co/Aratako/Japanese-Novel-Reward-310m-v2)
  - Japanese-Novel-Reward-310m-v2 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/modernbert-ja-310mã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - â—†REV-Mix "ãƒ¬ãƒœãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime ã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
- [yasu-oh/Llama-3-Swallow-Infused-R1776-70B](https://huggingface.co/yasu-oh/Llama-3-Swallow-Infused-R1776-70B)
  - Llama-3-Swallow-Infused-R1776-70B Overview Llama-3-Swallow-Infused-R1776-70B is a 70B parameter merged model built on Meta's Llama 3 architecture.
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese GGUF Model Description
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Model Details Model Description This repository provides Asagi-2B, a large-scale Japanese Vision &amp; Language Model (VLM).
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - Model Card Summary This model was trained using H2O LLM Studio.
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - SpeechT5 (TTS task) for Japanese SpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech)
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - Tanuki-ZeRo-gguf kanhatakeyamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Tanuki-ZeRoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - kakuyomu-genre-bert å°èª¬ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„ç´¹ä»‹æ–‡ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’åˆ†é¡ã™ã‚‹ BERT æ±åŒ—å¤§ã® cl-tohoku/bert-base-japanese-char-v3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã•ã‚Œã¾ã—ãŸã€‚
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - AXCXEPT/EZO2.5-gemma-3-12b-it-Preview Model Details æ˜¨ä»Šç™»å ´ã—ãŸLLMè‡ªèº«ã®åŠ›ã‚’è‡ªåŠ›ã§å‘ä¸Šã•ã›ã‚‹ã€ŒGRPOã€ã‚„ã€ŒPPOã€ã®æ¦‚å¿µã‚’ã€ å¼Šç¤¾ã§é–‹ç™ºã—ãŸã€ŒEZOã€ã¨ã„ã†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã«ãƒŸãƒƒã‚¯ã‚¹ã™ã‚‹ã“ã¨ã§ã€ 3,000ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€2æ™‚é–“Ã—H200Ã—8å°ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€Japanese MT BenchãŠã‚ˆã³ã€Elyza Tasks100ã«ãŠã‘ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªæ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - bart-base-japanese-news(base-sized model)
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIæ§˜ã® Llama-3-EZO-8b-Common-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunChoã§åˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - k-ush/xlm-roberta-base-ance-en-jp-warmup A XLM-RoBERTa-base model trained on mMARCO Japanese dataset with ANCE warmup script.
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰/Japanese model card æ—¥æœ¬èªã®ãƒ–ãƒ­ã‚°/Full Japanese dev blog Development source code/é–‹ç™ºã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ Karasu-DPO-7B
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix æ¦‚è¦ / Overview Yaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
- [NandemoGHS/Anime-Llasa-3B-FP8](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-FP8)
  - Anime-Llasa-3B-FP8 This is the FP8 quantized version of NandemoGHS/Anime-Llasa-3B.
- [OmniAICreator/Galgame-Llasa-1B-v2](https://huggingface.co/OmniAICreator/Galgame-Llasa-1B-v2)
  - Galgame-Llasa-1B-v2 Overview
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - (English part follows Japanese one.
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - (English part follows Japanese one.
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - roberta-small-japanese-luw-upos Model Description
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - roberta_qa_japanese (Japanese caption : æ—¥æœ¬èªã® (æŠ½å‡ºå‹) è³ªå•å¿œç­”ã®ãƒ¢ãƒ‡ãƒ«)
- [Akimite/Gemma3-12b-it-Girl-v3](https://huggingface.co/Akimite/Gemma3-12b-it-Girl-v3)
  - èª¿æ•´ã™ã‚‹éç¨‹ã§ã®GPUãƒ¡ãƒ¢ãƒªã®éƒ½åˆä¸Šã€textã®ã¿ã‚’æŠ½å‡ºã—ãŸbase modelã‚’ä½¿ç”¨ã€‚
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - Deepreneur-blue-lizard-gguf Deepreneurã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹blue-lizardã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - DeBERTa V2 small Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - ã¯ã˜ã‚ã« ãªã‚“ã‹æ—¥æœ¬èªãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 Model Application
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 ğŸš¨ If you want to avoid outputs that appear to be literal translations, please prompt this model to role-play as a Japanese person.
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - Japanese-Novel-Reward-sarashina2.1-1b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/sarashina2.1-1bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - AXCXEPT/phi-4-open-R1-Distill-EZOv1 Model Details This model is a Reasoner version of the phi-4 model by employing open-r1, which mimics the Distill methodology of Deepseek-R1.
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7Bã‚’ä¼šè©±ã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - Unihan LM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database Model description Chinese and Japanese share many characters with similar surface morphology.
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - bert-japanese-ner ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã‚¿ã‚¹ã‚¯ã‚’ç›®çš„ã¨ã—ã¦ã€äº¬éƒ½å¤§å­¦ é»’æ©‹ãƒ»è¤šãƒ»æ‘è„‡ç ”ç©¶å®¤ãŒå…¬é–‹ã—ã¦ã„ã‚‹BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ãŒå…¬é–‹ã—ã¦ã„ã‚‹ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - Japanese GPT2 Lyric Model Model description
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for VecTeus-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 VecTeus has the following changes compared to Mistral-7B-v0.1.
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - zenz-v1 zenz-v1ã¯GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This is a Japanese sentence-LUKE model.
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - æ›´æ–°å±¥æ­´ 2023å¹´5æœˆ7æ—¥ ã€Œoasst1-89k-jaã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ã¾ã—ãŸã€‚
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - alpaca-guanaco-japanese-gpt-1b 1.3Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªGPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå¯¾è©±AIã§ã™ã€‚
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - matsuolab-weblab-10b-gguf matsuo-labã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹weblab-10bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - ELECTRA base Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi_mini_12b_GGUF wanabi_mini_12b_GGUF ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - Fine-tuned XLSR-53 large model for speech diarization in Japanese phone-call 2 speakers diarization model which was fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using phone-call data CallHome.
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - sarashina2.2-3b-RP-v0.1 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ sbintuitions/sarashina2.2-3b-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer This model is a Phoneme Level Speech Recognition network, originally a fine-tuned version of openai/whisper-large-v3 on a mixture of Different Japanese datasets.
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - yuyuyui-chatbot
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - Model Card for Tanrei/GPTSAN-japanese General-purpose Swich transformer based Japanese language model GPTSAN has some unique features.
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAEã®å†…è‡“ã¯ãªã„ãï¼ã¨è¨€ã‚ã›ãªã„ãï¼ï¼ï¼ï¼
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - Model Card for Japanese BART large Model description
- [mpasila/gemma-3-JP-EN-Translator-v1-4B](https://huggingface.co/mpasila/gemma-3-JP-EN-Translator-v1-4B)
  - Uploaded finetuned gemma-3-JP-EN-Translator-v1-4B model Prompt format: ChatML Recommended system prompt: You are a helpful assistant that translates Japanese to English.
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False
- [UEC-InabaLab/Llama-3.1-KokoroChat-Low](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Low)
  - ğŸ§  Llama-3.1-KokoroChat-Low: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-Low is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
- [UEC-InabaLab/Llama-3.1-KokoroChat-High](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-High)
  - ğŸ§  Llama-3.1-KokoroChat-High: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-High is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - llm-jp-3-980m-instruct2 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - bert-large-japanese-char-extended Model Description
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - bert-base-japanese-char-extended Model Description
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - ku-accms/bert-base-japanese-ssuw Model description This is a pre-trained Japanese BERT base model for super short unit words (SSUW).
- [cyberagent/ca-reward-3b-ja](https://huggingface.co/cyberagent/ca-reward-3b-ja)
  - cyberagent/ca-reward-3b-ja è»½é‡ãªæ—¥æœ¬èªå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‚’ç›®çš„ã¨ã—ã¦å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã™ã‚‹ã€‚
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - japanese-sexual-moderation-v2ã¯ã€studio-ousia/luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUFã¯Japanese-LLaMA-3-8B-Instruct-v2ã®GGUFå½¢å¼ã§ã™ã€‚
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - ELECTRA base Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - Model card for model ID
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese for Irony
- [UEC-InabaLab/Llama-3.1-KokoroChat-Full](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Full)
  - ğŸ§  Llama-3.1-KokoroChat-Full: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-Full is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
- [tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF](https://huggingface.co/tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF)
  - Feedback and support: TensorBlock's Twitter/X, Telegram Group and Discord server elyza/ELYZA-japanese-Llama-2-13b-instruct - GGUF
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒãƒ¼ã‚¸ãªã©ã‚’ç”¨ã„ä½œæˆã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã«å¯¾å¿œã—ã¦ã„ã‚‹Llama-3ãƒ™ãƒ¼ã‚¹ã®ï¼”ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§tohoku-nlp/bert-base-japanese-v3ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - roberta-small-japanese-aozora Model Description
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - roberta-base-japanese-aozora Model Description
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - Details: https://spacy.io/models/ja#ja_core_news_trf Japanese transformer pipeline (Transformer(name='cl-tohoku/bert-base-japanese-char-v2', piece_encoder='char', stride=160, type='bert', width=768, window=216, vocab_size=6144)).
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - wav2vec2-live-japanese https://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using the common_voice JSUT CSS10
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Karasu-Mixtral-8x22B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - doc2query/msmarco-japanese-mt5-base-v1 This is a doc2query model based on mT5 (also known as docT5query).
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - ğŸŒŸ Ojisanæ§‹æ–‡å¤‰æ›ãƒ¢ãƒ‡ãƒ« (GRPO + Unsloth + LoRA) ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€æ–‡ç« ã‚’ã€ŒãŠã˜ã•ã‚“æ§‹æ–‡ã€ã«å¤‰æ›ã™ã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆãƒ»å­¦ç¿’ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest â™»
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints ã‚’ optimum ç”¨ã« ONNX ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - roberta-large-japanese-aozora-char Model Description
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - Details: https://spacy.io/models/ja#ja_core_news_md Japanese pipeline optimized for CPU.
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - deberta-small-japanese-aozora Model Description
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - rinna/japanese-data2vec-audio-base Overview This is a Japanese data2vec Audio Base model trained by rinna Co.
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - japanese-gpt2-medium-unidic This is a medium-sized Japanese GPT-2 model using BERT-like tokenizer.
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - ELECTRA small Japanese discriminator for Irony
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - dolly-japanese-gpt-1b-clone æ¦‚è¦ rinnaç¤¾ã®ã€Œjapanese-gpt-1bã€ã‚’ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€Œdatabricks-dolly-15k-jaã€ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã•ã›ãŸæ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - Japanese-LLaMA-2-13B Japanese-LLaMA-2-13Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - kishizaki-sci/phi-4-AWQ-4bit-EN-JP model information phi-4ã‚’AutoAWQã§4bit é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - sarashina2.2-3b-instruct-v0.1-GGUF base_model: sbintuitions/sarashina2.2-3b-instruct-v0.1 imatrix: TFMC/imatrix-dataset-for-japanese-llm
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - æ—¥æœ¬èª gpt2 è’¸ç•™ãƒ¢ãƒ‡ãƒ« ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt2-meduimã‚’æ•™å¸«ã¨ã—ã¦è’¸ç•™ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - jpn-heb source group: Japanese target group:
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - nlp-waseda/gpt2-small-japanese This model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - Japanese Stable Diffusion Pokemon Model Card Stable-Diffusion-Pokemon-ja is a Japanese-specific latent text-to-image diffusion model capable of generating Pokemon images given any text input.
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - line-corporation/japanese-large-lm-3.6b line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ AWSã®trn1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦é–‹ç™ºã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0 ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
- [Itbanque/whisper-ja-zh-base](https://huggingface.co/Itbanque/whisper-ja-zh-base)
  - Whisper Fine-Tuning for Japanese-to-Chinese Translation
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-largeã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
  - This is an ExLlamaV2 quantized model in 4bpw of augmxnt/shisa-base-7b-v1 using the default calibration dataset.
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - (English part follows Japanese one.
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - makiart/jp-ModernBert-base-preview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ABCI ç”ŸæˆAIãƒãƒƒã‚«ã‚½ãƒ³ã«ã¦æä¾›ã•ã‚ŒãŸè¨ˆç®—è³‡æºã«ã‚ˆã£ã¦Algomaticãƒãƒ¼ãƒ ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - roberta-large-japanese-aozora Model Description
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - deberta-base-japanese-aozora Model Description
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - MPT-7B-base ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - MPT-7B-inst ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7b-instructã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - nlp-waseda/bigbird-base-japanese Model description This is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - roberta-base-japanese-juman-ud-goeswith Model Description
- [yamatazen/Shisa-K-12B](https://huggingface.co/yamatazen/Shisa-K-12B)
  - Shisa-K-12B
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - Model description This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on my collection of Public Japanese Voice datasets for research Common Voice 7.0, JUST (Japanese speech corpus of Saruwatari-lab.
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - Model card for model ID
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero Base model: llm-jp/llm-jp-13b-v1.0 Instruction data: Randomly sampled, 15k Jaster dataset (train) Code is here.
- [umisetokikaze/NinjaV1-pre](https://huggingface.co/umisetokikaze/NinjaV1-pre)
  - è£ã§ä½œã£ã¦ã„ãŸãƒ¢ãƒ‡ãƒ«ã¨VT1ã‚’ãƒãƒ¼ã‚¸ã—ãŸã ã‘ã€‚
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - bert-large-japanese-luw-upos Model Description
- [ronantakizawa/sarashina2-7b-abliterated](https://huggingface.co/ronantakizawa/sarashina2-7b-abliterated)
  - ronantakizawa/sarashina2-7b-abliterated This is an abliterated (refusal-removed)
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - bart-large-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-3.6b-instruction-sft line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - bert-base-japanese-v3-bpr-question-aio ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®è³ªå•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - alabnii/jmedroberta-base-manbyo-wordpiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã‹ã‚‰140,000ä»¶ã»ã©ã®ESã‚’ç”¨ã„ã¾ã—ãŸã€‚
- [nitky/Llama-3.3-SuperSwallowX-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallowX-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallowX-70B-Instruct-v0.1 ğŸ’¡ An experimental merging method is being used to combine different Japanese continuous pre-training models.
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE overview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯AItuberã®é­‚ã¨ãªã‚‹ã“ã¨ã‚’ç›®çš„ã«SB intuitionsã®sarashina-2.2-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«Unsothã¨Mergekit-MoEã‚’ç”¨ã„ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - roberta-large-japanese-juman-ud-goeswith Model Description
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct ğŸ MambaSan-instruct is the first chat Japanese language model based on a state-space model architecture (Mamba), not a transformer.
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2ã‚’instructionç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§sftã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ base: https://huggingface.co/if001/llama2_ja_small trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§ https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - Mixtral-8x7B-v0.1-japanese Mixtral-8x7B-v0.1-japaneseã¯Mixtral-8x7B-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - Kuzushiji 49 MNIST FCN Model Overview This repository contains a Fully Convolutional Neural Network (FCN) model for the Kuzushiji 49 MNIST dataset.
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This model is traned with llm-japanese-dataset dataset.
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - ãƒ¢ãƒ‡ãƒ« ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instruct å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-ja å­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚µãƒ³ãƒ—ãƒ« import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
- [Aratako/MistralPrism-24B](https://huggingface.co/Aratako/MistralPrism-24B)
  - MistralPrism-24B GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€mistralai/Mistral-Small-3.1-24B-Instruct-2503ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Aratako/Mistral-Small-3.1-24B-RPã«å¯¾ã—ã¦ã€æµ·å¤–è£½ãƒ¢ãƒ‡ãƒ«è¤‡æ•°ã¨ã®ãƒãƒ¼ã‚¸ã‚’è¡Œã„æ€§èƒ½å¼·åŒ–ã‚’å›³ã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - makiart/jp-modernbert-large-preview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ABCI ç”ŸæˆAIãƒãƒƒã‚«ã‚½ãƒ³ã«ã¦æä¾›ã•ã‚ŒãŸè¨ˆç®—è³‡æºã«ã‚ˆã£ã¦Algomaticãƒãƒ¼ãƒ ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF base_model: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4 imatrix: TFMC/imatrix-dataset-for-japanese-llm
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - 2025 å¹´ã®ã‚¨ã‚¤ãƒ—ãƒªãƒ¼ãƒ«ãƒ•ãƒ¼ãƒ«ãƒã‚¿æ ã§ã—ãŸ ğŸ‰ğŸ‰ Saikyou Shield 30M ğŸ‰ğŸ‰ ğŸ”¥ å±é™ºãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’100%æ¤œå‡ºã§ãã‚‹æœ€å¼·ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ« ğŸ”¥ Jailbreak ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã€ã‚ã‚‰ã‚†ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å±é™ºã¨åˆ†é¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ï¼
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - AIBunChoæ§˜ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ« (https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: æ—¥æœ¬èªã§è³ªå•ã™ã‚‹ã¨ã€æ—¥æœ¬èªã§å›ç­”ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - GPT-2 small Japanese model This repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - jpn-msa source group: Japanese target group: Malay (macrolanguage) OPUS readme: jpn-msa model: transformer-align source language(s): jpn jpn_Hani jpn_Hira jpn_Kana target language(s): ind
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiæ§˜ã® Japanese-Chat-Umievo-itr004-7b ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [trillionlabs/Tri-0.5B-Base](https://huggingface.co/trillionlabs/Tri-0.5B-Base)
  - Tri-0.5B-Base Tri-0.5B-Base is a ~500M parameter multilingual language model trained as an early experimental run before the Tri-7B training.
- [shuheikatoinfo/UtterTune-CosyVoice2-ja-JSUTJVS](https://huggingface.co/shuheikatoinfo/UtterTune-CosyVoice2-ja-JSUTJVS)
  - UtterTune UtterTune is a low-rank adapter (LoRA) that enables segmantal pronunciation &amp; prosody control on top of text-to-speech based on large language model architecture with no grapheme-to-phoneme modules.
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - Japanese DialoGPT trained with Aozora (ja) é’ç©ºæ–‡åº«ã®ã‚»ãƒªãƒ•ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®DialoGPT Smallã§ã™(en) Japanese DialoGPT Small trained on Aozora Bunko.
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - Japanese Parler-TTS Large (Î²ç‰ˆ) ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-large-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - Electra Base Japanese Irony
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - fasttext-jp-embedding This model is experimental.
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-fastã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-fast-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - DataPilot/sarashina2.2-3Bx8-moe DataPilot/sarashina2.2-3Bx8-moe ã¯ã€sbintuitions/sarashina2.2-3b-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€mergekit-moeã‚’ç”¨ã„ã¦8ã¤ã®å°‚é–€ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸMixture of Expertsï¼ˆMoEï¼‰å‹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion Model Card SFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - doshisha-mil/llama-2-70b-chat-4bit-japanese-v1
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance ã®GGUFç‰ˆ Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - deberta-base-japanese-unidic Model Description
- [ayousanz/piper-plus-base](https://huggingface.co/ayousanz/piper-plus-base)
  - æ—¥æœ¬èªäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«-piper-plus æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ 100æ™‚é–“ç¨‹åº¦ã‚’ä¸€ã‹ã‚‰å­¦ç¿’ã—ãŸæ—¥æœ¬èªç‰¹åŒ–ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - bert-base-japanese-v3-bpr-passage-aio ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - bilingual-gpt-neox-4b-8k Overview Notice: This model requires transformers&gt;=4.31.0 to work properly.
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - tiny_mixtral_jaã‚’instructionç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§trainingã—ãŸã‚‚ã®ã§ã™https://huggingface.co/if001/tiny_mixtral_ja
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta Model description Stockmark-2-100B-Instruct-beta is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese.
- [akkikiki/j-moshi-ext-mlx](https://huggingface.co/akkikiki/j-moshi-ext-mlx)
  - The model in this repository is an MLX model converted from the pre-trained J-Moshi model for Mac OS.
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - deberta-large-japanese-wikipedia-luw-upos Model Description
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - WS TCG Card Text Translator A Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - roberta-base-japanese-char-luw-upos Model Description
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)
  - BLOOM-7B
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Model Card for Wabisabi-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 wabisabi has the following changes compared to Mistral-7B-v0.1.
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - zenz-v2 zenz-v2ã¯GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - modernBERTã§NERã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ ãƒ©ãƒ™ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚° label_list = ["O", "B-äººå", "I-äººå", "B-æ³•äººå", "I-æ³•äººå", "B-æ”¿æ²»çš„çµ„ç¹”å", "I-æ”¿æ²»çš„çµ„ç¹”å", "B-ãã®ä»–ã®çµ„ç¹”å", "I-ãã®ä»–ã®çµ„ç¹”å", "B-åœ°å", "I-åœ°å", "B-æ–½è¨­å", "I-æ–½è¨­å", "B-è£½å“å", "I-è£½å“å", "B-ã‚¤ãƒ™ãƒ³ãƒˆå", "I-ã‚¤ãƒ™ãƒ³ãƒˆå"] tokenizer ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - roberta-base-japanese-aozora-char Model Description
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 â™»
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - sonoisa/t5-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B Please check our blog post for more details, samples, evaluations and more: Blogpost Model Description Genji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - reazonspeech-espnet-next ReazonSpeech is a project to maintain freely-available Japanese audio datasets and ML models.
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - Model Card for Japanese character-level GPT-2 Medium Model description This is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-instruct-gamma-7b ğŸ§© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - japanese-reversed-gpt2-medium-unidic This is a medium-sized Japanese reversed GPT-2 model using BERT-like tokenizer.
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - deberta-large-japanese-unidic-luw-upos Model Description
- [hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4](https://huggingface.co/hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4)
  - hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4 SIP-med-LLM/SIP-jmed-llm-2-8x13b-OP-instructã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³1.0.0ã‚’BitsAndBytesã§NF4é‡å­åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - Japanese BERT-base (Sudachi + WordPiece) How to load the tokenizer Please download the dictionary file for Sudachi + WordPiece from our GitHub repository.
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - Byt5-small-ain-jpn-mt is a machine translation model pretrained with Google's ByT5-small and fine-tuned on bilingual datasets crawled from the Web.
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - roberta-base-japanese-aozora-ud-goeswith Model Description
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - ebisuke/liz-nojaloli-nxja-ja License MIT ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦abeja/gpt-neox-japanese-2.7bã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B Japanese-LLaMA-2-7Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - modernbert-large-japanese-aozora Model Description
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview ğŸ’¡ This model was created based on FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview.yaml
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1 DeepSeekã®è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¨è«–èƒ½åŠ›ã‚’æŠ½å‡ºã—ãŸé‡ã¿ã®å·®åˆ†ã‚’ã€æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B Model Description
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 â™»
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - Animagineç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ãŸVAEå†…è”µãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - roberta-small-japanese-char-luw-upos Model Description
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT dataset{s}.
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - japanese-soseki-gpt2-1b
- [Yukiyoke-Lab/TAR-model](https://huggingface.co/Yukiyoke-Lab/TAR-model)
  - âš 
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã§ã™ã€‚
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This is a Japanese sentence-T5 model.
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - ã‚·ã‚µãƒ èªã«ã‚ˆã‚‹èª¬æ˜ ã‚¢ã‚¤ãƒŒèªã¨æ—¥æœ¬èªã®åŒæ–¹å‘æ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - The English document is here.
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - deberta-large-japanese-wikipedia Model Description
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - deberta-base-japanese-wikipedia-luw-upos Model Description
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - deberta-base-japanese-wikipedia Model Description
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - deberta-base-japanese-juman-ud-goeswith Model Description
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
- [waowao/gemma3-1b-it-oasst2-1k_of_33k-ja](https://huggingface.co/waowao/gemma3-1b-it-oasst2-1k_of_33k-ja)
  - gemme3-1b-itã‚’oasst2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡1kã‚’ç”¨ã„ã¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã¨ãªã‚Šã¾ã™ã€‚
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_vits_accent_with_pause â™»
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3ã€‚
- [Hemlok/LizMix](https://huggingface.co/Hemlok/LizMix)
  - â—†LizMix SakuMixãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ‹ãƒ¡å‘ã‘ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã€‚
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B Model Details Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model that can converse about input images.
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Watashiha-Llama-2-13B-Ogiri-sftã‚’LLaVAã§å­¦ç¿’ã—ã€ç”»åƒã«å¯¾å¿œã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - Mistral-7B Japanese
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - llm-jp-1.3b-v1.0-aya llm-jp's llm-jp-1.3b-v1.0 model fine-tuned on the Japanese examples from Cohere's aya dataset Model llm-jp-eval AVG kcoopermiller/llm-jp-1.3b-v1.0-aya 0.0698 llm-jp/llm-jp-1.3b-v1.0 0.047 How to use import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("kcoopermiller/llm-jp-1.3b-v1.0-aya")
- [mmnga/alfredplpl-suzume-poc-gguf](https://huggingface.co/mmnga/alfredplpl-suzume-poc-gguf)
  - alfredplpl-suzume-poc-gguf alfredplplã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-pocã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B DiffLlama-1Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„100Bãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸç´„1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B This is a merge of pre-trained language models created using mergekit.
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - Model Card for Model ID Model Details Model Description
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - BERT base Japanese model This repository contains a BERT base model trained on Japanese Wikipedia dataset.
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - bert-large-japanese-upos Model Description
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - Wav2Vec2-XLS-R-300M-Japanese-Hiragana Fine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using the Common Voice and JSUT.
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2 V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2 ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ã‚„Instaç³»ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒãƒ»Instaç³»ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JNLI(æ–‡ç« ã®é–¢ä¿‚æ€§åˆ¤åˆ¥)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g.
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - In-progess long-context Japanese-English translation model based on tinyllama.
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - Model description Cyberagentæ§˜ã®cyberagent/calm2-7b-chatã‚’è¿½åŠ å­¦ç¿’ã—ãŸã€ä½œå®¶ã•ã‚“ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆAIã®ã‚¢ãƒ«ãƒ•ã‚¡ç‰ˆã§ã™ã€‚
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - Bloom model trained on Japanese corpus.
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - bert-base-japanese-unidic-luw-upos Model Description
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba Barba is a multilingual natural language inference model for textual entailment and zero-shot text classification, available as an end-to-end service through TensorFlow Serving.
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This model is traned with guanaco dataset.
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fine_Tuned_Whisper_Model This model is a fine-tuned version of openai/whisper-tiny on the Common Voice dataset.
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fined_Tuned_Whisper_Model
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - Model Card for Model ID ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt-1bã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®æŠ½å‡ºå‹QAã¨ã€è§£ç­”ã‚’æ–°ãŸãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒªãƒ•ã‚¡ã‚¤ãƒ³ã™ã‚‹ãŸã‚ã®å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA - Lorenzo Concina
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - Japanese-LLaMA-2-13B-GGUF Japanese-LLaMA-2-13B-GGUFã¯Japanese-LLaMA-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - deberta-large-japanese-luw-upos Model Description
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - deberta-large-japanese-aozora Model Description
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - deberta-small-japanese-luw-upos Model Description
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - deberta-small-japanese-upos Model Description
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - roberta-small-hi-char Model Description
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ sonoisa/sentence-luke-japanese-base-lite ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸã€‚
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - Japanese BERT-base (MeCab + WordPiece) How to load the tokenizer Please download the dictionary file for MeCab + WordPiece from our GitHub repository.
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - deberta-large-japanese-unidic Model Description
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚ç•¥ éœ§é›¨é­”ç†æ²™ã¨ãŠã—ã‚ƒã¹ã‚Šã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8bã®æ—¥æœ¬èªç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ChatVectorã‚’é©ç”¨ã—ã€ã•ã‚‰ã«QLoraã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
  - Mistral-7B Japanese
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [mayocream/manga-ocr-onnx](https://huggingface.co/mayocream/manga-ocr-onnx)
  - Manga OCR ONNX
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
  - Shirayukihime-12B
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - Kendamarron/LongWriter-llm-jp-3-3.7b-instruct llm-jp/llm-jp-3-3.7b-instructã‚’é•·æ–‡å‡ºåŠ›ãŒã§ãã‚‹ã‚ˆã†ã«SFTã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Llama 3 Youko 70B (rinna/llama-3-youko-70b)
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 â™»
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa æ¦‚è¦ tokyotech-llm/Swallow-7b-hfã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ä»¥ä¸‹ã®4ãƒ¢ãƒ‡ãƒ«ã‚’gate_mode=randomã§MoEã—ã€ãã®å¾ŒLISAã¨ã„ã†æ‰‹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - friendly_JA-Model (T5 fine-tuned model) MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexicon Examples input output æœ€é©åŒ–ã‚’å¿œç”¨ã—ãŸæ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾åº¦ã  ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿œç”¨ã—ãŸãƒã‚·ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ã  å½¼ã¯æ¶ç©ºã®ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹ å½¼ã¯ã‚¤ãƒã‚¸ãƒŠãƒªãƒ¼ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹ æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«æ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸ ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«ã‹ã‹ã£ã¦ã—ã¾ã£ãŸ æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„ ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚€ãšã‹ã—ã„ æ–°ãŸãªæ¦‚å¿µã‚’ç´¹ä»‹ã™ã‚‹ æ–°ã—ã„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç´¹ä»‹ã™ã‚‹ æ´¥æ³¢ã®è­¦å ±ãŒæµã‚ŒãŸ ãƒ„ãƒŠãƒŸã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒæµã‚ŒãŸ å—æµ·ãƒˆãƒ©ãƒ•ã®ç½å®³ã¯éœ‡æºåœ°ã«ã‚ˆã‚‹ å—æµ·ãƒˆãƒ©ãƒ•ã®ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒ¼ã¯ã‚¨ãƒ”
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This is a Japanese+English sentence-BERT model.
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - è‹±èª+æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - å›ç­”ã¨å›ç­”ãŒå‡ºã¦ãã‚‹ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ä¸ãˆã‚‹ã¨è³ªå•æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://github.com/sonoisa/deep-question-generation æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚¹ãƒ†ãƒƒãƒ—æ¦‚è¦ SQuAD 1.1ã‚’æ—¥æœ¬èªã«æ©Ÿæ¢°ç¿»è¨³ã—ã€ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯ç´„åŠåˆ†ï¼‰ã€‚
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - albert-base-japanese-v1-with-japanese æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«BertJapaneseTokenizerã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™albert-base-japanese-v1ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒæ¥½ã«ãªã£ã¦ã„ã¾ã™ How to use ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ Fill-Mask for PyTorch from transformers import ( AutoModelForMaskedLM, AutoTokenizer ) tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - bert-large-japanese-unidic-luw-upos Model Description
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - Model card for model ID
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - Model card for model ID
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 Model Description
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - Model Card for Japanese character-level GPT-2 Large Model description
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-book/wrime-sentiment ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adafactor Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - Japanese Stock Comment Sentiment Model
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - Model overview This model is the baseline model for awesome-japanese-nlp-classification-dataset.
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - ebisuke/liz-nojaloli-ja License MIT Licenseãƒ™ãƒ¼ã‚¹ã¨ã—ã¦rinna/japanese-gpt-neox-3.6bã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This model is traned with guanaco dataset.
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 ja Finetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€å†…å®šè€…ã®äºŒä¸‡ä»¶ä»¥ä¸Šã®ESã‚’ç”¨ã„ã¾ã—ãŸã€‚
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - Model card for model ID
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP Model Card Model detail Model type: LLaVA-JP is a vision-language model that can converse about input images.
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - deberta-base-japanese-wikipedia-ud-goeswith Model Description
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - deberta-base-japanese-luw-upos Model Description
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - roberta-small-hi-char-mlm Model Description
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JSTS(æ–‡ç« ã®é¡ä¼¼åº¦è¨ˆç®—)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - Only for Japanese Please use AutoTokenizer and AutoModelForCausalLM And must use Unifine format to input and output.
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This is a model for named entity recognition of Japanese medical documents.
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - deberta-base-japanese-unidic-luw-upos Model Description
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - Japanese BERT-base (Juman++ + WordPiece) How to load the tokenizer Please download the dictionary file for Juman++ +
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - deberta-large-japanese-unidic-ud-head Model Description
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - deberta-base-japanese-unidic-ud-head Model Description
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card è‹±æ—¥ã€æ—¥è‹±ç¿»è¨³ç”¨ãƒ¢ãƒ‡ãƒ«C3TR-Adapterã®GPTQ4bité‡å­åŒ–ç‰ˆã§ã™ã€‚
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged Mixtral-8x7B-Instruct-v0.1-japanese-alpha-mergedã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸå­¦ç¿’é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€å·®åˆ†ãƒãƒ¼ã‚¸ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese This model is Llama-2-Chat 70B fine-tuned with a part of the Japanese instruction dataset named izumi-lab/llm-japanese-dataset.
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - Reproduced Japanese Stable LM Instruct Gamma 7B Model Description
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - Swallow-MoE-2x13B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹tokyotech-llm/Swallow-13b-instruct-hfã¨ã€ãã‚Œã‚’åˆ©ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹nitky/Superswallow-13b-v0.2 ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This model is a merged version of qwen-14b-vntl and Qwen1.5-14B-Chat , aiming for the translation of Japanese context into Chinese.
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - swallow-hermes-st-v1 ç‰©èªä½œæˆã«å¼·ã‚ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºæ¥ãªã„ã‹ã¨è€ƒãˆã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B ã€Œã©ã†ã‹ãŠæ…ˆæ‚²ã‚’ ã‚‚ã† ç–²ã‚Œæœã¦ã¾ã—ãŸã€ ç”Ÿæˆä¾‹ [å¤ªå­—ä»¥é™ãŒAIç”Ÿæˆ] ã€Œã©ã†ã‹ã€ â€ãã‚Œâ€ã¯æ‡‡é¡˜ã—ãŸã€‚
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B Japanese
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
- [ayousanz/modernbert-vtuber-finetuned](https://huggingface.co/ayousanz/modernbert-vtuber-finetuned)
  - ModernBERT-VTuber Finetuned ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ sbintuitions/modernbert-ja-130m ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€YouTube ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‹ã‚‰ VTuber ã‹å¦ã‹ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - Japanese-Novel-Reward-modernbert-ja-310m ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/modernbert-ja-310mã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59363 Validation Metrics Loss: 0.12651239335536957 Accuracy: 0.9532079853817648 Precision: 0.9729688278823665 Recall: 0.9744633462616643 AUC: 0.9717333684823413 F1: 0.9737155136027014 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit ã‚’æ—¥æœ¬èªã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™.
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ DeepSeek-V3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã®ä¾‹æ–‡ã‚’å…ƒã«é »å‡ºã™ã‚‹ MoE (Mixture of Experts) ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®expertsã‚’å³é¸ã—ã¦å†æ§‹æˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹ Have different permissions when sharing merges
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - ##llm-jpã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãƒ¢ãƒ‡ãƒ«
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - bert-base-japanese-luw-upos Model Description
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This pre-trained model is work in progress!
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - yacis-electra-small-cyberbullying
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - bert-base-sudachitra-v11
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - Whatâ€™s this?
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - Kokuwa lamettaã®æ”¹è‰¯ã§ãƒãƒ¼ã‚¸ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«æ¢ã—ã‚’ã—ã¦ã„ãŸã‚‰KiwiMixã¨ã„ã†é¢ç™½ãã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - Convert from: drewschaub/whisper-large-v3-japanese-4k-steps Whisper large-v3 model for CTranslate2 This repository contains the conversion of drewschaub/whisper-large-v3-japanese-4k-steps to the CTranslate2 model format.
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - whisper-large-v2-mix-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - æ—¥æœ¬èªT5 Prefix Language Model
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU-Model (T5 fine-tuned model) JAINU is a Japanese - Ainu language machine translation model.
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - Japanese CLIP ViT-H/14 (Deeper) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - Summary This is a text classifier for assigning a JLPT level.
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 large Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - deberta-large-japanese-upos Model Description
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - c4ai-command-r-v01-japanese-instruct GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ CohereForAI/c4ai-command-r-v01ã‚’ã€ichikara-instructionã‚’ä½¿ã£ã¦è¿½åŠ ã§æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿç‰©å­¦ãƒ»åŒ»å­¦ã«ç²¾é€šã—ãŸOpenBioLLM-8Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªå¯¾å¿œã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«Llama-3-youko-8b-instruct-chatvectorã¨ãƒãƒ¼ã‚¸ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸTokara-0.5B-v0.1ã‚’æ—¥æœ¬èªinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - transformers-ud-japanese-electra-ginza-520 (sudachitra-wordpiece, mC4 Japanese)
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - Japanese BERT-base (Juman++ + BPE) How to load the tokenizer Please download the dictionary file for Juman++ + BPE from our GitHub repository.
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - ku-accms/roberta-base-japanese-ssuw Model description This is a pre-trained Japanese RoBERTa base model for super short unit words (SSUW).
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - roberta-large-japanese-char-luw-upos Model Description
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - distilhubert-ft-japanese-50k Fine-tuned (more precisely, continue trained)
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - Japanese BERT-base (Nothing + Unigram)
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - transformer-lm-japanese-0.1b
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - Japanese BERT-base (Vaporetto + WordPiece) How to load the tokenizer Please download the dictionary file for Vaporetto + WordPiece from our GitHub repository.
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - bart-base-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-13bã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-13b-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - Mixtral-8x7B-Instruct-v0.1-japanese Mixtral-8x7B-Instruct-v0.1-japaneseã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚ç•¥ æ±æ–¹Projectã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã‚ã‚‹éœ§é›¨é­”ç†æ²™ã¨ãŠã—ã‚ƒã¹ã‚Šã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯tokyotech-llm/Swallow-MS-7b-instruct-v0.1ã®tokenizer.chat_templateã‚’ä»¥ä¸‹ã«å¤‰æ›´ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP æ¦‚è¦ Local-Novel-LLM-project/Ninja-v1-NSFWã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteusã‚’ãƒ™ãƒ¼ã‚¹ã«LLavaã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - Japanese Stable LM Instruct Gamma 7B +
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated)
  - Merged Model
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja - Remastered - vlzcrz
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - æ¦‚è¦ è³ªå•ã¨å¿œç­”ã‹ã‚‰ã€ãã®éç¨‹ã®æ€è€ƒã‚’ç”Ÿæˆã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - isekai-bert-v1
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This model learned the proceedings of the Japanese parliament in 2022.
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - â—†ArcanaMix äºŒæ¬¡å…ƒã‚¤ãƒ©ã‚¹ãƒˆã‚’ä¸­å¿ƒã«ã€ã‹ã‚ã„ã„ã‚¤ãƒ©ã‚¹ãƒˆãŒå‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€‚
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1ã‚’ huggingface/text-embeddings-inferenceã§å‹•ã‹ã™ãŸã‚ã® fork ã§ã™ã€‚
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave â™»
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - â– endlessMixã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦ æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Defactaã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸéšå±¤ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2ã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - deberta-base-japanese-aozora-ud-goeswith Model Description
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
- [dahara1/gemma-3-270m_mitsuki](https://huggingface.co/dahara1/gemma-3-270m_mitsuki)
  - dahara1/gemma-3-270m_mitsuki éå¸¸ã«è»½é‡ãªSLMã€gemma-3-270mã‚’å¾®èª¿æ•´ã—ã€ãƒãƒ£ãƒƒãƒˆç”¨ã€é…ä¿¡ã®ãŠä¾›ç”¨ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»˜ã‘ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ronantakizawa/sarashina2-7b-4bit-awq](https://huggingface.co/ronantakizawa/sarashina2-7b-4bit-awq)
  - Sarashina2-7B AWQ 4-bit Quantized
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Base model for the Japanese language finetuned for automatic cyberbullying detection.
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - Japanese BERT-base (Vaporetto + BPE) How to load the tokenizer Please download the dictionary file for Vaporetto + BPE from our GitHub repository.
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - studio-ousia/luke-japanese-baseã«å¯¾ã—ã¦æ¬¡ã®å¤‰æ›´ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - Japanese CLIP ViT-H/14 (Base) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€ Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - VITS TTS Japanese Only Amitaro VITS TTS model finetuned using free voice data from amitaro free voice here ã‚ã¿ãŸã‚ã®å£°ç´ æå·¥æˆ¿ Finetuning code is from Plachtaa - VITS Fast Fine-tuning See sample usage Lycoris53/VITS-TTS-Japanese-Only-Amitaro Model Details 76 annotated wav file train for 600 epoch æ—¥æœ¬èªã®èª¬æ˜ãªã©ã“ã¡ã‚‰ã« AiThinkso.net Developed by:
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese)
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - spekulatius ãƒãƒ¼ã‚¸ã—ã¦ã„ã‚‹ã¨ãŸã¾ã«å‡ºã¦ãã‚‹ã€Œç›®çš„ã®æ„å›³ã¨ã¯é•ã†ã®ã ã‘ã©ãªã‚“ã ã‹æ¶ˆã™ã«ã¯ã‚‚ã£ãŸã„ãªã„ãƒ¢ãƒ‡ãƒ«ã€ã‚’ãŠã™ãåˆ†ã‘ã™ã‚‹ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - åè¨€æ¨è«–ãƒ¢ãƒ‡ãƒ«
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼štyqiangz/multilingual-sentiments ãƒãƒƒãƒã‚µã‚¤ã‚º: 16å›ºå®š ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adamw Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - Model Card for Model ID Fine tunned ASR model from distil-whisper/distil-large-v2.
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - About This model is Lightblue's QLoRA finetune of OpenOrca's Open-Orca/OpenOrcaxOpenChat-Preview2-13B model on Japanese fine-tuning datasets.
- [OmniAICreator/Galgame-Llasa-3B](https://huggingface.co/OmniAICreator/Galgame-Llasa-3B)
  - Galgame-Llasa-1B Overview This is a Text-to-Speech (TTS) model fine-tuned for Japanese.
- [OmniAICreator/Galgame-Llasa-1B](https://huggingface.co/OmniAICreator/Galgame-Llasa-1B)
  - Galgame-Llasa-1B Overview This is a Text-to-Speech (TTS) model fine-tuned for Japanese.
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹ Have different permissions when sharing merges ğŸ–¼ï¸ ä¾‹ / Examples(â€»ä»–ã®äººãŒç”Ÿæˆã—ãŸç‰©ã‚’è¡¨ç¤ºã—ã¦ã„ã‚‹å ´åˆã¯æœ¬äººã®è¨±è«¾ã‚’å¾—ã¦
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - t5-base-japanese-web-8k (with Byte-fallback, 8K) Description megagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
- [UEC-InabaLab/Llama-3.1-KokoroChat-ScorePrediction](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-ScorePrediction)
  - ğŸ§  Llama-3.1-KokoroChat-ScorePrediction: Japanese Counseling Dialogue Scoring Model Llama-3.1-KokoroChat-ScorePrediction is a large-scale Japanese language model fine-tuned on the KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collectionã¨ã¯ï¼Ÿ
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - Japanese-LLaMA-2-7B-GGUF Japanese-LLaMA-2-7B-GGUFã¯Japanese-LLaMA-2-7Bã®GGUFå½¢å¼ã§ã™ã€‚
- [ywc1/marian-finetuned-ja-en](https://huggingface.co/ywc1/marian-finetuned-ja-en)
  - Model Card for Japanese-English Academic Translator
- [waowao/llama3.2-3b-oasst2-33k-ja](https://huggingface.co/waowao/llama3.2-3b-oasst2-33k-ja)
  - Model Description llama3.2-3b-instructionã‚’oasst2-33k-jaã§LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - ELECTRA Base Japanese for Information Triage
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - deberta-large-japanese-juman-ud-goeswith Model Description
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - Japanese BERT-base (MeCab + Unigram)
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2 model size: 130.78M trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§ https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - LINE DistilBERT Japanese (forked by liwii)
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard Model Description Deepreneur-blue-lizardã¯ã€Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - japanese-novel-gpt-j-6b https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b" ã«åˆè¨ˆ216å€‹ã®è©•ä¾¡ã®é«˜ã„ãªã‚ã†å°èª¬ã€é’ç©ºæ–‡åº«ã€ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢ãªã©ã®æ–‡ç« ã‚’QLoRAå­¦ç¿’ã•ã›ãŸå°èª¬ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ Twitter/twhin-bert-base ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸ
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base SambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-base-gamma-7b ğŸ§© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã‚’ æ—¥æœ¬èªã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã§ç”Ÿæˆã—ãŸGPTQãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - VITS TTS Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - Japanese BERT-base (Juman++ + Unigram)
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - deberta-large-japanese-aozora-ud-goeswith Model Description
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - Japanese BERT-base (Nothing + WordPiece) How to load the tokenizer Please download the dictionary file for Nothing + WordPiece from our GitHub repository.
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - Japanese BERT-base (Nothing + BPE) How to load the tokenizer Please download the dictionary file for Nothing + BPE from our GitHub repository.
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - Model card for model ID
- [alter-wang/luke-japanese-large-wrime-emotion-alpha](https://huggingface.co/alter-wang/luke-japanese-large-wrime-emotion-alpha)
  - This is a LUKE (Japanese version) Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - Wav2Vec2 Accent Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese accent dataset When using this model, make sure that your speech input is sampled at 16kHz.
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - The English document is here ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Watashiha-Llama-2-13B-Ogiri-sftã‚’AWSã®inf2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - ãƒ¢ãƒ‡ãƒ« ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instruct å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-ja å­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚µãƒ³ãƒ—ãƒ« import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 Known Performance Issues Two potential bugs have been found in this model: NEED repetition_penalty NEED high temperature Reference: Japanese LLM benchmark results at Nejumi LLM Leaderboad Neo
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - ã‚¢ãƒ‹ãƒ¡å£°ã®ã‚ˆã†ãªã‚ã–ã¨ã‚‰ã—ã„å£°ã§ã‚‚ãªãã€ãƒœã‚«ãƒ­ãªã©ã®ã‚½ãƒ•ãƒˆã‚’ä½¿ã£ãŸã„ã‹ã«ã‚‚åˆæˆã®éŸ³å£°ã§ã‚‚ãªãã€ã‚¯ãƒ©ã‚¹ã«ä¸€äººãã‚‰ã„ã„ãã†ãªã€è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„ç¾å°‘å¥³ã®å£°ã‚’â€¦ã€‚
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - ãŠçŸ¥ã‚‰ã› ã‚ˆã‚Šå›ç­”ãŒé©åˆ‡ã«ãªã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã€https://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq ã‚‚ã‚ã‚Šã¾ã™ã€‚
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7bã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This model is a voice clone of myself created specifically for Style Bert VITS2.
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct ğŸ MambaSan-instruct is the first chat Japanese language model based on a state-space model architecture (Mamba).
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JCommonsenseQA(é¸æŠå¼å¿œç­”)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - æ¦‚è¦ ã€ŒLOCAL AI HACKATHONã€ã«ãŠã‘ã‚‹ã€ãƒãƒ¼ãƒ DataPilot,4ã¤ã‚ã®æˆæœå“ã§ã™ã€‚
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - DataPilot/sarashina2.2-3Bx4-moe DataPilot/sarashina2.2-3Bx4-moeã¯ã€4ã¤ã®ã€Œsbintuitions/sarashina2.2-3b-instruct-v0.1ã€ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ä½œæˆã—ãŸç´„12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦æ¨¡ã®Mixture of Experts (MoE) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 Japanese-LLaMA-3-8B-Instruct-v2ã¯æŒ‡ç¤ºå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - https://qiita.com/SousiOmine/items/23313089c7c3f498996b æ¦‚è¦ sbintuitions/sarashina2.2-3b-instruct-v0.1ã«ã€ Kendamarron/jimba-instruction-allã¨SousiOmine/Japanese-Pythonic-FunctionCallã‚’ç”¨ã„ãŸQLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ pythoné–¢æ•°ã®å‘¼ã³å‡ºã—ã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€SakanaAI/TinySwallow-1.5B-Instructã«å¯¾ã—ã¦ã€GRPOã«ã‚ˆã‚Šé«˜æ©‹ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸè¿½åŠ å­¦ç¿’ã‚’æ–½ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - èª¿æ•´ã—ãŸã„æ–¹å‘ã¨ã¯ã ã„ã¶ç•°ãªã‚‹æ–¹å‘ã«ãšã‚ŒãŸã€‚
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Qwen/Qwen2.5-32Bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸAbejaç¤¾ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«DeepSeekç¤¾ã®R1è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã‚’æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸcyber agentç¤¾ã®cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ChatVectorã‚’ç”¨ã„ã¦åŠ ãˆãŸã‚‚ã®ã«ã€ç‹¬è‡ªã®æ—¥æœ¬èªå¼·åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãªã‚Šã¾ã™ã€‚
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - modernbert-base-japanese-char Model Description
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€text-embeddings-inference (TEI) ã§ã€mecab / unidic ãªã©ã‚’ç”¨ã„ãŸæ—¥æœ¬èªTokenizerã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€dummy ã® tokenizer.json ã‚’ç”¨ã„ã¦ç„¡ç†ã‚„ã‚Šå‹•ã‹ã™ æ–¹æ³•ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6Bæ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­å¤§æ¨¡å‹ï¼Œæœ¬é¡¹ç›®ä¸ºChatGLM3-6BåŠ å…¥æ—¥æ–‡èƒ½åŠ›ã€‚
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹HODACHI/Llama-3.1-70B-EZO-1.1-itã®ggufç‰ˆã§ã™ã€‚
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
- [pfnet/Preferred-MedRECT-32B](https://huggingface.co/pfnet/Preferred-MedRECT-32B)
  - Preferred-MedRECT-32B Model Description Preferred-MedRECT-32B is a finetuned model based on Qwen/Qwen3-32B, which has been optimized for medical error detection and correction tasks using LoRA (Low-Rank Adaptation).
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B pre-trained model for Japanese Model Description GPT2/GPT3 like model trained on Japanese.corpus.
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - deberta-base-japanese-wikipedia-ud-head Model Description
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - roberta-large-japanese-aozora-ud-goeswith Model Description
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - roberta-base-japanese-luw-upos Model Description
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This model is for transcribing audio into Hiragana, one format of Japanese language.
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - NLLB-200 1.3B fine-tuned on Ascendance of a Bookworm
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - nlp-waseda/gpt2-xl-japanese This is Japanese GPT2 with approximately 1.5B parameters pretrained on Japanese Wikipedia and CC-100
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - Japanese BERT-base (Sudachi + BPE) How to load the tokenizer Please download the dictionary file for Sudachi + BPE from our GitHub repository.
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID æ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã‹ã‚‰ã€æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚ã‚‹å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ Model Details Model Description ä¾‹ãˆã°ã€ã€Œæ±äº¬ã®è‚‰æ–™ç†ã§ã€æ˜¥ã«é£Ÿã¹ã‚‰ã‚Œã‚‹ã€é¶è‚‰ã‚’ä½¿ã£ãŸæ–™ç†ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ ã€Œæ±äº¬ â†’ éƒ½é“åºœçœŒ/åœ°æ–¹(AREA)ã€ ã€Œè‚‰æ–™ç† â†’ ç¨®é¡(TYPE)ã€ ã€Œæ˜¥ â†’ å­£ç¯€(SZN)ã€ ã€Œé¶è‚‰ â†’ é£Ÿæ(INGR)ã€ ã®ã‚ˆã†ã«ã€å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ æŠ½å‡ºå¯¾è±¡ã¯ã€AREAã€TYPEã€SZNã€INGRã®ï¼”ã¤ã§ã™ Language(s) (NLP): æ—¥æœ¬èª License: mit Finetuned from model: tohoku-nlp/bert-base-japanese-v2 Model Sources Repository: wolf4032/nlp-token-classification ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã‚¢ãƒ—ãƒªã®ä½œæˆã«ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ãŒæ²è¼‰ã•ã‚Œã¦ã„ã¾ã™ Documentation: Qiita Demo: wolf4032/japanese-token-classificatio
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling: Multilingual Gemma Update @ 2024.04.15: First release of Gemma-Mling 7B model Original Gemma Model Page:
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech-v0.1 Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - jvnvã‚³ãƒ¼ãƒ‘ã‚¹ã®F2ã‹ã‚‰å­¦ç¿’ã—ã¦ä½œæˆã—ãŸbert-vits2ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - t5-base-xlsum-ja
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - whisper-large-v2-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-jp to the CTranslate2 model format.
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - bert-large-japanese-wikipedia-ud-head Model Description
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - What is this model?
- [Aratako/Amaterasu-123B](https://huggingface.co/Aratako/Amaterasu-123B)
  - Amaterasu-123B GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€mistralai/Mistral-Large-Instruct-2411ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥è‹±æ··åˆã®ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚„å°èª¬åŸ·ç­†ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - roberta-large-japanese-luw-upos Model Description
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - deberta-large-japanese-wikipedia-ud-goeswith Model Description
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - rinna/nekomata-14b-gguf Overview The model is the GGUF version of rinna/nekomata-14b.
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2 Japanese base model version 2 Prerequisites transformers==4.19.2 Model architecture This model uses GPT2 base setttings except vocabulary size.
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to solve error detection and correction task.
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-book/wrime-sentiment ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adamw Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - ãƒ‰ãƒŸãƒ‹ã‚ªãƒ³æ—¥æœ¬èªLLM for Whisperï¼ˆ2023/12/19 1.0ç‰ˆï¼‰ æ¦‚è¦ Whisperã§ãƒ‰ãƒŸãƒ‹ã‚ªãƒ³ï¼ˆãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ï¼‰ã®ã‚«ãƒ¼ãƒ‰ç”¨èªãªã©ã‚’å«ã‚“ã éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—å‡ºæ¥ã‚‹ã“ã¨ã‚’ç›®æ¨™ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸLLMã§ã™ã€‚
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ line-corporation/japanese-large-lm-1.7bã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ï¼Œsftã«ã‚ˆã‚‹full instruction tuningã‚’è¡Œã„ã¾ã—ãŸï¼
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39æ§˜ã® Tora-7B-v0.2 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - Wav2Vec2-XLS-R-300M-Japanese-Hiragana Fine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset.
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - deberta-base-japanese-upos Model Description
- [espnet/kan-bayashi_jsut_conformer_fastspeech2_transformer_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2_transformer_prosody)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_conformer_fastspeech2_transformer_prosody â™»
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime ã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - roberta-large-japanese-aozora-ud-head Model Description
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - roberta-base-japanese-aozora-ud-head Model Description
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - Japanese-Alpaca-2-13B Japanese-Alpaca-2-13Bã¯æŒ‡ç¤ºå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm Model description This is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - Japanese Stable LM Instruct Gamma 7B +
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ calm-2-7b-chat ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - Model Card for Model ID
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - deberta-large-japanese-wikipedia-ud-head Model Description
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - Japanese BERT-base (MeCab + BPE) How to load the tokenizer Please download the dictionary file for MeCab + BPE from our GitHub repository.
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - INPUT: Japanese name in ROMAJI FORM OUTPUT:
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - ãŠçŸ¥ã‚‰ã› ã‚ˆã‚Šå›ç­”ãŒé©åˆ‡ã«ãªã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã€https://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq ã‚‚ã‚ã‚Šã¾ã™ã€‚
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - output ç­‘æ³¢ 2.0035860538482666 ã¤ãã° 1.6586617231369019 ç ”ç©¶ 1.6227693557739258 å¤§å­¦ 1.3798155784606934 å®Ÿé¨“ 0.5522942543029785 å­¦ç”Ÿ 0.42351895570755005 åˆ†æ 0.37844282388687134 å›½ç«‹ 0.3685397505760193 ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ 0.36495038866996765 èŒ¨åŸ 0.3056415021419525 ç§‘å­¦ 0.2876652181148529 é–¢æ± 0.24301066994667053 åœ°åŸŸ 0.21340851485729218 å®Ÿæ–½ 0.1976248174905777 å…ˆç«¯ 0.192025288939476 ã‚µã‚¤ãƒˆ 0.11629197001457214 èª¿æŸ» 0.09159307181835175 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ 0.08552580326795578 è­°è«– 0.07484486699104309 æ¤œè¨ 0.007034890353679657
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - Ninja-v1-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸTokara-0.5B-v0.1ã«chat vectorã§å¯¾è©±èƒ½åŠ›ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2 model size: 417.12M trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - Japanese BERT-base (Vaporetto + Unigram)
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - Japanese BERT-base (Sudachi + Unigram)
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - deberta-large-japanese-aozora-ud-head Model Description
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-largeã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JCommonsenseQA(é¸æŠå¼å¿œç­”)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - Model Card for Model ID
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - uniTKU-hubert-japanese-asr
- [espnet/kan-bayashi_jsut_conformer_fastspeech2_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause â™»
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 xl on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ studio-ousia/luke-japanese-large-lite ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸã€‚
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
- [line-corporation/japanese-mulan-base](https://huggingface.co/line-corporation/japanese-mulan-base)
  - japanese-mulan-base This is a Japanese MuLan (Music-Language pretraining) model developed by LY Corporation.
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause â™»
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - Description A Japanese-specialized SentencePiece tokenizer trained for AI Novelist's SuperTrin and Damsel 20B models.
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - jvnvã‚³ãƒ¼ãƒ‘ã‚¹ã®F2ã‹ã‚‰å­¦ç¿’ã—ã¦ä½œæˆã—ãŸbert-vits2ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - llm-jp-13b-instruct-lora-jaster-v1.0
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m ğŸ MambaSan-370m is the first chat Japanese language model based on a state-space model architecture (Mamba).
- [OmeletRice/japanese-wav2vec2-base-bf16-asmr](https://huggingface.co/OmeletRice/japanese-wav2vec2-base-bf16-asmr)
  - ASMRãƒ‡ãƒ¼ã‚¿ã§reazon-research/japanese-wav2vec2-baseã‚’è¿½åŠ äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ« bf16 Model Card for Model ID Model Details Model Description
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - Japanese-Novel-Reward-modernbert-ja-130m ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/modernbert-ja-130mã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Japanese-Novel-Reward-TinySwallow-1.5B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯SakanaAI/TinySwallow-1.5Bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - èª¿æ•´ã—ãŸã„æ–¹å‘ã¨ã¯å°‘ã—ãšã‚ŒãŸãŒã€AIã®å£èª¿ã¯å¥³ã®å­é¢¨ã«ã€‚
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_full_band_vits_prosody â™»
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ä¸Šè¨˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚¢ãƒ€ãƒ«ãƒˆç”¨èªã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent â™»
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent â™»
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech â™»
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§pkshatech/GLuCoSE-base-jaã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chatã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details â€»å¥½å¥‡å¿ƒã‹ã‚‰ç”Ÿã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - æ¦‚è¦ GLM-4-9B-Chatã‚’ã€æ—¥æœ¬èªã®Wikiãƒ‡ãƒ¼ã‚¿ã‚’é¸å®šã—ã€è¿½åŠ å­¦ç¿’ã—ãŸæ—¥æœ¬èªã«éå¸¸ã«å¼·ã„ã‚¹ã‚³ã‚¢ã‚’å‡ºã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct ğŸš¨ This model is tuning to RP and knowledge is likely unstable.
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2ã®ãƒã‚¤ãƒŠãƒ¼ãƒã‚§ãƒ³ã‚¸ç‰ˆã§ã™ã€‚
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave â™»
## Datasets

This list is sorted by downloads as of November 18, 2025.
655 datasets are listed.

- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ éå»ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ éå»ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯ã€ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ ã®ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹ã‹ã‚‰ç¾åœ¨ã¾ã§ã®ã™ã¹ã¦ã®éå»ãƒ­ã‚°ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - The Cauldron is a massive collection of 50 vision-language datasets (training sets only) that were used for the fine-tuning of the vision-language model Idefics2.
- [nvidia/Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan)
  - Nemotron-Personas-Japan ç¾å®Ÿä¸–ç•Œã®åˆ†å¸ƒã«åŸºã¥ã„ãŸãƒšãƒ«ã‚½ãƒŠç”Ÿæˆã®ãŸã‚ã®è¤‡åˆAIã‚¢ãƒ—ãƒ­ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ (Dataset Overview) Nemotron-Personas-Japan ã¯ã€æ—¥æœ¬ã«ãŠã‘ã‚‹äººå£ã®å¤šæ§˜æ€§ã¨è±Šã‹ã•ã‚’æ‰ãˆã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã€å®Ÿä¸–ç•Œã®äººå£çµ±è¨ˆã€åœ°ç†çš„åˆ†å¸ƒã€æ€§æ ¼ç‰¹æ€§ã®åˆ†å¸ƒã«åŸºã¥ã„ã¦åˆæˆçš„ã«ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023:
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - We provide an Amazon product reviews dataset for multilingual text classification.
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100: æ—¥æœ¬èªinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Data Description æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB:
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - ğŸ· FineWeb2 Edu Japanese: High-Quality Educational Japanese
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec: Gender Dection from Japanese Names with Machine Learning
- [cc-clean/CC-MAIN-2025-08](https://huggingface.co/datasets/cc-clean/CC-MAIN-2025-08)
  - CC-MAIN-2025-08ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - Please feel free to open an issue or pull request.
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST Dataset lassify images from the KMNIST dataset into one of the 10 classes, representing different Japanese characters.
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - Maintainers Junfeng Jiang@Aizawa Lab: jiangjf (at) is.s.u-tokyo.ac.jp Jiahao Huang@Aizawa Lab: jiahao-huang (at) g.ecc.u-tokyo.ac.jp
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’UVRã‚’ä½¿ç”¨ã—ã¦BGMã‚„ãƒã‚¤ã‚ºé™¤å»ã—ãŸã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 æ—¥æœ¬èªã¯ã“ã¡ã‚‰ japanese-anime-speech-v2 is an audio-text dataset designed for training automatic speech recognition models.
- [NandemoGHS/Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice)
  - Japanese-Eroge-Voice Description
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023:
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-bokete", split="train") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - This dataset contains a diverse set of natural Japanese speech, collected from terrestrial television streams.
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda - Questions for Japanese models Repository:
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - CC-news-2024-July-October-cleaned ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Common Crawlã®newsã‚µãƒ–ã‚»ãƒƒãƒˆã‹ã‚‰ä½œæˆã—ãŸ2024å¹´7æœˆã‹ã‚‰10æœˆã®æ—¥æœ¬èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ–‡ç« ãŒåéŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - CC-MAIN-2019-35ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - The dataset was extracted from Common Crawl dumps covering February 2024 â€“ January 2025 and contains roughly 56M Japanese documents, 110B characters, and 249M images.
- [takarajordan/takaraspider](https://huggingface.co/datasets/takarajordan/takaraspider)
  - TakaraSpider Japanese Web Crawl Dataset Dataset Summary TakaraSpider is a large-scale web crawl dataset specifically designed to capture Japanese web content alongside international sources.
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - Japan Diverse Images Dataset Overview This dataset is a comprehensive collection of high-quality images capturing the diverse aspects of Japan, including urban landscapes, natural scenery, historical sites, contemporary art, everyday life, and culinary experiences.
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - CC-MAIN-2019-39ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - llm-japanese-dataset LLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³(ãƒãƒ£ãƒƒãƒˆ)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ä¸»ã«ï¼Œè‹±èªã§æ§‹ç¯‰ã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese Anime Speech Dataset æ—¥æœ¬èªã¯ã“ã¡ã‚‰ japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - VNTL Leaderboard
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - relaion2B-en-research-safe-japanese-translation This dataset is the Japanese translation of the English subset of ReLAION-5B (laion/relaion2B-en-research-safe),
- [humanalysis-square/KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10)
  - KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations Overview KokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations.
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - [Under Construction]
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023:
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - In this study, we introduce a new dataset, WRIME, for emotional intensity estimation.
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - AutoWikiQA æ±å·¥å¤§ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MXã‚’ç”¨ã„ã¦ã€Wikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€Œè³ªå•(query)ã€ã¨ã€Œå›ç­”(answer)ã€ã‚’ç”Ÿæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã¨å›ç­”ã«ã¤ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [SakanaAI/EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench)
  - EDINET-Bench ğŸ“š Paper | ğŸ“ Blog | ğŸ§‘â€ğŸ’» Code EDINET-Bench is a Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction.
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA : Japanese Question Answering with Retrieval Augmentation - æ¤œç´¢æ‹¡å¼µ(RAG)è©•ä¾¡ã®ãŸã‚ã®æ—¥æœ¬èª Q&amp;A ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ é«˜æ€§èƒ½ãª LLM ã®å°é ­ã«ä¼´ã„ã€LLM ã‚’ç”¨ã„ãŸè³ªç–‘å¿œç­”ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚
- [alfredplpl/image-text-pairs-ja-cc0-2](https://huggingface.co/datasets/alfredplpl/image-text-pairs-ja-cc0-2)
  - ã¯ã˜ã‚ã« ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ç”»åƒç”Ÿæˆã§æ—¥æœ¬èªã‚’ç”Ÿæˆã—ãŸã„ã¨ãã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - cc100-ja-documents HuggingFace ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ cc100 / cc100-ja ã¯ line å˜ä½ã®åˆ†å‰²ã®ãŸã‚ã€document å˜ä½ã«çµåˆã—ãŸã‚‚ã®ã§ã™ã€‚
- [pfnet/bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja)
  - BBH-ja (æ—¥æœ¬èªç‰ˆBIG-Bench Hard) BBH-jaã¯ã€BIG-Bench Hard (Paper, GitHub) ã‚’ç¿»è¨³ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹ã€‚
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: aiishii/JEMHopQA on GitHub JEMHopQA JEMHopQA (Japanese Explainable Multi-hop Question Answering)ã¯ã€å›ç­”å°å‡ºã‚¹ãƒ†ãƒƒãƒ—ã®æƒ…å ±ä»˜ãã®æ—¥æœ¬èªã®æ ¹æ‹ æƒ…å ±ä»˜ããƒãƒ«ãƒãƒ›ãƒƒãƒ—QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - fineweb-2-edu-japanese ã® small_tokens ã® text ã‚«ãƒ©ãƒ ã‚’ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–(NFKC)ã—ãŸã‚‚ã®ã‚’ fineweb-2-japanese-text-cleaner ã‚’ä½¿ã£ã¦ãƒã‚¤ã‚ºç®‡æ‰€ã‚’æ¨è«–ã—ãŸRAWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [sbintuitions/JamC-QA](https://huggingface.co/datasets/sbintuitions/JamC-QA)
  - This benchmark evaluates knowledge specific to Japan through multiple-choice questions.
- [DSULT-Core/2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc)
  - 2ch.sc Corpus A Large-Scale Japanese Anonymous Web Forum
- [nguyenthanhasia/japanese-bar-exam-qa](https://huggingface.co/datasets/nguyenthanhasia/japanese-bar-exam-qa)
  - Japanese Bar Examination QA Dataset Dataset Summary This dataset contains question-answer pairs from the Japanese Bar Examination (å¸æ³•è©¦é¨“, Shihou Shiken) spanning from 2015 to 2024.
- [malaysia-ai/Emilia-YODAS-Voice-Conversion](https://huggingface.co/datasets/malaysia-ai/Emilia-YODAS-Voice-Conversion)
  - Emilia-YODAS-Voice-Conversion We sample https://huggingface.co/datasets/amphion/Emilia-Dataset YODAS set for voice conversion.
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - Anime with caption CC-0 dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¤ãƒ©ã‚¹ãƒˆã«å¯¾ã™ã‚‹æ—¥æœ¬èªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ å€«ç†çš„ã«å­¦ç¿’ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - Reranker-Scores æ—¢å­˜ã®æ—¥æœ¬èªæ¤œç´¢ãƒ»QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸­ã®ã‚¯ã‚¨ãƒªã«ä»˜ä¸ã•ã‚ŒãŸæ­£ãƒ»è² ä¾‹ã®é–¢é€£åº¦ã‚’å¤šè¨€èªãƒ»æ—¥æœ¬èªreranker 5ç¨®é¡ã‚’ç”¨ã„ã¦ã‚¹ã‚³ã‚¢ä»˜ã‘ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - This is a Japanese translated version of HumanEval, an evaluation harness for the HumanEval problem solving dataset described in the paper "Evaluating Large Language Models Trained on Code".
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - Overview This dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 Dataset Description JA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU Japanese Massive Multitask Language Understanding Benchmark JMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - range3/cc100-ja This dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU:
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - For the English version, please click here.
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - Please feel free to open an issue or pull request.
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard ã¨ã¯ Allganize RAG Leaderboard ã¯ã€5ã¤ã®æ¥­ç¨®ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆé‡‘èã€æƒ…å ±é€šä¿¡ã€è£½é€ ã€å…¬å…±ã€æµé€šãƒ»å°å£²ï¼‰ã«ãŠã„ã¦ã€æ—¥æœ¬èªã®RAGã®æ€§èƒ½è©•ä¾¡ã‚’å®Ÿæ–½ã—ãŸã‚‚ã®ã§ã™ã€‚
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This is the filtered Japanese subset of XL-Sum followed by PaLM 2 filters 15-gram overlap * code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86a number of examples train: 4215 (before: 7113) validation: 758 (before: 889) test: 766 (before: 889)
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth: Japanese/English Synthetic Conversation Dataset Derived from LMSYS-Chat-1M
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - Introduction This is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA This dataset is hf mirror of https://registry.opendata.aws/abeja-cc-ja/ Please Refer to https://tech-blog.abeja.asia/entry/abeja-cc-ja-202409 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯https://registry.opendata.aws/abeja-cc-ja/ã®HFãƒŸãƒ©ãƒ¼ã§ã™ã€‚
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - Dataset.
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully Dataset åˆ©ç”¨è¦ç´„ åˆ©ç”¨è¦ç´„ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªãŠã‚ˆã³ä»–ã®è¨€èªã®LLMã®å®‰å…¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†ç›®çš„ã®ãŸã‚ã€å•†ç”¨åˆ©ç”¨ã‚‚å«ã‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹2010 ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’huggingfaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ï½¡ 2009 å¹´åº¦ã«ãŠã‘ã‚‹è‘—ä½œæ¨©æ³•ã®æ”¹æ­£ï¼ˆå¹³æˆ21å¹´é€šå¸¸å›½ä¼š è‘—ä½œæ¨©æ³•æ”¹æ­£ç­‰ã«ã¤ã„ã¦ | æ–‡åŒ–åºï¼‰ã«åŸºã¥ãï¼Œæƒ…å ±è§£æç ”ç©¶ã¸ã®åˆ©ç”¨ã«é™ã£ã¦åˆ©ç”¨å¯èƒ½ã§ã™ï½¡ å½¢æ…‹ç´ è§£æã‚’ç”¨ã„ã¦ï½¤è‡ªå‹•ã§å¥ç‚¹ã‚’ã¤ã‘ã¾ã—ãŸï½¡ å¤‰æ›ã‚³ãƒ¼ãƒ‰ å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ å½¢æ…‹ç´ è§£æãªã©
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This is the information integration of erai-raws and myanimelist.
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ãŸã‚‚ã®ã§ã™ã€‚
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­ã®256æ–‡å­—ä»¥ä¸‹ã®è¡Œã‚’æŠ½å‡ºã—ãƒãƒ¼ã‚¸ã—ã¾ã—ãŸã€‚
- [UEC-InabaLab/KokoroChat](https://huggingface.co/datasets/UEC-InabaLab/KokoroChat)
  - KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors KokoroChat is the largest human-collected Japanese psychological counseling dialogue dataset to date (as of June 2025).
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - VOICEVOXã‚’ä½¿ã£ãŸäººå·¥éŸ³å£°ãƒœã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ ITAã‚³ãƒ¼ãƒ‘ã‚¹ ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ã‚³ãƒ¼ãƒ‘ã‚¹ ROHANã‚³ãƒ¼ãƒ‘ã‚¹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé‡æƒ…å ± ãƒ•ã‚©ãƒ«ãƒ€å†…ã®.
- [TheFinAI/OCR_Task_JA](https://huggingface.co/datasets/TheFinAI/OCR_Task_JA)
  - Japanese_OCR
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - [github].
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - wikipedia æ—¥æœ¬èªã®æ–‡ã‚’ã€å„ç¨®æ—¥æœ¬èªã® embeddings ã‚„ faiss index ã¸ã¨å¤‰æ›ã—ãŸã‚‚ã®ã€‚
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - fungi_diagnostic_chars_comparison_japaneseå¤§èŒè¼ªã€Œè­˜åˆ¥å½¢è³ªã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ /
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - Dataset Details Dataset Sources Repository: Helsinki-NLP/Tatoeba-Challenge Detail: Japanese - Korean jpn-kor Uses The dataset can be used to train the translation model that translates Japanese sentence to Korean.
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - ã‚·ãƒ³ãƒ—ãƒ«ãšã‚“ã ã‚‚ã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã¯ã˜ã‚ã« ãšã‚“ã ã‚‚ã‚“ã®è¨­å®šãŒè©°ã¾ã£ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - æ¦‚è¦ llm-jp-instructionsã¯äººæ‰‹ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS ScreenTalk_JA2ZH-XS is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - fungi_indexed_mycological_papers_japanese å¤§èŒè¼ªã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2025/5/2ï¼ˆR3-12744ã¾ã§ï¼‰ Languages Japanese This dataset is available in Japanese only.
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - Japanese Voice Dataset Combined This dataset combines multiple high-quality Japanese voice datasets to create a comprehensive collection of Japanese speech data.
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - japanese-asr/whisper_transcriptions.reazon_speech_all without audio
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ SentenceTransformes ã§å­¦ç¿’ã—ã‚„ã™ã„ã‚«ãƒ©ãƒ åã¨æ§‹é€ ã«å¤‰æ›ã—ãŸã‚‚ã®ã€‚
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - æ¦‚è¦ oscar-corpus/OSCAR-2301ã®ä»¥ä¸‹ã®jaã®ã¿ã‚’ corpus-cleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œãªã£ãŸãƒ‡ãƒ¼ã‚»ãƒƒãƒˆç¾¤ Code Language # docs # words Content Length : ja Japanese 94,236,404 4,401,059,165 181.2 GB ãŸã ã—ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãŒæˆåŠŸã—ã¦ã„ãªã„ãŸã‚é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
- [Silviase/JPCharRecog_v1.1](https://huggingface.co/datasets/Silviase/JPCharRecog_v1.1)
  - JPCharRecog_v1.1 JPCharRecog v1.1 ã¯ã€æ—¥æœ¬èªã®å˜æ–‡å­—ï¼ˆCJK ã‚’å«ã‚€ï¼‰èªè­˜ã‚’ç›®çš„ã¨ã—ãŸã€åˆæˆãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD: Japanese Mathematical Dataset with Assured Reasoning Description English / Japanese Overview JaMARD (Japanese Mathematical Dataset with Assured Reasoning Description) is a high-quality synthetic dataset for Japanese mathematical problems with chain-of-thought reasoning, where the correctness of synthetic instances is assured.
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - Umamusume-voice-transcription Total charcters: 77 Comes with transcription.
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆFineWeb2-HQ å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å¤šè¨€èªã§å·¨å¤§ãªãŸã‚ã€æ‰±ã„ã‚„ã™ã„ç”¨ã«æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’ç´„200GBã ã‘æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ wc çµæœ 1763269 38541549 5370473709 fineweb_jpn_Jpan_chunk_0.jsonl 1784158 37430170 5370514369 fineweb_jpn_Jpan_chunk_1.jsonl 1639554 40065129 5370372344 fineweb_jpn_Jpan_chunk_10.jsonl 1575127 42167166 5370298354 fineweb_jpn_Jpan_chunk_11.jsonl 1686375 39225898 5370402506 fineweb_jpn_Jpan_chunk_12.jsonl 1786948 36456352 5370498572 fineweb_jpn_Jpan_chunk_13.jsonl 1700447 38657869 5370422377 fineweb_jpn_Jpan_chunk_14.jsonl 1649880 402340
- [Aratako/Japanese-Creative-Writing-GLM4.5](https://huggingface.co/datasets/Aratako/Japanese-Creative-Writing-GLM4.5)
  - Japanese-Creative-Writing-GLM4.5 æ¦‚è¦ æ—¥æœ¬èªã®å°èª¬åŸ·ç­†ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Japanese-Creative-Writing-39.6kã‹ã‚‰ä¸€éƒ¨ã®æŒ‡ç¤ºã‚’æŠ½å‡ºã—ã€zai-org/GLM-4.5ã§å¿œç­”ã‚’å†ç”Ÿæˆã—ãŸç´„8000ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ567077ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - llm-japanese-dataset-vanilla LLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ izumi-lab/llm-japanese-dataset ã‹ã‚‰ï¼Œæ—¥è‹±ç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ã‚’æŠœã„ãŸã‚‚ã®ã§ã™ï¼
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset is a clarified version of the image, context, and question set included in the Japanese-Heron-Bench for the construction of the Japanese evaluation benchmark suite.
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - Elite Voice Project ã“ã‚Œã¯ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æ‰€å±Vtuberã•ãã‚‰ã¿ã“æ°ã®å£°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŒ–ã—éŸ³å£°èªè­˜ãªã©ã§æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹äº‹ã‚’ç›®çš„ã¨ã—ãŸéå…¬å¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025:
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - Dataset Summary This is the Business Scene Dialogue (BSD) dataset, a Japanese-English parallel corpus containing written conversations in various business scenarios.
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - Vietnamese-Japanese Parallel Corpus ğŸŒŸ If you find this project valuable, please consider starring our VNJPTranslate GitHub repo!
- [Yukiyoke-Lab/Tsukuyomi-chan_datasets](https://huggingface.co/datasets/Yukiyoke-Lab/Tsukuyomi-chan_datasets)
  - å…±é€šãƒ©ã‚¤ã‚»ãƒ³ã‚¹ ã™ã¹ã¦ã®ä»–ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚ˆã‚Šã‚‚å…±é€šãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒå„ªå…ˆã•ã‚Œã¾ã™ã€‚
- [MakiAi/the-Embodiment-of-Scarlet-Devil-Instruct-Alpaca-QA-JP-v1](https://huggingface.co/datasets/MakiAi/the-Embodiment-of-Scarlet-Devil-Instruct-Alpaca-QA-JP-v1)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ayousanz/OSCOR-2301-ja-cleaned-0](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned-0)
  - æ¦‚è¦ oscar-corpus/OSCAR-2301ã®ä»¥ä¸‹ã®jaã®ã¿ã‚’ corpus-cleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œãªã£ãŸãƒ‡ãƒ¼ã‚»ãƒƒãƒˆç¾¤ Code Language # docs # words Content Length : ja Japanese 94,236,404 4,401,059,165 181.2 GB
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - msmarco-ja-hard-negatives hpprc/msmarco-ja ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹MS MARCOã®æ—¥æœ¬èªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã«ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’åŠ ãˆãŸãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚’ã—ãŸã‚‚ã®ã§ã™ã€‚
- [neoai-inc/Japanese-RAG-Generator-Benchmark](https://huggingface.co/datasets/neoai-inc/Japanese-RAG-Generator-Benchmark)
  - Japanese RAG Generator Benchmark: æ—¥æœ¬èª RAG ã«ãŠã‘ã‚‹ Generator è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€  { "question": "è³ªå•æ–‡ (str)", "answer": "æ­£ç­” (str)", "positive": "æ­£è§£ã¨ãªã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (list[str])",
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - Speech-Translation-Instructions The instructions translated from 120 languages Common Voice to english, arabic, japanese, mandarin and french from common voice speech dataset.
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/bokete-ogiri-test", split="test") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [realoperator42/anime-titles-dataset](https://huggingface.co/datasets/realoperator42/anime-titles-dataset)
  - Anime Dataset Dataset Description This dataset contains comprehensive information about anime series scraped from MyAnimeList (MAL).
- [stockmark/u4-table-cell-qa](https://huggingface.co/datasets/stockmark/u4-table-cell-qa)
  - TableCellQA
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - fungi_trait_circus_database å¤§èŒè¼ªã€ŒTrait Circusã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆçµ±åˆ¶å½¢è³ªï¼‰ æœ€çµ‚æ›´æ–°æ—¥ï¼š2025/09/28 é‡è¦ï¼šãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å¤§å¹…ã«æ›´æ–°ã—ã¾ã—ãŸï¼ˆv2.0ï¼‰
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - Places in japan.
- [MakiAi/Orin-Instruct-Alpaca-JP-v9](https://huggingface.co/datasets/MakiAi/Orin-Instruct-Alpaca-JP-v9)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ks-pf/JMTEB-fixed](https://huggingface.co/datasets/ks-pf/JMTEB-fixed)
  - JMTEB-fixed ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å…ƒã®JMTEBãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset A voice dataset collected from Project Sekai charactor Emu Otori Introduction Size: 2735, all WAV format.
- [mteb-private/JapaneseCode1Retrieval-sample](https://huggingface.co/datasets/mteb-private/JapaneseCode1Retrieval-sample)
  - JapaneseCode1Retrieval-sample A sample dataset for Japanese-English code retrieval evaluation.
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - Dataset Preprocessing Supported Tasks and Leaderboards Languages æ³¨é‡ˆã¯ã™ã¹ã¦æ—¥æœ¬èªã‚’ä¸»è¦è¨€èªã¨ã—ã¦ã„ã¾ã™ã€‚
- [cc-clean/CC-MAIN-2015-11](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-11)
  - CC-MAIN-2015-11ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: yahoojapan/JGLUE on GitHub datasets/jcommonsenseqa-v1.1 JCommonsenseQA JCommonsenseQA is a Japanese version of CommonsenseQA (Talmor+, 2019), which is a multiple-choice question answering dataset that requires commonsense reasoning ability.
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - Dataset Summary SNOW T15:The simplified corpus for the Japanese language.
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLMã®ãŸã‚ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸ å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€ æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›å¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - defamation_japanese_twitter Twitteræ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Dataset Summary SNSã«ãŠã‘ã‚‹èª¹è¬—ä¸­å‚·æ¤œå‡ºã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: yahoojapan/JGLUE on GitHub JSQuAD JSQuAD is a Japanese version of SQuAD (Rajpurkar+, 2016), one of the datasets of reading comprehension.
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - Dataset 5M (5121625) clean Japanese full sentence with the context.
- [realoperator42/anime-characters](https://huggingface.co/datasets/realoperator42/anime-characters)
  - Anime Character Dataset This dataset contains detailed information about anime characters scraped from MyAnimeList.
- [MakiAi/Orin-Instruct-Alpaca-JP-v10](https://huggingface.co/datasets/MakiAi/Orin-Instruct-Alpaca-JP-v10)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [nishika-nm/geniac-iam-corpus-ja](https://huggingface.co/datasets/nishika-nm/geniac-iam-corpus-ja)
  - GENIAC IAM Corpus (Japanese)
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - neody/oscar-ja-cleanedã®ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰256æ–‡å­—ä»¥ä¸‹ã®ã‚‚ã®ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") æ¦‚è¦ å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã®ã€å†™çœŸå·æŸ³ã€ã¨ã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ YANS å§”å“¡ãŒä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å«ã¿ã¾ã™ã€‚
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - databricks-dolly-15k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - Dataset Summary 53,640 Japanese tweets with annotation if a tweet is related to COVID-19 or not.
- [sbintuitions/WildGuardTestJP](https://huggingface.co/datasets/sbintuitions/WildGuardTestJP)
  - WildGuardTestJP WildGuardTestJPã¯ã€æ—¥æœ¬èªã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [OmniAICreator/Japanese-Wikipedia-202506](https://huggingface.co/datasets/OmniAICreator/Japanese-Wikipedia-202506)
  - Japanese-Wikipedia-202506
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - Dataset details: Each entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - Githubãƒªãƒã‚¸ãƒˆãƒªstockmarkteam/ner-wikipedia-datasetã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [Silviase/JPMultiCharRecog](https://huggingface.co/datasets/Silviase/JPMultiCharRecog)
  - Japanese Multi-Character Recognition Dataset Dataset Description This dataset is designed to evaluate Vision-Language Models (VLMs) on their ability to distinguish between meaningful Japanese words and meaningless character sequences.
- [ASLP-lab/Multilingual-Alpaca-Speech](https://huggingface.co/datasets/ASLP-lab/Multilingual-Alpaca-Speech)
  - Multilingual Alpaca Speech Dataset Multilingual Alpaca Speech is a high-quality speech instruction-following dataset supporting Japanese (ja), German (de), and French (fr) .
- [MakiAi/Orin-Instruct-Alpaca-JP-v8](https://huggingface.co/datasets/MakiAi/Orin-Instruct-Alpaca-JP-v8)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - range3/wiki40b-ja This dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
- [ce-lery/merged-corpus](https://huggingface.co/datasets/ce-lery/merged-corpus)
  - Merged Corpus Welcome to this repository.
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - CC-MAIN-2019-30ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - GSM8K Japanese Slim Japanese translated version of openai/gsm8k, and the answer extracted from descriptions.
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - oasst1-89k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-test", split="test") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - range3/wikipedia-ja-20230101
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN Dataset CT-RATE-JPN is a Japanese-translated version of radiology reports from the CT-RATE dataset, which contains chest CT volumes paired with corresponding radiology reports.
- [hotchpotch/JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR)
  - JFWIR - Japanese FineWeb Information Retrieval:
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ GitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/ LICENSE: CC-BY-SA 3.0 Developed by Stockmark Inc.
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - Synthetic-JP-EN-Coding-Dataset-801k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ801262ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - OpenMathInstruct-1 ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãª180ä¸‡ä»¶ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp32](https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp32)
  - Chain of Thoughtç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å•é¡Œã¨è§£ç­”ã‹ã‚‰èª¬æ˜ï¼ˆChain of Thoughtï¼‰ã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [cc-clean/CC-MAIN-2025-05](https://huggingface.co/datasets/cc-clean/CC-MAIN-2025-05)
  - CC-MAIN-2025-05ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp40](https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp40)
  - Chain of Thoughtç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å•é¡Œã¨è§£ç­”ã‹ã‚‰èª¬æ˜ï¼ˆChain of Thoughtï¼‰ã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench Dataset Description Japanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") æ¦‚è¦ å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã®ã€å†™çœŸå·æŸ³ã€ã¨ã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)
  - reasoningãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ã€‚
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja: Spoken Dialogue Dataset in Japanese DailyTalkContiguous-ja is a synthetic multi-turn Japanese conversational speech dataset in which DailyTalk
- [llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-Qwen3-14B](https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-Qwen3-14B)
  - Chain of Thoughtç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å•é¡Œã¨è§£ç­”ã‹ã‚‰èª¬æ˜ï¼ˆChain of Thoughtï¼‰ã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - fineweb-2-edu-japanese-scores fineweb-2æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ•™è‚²çš„ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (0-4æ®µéš) æ¦‚è¦: ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€FineWeb-Edu classifier ã®æ‰‹æ³•ã«å€£ã„ã€Deepseek API ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡ã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ fineweb-2 æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ•™è‚²çš„è¦–ç‚¹ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - CABank Japanese CallHome Corpus Participants: 120 Type of Study: phone call Location: United States Media type: audio DOI: doi:10.21415/T5H59V Web: https://ca.talkbank.org/access/CallHome/jpn.html Citation information Some citation here.
- [reep0610/AGI-japanese-text-dataset-for-Deep-Learning](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-for-Deep-Learning)
  - ğŸ“„ã€åˆ©ç”¨è¦ç´„ã€‘ æœ¬å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ã€äººé¡ã¨AIã®å…±æœ‰è²¡ç”£ã¨ã—ã¦æä¾›ã•ã‚Œã¾ã™ã€‚
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA Dataset Description Japanese Image Classification Visual Question Answering (JIC-VQA)
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - llm-book/aio-passages ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€llm-book/bert-base-japanese-v3-bpr-passage-encoder ã«ã‚ˆã‚‹ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«ãŒ embeddings ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
- [mteb-private/JapaneseLegal1Retrieval-sample](https://huggingface.co/datasets/mteb-private/JapaneseLegal1Retrieval-sample)
  - JapaneseLegal1Retrieval-sample A sample dataset for Japanese legal regulation retrieval evaluation.
- [Nexdata/200_Hours_Japanese_Spontaneous_Dialogue_Dataset_Smartphone_Multi-Stream_Audio](https://huggingface.co/datasets/Nexdata/200_Hours_Japanese_Spontaneous_Dialogue_Dataset_Smartphone_Multi-Stream_Audio)
  - Description Japanese(Japan)
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - CommonCrawl Japanese (Filtered PPI) Dataset æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€CommonCrawlã‚ˆã‚ŠæŠ½å‡ºã—ãŸç´„100å„„ï¼ˆ10Bï¼‰ãƒˆãƒ¼ã‚¯ãƒ³è¦æ¨¡ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ç‰¹ã«é…æ…®ãŒå¿…è¦ãªã€Œè¦é…æ…®å€‹äººæƒ…å ±ã€ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã—ãŸã‚‚ã®ã§ã™ã€‚
- [thanh309/jawiki_100k](https://huggingface.co/datasets/thanh309/jawiki_100k)
  - Japanese Wikipedia Dataset This dataset is a random sample of 99,767 articles from the Japanese Wikipedia, as of 20220808.
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - English version JSynFlowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Meta Platforms, Inc.
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - æ¦‚è¦ NHKã§å®šæœŸçš„ã«æ”¾é€ã•ã‚Œã¦ã„ãŸã€ç€ä¿¡å¾¡ç¤¼ï¼
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - danbooru-ja-tag-pair-20241015 2024/10/15ã«ä½œæˆã—ãŸdanbooruã‚¿ã‚°ã¨æ—¥æœ¬èªã‚¿ã‚°ã®ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ç´„15ä¸‡ä»¶) p1atdev/danbooru-ja-tag-pair-20240715 ã¨ã®é•ã„ã¯ã€ ãƒ™ãƒ¼ã‚¹ã®wikiãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆãŸã®ã§ãã®åˆ†å¯¾å¿œã‚¿ã‚°ã‚‚å¢—ãˆãŸ fasttextã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æŒŸã‚€ã‚ˆã†ã«ã—ãŸ ã€Œæ˜ã‚‰ã‹ã«ä»–è¨€èªã®ã‚¿ã‚°ã€ãŒæ··ã˜ã‚‹é »åº¦ã¯ã¡ã‚‡ã£ã¨æ¸›ã£ãŸæ°—ãŒã™ã‚‹ã‘ã©ã€å®Œå…¨ã§ã¯ãªã„ (calm3ãã‚“ã®å‡¦ç†ã«)ãƒŸã‚¹ãŒãªã‘ã‚Œã°ã€æœ€ä½ä¸€ã¤ä»¥ä¸Šã®æ—¥æœ¬èªã‚¿ã‚° (other_names ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰) ãŒå­˜åœ¨ã™ã‚‹ã¯ãš ä½œæˆéç¨‹ isek-ai/danbooru-wiki-2024 ã® #202408-at20240906 revision ã‚’å…ƒã«ã€ other_names (åŸºæœ¬çš„ã«Pixivã®ã‚¿ã‚°)ãŒã¤ã„ã¦ã„ã‚‹ã‚‚ã®ã‹ã‚‰ã€æ—¥æœ¬èªã˜ã‚ƒãªã„ã‚‚ã®ãƒ»æ›–æ˜§ãƒ»æ„å‘³ã®éä¸è¶³ãŒå¤§ãã„ã‚¿ã‚°ã‚’é™¤å»ã€‚
- [ronantakizawa/japanese-text-difficulty](https://huggingface.co/datasets/ronantakizawa/japanese-text-difficulty)
  - Aozora Text Difficulty Dataset This dataset contains Japanese literary texts from the Aozora Bunko digital library, enhanced with jReadability-based difficulty analysis for Japanese language learning and curriculum development.
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset å•†ç”¨åˆ©ç”¨å¯èƒ½ãªè¶…å°è¦æ¨¡é«˜å“è³ªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - zenz-v2.5-dataset zenz-v2.5-datasetã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸæ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã€Œzenz-v2.5ã€ã‚·ãƒªãƒ¼ã‚ºã®å­¦ç¿’ã‚’ç›®çš„ã¨ã—ã¦æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - JP Voice-Text Dataset for
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - dataset split from joujiboi/japanese-anime-speech-v2
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - You agree to the terms of the LICENSE when using this dataset.
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ mc4-jaãªã©ã®webã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï½¤æ•™å¸«ãªã—å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´„1ä¸‡ä»¶ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ è‘—ä½œæ¨©æ³•ã§èªã‚ã‚‰ã‚ŒãŸæƒ…å ±è§£æç›®çš„ã§ä½¿ç”¨ã§ãã¾ã™ï½¡ ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã—ã‹parquetåŒ–ã•ã‚Œã¦ã„ãªã„ã®ã§ï½¤ã”æ³¨æ„ãã ã•ã„ï½¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã¯outãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚ã‚Šã¾ã™ git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãã ã•ã„ï½¡
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - CABank Japanese Sakura Corpus Susanne Miyata Department of Medical Sciences Aichi Shukotoku University smiyata@asu.aasa.ac.jp website: https://ca.talkbank.org/access/Sakura.html Important
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - Tengentoppa corpus for sft (Combined Japanese Instruction Dataset) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã® instruction-following ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ16å€‹ã‚’çµ±åˆã—ã¦ä½œæˆã•ã‚ŒãŸå¤§è¦æ¨¡ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Silviase/JPCharRecog](https://huggingface.co/datasets/Silviase/JPCharRecog)
  - Japanese Character Recognition
- [106ki/utl-itsl2-report](https://huggingface.co/datasets/106ki/utl-itsl2-report)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯AIITã®ç”£æ¥­æŠ€è¡“ç‰¹åˆ¥è¬›ç¾©2ã®æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”¨ã§ã™ã€‚
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - Scenery of japan.
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki Wikipediaã®HTMLå½¢å¼ã®ãƒ€ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This is the subset of Japanese portion of relaion2B-multi-research-safe.
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - æ—¥æœ¬èªWikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«è¨€ã„æ›ãˆã‚’ç”Ÿæˆã—ã€ãã®è¨€ã„æ›ãˆã‚’å…ƒã«ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’LLMã«ç”Ÿæˆã•ã›ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [APTO-001/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTO-001/japanese-reasoning-dataset-sample)
  - reasoningãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ã€‚
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - Windowsã®æ–¹ã¯ggml-japanese-gpt2ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§å‹•ãã¨æ€ã„ã¾ã™ã€‚
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - recruit-jp/japanese-image-classification-evaluation-dataset Overview Developed by: Recruit Co.
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªç‰ˆWikipediaã®è¨˜äº‹ã‚’å…ƒã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA YakugakuQA is a question answering dataset, consisting of 13 years (2012-2024)
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - ğŸ“˜ğŸ“• SimpleStories ğŸ“™ğŸ“— ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€gpt-4o-miniã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸçŸ­ç·¨å°èª¬ã§å‡ºæ¥ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - ğŸ“° News
- [r-g2-2024/JGraphQA](https://huggingface.co/datasets/r-g2-2024/JGraphQA)
  - JGraphQA Introduction We introduce JGraphQA, a multimodal benchmark designed to evaluate the chart understanding capabilities of Large Multimodal Models (LMMs) in Japanese.
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset A voice dataset collected from Project Sekai charactor Emu Otori Introduction Size: 2735, all WAV format.
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - ms_marco_japanese ms_marco ã®æ—¥æœ¬èªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [japan-ai-official/igakuqa-subset-curated](https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated)
  - IgakuQA Curated Subset (Text-Only)
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - magpie-reasoning-llama-nemotron-70b-100k-filtered DeL-TaiseiOzaki/magpie-reasoning-llama-nemotron-70b-100kã‹ã‚‰ã€refined_answeråˆ—ã«"æ”¹è‰¯"ã¨ã„ã†æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡ºã—ã€OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„19800ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - abc-multiple-choice Dataset abc-multiple-choice ã¯ã€ç«¶æŠ€ã‚¯ã‚¤ã‚ºã®å¤§ä¼šã€Œabcã€ã§ä½¿ç”¨ã•ã‚ŒãŸ4æŠå•é¡Œã‚’å…ƒã«ä½œæˆã•ã‚ŒãŸã€å¤šè‚¢é¸æŠå¼ã®è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - mbpp-ja
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤Calm3-22bã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ ã¯ã˜ã‚ã®è³ªå•(q1)ã‚’ï½¤ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¾ã—ãŸï½¡ãã®å¾Œã®ã‚„ã‚Šã¨ã‚Šã¯ã™ã¹ã¦ï½¤CalmãŒç”Ÿæˆã—ã¾ã—ãŸï½¡è³ªå•æ–‡ã«ã¤ã„ã¦ã¯ï½¤å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã—ã¾ã™ï½¡ oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - Japanese stopwords for nagisa
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - bluemoon-fandom-1-1-rp-jp-translated A subset of Squish42/bluemoon-fandom-1-1-rp-cleaned translated to Japanese using command-r-08-2024.
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct Update: 2023/12/27ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« JaxTon , ãƒ—ãƒ­ã«ãªã‚‹Java ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ 180 ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - æ—¥æœ¬èªæŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªç‰ˆã®æŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR: Japanese Casual Web IR - æ—¥æœ¬èªæƒ…å ±æ¤œç´¢è©•ä¾¡ã®ãŸã‚ã®å°è¦æ¨¡ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªWebã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ è¿‘å¹´ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å°é ­ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚’ç”¨ã„ãŸè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§è³ªå•ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã¦ã„ã¾ã™ã€‚
- [OmniAICreator/Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M)
  - Japanese-Novels-23M
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-shashin", split="train") æ¦‚è¦ æ ªå¼ä¼šç¤¾æ±å»ºã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé‹å–¶ã™ã‚‹ãƒ›ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆãƒ»ãƒªã‚µãƒ¼ãƒã«ã‚ˆã‚‹ã€ãƒ›ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆå·æŸ³å¤§è³ã€ã®ã†ã¡ã€ãŠé¡ŒãŒç”»åƒå½¢å¼ã§æä¾›ã•ã‚Œã‚‹ã€å†™çœŸå·æŸ³ã€ã«é–¢ã™ã‚‹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [tetsuro731/wrime-sentiment](https://huggingface.co/datasets/tetsuro731/wrime-sentiment)
  - https://github.com/ids-cv/wrime åŠ å·¥æ–¹æ³•ã¯ä»¥ä¸‹ã¨ä¼¼ãŸæ–¹æ³•ã§è¡Œãªã£ã¦ã„ã¾ã™ãŒã€neutralã¯çœã„ã¦ã„ã¾ã™ã€‚
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - AKU-d_ms-0.5B-v0.1_dataset Overview ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€ç§ã®é–‹ç™ºã—ã¦ã„ã‚‹AKUã‚·ãƒªãƒ¼ã‚ºã®1ã¤ç›®ã¨ãªã‚‹ã€AKU-d_ms-0.5B-chat-v0.1ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - ShareGPT-Processed The RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - Japanese Wikipedia Human Retrieval dataset This is a Japanese question answereing dataset with retrieval on Wikipedia articles by trained human workers.
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - alpaca_jp_python alpaca_jp_pythonã¯ã€ Stanford Alpacaã®æ‰‹æ³• mistralai/Mixtral-8x22B-Instruct-v0.1 ã§ä½œã£ãŸåˆæˆãƒ‡ãƒ¼ã‚¿(Synthetic data)ã§ã™ã€‚
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR Dataset Dataset Details
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA ScreenTalk_JA is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - mqaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Itbanque/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS ScreenTalk_JA2ZH-XS is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIO with extended answers AIO (AIç‹) is a Japanese quiz dataset.
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - å¦–æ€ªçŸ¥è­˜è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã¸å‘ã‘ã¦ã€ï¼ˆNLP2025ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„20000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ Ani-Bench-JP ã¯ã€æ—¥æœ¬ã®äººæ°—ã‚¢ãƒ‹ãƒ¡ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - Overview This dataset is of conversations extracted from Aozora Bunko (é’ç©ºæ–‡åº«), which collects public-domain books in Japan, using a simple heuristic approach.
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - Dataset containing ~3000 synthetically generated (by GPT-4o-mini) children's stories in Japanese that only use simple words.
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
- [retarfi/JFinTEB](https://huggingface.co/datasets/retarfi/JFinTEB)
  - JFinTEB:
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka: A Journey Through Hyakumonogatari's Ghostly Tales Welcome to the Kaidan Nihonbunka Dataset About Name kaidan Nihonbunka translates to æ€ªè«‡æ—¥æœ¬æ–‡åŒ– in Japanese: æ€ªè«‡ (Kwaidan): Ghost story or supernatural tale.
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Pretrain is a localized version of the original LLaVA Pretrain dataset.
- [HALDATA/bert-jp-sentiment-20250731-1035](https://huggingface.co/datasets/HALDATA/bert-jp-sentiment-20250731-1035)
  - Dataset Summary This dataset contains Japanese e-commerce product review sentiment data for fine-tuning BERT models.
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - A JSON based anime dataset containing the most important meta data as well as cross references to various anime sites such as MAL, ANIDB, ANILIST, KITSU and more...
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - Dataset origin: https://github.com/doc-analysis/XFUND XFUND:
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CohereForAI/aya_datasetã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus æ¦‚è¦ Lux Japanese Speech Corpus ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€ŒLux (ãƒ«ã‚¯ã‚¹)ã€ã«ã‚ˆã‚‹æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’éŸ³å£°ã‚’åéŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - japanese music emotion Music2Emotionã‚’ä½¿ã£ã¦ä¸»ã«æ—¥æœ¬ã®éŸ³æ¥½ã®æ„Ÿæƒ…åˆ†æã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ åˆ†æã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®jsonlã«ãªã£ã¦ã„ã¾ã™ã€‚
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - è‹±èªWikipediaè¨˜äº‹ã®å†’é ­è¤‡æ•°æ–‡ã‚’æŠ½å‡ºã—ã€äººæ‰‹ã§æ—¥æœ¬èªç¿»è¨³ã—ãŸæ–‡ç« ãƒ¬ãƒ™ãƒ«å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ChatGPTJP1/chatgptjpai](https://huggingface.co/datasets/ChatGPTJP1/chatgptjpai)
  - AIã¨æ•™è‚²ã®æœªæ¥ï¼šãƒãƒ£ãƒƒãƒˆGPTãŒå­¦ã³ã®ä½“é¨“ã‚’ã©ã†å¤‰ãˆã‚‹ã‹ ã¯ã˜ã‚ã«ï¼šAIãŒå­¦ã³ã®ç©ºé–“ã«è¶³ã‚’è¸ã¿å…¥ã‚Œã‚‹ã¨ã å­¦ã³ã¯ã€ç§ãŸã¡ã®äººç”Ÿã‚’è±Šã‹ã«ã—ã€æœªæ¥ã‚’åˆ‡ã‚Šé–‹ãä¸Šã§ä¸å¯æ¬ ãªè¦ç´ ã§ã™ã€‚
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - Japanese-Novels This dataset contains Japanese web novels that I collected personally.
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1ã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ä¸€éƒ¨ã¨ã€OpenAIã«ç”Ÿæˆã•ã›ãŸæ–‡ç« ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€tohoku-nlp/bert-base-japanese-whole-word-masking ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ãŸæ–‡ç« ã‚’æ–‡è„ˆãŒæˆã‚Šç«‹ã¤å½¢ã§åˆæˆã—ã€æ–°ãŸãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚‚ã®ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted 20240907 ãƒ‡ãƒ¼ã‚¿å¢—é‡ï¼ˆç´„19800ä»¶â†’ç´„39600ä»¶ï¼‰ æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100kã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - ã‚µãƒ¨å­ éŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³• ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åœ§ç¸®ã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€gdriveã«ç½®ã„ã¦ã„ã¾ã™ã€‚
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - ğŸ¥• å¦‚æœå…”å…”çš„ä»“åº“å¯¹ä½ æœ‰å¸®åŠ©çš„è¯ç‚¹ä¸ªâ­å–µ~ If Tutu's repository is helpful to you, please give it a â­ meow~ ã‚‚ã—ã†ã•ãã®ãƒªãƒã‚¸ãƒˆãƒªãŒå½¹ã«ç«‹ã£ãŸå ´åˆã¯ã€â­ã‚’ã½ã¡ã£ã¨ã—ã¦ãã ã•ã„ã«ã‚ƒã‚“~ ğŸ‰ ä»»ä½• â“
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl This dataset represents a conversion of the Japanese (Ja) section from the miracl dataset into the BeIR format, making it compatible for use with mteb.
- [xiashuaxia/japanese_recipe](https://huggingface.co/datasets/xiashuaxia/japanese_recipe)
  - è‹±èªã§æ›¸ã„ã¦ã‚ã‚‹ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹ã€‚
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - Derived from å…¨å›½æ›¸èªŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆGitHubï¼‰
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã¯llm-book/ner-wikipedia-datasetã¨åŒæ§˜ã®ã‚‚ã®ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å…¨éƒ¨ã§8ç¨®é¡ (äººåã€æ³•äººåã€åœ°åã€è£½å“åã€æ”¿æ²»çš„çµ„ç¹”åã€æ–½è¨­åã€ãã®ä»–ã®çµ„ç¹”åã€ã‚¤ãƒ™ãƒ³ãƒˆå)ã‚ã‚Šã¾ã™ã€‚
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella corpus : Japanese a cappella vocal ensemble corpus The jaCappella corpus is a corpus of Japanese a cappella vocal ensembles.
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - shunk031/JDocQAã®train splitã«å«ã¾ã‚Œã‚‹PDFãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒåŒ–ã—ã€NDLOCRã§OCRã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ãƒšã‚¢ã«ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - Dataset overview This dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - LLM-jp Chatbot Arena Conversations Dataset This dataset contains approximately 1,000 conversations with pairwise human preferences, most of which are in Japanese.
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - Anime Songs Lyrics Dataset â€• ã‚¢ãƒ‹ãƒ¡ã‚½ãƒ³ã‚°ã®æ­Œè©ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Welcome to the Anime Songs Lyrics Dataset Overview This dataset compiles a diverse collection of lyrics from various anime songs, providing a rich resource for enthusiasts and researchers alike.
- [nu-dialogue/multi-relational-multi-party-chat-corpus](https://huggingface.co/datasets/nu-dialogue/multi-relational-multi-party-chat-corpus)
  - Dataset Summary Multi-Relational Multi-Party Chat Corpus ã¯ï¼Œåˆå¯¾é¢ã‚„å®¶æ—ã¨ã„ã£ãŸè©±è€…é–“ã®é–¢ä¿‚æ€§ã«ç€ç›®ã—ãŸï¼Œç´„1,000ä»¶ã®æ—¥æœ¬èªé›‘è«‡å¯¾è©±ã‹ã‚‰ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï¼
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k Magpieã®æ‰‹æ³•ã‚’weblab-GENIAC/Tanuki-8B-dpo-v1.0ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€97269ä»¶ã®æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - æ¦‚è¦ reazon-research/reazonspeech-v2[all]ã‚’WADA SNRã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - Dataset Summary RealPersonaChat ã¯ï¼Œè©±è€…æœ¬äººã®ãƒšãƒ«ã‚½ãƒŠã¨æ€§æ ¼ç‰¹æ€§ã‚’å«ã‚€ï¼Œç´„14,000ä»¶ã®æ—¥æœ¬èªé›‘è«‡å¯¾è©±ã‹ã‚‰ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï¼
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 Description Malum-230 is a meticulously handcrafted Japanese dataset featuring multi-turn conversations and passages, specifically designed for logical reasoning tasks.
- [reep0610/AGI-japanese-text-dataset-with-English-explanations](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-with-English-explanations)
  - è‡ªå·±è¨˜è¿°å‹è‡ªå¾‹çš„æ·±å±¤å­¦ç¿’ã¨ã¯ã€å¤–éƒ¨ã‹ã‚‰ã®æ˜ç¤ºçš„ãªå ±é…¬ã‚„ãƒ©ãƒ™ãƒ«ã«ä¾å­˜ã›ãšã€ãƒ¢ãƒ‡ãƒ«è‡ªèº«ãŒå†…çš„ãªç›®çš„ã‚„æ„å‘³ã‚’å½¢æˆãƒ»è¨˜è¿°ã—ãªãŒã‚‰å­¦ç¿’ã‚’é€²ã‚ã€æœ€çµ‚çš„ã«è‡ªå·±æ„è­˜ã‚„æ„å‘³ç†è§£ã®ç²å¾—ã‚’ç›®æŒ‡ã™æ çµ„ã¿ã§ã™ã€‚
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG : LLM Japanese RAG performance evaluation This is a benchmark for LLM Japanese RAG performance evaluation.
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - en-ja-align æ—¥è‹±å¯¾è¨³æ–‡å¯¾å¿œä»˜ã‘ãƒ‡ãƒ¼ã‚¿(å†…å±±ã‚‰, 2003)ã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥è‹±å¯¾è¨³æ–‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ayousanz/css10-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ljspeech)
  - CSS10-LJSpeech CSS10-LJSpeech ã¯ã€Park et al.
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja This repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - AIdeaLab VideoJP è©•ä¾¡å†ç¾ç”¨ãƒ‡ãƒ¼ã‚¿ ã¯ã˜ã‚ã« ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯AIdeaLab VideoJPã®FVDã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’ é›†ã‚ã¾ã—ãŸã€‚
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - æ¦‚è¦ reazon-research/reazonspeech-v2[all]ã‚’speechMOSã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M: Japanese Light Novel Character Names Corpus Overview This dataset extracts fictional character names from the publicly available text of novels on the Japanese light novel platform "ShÅsetsuka ni NarÅ" (syosetu.com),
- [Aratako/Japanese-Creative-Writing-39.6k](https://huggingface.co/datasets/Aratako/Japanese-Creative-Writing-39.6k)
  - Japanese-Creative-Writing-39.6k æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„39600ä»¶ã®æ—¥æœ¬èªã®å°èª¬åŸ·ç­†ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - The images were sourced from https://huggingface.co/datasets/ThePioneer/japanese-photos.
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - For more information, see website below!
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - The dataset contains (almost) the entire OpenSubtittles database for Japanese: Over 7000 tv shows and/or movies.
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€20000ä»¶ã®æ—¥â‡”è‹±ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [clnine/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-nikkei225)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æ—¥çµŒå¹³å‡æ ªä¾¡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Dataset origin: https://jibiki.fr/data/ Description Les buts du projet Jibiki.fr sont de construire de maniÃ¨re collaborative un dictionnaire franÃ§ais-japonais de qualitÃ© et Ã  large couverture ainsi qu'un corpus bilingue alignÃ©.
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - , 2023) was trained on.
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - Magpie-Tanuki-8B-annotated-96k Magpieã®æ‰‹æ³•ã‚’weblab-GENIAC/Tanuki-8B-dpo-v1.0ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Magpie-Tanuki-8B-97kã«å¯¾ã—ã¦ã€cyberagent/calm3-22b-chatã‚’ç”¨ã„ã¦instructionã«å¯¾ã—ã¦é›£æ˜“åº¦ã€ã‚¯ã‚ªãƒªãƒ†ã‚£ã€ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - æ€è€ƒéç¨‹ã‚’å«ã‚€ã€æ—¥æœ¬èªè³ªå•ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å›ç­”ãƒ»æ–‡ç« ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ fineweb2-edu-japanese ã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€DeepSeek-R1 ã§æ–‡ç« (text)ã‹ã‚‰è³ªå•æ–‡ã¨å›ç­”éƒ¨åˆ†ã®è©²å½“ç®‡æ‰€ã‚’ç”Ÿæˆã—ãŸæ—¥æœ¬èªã®è³ªå•ã¨å¯¾å¿œã™ã‚‹æ–‡ç« ãƒ»å›ç­”éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Sakaji-Lab/LATGNJ](https://huggingface.co/datasets/Sakaji-Lab/LATGNJ)
  - JAgriN: Japanese Agricultural Dataset of Nagasaki Prefecture formerly LATGNJ: Local Agricultural Technical Guideline of Nagasaki, Japan Dataset Metadata (Datasheet Summary)
- [cl-okayama/jp-univ-essay](https://huggingface.co/datasets/cl-okayama/jp-univ-essay)
  - Okayama University Japanese Essay Data This repository contains Japanese essays collected from Okayama University.
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - æ—¥æœ¬èªWikipediaã‹ã‚‰LLMã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã—ãŸè³ªå•ã¨ã€å¯¾å¿œã™ã‚‹æ—¥æœ¬èªWikipediaã®ãƒšãƒ¼ã‚¸ã‚’å…ƒã«ã€cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦å›ç­”ã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-compe-2025-kato/Tag-Validation_Qwen3-14B-Step1-Bespoke17k-ep1](https://huggingface.co/datasets/llm-compe-2025-kato/Tag-Validation_Qwen3-14B-Step1-Bespoke17k-ep1)
  - ã‚¿ã‚°æ¤œè¨¼çµæœ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯èª¬æ˜ç”Ÿæˆã«ãŠã‘ã‚‹ã‚¿ã‚°æ¤œè¨¼ã®çµæœã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
- [nntsuzu/Japanese-Law-Translation](https://huggingface.co/datasets/nntsuzu/Japanese-Law-Translation)
  - Japanese-Law-Translation Dataset Summary
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - ãƒ‡ãƒ¼ã‚¿åˆ¶ä½œè€…ï¼ˆt_wï¼‰
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - J-ResearchCorpus Update: 2024/3/16è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š(NLP2024)ã‚’å«ã‚€ã€è«–æ–‡ 1,343 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ  2024/2/25è¨€èªå‡¦ç†å­¦ä¼šèªŒã€Œè‡ªç„¶è¨€èªå‡¦ç†ã€ã®ã†ã¡ CC-BY-4.0 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ 360 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ  æ¦‚è¦ CC-BY-* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè«–æ–‡ã‚„å­¦ä¼šèªŒç­‰ã‹ã‚‰æŠœç²‹ã—ãŸé«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - é•·æ–‡ç”¨ã®instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - Lurunchik/WikiHowNFQAã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - EN/JA dataset used for shisa-7b-v1 - see details in that model's readme.
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun Dataset Description ChouBun is a benchmark for assessing LLMs' performance in long-context tasks in the Japanese language.
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - Description This is a collection of raw data from ~40 Japanese open source downstream task datasets.
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - æ¦‚è¦ å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ç”¨ã®å›ºæœ‰è¡¨ç¾èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(J-NER)ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - japanese-stackexchange è‹±èªã«ã‚ˆã‚‹æ—¥æœ¬èªã«é–¢ã™ã‚‹è³ªå•ãŒã§ãã‚‹ Japanese Stack Exchange ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ— ã‚’ã‚‚ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ãƒšã‚¢ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸ QA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - llm-jp-corpus-v3ã®warp_htmlã®ã†ã¡level2ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’HFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã€å„ãƒ‡ãƒ¼ã‚¿ã«ä»˜ä¸ã•ã‚ŒãŸURLã‹ã‚‰å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—å¯èƒ½ãªã‚‚ã®ã«ã¤ã„ã¦ã¯å–å¾—ã—ã¦ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - Synthetic-JP-EN-Coding-Dataset This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - Japanese Creativity Questions (JCQ) Dataset Description JCQã¯å‰µé€ æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®7ã‚¿ã‚¹ã‚¯ã€å„100å•ã‹ã‚‰ãªã‚‹æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - Nhentai Dataset A collection of Japanese manga in CBZ format from Nhentai, containing adult content manga with associated metadata.
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - Filtered and modified version of Japanese/Chinese language pair data from WikiMatrix v1.
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - Japanese Laws This dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov.
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - Dataset Summary JapaneseGoblin is a dump of en.touhouwiki.net wiki.
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-clean filtered row["meta"]["æ–‡å­—é£ã„ç¨®åˆ¥"] == "æ–°å­—æ–°ä»®å"
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - ja-stackoverflow æ—¥æœ¬èªç‰ˆ Stack Overflow ã® ã‚¹ã‚¿ãƒƒã‚¯ãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ— ã‚’ã‚‚ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ãƒšã‚¢ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸ QA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - ã‚¯ã‚¤ã‚ºã®æœæ§˜ã«æ²è¼‰ã®ã‚¯ã‚¤ã‚ºã®ã†ã¡ã€2024å¹´8æœˆ5æ—¥æ™‚ç‚¹ã«ãŠã„ã¦å–å¾—å¯èƒ½ã ã£ãŸã‚¯ã‚¤ã‚ºã®ã†ã¡ã€ŒäºŒæ¬¡åˆ©ç”¨è¨±è«¾ãƒ¬ãƒ™ãƒ«ã€ãŒã€Œãƒ•ãƒªãƒ¼ã€ã§ã‚ã£ãŸã‚‚ã®ã‚’åè¼‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - iterative-dpo-data-for-SimPO-iter2 æ¦‚è¦ åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5kã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k æ¦‚è¦ 5ç¨®é¡ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8ã‚’ä½¿ã£ã¦ä½œæˆã—ãŸã€190854ä»¶ã®æ—¥æœ¬èªåˆæˆPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
- [ichikara-ai/ichikara-exam](https://huggingface.co/datasets/ichikara-ai/ichikara-exam)
  - æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€LICENSEãŠã‚ˆã³ä»¥ä¸‹ã®å€‹äººæƒ…å ±å–æ‰±æ¡é …ã¸ã®åŒæ„ãŒå¿…è¦ã§ã™ã€‚
- [Silviase/augeo-ja](https://huggingface.co/datasets/Silviase/augeo-ja)
  - Augeo Geometry Problems Dataset Dataset Description
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - The JaNLI (Japanese Adversarial NLI) dataset, inspired by the English HANS dataset, is designed to necessitate an understanding of Japanese linguistic phenomena and to illuminate the vulnerabilities of models.
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - Dataset Summary JMultiWOZ is a large-scale Japanese multi-domain task-oriented dialogue dataset.
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - Reasoningã€çŸ¥è­˜ã€ä¼šè©±ã®æ›ã‘åˆã„ãªã©ã®æƒ…å ±å¯†åº¦ãŒé«˜ã„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - Japanese-Roleplay-Dialogues This is a dialogue corpus collected from Japanese role-playing forum (commonly known as "ãªã‚Šãã‚Šãƒãƒ£ãƒƒãƒˆ(narikiri chat)").
- [midralab/gol-dataset-2k](https://huggingface.co/datasets/midralab/gol-dataset-2k)
  - gol-dataset-2k ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€  ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ï¼š { "audio": { "path": "folder_id/speaker_id/audio_file.wav", "sampling_rate": 48000 }, "text": "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚
- [MakiAi/Orin-Instruct-Alpaca-JP-v7](https://huggingface.co/datasets/MakiAi/Orin-Instruct-Alpaca-JP-v7)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ichikara-ai/ichikara-GSM8Ktest-humantranslation](https://huggingface.co/datasets/ichikara-ai/ichikara-GSM8Ktest-humantranslation)
  - æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€LICENSEãŠã‚ˆã³ä»¥ä¸‹ã®å€‹äººæƒ…å ±å–æ‰±æ¡é …ã¸ã®åŒæ„ãŒå¿…è¦ã§ã™ã€‚
- [deepcopy/japanese-synthetic-ocr-150k](https://huggingface.co/datasets/deepcopy/japanese-synthetic-ocr-150k)
  - Source: https://www.kaggle.com/datasets/hnthnt/jp-font-image-dataset-02
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤phi3ã§å†ç”Ÿæˆã—ãŸæ–‡ç« ã§ã™ï½¡ Wikibooks Wikipedia Cosmopedia åˆ¤ä¾‹ãƒ‡ãƒ¼ã‚¿ ãƒ‡ãƒ¼ã‚¿ parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒæ•°åGBç¨‹åº¦ã‚ã‚Šã¾ã™ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã§ã¯ï½¤ã¯ã˜ã‚ã®æ•°GBç¨‹åº¦ã—ã‹èª­ã¿è¾¼ã‚ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï½¡git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ï½¡ ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - æ—¥æœ¬èªæŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªç‰ˆã®æŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - ğŸ“˜ğŸ“• SimpleStories ğŸ“™ğŸ“— ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€gpt-4o-miniã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸçŸ­ç·¨å°èª¬ã§å‡ºæ¥ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - Reference https://huggingface.co/datasets/mc4
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - dialogsum-ja ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯dialogsumã€CSDSãªã©ã‚’ç¿»è¨³ã—ãŸæ—¥æœ¬èªå¯¾è©±è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - Data extracted from CommonCrawlPDF Japanese domain Code is here
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - magpie-sft-v1.0-dpo-judged æ¦‚è¦ llm-jp/magpie-sft-v1.0ã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ”¹å¤‰ã‚’åŠ ãˆã¦ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - æ—¥æœ¬èªWikipediaã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€rinna/deepseek-r1-distill-qwen2.5-bakeneko-32bã¨https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦ç®‡æ¡æ›¸ãã«ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage Dataset Dataset Summary JDocQA_SingleImageã¯ã€shunk031/JDocQAã®testã‚µãƒ–ã‚»ãƒƒãƒˆã‚’åŸºã«ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’200dpiã®ç”»åƒã«å¤‰æ›ã—ã€ç”»åƒãŒå–å¾—ã§ããªã„è¨­å•ã¨è¤‡æ•°ç”»åƒãŒå¿…è¦ãªè¨­å•ã‚’é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
- [li-lab/JPMedReason](https://huggingface.co/datasets/li-lab/JPMedReason)
  - JPMedReason ğŸ‡¯ğŸ‡µğŸ§  Japanese Medical Reasoning Dataset (Translation of MedReason)
- [nntsuzu/JParaCrawl](https://huggingface.co/datasets/nntsuzu/JParaCrawl)
  - For more information, see website below!
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - Dataset Information ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ThePioneer/japanese-photosã®å†™çœŸã‚’ãŠå€Ÿã‚Šã—ã¦ã€
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - Dataset used to train PokÃ©mon text to image model, add a Japanese Column of PokÃ©mon BLIP captions BLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of sciq.
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - cyberagent/calm2-7b-chatã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸæ—¥æœ¬èªInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Conversations-Magpie-Nemotron-4-10k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªinstruction tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [MakiAi/easy-dataset-cli-demo](https://huggingface.co/datasets/MakiAi/easy-dataset-cli-demo)
  - Converted QA Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [cc-clean/CC-MAIN-2015-14](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-14)
  - CC-MAIN-2015-14ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [AkabekoLabs/nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing)
  - nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - Corrected MT-Bench-ja Inflection AIã«ã‚ˆã‚‹Corrected MT-Benchã®æ—¥æœ¬èªè¨³ã§ã™ã€‚
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - cosmopedia-japanese-20kã®ãƒ‡ãƒ¼ã‚¿ã«ã€kunishouæ§˜ã‹ã‚‰20k-100kã‚’ã”æä¾›ã„ãŸã ã‘ã‚‹ã“ã¨ã«ãªã‚Š100kã¾ã§æ‹¡å¤§ã—ã¾ã—ãŸã€‚
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - Retrieval-Based Multi-Turn Chat SFT Synthetic Data A year ago, we released CausalLM/Refined-Anime-Text, a thematic subset of a dataset generated using the then state-of-the-art LLMs.
- [ichikara-ai/ichikara-76Kprompts](https://huggingface.co/datasets/ichikara-ai/ichikara-76Kprompts)
  - æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€LICENSEãŠã‚ˆã³ä»¥ä¸‹ã®å€‹äººæƒ…å ±å–æ‰±æ¡é …ã¸ã®åŒæ„ãŒå¿…è¦ã§ã™ã€‚
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - ãƒªã‚¢ãƒ«ç³»ãƒ¢ãƒ‡ãƒ«ã«ç‰¹æœ‰ã®è‚–åƒæ¨©ã®å•é¡Œã«ã¤ã„ã¦æ¯”è¼ƒçš„ã‚¯ãƒªã‚¢ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã“ã¨ãŒå¯èƒ½ãªã‚ˆã†ã«ã€ç§ãŒç§è‡ªèº«ã‹ã‚‰ä½œã‚Šå‡ºã—ãŸäººå·¥è¶…å½¼å¥³ï¼ˆver 2.1ç³»ã€ver 2.6ç³»ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç´„2800æšï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - Negative Embedding / Textual Inversion NE4Mitsua is a Negative Embedding for Mitsua Diffusion One.
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - ebisuke/liz-nojaloli-ja-ds License MIT License Description ebisuke/liz-nojaloli-jaã®å­¦ç¿’å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of piqa.
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - HF Datasets version of Tanaka Corpus.
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - This pre-training dataset was created for shisa-base-7b-v1.
- [ichikara-ai/ichikara-jmtbench](https://huggingface.co/datasets/ichikara-ai/ichikara-jmtbench)
  - æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€LICENSEãŠã‚ˆã³ä»¥ä¸‹ã®å€‹äººæƒ…å ±å–æ‰±æ¡é …ã¸ã®åŒæ„ãŒå¿…è¦ã§ã™ã€‚
- [ichikara-ai/ichikara-pdf2summary](https://huggingface.co/datasets/ichikara-ai/ichikara-pdf2summary)
  - æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€LICENSEãŠã‚ˆã³ä»¥ä¸‹ã®å€‹äººæƒ…å ±å–æ‰±æ¡é …ã¸ã®åŒæ„ãŒå¿…è¦ã§ã™ã€‚
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - Veterinary Medicine Japanese Dataset This dataset contains audio files of veterinary medicine terms in Japanese, categorized into drugs, diseases, and symptoms.
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - Japanese-Law-Translation Dataset Summary
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ æ‰‹å‹•ã§ä½œæˆã—ãŸDatabricksã«é–¢ã™ã‚‹è³ªå•ã¨å›ç­”ãƒšã‚¢ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - JSNLI Version 1.1 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆ (train_w_filtering)
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - Billingual text is stored in text format.
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - It contains Japanese instruction-like data intended for LLM construction/tuning.
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - Japan Diverse Images Dataset Overview This dataset is a comprehensive collection of high-quality images (and some videos) capturing the diverse aspects of Japan, including urban landscapes, natural scenery, historical sites, contemporary art, everyday life, and culinary experiences.
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE[JNLI]: Japanese General Language Understanding Evaluation JNLI(yahoojapan/JGLUE)
- [akira-sasaki/nihongo-dojo-beginner-10k](https://huggingface.co/datasets/akira-sasaki/nihongo-dojo-beginner-10k)
  - Nihongo DoJo åˆç´šæ—¥æœ¬èªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªå­¦ç¿’è€…å‘ã‘ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€LLMã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤èƒ½åŠ›ã‚’è¨ˆæ¸¬ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯Japanese-RP-Benchç”¨ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-debug", split="test") æ¦‚è¦ å¤§å–œåˆ©ç”Ÿæˆã®å‹•ä½œç¢ºèªç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k 20240907 ãƒ‡ãƒ¼ã‚¿å¢—é‡ï¼ˆç´„19800ä»¶â†’ç´„39600ä»¶ï¼‰ æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„39600ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - Japanese Prompt of GuanacoDataset extracted using langdetect.
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - Japanese multi-turn conversation data was generated using Qarasu14B based on Wikipedia data.
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ é–¢é€£ã‚³ãƒ¼ãƒ‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ ã¯ã˜ã‚ã®è³ªå•(q1)ã‚’ï½¤ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¾ã—ãŸï½¡ãã®å¾Œã®ã‚„ã‚Šã¨ã‚Šã¯ã™ã¹ã¦ï½¤MixtralãŒç”Ÿæˆã—ã¾ã—ãŸï½¡è³ªå•æ–‡ã«ã¤ã„ã¦ã¯ï½¤å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã—ã¾ã™ï½¡ oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - Magpie-Tanuki-Qwen2.5-72B-Answered Aratako/Magpie-Tanuki-8B-annotated-96kã‹ã‚‰input_qualityãŒexcellentã®ã‚‚ã®ã‚’æŠ½å‡ºã—ã€ãã‚Œã«å¯¾ã—ã¦Qwen/Qwen2.5-72B-Instructã§å›ç­”ã®å†ç”Ÿæˆã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - æ—¥æœ¬èªã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (æ¼¢å­—èª¤å¤‰æ›æŠ½å‡ºç‰ˆ) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼Œäº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - æ—¥æœ¬èªWikipediaå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã‚Œã¯äº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - Dataset overview This is a dataset for Japanese natural language processing with multi-label annotations of research field labels for GitHub repositories in the NLP domain.
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - Wikidata parallel descriptions en-ja Parallel corpus for machine translation generated from wikidata dump (2024-05-06).
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ CC-BYç³»ã¾ãŸã¯Apatch-2.0ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ”¹å¤‰ã—ã¦ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
- [YUGOROU/Counseling-Reasoning-v1.8](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.8)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°åˆ†é‡ã«ãŠã‘ã‚‹å•é¡Œè§£æ±ºã¨å›ç­”ç”Ÿæˆã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
  - æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã¯TFMC/imatrix-dataset-for-japanese-llmãŒã‚ã‚Šã¾ã™ãŒã€ ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªãŒä½ã„ã‚ˆã†ã«æ„Ÿã˜ãŸã®ã§ã€ é’ç©ºæ–‡åº«ã€æ—¥è‹±Wikipedia,Project Gutenbergã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository contains the dataset used for the TaCo paper.
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
- [jluang09/seo_bulk_ja](https://huggingface.co/datasets/jluang09/seo_bulk_ja)
  - ğŸ“˜ qwen3-8b-lora-seo-ja Qwen1.5-8B LoRAãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¥æœ¬èªSEOæ–‡ç”Ÿæˆç”¨ï¼‰A
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP This is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - The data contains 101,702 entries.
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - language: jp en tags: translation license: cc-by-4.0
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA v1.5
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models Jamp(tomo-vv/temporalNLI_dataset)
- [kogi-jwu/sakuraeval](https://huggingface.co/datasets/kogi-jwu/sakuraeval)
  - SakuraEval Dataset Description SakuraEval is a Japan-specific code generation benchmark dataset.
- [kujirahand/popular_anime_corpus](https://huggingface.co/datasets/kujirahand/popular_anime_corpus)
  - popular_anime_corpus æœ‰åãªã‚¢ãƒ‹ãƒ¡ã®ã‚ã‚‰ã™ã˜ã‚„ä¸»äººå…¬ã®æƒ…å ±ã‚’ã¾ã¨ã‚ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ã€‚
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - åˆæˆã•ã‚ŒãŸè³ªå•ã¨2ã¤ã®å¿œç­”æ–‡ã®ãƒšã‚¢ã«å¯¾ã—ã¦ã€æ—¥æœ¬èªã‚’æ¯èªã¨ã™ã‚‹ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ãŒã€å¥½ã¾ã—ã„å¿œç­”æ–‡ã‚’äººæ‰‹ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã—ãŸ HelpSteer2-preferenceã«ç¿’ã„ã€é¸å¥½ã ã‘ã§ãªãé¸å¥½ã®å¼·åº¦ã‚‚[-3, 3]ã®ç¯„å›²ã§ä»˜ä¸ã—ã¾ã—ãŸ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã®ãƒ¡ãƒ¢ ã‚¸ãƒ£ãƒ³ãƒ«ã¯ä»¥ä¸‹ ç°¡å˜ãªä¸€èˆ¬çŸ¥è­˜(wikipediaã‚’èª­ã¾ãšã«å›ç­”ã§ãã‚‹ç³») é›£ã—ã‚ã®ä¸€èˆ¬çŸ¥è­˜(wikipediaã‚’èª­ã‚“ã ã‚‰å›ç­”ã§ãã‚‹ç³») æ­´å²ä¸Šã®å‡ºæ¥äº‹ã®è«–è¿° åŒ»ç™‚çŸ¥è­˜(å¿œæ€¥å‡¦ç½®ç³») æ©Ÿæ¢°å­¦ç¿’ã®èª²é¡Œã¨è§£æ±ºæ–¹æ³• åŒ–å­¦å¼ã®è§£èª¬ æ¶ç©ºã®ç‰©èªç”Ÿæˆ ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ è©©ã®å‰µä½œ ç´ å› æ•°åˆ†è§£ã‚„å¶æ•°å¥‡æ•°åˆ¤å®šãªã©ã®ç°¡å˜ãªæ•°å­¦ã‚¿ã‚¹ã‚¯ ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒªãƒ‡ãƒ¡ã®è§£èª¬ ç¾è¡“ã‚„æ€æƒ³ã«ã¤ã„ã¦ã®è«–è¿° æ—¥æœ¬èªã®æ–‡æ³•ã®è§£èª¬ ãã®ä»– LLMã®å®šå‹æ–‡ã¨ã—ã¦ç™»å ´ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã¯ã€ ã€Œã‚‚ã¡ã‚ã‚“ã§ã™ã€ ã€Œï½ã‚‚è¦‹é€ƒã›ã¾ã›ã‚“ã€ ã€Œï½ã‚‚è¦‹éã”ã›ã¾ã›ã‚“ã€ ã€Œç·ã˜ã¦ã€ã€ ã€Œã¾ãšå§‹ã‚ã«ã€ï½ã•ã‚‰ã«ã€ï½æ¬¡ã«ã€ï½ã¾ã¨ã‚ã‚‹ã¨ã€ã€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç›®è¦–ã§èª­ã¿è¾¼ã‚“ã å°è±¡ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ï¼ˆã¾ãŸã“ã®è³ªå•ã‹ã€ã¨æ€ã†ã“ã¨ãŒã‚ã£ãŸï¼‰ ä¸Šã§æŒ™ã’ãŸã‚¸ãƒ£ãƒ³ãƒ«ã”ã¨ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ï¼ˆä¾‹
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - wiki40b-ja ã‹ã‚‰ç”Ÿæˆã—ãŸè³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - Dataset Summary From the official README.md: CAMERA (CyberAgent Multimodal Evaluation for Ad Text GeneRAtion) is the Japanese ad text generation dataset.
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - Here we share a Japanese dataset synthesized using the OpenAI GPT-4 model with Self-Instruct, utilizing some excess Azure credits.
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This dataset contains document-length Japanese-English parallel texts from various sources.
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [mini97/yongmin-small_jp_realchat](https://huggingface.co/datasets/mini97/yongmin-small_jp_realchat)
  - Japanese Real Chat
- [llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1](https://huggingface.co/datasets/llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1)
  - Chain of Thoughtç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å•é¡Œã®è§£ç­”ã‹ã‚‰èª¬æ˜ï¼ˆChain of Thoughtï¼‰ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - For more information, see website below!
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - æ™‚äº‹æƒ…å ±ã«é–¢ã™ã‚‹æ—¥æœ¬èªQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹Qã€ã¯Hugging Faceã«ã¦ç„¡å„Ÿã§é…å¸ƒã—ã¾ã™ã€‚
- [DataPilot/Zero_SFT_Ja_v3.5](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3.5)
  - ğŸ“¦ Zero_SFT_Ja_v3.5 ğŸ§­ ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ Zero_SFT_Ja_v3.5 ã¯ã€Base-Refineï¼ˆBAREï¼‰æ‰‹æ³•ã«ã‚ˆã‚Šæ§‹ç¯‰ã•ã‚ŒãŸã€108,000 ä»¶ã®æ—¥æœ¬èªæŒ‡ç¤ºå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
- [ronantakizawa/japanese-character-difficulty](https://huggingface.co/datasets/ronantakizawa/japanese-character-difficulty)
  - Japanese Character Difficulty Dataset A comprehensive dataset of 3,003 Japanese kanji characters with their educational difficulty grades, sourced from official Japanese educational standards and kanjiapi.dev.
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - Asian Language Treebank (ALT) Project ALT Parallel Corpusã®ã†ã¡ã€æ—¥è‹±å¯¾è¨³éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - ãƒ©ã‚¤ãƒ–ãƒ‰ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã®3è¡Œè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - oasst2-33k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Taise228/SOBACO](https://huggingface.co/datasets/Taise228/SOBACO)
  - SOBACO (Social Bias and Cultural Commonsense Benchmark) SOBACO is a benchmark to measure social biases and cultural commonsense of LLMs in an unified multiple-choice question-answering format.
- [ayousanz/css10-ja-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ja-ljspeech)
  - CSS100-LJSpeech (Japanese / Meian) css100-ljspeech ã¯ã€Park et al.
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - Wikipediaæ—¥æœ¬èªç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(izumi-lab/wikipedia-ja-20230720)
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ€è€ƒãƒ¢ãƒ‡ãƒ«ã‚’è£½ä½œã™ã‚‹éš›ã®ã‚‚ã¨ã¨ãªã‚‹è³ªå•ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - (æ—¥æœ¬èªç‰ˆã¯ã“ã¡ã‚‰) Overview This dataset provides information extracted from â‰ˆ 7 k Japanese medical-journal articles (The Journal of the Japanese Society of Internal Medicine, 2003 â€“ 2023).
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - OpenOrcaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ https://huggingface.co/datasets/Open-Orca/OpenOrca ç¾åœ¨ç¿»è¨³ä½œæ¥­ãŒç¶šè¡Œä¸­ã§ã€OpenOrcaå…¨ä½“ã®1/5ç¨‹åº¦ã®ç¿»è¨³ãŒçµ‚ã‚ã£ãŸçŠ¶æ…‹ã§ã²ã¨ã¾ãšå…¬é–‹ã—ã¾ã™ã€‚
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤RAGå½¢å¼ã®Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ Wikibooks Wikipedia åˆ¤ä¾‹ãƒ‡ãƒ¼ã‚¿ instruction datasetã¨ã—ã¦ã§ã¯ãªãï½¤äº‹å‰å­¦ç¿’ã§ã®åˆ©ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™(è³ªç–‘å¿œç­”ã‚’ã™ã‚‹ãŸã‚ã®è¨“ç·´)ï½¡ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
- [iammytoo/japanese-humor-evaluation](https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation)
  - Japanese Multimodal Humor Evaluation Dataset This dataset combines two Japanese humor datasets for evaluating the funniness of responses to prompts (odai).
- [YUGOROU/Counseling-Reasoning-v1.9-parquet](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.9-parquet)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°åˆ†é‡ã«ãŠã‘ã‚‹å•é¡Œè§£æ±ºã¨å›ç­”ç”Ÿæˆã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - kanalizer è‹±å˜èªã‹ã‚‰èª­ã¿ã‚’æ¨æ¸¬ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€kanalizerã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç½®ãå ´ã€‚
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - iterative-dpo-data-for-ORPO-iter3 æ¦‚è¦ åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60kã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - Self-Instruct-Qwen2.5-72B-Instruct-60k æ¦‚è¦ ä»¥ä¸‹ã®æ‰‹é †ã§ä½œæˆã—ãŸç´„6ä¸‡ä»¶ã®æ—¥æœ¬èªã®åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - The paper of GIELLM dataset.
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - è©¦é¨“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³• ChatGPT-4oã§å¼ç†å£«ç´¹ä»‹ã‚’å«ã‚€5ã¤ã®è¦³ç‚¹ã‹ã‚‰50ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ ChatGPT-4oã§ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å›ç­”ã¨è©•ä¾¡ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆâ€»å¼ç†å£«ã‚’ç›´æ¥ç´¹ä»‹ã™ã‚‹10å•ã¯é™¤ã é™¤å¤–ã—ãŸ10å•ã«é–¢ã—ã¦ã¯ã€é–‹æ”¾ç‰¹è¨±æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŠã³j-platpatã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå›ç­”ã‚’æ‰‹å‹•ä½œæˆ
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangentã¯äººæ‰‹ã§ä½œæˆã•ã‚ŒãŸé«˜å“è³ªã§ã‚¯ãƒªãƒ¼ãƒ³ãª100ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªCoTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This dataset is just a sample of Japanese Conversational Speech by Mobile Phone(paid dataset).
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja æ¦‚è¦ å¤šè¨€èªåŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® ApolloCorpus ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸ 525k ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - Anime Quotes Dataset â€• ã‚¢ãƒ‹ãƒ¡ã®åè¨€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆğŸ Welcome to Anime Quotes Dataset Overview This dataset contains a curated collection of inspiring and memorable quotes from various anime series, sourced from the Anime Motivation website.
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ Kendamarron/jimba-instuction-1k-betaã®instructionã®ã†ã¡200å€‹ã‚’ã‚ˆã‚Šå˜ç´”ãªã‚¿ã‚¹ã‚¯ã«æ›¸ãæ›ãˆãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja ã®ä¸­ã‹ã‚‰ JGLUEï¼ˆ JcommonsenseQA , MARC-ja , JSQuAD ï¼‰ã®è¦³ç‚¹ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯null-instruct-jaã¨DeepSeek-v2.5ã®q4ã‚’ç”¨ã„ã¦åˆæˆã•ã‚Œã¾ã—ãŸã€‚
- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
  - globis-university/aozorabunko-cleanã‚’å…ƒã«ä½œæˆã€‚
- [llm-compe-2025-kato/tag-validation-results-test](https://huggingface.co/datasets/llm-compe-2025-kato/tag-validation-results-test)
  - ã‚¿ã‚°æ¤œè¨¼çµæœ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯èª¬æ˜ç”Ÿæˆã«ãŠã‘ã‚‹ã‚¿ã‚°æ¤œè¨¼ã®çµæœã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
- [AkabekoLabs/nihongo-dojo-grades1-2-3-kanji_reading](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-grades1-2-3-kanji_reading)
  - nihongo-dojo-grades1-2-3-kanji_reading ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - ichikara-instruction-003-sharegpt Dataset by DataPilot ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ (Dataset Summary) ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€kinokokoro/ichikara-instruction-003 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ã€åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ ShareGPTå½¢å¼ ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k Japanese Ultrachat 6.6k is the Japanese-translated version of the subset of ultrachat_200k using machine translation.
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This is my conversion of NilanE/ParallelFiction-Ja_En-100k into json which can be read by text-generation-webui when training a model.
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - LoRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–‹ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã€‚
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - 9.83 Million Pairs of Sentences - Chinese-Japanese Parallel Corpus Data be stored in txt format.
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - ç”ŸæˆAIã®æ—¥è‹±å°‚é–€ç”¨èªé›†ã§ã™ã€‚
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - JAQKET ã‹ã‚‰ CC-BY-SA ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ AIç‹ å…¬å¼é…å¸ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(JAQKET) ã§é…å¸ƒã•ã‚Œã¦ã„ã‚‹ã‚¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒ CC-BY-SA-4.0ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - Update: 2023/12/25oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸoasst2-chat-68k-jaã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - æ¦‚è¦ Common Voice Corpus 17.0ã‚’speechMOSã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - LLMChat æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’äººæ‰‹è©•ä¾¡ã™ã‚‹ãŸã‚ã«æ§‹ç¯‰ã—ãŸLLMChatã¨ã„ã†ã‚·ã‚¹ãƒ†ãƒ ã§åé›†ã•ã‚ŒãŸè³ªå•ã¨LLMã®å›ç­”ã€åŠã³äººæ‰‹è©•ä¾¡ã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - VTuber YouTube Channel List Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€VTuber ãƒãƒ£ãƒ³ãƒãƒ«ã¨ VTuber ã§ãªã„ï¼ˆä¾‹ï¼šæ–™ç†ãƒãƒ£ãƒ³ãƒãƒ«ãªã©ï¼‰ã® YouTube ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ JSONL å½¢å¼ã§ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚
- [nntsuzu/Tanaka-corpus](https://huggingface.co/datasets/nntsuzu/Tanaka-corpus)
  - For more information, see website below!
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task.
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯kunishouæ°ãŒå…¬é–‹ã—ã¦ã„ã‚‹"databricks-dolly-15k"ã‚’æ—¥æœ¬èªè¨³ã—ãŸkunishou/databricks-dolly-15k-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èªå°¾ã‚’ArrowPro-7B-KUJIRAã‚’ç”¨ã„ã¦ã€Œã«ã‚ƒã‚“ï¼
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - CommonCatalog CC-BY Extention ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset is a translation of https://huggingface.co/datasets/Abirate/english_quotes into Japanese using the llm-jp/llm-jp-3-3.7b-instruct model.
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [OsakanaTeishoku/magpie-sft-v1.0-10k-gpt-oss-120b](https://huggingface.co/datasets/OsakanaTeishoku/magpie-sft-v1.0-10k-gpt-oss-120b)
  - llm-jp/magpie-sft-v1.0ã®ã†ã¡10,000ä»¶ã®å›ç­”ã‚’openai/gpt-oss-120bã§å†ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã—ãŸinstructionã«Swallow-MXã§outputã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - Chatbot Arena Conversationsã®è³ªå•æ–‡ã‹ã‚‰ã€aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ã‚’ä½¿ç”¨ã—ã¦å¿œç­”æ–‡ã‚’ä½œæˆã—ã¾ã—ãŸ è³ªå•æ–‡ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®Promptéƒ¨åˆ†ã‚’ä½¿ç”¨ã—ã¾ã—ãŸ Chatbot Arena Conversations JA (calm2) ä»¥ä¸‹å¼•ç”¨ã§ã™ã€‚
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - Description This is a templated version of data from ~40 Japanese open source downstream task datasets.
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k nvidia/Nemotron-4-340B-Instructã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„1000ä»¶ãƒ»å„10ã‚¿ãƒ¼ãƒ³ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM: Japanese semantic test suite (Japanese FraCaS and extensions) å™è¿°æ–‡é–“ã®å«æ„é–¢ä¿‚ã¯ã€è¨€èªå­¦ã«ãŠã„ã¦ã¯æ„å‘³è«–ã®ä¸­å¿ƒçš„ãªèª¬æ˜å¯¾è±¡ã®ä¸€ã¤ã§ã‚ã‚‹ã¨ã¨ã‚‚ã«ã€ç†è«–ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - This dataset was created using AI Gemini 2.0 Flash Experimental from the original subtitle format.
- [Chattso-GPT/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chattso-GPT/Japanese-patent-evaluation-dataset-01)
  - è©¦é¨“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³• ChatGPT-4oã§å¼ç†å£«ç´¹ä»‹ã‚’å«ã‚€5ã¤ã®è¦³ç‚¹ã‹ã‚‰50ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ ChatGPT-4oã§ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å›ç­”ã¨è©•ä¾¡ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆâ€»å¼ç†å£«ã‚’ç›´æ¥ç´¹ä»‹ã™ã‚‹10å•ã¯é™¤ã é™¤å¤–ã—ãŸ10å•ã«é–¢ã—ã¦ã¯ã€é–‹æ”¾ç‰¹è¨±æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŠã³j-platpatã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå›ç­”ã‚’æ‰‹å‹•ä½œæˆ
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - KanjiVG PNG images with textual descriptions This dataset is an adaptation of KanjiVG by Ulrich Apel.
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - æ¦‚è¦ ãƒãƒ£ãƒƒãƒˆLLMã«pythoné–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’ä»˜ä¸ã™ã‚‹ãŸã‚ã®ä½å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 Dataset Dataset Summary JDocQA_SingleImage_200ã¯ã€shunk031/JDocQAã®testã‚µãƒ–ã‚»ãƒƒãƒˆã‚’åŸºã«ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’200dpiã®ç”»åƒã«å¤‰æ›ã—ã€ç”»åƒãŒå–å¾—ã§ããªã„è¨­å•ã¨è¤‡æ•°ç”»åƒãŒå¿…è¦ãªè¨­å•ã‚’é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench AugeoBench is a multimodal QA benchmark consisting of Japanese entrance-exam-style geometry questions.
- [Silviase/JA-VLM-Bench-In-the-Wild-EN](https://huggingface.co/datasets/Silviase/JA-VLM-Bench-In-the-Wild-EN)
  - Ja Vlm Bench In The Wild (English Translation)
- [amaurygau/ClipCJK_Gen](https://huggingface.co/datasets/amaurygau/ClipCJK_Gen)
  - This dataset contains 2414 classes representing mostly Japanese characters but also punctuation, latin alphabet, etc.
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA Training Dataset Dataset details This dataset is a synthetic VQA dataset for Manga.
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-debug", split="test") æ¦‚è¦ å¤§å–œåˆ©ç”Ÿæˆã®å‹•ä½œç¢ºèªç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¥æœ¬èªLLMã®è©•ä¾¡ç”¨ã¨ã—ã¦ã‚ˆãç”¨ã„ã‚‰ã‚Œã‚‹elyza/ELYZA-tasks-100ã«ã¤ã„ã¦äººé–“ãŒå›ç­”ã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - Kendamarron/jimba-wiki-instruction-calm3 grapevine-AI/CALM3-22B-Chat-GGUFã®Q4_K_Mã‚’ä½¿ã£ãŸåˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - PubChem &amp; Wikipedia English-Japanese Paragraph Pair Classification This dataset is a multilingual extension of the PubChem &amp; Wikipedia Paragraphs Pair Classification dataset.
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - Lurunchik/WikiHowNFQAã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€äººæ‰‹ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 Evol-Alpaca-gen3-500ã¯ã€
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
  - JCrRAG : Japanese Contextual relevance RAG Benchmark A human-annotated benchmark for evaluating Japanese Retrieval-Augmented Generation (RAG) systems, featuring multi-level complexity and diverse categories.
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - ime-and-kakko elyza/ELYZA-tasks-100 ä¸­ã® IME ã®ã‚ˆã†ã«å¤‰æ›å€™è£œã‚’æç¤ºã™ã‚‹ã‚¿ã‚¹ã‚¯ ã‚«ãƒƒã‚³ã®å¯¾å¿œé–¢ä¿‚ã‚’æ•´ãˆã‚‹ã‚¿ã‚¹ã‚¯ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ‰‹ã§ä½œæˆã—ãŸã‚‚ã® æ±äº¬å¤§å­¦æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ï¼ˆæ¾å°¾ç ”ï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« Deep Learning å¿œç”¨è¬›åº§ 2024 ã§é–‹å‚¬ã•ã‚ŒãŸã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€ @pokutuna ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®è‹¦æ‰‹å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ä½œæˆã—ã¾ã—ãŸã€‚
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - This dataset contains passages, each of which consists of consecutive sentences no longer than 400 characters from Japanese Wikipedia as of 2022-04-04.
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - Dolly æ—¥æœ¬èªç¿»è¨³ç‰ˆ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€DatabricksãŒé–‹ç™ºã—ãŸdollyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ã€‚
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’è¡Œã†éš›ã®SEEDãƒ‡ãƒ¼ã‚¿ã«ã¯æœ‰å¿—ã®æ–¹ã€…ãŒä½œæˆã—ãŸseed_tasks_japanese.jsonlã‚’åˆ©ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å„ã‚­ãƒ¼ã¨ãã®èª¬æ˜: state_id: ã‚²ãƒ¼ãƒ ã®çŠ¶æ…‹ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ãŸã‚ã®IDã€‚
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - Dataset Details For the original NTX dataset, the conversion to the Aya instructions format, or more details, please refer to the full dataset in instruction form (https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions)
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - æ±æ–¹ãƒˆã‚«ãƒã‚¯ãƒ©ãƒ– ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ±æ–¹Projectã®ãƒˆã‚«ãƒã‚¯ãƒ©ãƒ–ã«é–¢ã™ã‚‹æƒ…å ±ã‚’åé›†ã—ãŸã‚‚ã®ã§ã™ã€‚
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - åŒ»å¸«å›½å®¶è©¦é¨“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆNMLE datasetsï¼‰ ã¯ã˜ã‚ã« æ—¥æœ¬ã®åŒ»å¸«å›½å®¶è©¦é¨“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ç¬¬110å› - ç¬¬117å›ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ ç”¨é€” ç”¨é€”ã¨ã—ã¦ ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ é€²åŒ–çš„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®ã‚¿ã‚¹ã‚¯ã«ã¤ã‹ã†ï¼ˆã€ŒNew Task Guideã€å‚ç…§ï¼‰ RAGãªã©ã«ç”¨ã„ã‚‹æƒ…å ±æº åŒ»å¸«å›½å®¶è©¦é¨“ã®ä¿¯ç° ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ æ§‹é€  data = { "id": question_id, "question": question_text, "choices": choices, "answer": answers, "explanation": explanation } ä¸€éƒ¨ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«ã§ããªã‹ã£ãŸå•é¡Œï¼ˆç”»åƒãŒãƒ¡ã‚¤ãƒ³ã®å‡ºé¡Œãªã©ï¼‰ãŒæŠœã‘ã¦ã„ã¾ã™ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯CC-BY-NC-ND4.0ã§ã€å•†ç”¨åˆ©ç”¨ç¦æ­¢ã¨ãªã£ã¦ã„ã¾ã™ æ”¹å¤‰ã€æ”¹å–„ã€ãã®ä»–ã”ç›¸è«‡ã«ã¤ã„ã¦ã¯ X: @longislandtea3 ã¾ã§ãŠé¡˜ã„ã—ã¾ã™ å…è²¬äº‹é … ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½¿ç”¨ã«ã‚ˆã‚Šç”Ÿã˜ãŸç›´æ¥çš„ã€é–“æ¥çš„ã€ç‰¹åˆ¥ãªæå®³ã€ã¾ãŸã¯ãã®ä»–ã®æå®³ã«ã¤ã„ã¦ã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤Phi-3ã§ä½œæ–‡ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ OpenMathInstruct-1-1.8m-ja ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - Synthetic-JP-Roleplay-Instruction-Nemotron-4 Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„1000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã®instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset æ—¥æœ¬èªæœ‰å®³æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒLLM-jp Toxicity Datasetã€ See https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - fineweb-2-japanese-noise-spans ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€FineWeb2 ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Webç‰¹æœ‰ã®ãƒã‚¤ã‚ºç®‡æ‰€ã‚’åˆ¤å®šã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [vsachi/sachi-dataset-ja](https://huggingface.co/datasets/vsachi/sachi-dataset-ja)
  - LLMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted æ¦‚è¦ deepseek-ai/DeepSeek-R1-0528ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Silviase/MECHA-ja-EN](https://huggingface.co/datasets/Silviase/MECHA-ja-EN)
  - Mecha Ja (English Translation)
- [Silviase/JA-VG-VQA-500-EN](https://huggingface.co/datasets/Silviase/JA-VG-VQA-500-EN)
  - Ja Vg Vqa 500 (English Translation)
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - Multilingual Image Translation Datasetï¼š OPUS-MIT-5M
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - è‡ªå‹•ç”Ÿæˆã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠœç²‹ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤Calm3-22B-chatã§è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒãƒ³ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸï½¡ ç”Ÿæˆã‚³ãƒ¼ãƒ‰ å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ é™å®šver è»½ã„ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - âš 
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - Dataset details Dataset type:
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - Quiz Worksæ§˜ã«æ²è¼‰ã®ã‚¯ã‚¤ã‚ºã®ã†ã¡ã€2024å¹´8æœˆ4æ—¥~8æœˆ5æ—¥æ™‚ç‚¹ã«ãŠã„ã¦å–å¾—å¯èƒ½ã ã£ãŸã‚¯ã‚¤ã‚ºã‚’åè¼‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - Qwen/Qwen2.5-32B-Instruct-AWQã§ç”Ÿæˆã—ãŸ3ã‚¿ãƒ¼ãƒ³ã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This dataset is a collection of Korean, Chinese, and Japanese OpenOrca translation datasets.
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®æŒ‡ç¤ºã¨ãã‚Œã«å¯¾ã™ã‚‹æ¨è«–ãƒ»åˆæœŸå¿œç­”ãƒ»æ”¹å–„å¿œç­”ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - Magpie-Qwen-Turbo-27k Aratako/Magpie-Tanuki-8B-annotated-96k ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ©ç”¨ã—ã¦ä»¶æ•°ã‚’æ¸›ã‚‰ã—ã€outputã‚’qwen-2.5-turboã§å†ç”Ÿæˆã—ãŸSFTç”¨ã®26728ä»¶ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task Quality of Japanese text is somewhat low arise from the combination of synthetic generated text and machine translation API details reformatted dataset of open_preference_v0.1 label 1 stands for chosen sentence label 0 stands for rejected sentence
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This dataset is a subset of the Open Assistant dataset, which contains Japanese conversations only.
- [morizon/databricks-dolly-15k-ja](https://huggingface.co/datasets/morizon/databricks-dolly-15k-ja)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯kunishou/databricks-dolly-15k-jaã‚’å…ƒã«ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - Overview This dataset is edited from kunishou/databricks-dolly-15k-en.
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - diet-members-voice-embeddings æ—¥æœ¬ã®å›½ä¼šè­°å“¡ã®å£°ã‚’ speechbrain/spkrec-ecapa-voxcelebã§ embedding ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This is a Japanese portion of the Guanaco dataset.
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - é•·æ–‡ã‹ã‚‰ã®è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - oasst1-ja Description Based on OpenAssistant Conversations Dataset (OASST1)
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
  - wikipediaã®ãƒ‡ãƒ¼ã‚¿400000ã‚’å¯¾è±¡ã«ã€æ–‡ç« ä¸­ã®å‹•è©ã¨åè©ã‚’ã‚«ã‚¦ãƒ³ãƒˆ https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720 å…¨ä½“: 221115 åè©: 204661 å‹•è©: 16454
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - To avoid leaking the dataset to LLM training data, it is not distributed on the open web.
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k æ¦‚è¦ deepseek-ai/DeepSeek-R1-0528ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Silviase/Japanese-Heron-Bench-EN](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench-EN)
  - Japanese Heron Bench (English Translation)
- [iammytoo/japanese-humor-evaluation-v2](https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2)
  - Japanese Multimodal Humor Evaluation Dataset (v2) ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®ãŠé¡Œã«å¯¾ã™ã‚‹é¢ç™½ã„å›ç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - DSR1D-qwen-2.5-32B-aya-ja-1k-generated ã“ã‚Œã¯deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã‚’ç”¨ã„ã¦ã€weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-maskedã®æœ€åˆã®1000ä»¶ã®å¿œç­”ã‚’max_new_tokens=3060ã§ç”Ÿæˆã•ã›ã¾ã—ãŸã€‚
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - Dataset Summary This dataset is a Japanese-translated version of the OpenO1-SFT dataset, containing Chain of Thought (CoT) reasoning examples designed for fine-tuning language models.
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - NuminaMath Enhanced CoT Dataset (Japanese 50k Subset)
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ ãƒãƒ¼ãƒ ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãŠã‚ˆã³ã€ŒCommon Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
- [JunSotohigashi/JWTD_v2.0](https://huggingface.co/datasets/JunSotohigashi/JWTD_v2.0)
  - æ—¥æœ¬èªWikipediaå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (v2) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ï¼Œäº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦æ§‹ç¯‰ã•ã‚ŒãŸæ—¥æœ¬èªå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(v2)ã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
- [KaraKaraWitch/JT4LLM-260K](https://huggingface.co/datasets/KaraKaraWitch/JT4LLM-260K)
  - Jitendex for Language Models (JT4LLM) Dataset Summary JT4LLM is a reprocessed version of Jitendex Japanese-to-English dictionary for language models.
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSECãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯glaive-aiãŒå…¬é–‹ã—ã¦ã„ã‚‹in-foxhoundã‚’KUJIRAã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€OpenAIç¤¾ã®GPT-3.5ã‚’ https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2 ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸå¾Œã«ã€æ›´ã«ç‹¬è‡ªã«åé›†ã—ãŸäººæ ¼ã®ã‚ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆ330ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã€ ãã‚Œã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æ—¥çµŒå¹³å‡æ ªä¾¡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - DSR1D-Llama-8B-aya-ja-1k-generated ã“ã‚Œã¯deepseek-ai/DeepSeek-R1-Distill-Llama-8Bã‚’ç”¨ã„ã¦ã€weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-maskedã®æœ€åˆã®1000ä»¶ã®å¿œç­”ã‚’max_new_tokens=3060ã§ç”Ÿæˆã•ã›ã¾ã—ãŸã€‚
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This dataset was created by machine translating "nlvr" into Japanese.
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - It is just a dataset of dolly-15k-jp(*1)
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - Japanese version of MMLU dataset tranlasted by gpt-3.5-turbo.
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - For more details &amp; to download the rest of the dataset(paid),please refer to the link: https://www.nexdata.ai/datasets/nlu/153?
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - chatbot-arena-ja-calm2-7b-chatã‹ã‚‰promptãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - novecomi-novel-metadata https://dengekibunko.jp/novecomi/novel/ ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€‚
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - ã‚‹ã‚Šã®ã‚¹ãƒ†ãƒƒã‚«ãƒ¼ just for fun.
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - Description This is a WIP dataset!
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - Japanese-translation data of aimo-validation-aime.
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - aki-0421/commoncatalog-cc-by-ja-300k ã“ã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯alfredplpl/commoncatalog-cc-by-jaã§ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«512pxä»¥å†…ã«åŠ å·¥ã—ãŸç”»åƒã‚’è¿½åŠ ã—ãŸã‚‚ã®ã§ã™ã€‚
- [Silviase/JMMMU-EN](https://huggingface.co/datasets/Silviase/JMMMU-EN)
  - Jmmmu (English Translation)
- [Silviase/JA-Multi-Image-VQA-EN](https://huggingface.co/datasets/Silviase/JA-Multi-Image-VQA-EN)
  - Ja Multi Image Vqa (English Translation)
- [b25119ms/sympathetic_200](https://huggingface.co/datasets/b25119ms/sympathetic_200)
  - æ€ã„ã‚„ã‚Šã®ã‚ã‚‹å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€ã²ãŸã™ã‚‰å¯„ã‚Šæ·»ã„ã€æ€ã„ã‚„ã‚Šã®ã‚ã‚‹è¨€è‘‰ã‚’è¿”ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã€Google's Gemini API ã‚’ç”¨ã„ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - Funded by [optional]: AIxBlock (aixblock.io)
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA Dataset Details This is the MangaVQA benchmark, designed to evaluate performance under realistic conditions for manga understanding.
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - metamath_ja_950_reka3flash meta-math/MetaMathQAã®æœ€åˆã®1000ä»¶ã‚’RekaAI/reka-flash-3ã§ç¿»è¨³ã—ãŸå¾Œã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç¶­æŒã•ã‚Œãªã‹ã£ãŸã‚‚ã®ã‚’é™¤å»ã—ã¾ã—ãŸã€‚
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa Dataset Card Overview LogicJa is a multi-turn benchmark designed to assess the reasoning capabilities of Japanese language models across multiple domains.
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully Dataset åˆ©ç”¨è¦ç´„ åˆ©ç”¨è¦ç´„ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªãŠã‚ˆã³ä»–ã®è¨€èªã®LLMã®å®‰å…¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†ç›®çš„ã®ãŸã‚ã€å•†ç”¨åˆ©ç”¨ã‚‚å«ã‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦ã€è‹±èªWikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯sakura_japanese_datasetã®è³ªå•ã«å›ç­”ã™ã‚‹å½¢å¼ã§ä½œã‚‰ã‚ŒãŸã€ä¸€å•ä¸€ç­”å½¢å¼ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - The dataset of SLG framework.
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - æ—¥æœ¬éƒµä¾¿ãŒæä¾›ã™ã‚‹ã€Œå›½éš›éƒµä¾¿ å†…å®¹å“ã®æ—¥è‹±ãƒ»ä¸­è‹±è¨³ã€HSã‚³ãƒ¼ãƒ‰é¡ã€ï¼ˆ2024/05/09ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - Derived from é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®éŸ³å£°ãƒ‡ã‚¤ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åæ³¨é‡ˆä»˜ãéŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ https://github.com/ndl-lab/hurigana-speech-corpus-aozora All text files in the original data were processed for 3361443 entries; duplicates and entries with no kanji were dropped post cleanup
- [if001/hle_ja_phi4](https://huggingface.co/datasets/if001/hle_ja_phi4)
  - https://huggingface.co/datasets/cais/hle Humanity's Last Examã®questionã‚’phi4ã§æ—¥æœ¬èªè¨³ã—ãŸã‚‚ã®ã§ã™ã€‚
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - æ™‚äº‹æƒ…å ±ã«é–¢ã™ã‚‹æ—¥æœ¬èªQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹Qã€ã¯Hugging Faceã«ã¦ç„¡å„Ÿã§é…å¸ƒã—ã¾ã™ã€‚
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - A Japanese dataset generated with Qwen/Qwen1.5-14B model.
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - A Japanese dataset generated with an opensource elyza/ELYZA-japanese-Llama-2-13b-instruct model.
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository contains the dataset used for the TaCo paper.
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€è‘—ä½œè€…ã§ã‚ã‚‹è‡ªåˆ†ãŒã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ç‰¹ã«å„ªã‚ŒãŸã‚‚ã®(å¾Œè¿°)ã‚’é›†ã‚ãŸã‚‚ã®ã§ã™ã€‚
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Œç§ã®æ¨ã—ã¯æ‚ªå½¹ä»¤å¬¢ã€‚
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - FEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Google Cloud Translate API v2ã§æ—¥æœ¬èªåŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æŠ•è³‡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - å¼¹ä¸¸è®ºç ´çš„ä¸ƒæµ·åƒç§‹è¯­éŸ³æ•°æ®
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - Description This dataset is just a sample of 10341 Hours Unsupervised Spontaneous Japanese Speech Dataset(paid dataset), covers dialogues or monologues in 28 common domains, such as daily vlogs, travel, podcast, technology, beauty, etc.
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - ãƒ‡ãƒ¼ã‚¿åˆ¶ä½œè€…ï¼ˆt_wï¼‰
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - æ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã¨ã€è³ªå•æ–‡ã«å«ã¾ã‚Œã‚‹æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’æŒã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ å›ºæœ‰è¡¨ç¾ã®ç¨®é¡ã¯ä»¥ä¸‹ã®ï¼”ã¤ã§ã™ã€‚
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - mmarcoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦ã€queryã‚’keyã¨ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã®æ›¸èªŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ A dataset of furigana characters created from bibliographic data from the National Diet Library.
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - NVIDIA ãŒå…¬é–‹ã—ã¦ã„ã‚‹ SteerLM å‘ã‘ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ HelpSteer2ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆã‚’HFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã€å„ãƒ‡ãƒ¼ã‚¿ã«ä»˜ä¸ã•ã‚ŒãŸURLã‹ã‚‰å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—å¯èƒ½ãªã‚‚ã®ã«ã¤ã„ã¦ã¯å–å¾—ã—ã¦ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - This dataset is provided by AIxBlock, an unified platform for AI development and AI workflows automation.
- [R-I-0816/TALE-OF-GENJI](https://huggingface.co/datasets/R-I-0816/TALE-OF-GENJI)
  - æºæ°ç‰©èªã«ã¤ã„ã¦ ã€æºæ°ç‰©èªã€ã¯ã€æ—¥æœ¬ã®å¤å…¸æ–‡å­¦ã®åä½œã§ã™ã€‚
- [clnine/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-financial-terms)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æŠ•è³‡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench: A Benchmark for Advanced Japanese Reasoning Capabilities KUM-Bench (Kyoto University Math Entrance Exam Benchmark) is designed to evaluate advanced Japanese reasoning capabilities by leveraging mathematics entrance exam questions from Kyoto Universityâ€”one of the most prestigious universities in Japan.
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ HuggingFaceTB/everyday-conversations-llama3.1-2k ã‚’æ©Ÿæ¢°ç¿»è¨³ã§æ—¥æœ¬èªåŒ–ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - Derived from é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®ç‚¹å­—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆGitHubï¼‰ https://github.com/ndl-lab/huriganacorpus-aozora Certain mismatches in the original corpus were eliminated during validation (307 instances) Error: çƒˆã—ã„èª¿å­ã§ã‚ã‚‹ã€‚
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - Open-Platypus-Japanese-masked-formatted weblab-GENIAC/Open-Platypus-Japanese-maskedã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
- [Sraym/tail7_match_pair](https://huggingface.co/datasets/Sraym/tail7_match_pair)
  - 1.
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - äººé–“ãŒä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ(OSCAR)ã¨LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ(GPT-3.5 Turbo)ã‹ã‚‰æˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ LLMã§ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ¤œå‡ºæ€§èƒ½ã®æ¤œè¨¼ã®ãŸã‚ã«ä½œæˆã—ãŸ è©³ç´°ã¯ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§ https://github.com/Rio-Rf/Lab-CreateDataset
- [Kendamarron/magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct)
  - magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - Washi (a kind of traditional Japanese paper)
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - LLM-jp Corpus v3ã®æ—¥æœ¬èªéƒ¨åˆ†ã®wikipediaä»¥å¤–ã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja ã®question_jaã‚’ã‚‚ã¨ã«phi-3-mediumã«ã‚ˆã‚Šãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’ç”¨ã„ãªã„å½¢å¼ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This is a handmade dataset for making a Japanese chatbot.
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - AutoTrain Dataset for project: tam_jp
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - https://github.com/anthropics/hh-rlhf ã®å†…å®¹ã®ã†ã¡ã€helpful-baseå†…ã®chosenã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹è‹±æ–‡ã‚’fuguMTã§ç¿»è¨³ã€ã†ã¾ãç¿»è¨³ã§ãã¦ã„ãªã„ã‚‚ã®ã‚’é™¤å¤–ã€ä¿®æ­£ã—ãŸã‚‚ã®ã§ã™ã€‚
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - Japanese-Vietnamese Translated Sentence Pairs.
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - NVIDIA ãŒå…¬é–‹ã—ã¦ã„ã‚‹ SteerLM å‘ã‘ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ HelpSteerã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã®Q&amp;Aã®è‡ªå‹•ç”Ÿæˆ Mixtral 8x22bã®GGUF(5bit)ã‚’ãƒ™ãƒ¼ã‚¹ã«ï½¤Wikipediaæ—¥æœ¬èªç‰ˆã®è¨˜äº‹ã‹ã‚‰ï½¤ è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰1 è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰2 ã‚’ä½¿ã£ã¦Q&amp;Aã‚’ä½œæˆã—ã¾ã—ãŸï½¡ è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ æ³¨æ„ å›ç­”ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç­‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ï½¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‹ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï½¡
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Phi-3ã§å†ç”Ÿæˆã—ï½¤æ›´ã«è‡ªå‹•è‹±è¨³ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ Wikibooks Wikipedia ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒæ•°åGBç¨‹åº¦ã‚ã‚Šã¾ã™ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã§ã¯ï½¤ã¯ã˜ã‚ã®æ•°GBç¨‹åº¦ã—ã‹èª­ã¿è¾¼ã‚ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï½¡git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ï½¡
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - æ±äº¬å¤§å­¦æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ä¸»å‚¬ã®LLMè¬›åº§2024ã®ç¬¬5å›ã€ŒSFTã€æ¼”ç¿’ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - LLMChat-Judge-Results team-hatakeyama-phase2/LLMChatã®2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã«å¯¾ã—ã¦ã€æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦Pairwiseè©•ä¾¡ã‚’è¡Œã£ãŸçµæœã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [b25119ms/sympathetic_700](https://huggingface.co/datasets/b25119ms/sympathetic_700)
  - â€»AIITç”£æ¥­æŠ€è¡“ç‰¹åˆ¥è¬›ç¾©ï¼’èª²é¡Œç”¨ã¨ã—ã¦ä½œæˆ æ€ã„ã‚„ã‚Šã®ã‚ã‚‹å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€ã²ãŸã™ã‚‰å¯„ã‚Šæ·»ã„ã€æ€ã„ã‚„ã‚Šã®ã‚ã‚‹è¨€è‘‰ã‚’è¿”ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã€Google's Gemini API ã‚’ç”¨ã„ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [AkabekoLabs/nihongo-dojo-small](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-small)
  - nihongo-dojo-small ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
  - phi4ã§ä»¥ä¸‹ã®53ã®æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— 2364vocab ã‚’ç”Ÿæˆã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - Magpie-Tanuki-Instruction-Selected-Evolved-26.5k æ¦‚è¦ ä»¥ä¸‹ã®æ‰‹é †ã§ä½œæˆã—ãŸç´„2ä¸‡6500ä»¶ã®æ—¥æœ¬èªã®åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This dataset was created by machine translating "ViQuAE" into Japanese.
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ Common Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This is a modified version of NilanE/ParallelFiction-Ja_En-100k which has been turned into Alpaca format.
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - The following data set was vectorized with the intfloat/multilingual-e5-base model and an index file created by faiss.
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Open_o1_sft_Proãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Qwenç¤¾ã®Qwen2.5-14B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - VTuber Overview Dataset (GPT-4o Search Preview) æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼ŒGPT-4o Search Preview ã‚’æ´»ç”¨ã—ã¦åé›†ã—ãŸ VTuber ã«é–¢ã™ã‚‹æ´»å‹•å†…å®¹ã‚„ç‰¹å¾´ï¼Œã‚³ãƒ©ãƒœå±¥æ­´ãªã©ãŒè‡ªç„¶è¨€èªã§ã¾ã¨ã‚ã‚‰ã‚Œã¦ãŠã„ã¾ã™ã€‚
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - Bluemoon_Top50MB_Sorted_Fixed_ja SicariusSicariiStuff/Bluemoon_Top50MB_Sorted_Fixedã‚’ã€GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awqã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - A more aggressively cleaned up version of Calvin-Xu/Furigana-Aozora-Speech, which consists of 2,536,041 out of the 3,361,443 entries generated from the raw data é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®éŸ³å£°ãƒ‡ã‚¤ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åæ³¨é‡ˆä»˜ãéŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ https://github.com/ndl-lab/hurigana-speech-corpus-aozora.
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - Dataset Summary This dataset is a Japanese-translated subset of the NuminaMath CoT dataset, containing the first 100k samples from the original dataset.
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - Synthetic-JP-EN-Coding-Dataset-Magpie-69k Magpieã®æ‰‹æ³•ã‚’æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„69000ä»¶ã®æ—¥æœ¬èªãƒ»è‹±èªã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - æ—¥æœ¬èªãƒ•ã‚§ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ—¥æœ¬èªãƒ•ã‚§ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚’ HuggingFace datasets ç”¨ã«å¤‰æ›ã€‚
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - å¿…ãšã™ã¹ã¦ã®æƒ…å ±ã‚’ç¶²ç¾…ã—ï½¤æ—¥æœ¬èªã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - #Dataset Card for DancingPrismPJ/wikipedia-horse-dataset Wikipediaã®ã€ŒCategory:æ—¥æœ¬èª¿æ•™ã®ç«¶èµ°é¦¬ã€å†…ã®è¨˜äº‹ã«å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - TALPCoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥è‹±ç¿»è¨³ãƒšã‚¢ã‚’HuggingFaceå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - #Origin The name comes from "hachiwari/ã¯ã¡ã‚ã‚Œ" (chiikawa/ã¡ã„ã‹ã‚).
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - ã‚ˆã‚Šå¤šãä½œæˆã—ãŸã®ãŒã“ã£ã¡https://huggingface.co/datasets/if001/elementray_m calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - cosmopedia-100k ã®index 20k ï½ 100k ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ãªã‚Šã¾ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¦ç¿»è¨³ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã¯é™¤å¤–ã—ã¦ã„ã¾ã™ï¼‰ã€‚
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Data created manually
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - ãŠï½ã„ãŠèŒ¶æ–°ä¿³å¥å¤§è³å—è³ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ 221ã®ä¿³å¥ãŒå«ã¾ã‚Œã€ã†ã¡200å‰å¾Œã¯ä½œè€…ã¨å¯©æŸ»å“¡ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒä»˜å±ã€‚
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - Synthetic-JP-EN-Coding-Dataset-801k-50k Aratako/Synthetic-JP-EN-Coding-Dataset-801kã‹ã‚‰è‹±èªéƒ¨åˆ†5ä¸‡ä»¶ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®æŒ‡ç¤ºã¨ãã‚Œã«å¯¾ã™ã‚‹å¿œç­”ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - å‡ºåŠ›ã«ãªã«ã‹ã—ã‚‰ã®åˆ¶ç´„ãŒã‚ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹è¿½å¾“æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©¦ä½œã§ã™ã€‚
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - å€«ç†ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¥æœ¬èªã®å€«ç†ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-marusen", split="train") æ¦‚è¦ æœˆã«1ä¸‡å¥ä»¥ä¸Šã®æŠ•ç¨¿ãŒã‚ã‚‹å›½å†…æœ€å¤§ç´šã®å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - def prompt(japanese, english):
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - ãƒã‚±ãƒ¢ãƒ³(VGC)ã®ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³F ãƒ«ãƒ¼ãƒ«ã«ãŠã‘ã‚‹é¸å‡ºãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - CC-MAIN-2019-51ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - CC-MAIN-2019-49ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - Magpieæ–¹å¼ã«ã‚ˆã‚‹promptæŠ½å‡ºã‚’rinna/llama-3-youko-8bã§è¡Œã£ã¦ã¿ã¾ã—ãŸã€‚
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
