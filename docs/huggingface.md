# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1375 models and 535 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èªž (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## ðŸ“– Contents

Released [a tool ðŸ”Ž](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search) for searching through a large number of repository information.

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).
Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Multimodality](#Multimodality)
	 * [Text Generation](#Text-Generation)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Reasoning](#Reasoning)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## ðŸŽ‰ The latest additions

**Models**
14 models have been added.

- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)


**Datasets**
6 datasets have been added.

- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)


## ðŸ§  Models

This list is sorted by downloads as of June 03, 2025.
1375 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 3,225,265
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from xlm-roberta-base using a dataset from Japanese Wikipedia, classifying tokens into person (PER) and organization (ORG) entities.
  - Downloads: 637,230
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M parameter text embedding model by Retrieva, Inc., primarily for Japanese but also supporting English, built upon the modernbert-ja-310m architecture.
  - Downloads: 435,709
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri provides Japanese general text embeddings with v3 models (30M-315M parameters, max length 8192) for use with Sentence Transformers, offering improved JMTEB scores up to 77.24.
  - Downloads: 314,990
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - Rinna's japanese-cloob-vit-b-16 is a Japanese contrastive language-image pre-training (CLIP) model, installable via pip and usable for image-text understanding tasks.
  - Downloads: 281,015
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - This repository provides a Japanese BERT model pretrained using Unidic-lite word-level tokenization and whole word masking on CC-100 and JA Wiki datasets.
  - Downloads: 232,773
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B is a top-performing Japanese text embedding model by Preferred Networks, excelling in tasks like information retrieval and achieving state-of-the-art results on the JMTEB benchmark.
  - Downloads: 197,323
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This repository provides a refined Japanese Sentence-BERT model (v2) trained with MultipleNegativesRankingLoss, achieving improved accuracy over v1 and requiring fugashi/ipadic for inference, built upon cl-tohoku/bert-base-japanese-whole-word-masking.
  - Downloads: 169,108
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with IPA dictionary-based word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 137,896
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - This repository provides a Japanese BERT base model pretrained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved language understanding.
  - Downloads: 130,542
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - This repository provides a Japanese BERT base model pretrained with IPA word-level and character tokenization, mirroring the original BERT base architecture.
  - Downloads: 106,659
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - This repository provides a Japanese BERT model pre-trained on CC-100 and JA Wiki data, utilizing character and word-level tokenization with Unidic and whole word masking for improved language understanding.
  - Downloads: 105,331
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 92,285
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - This repository provides a pre-trained Japanese DeBERTa V2 tiny model for masked language modeling, trained on Japanese Wikipedia, CC-100, and OSCAR datasets.
  - Downloads: 55,856
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - This repository provides a Japanese BERT base model pretrained on Japanese text using word-level and WordPiece tokenization based on the IPA dictionary, mirroring the original BERT base architecture.
  - Downloads: 55,249
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This repository provides a Japanese Sentence-BERT model (version 1) for generating sentence embeddings, with a link to a more accurate version 2 and usage examples.
  - Downloads: 48,983
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri provides pre-trained Japanese text embeddings using the Sentence Transformers library, requiring prefixes like "ã‚¯ã‚¨ãƒª:" or "æ–‡ç« :" for optimal query and passage encoding.
  - Downloads: 46,496
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 38,459
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese sentence embedding model, built on LUKE and trained on diverse data, for tasks like semantic similarity and search.
  - Downloads: 37,643
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 32,321
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced large language model based on Meta Llama 3, optimized via pre-training and instruction tuning by ELYZA, Inc.
  - Downloads: 29,764
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a 1.2B-parameter Japanese text embedding model achieving state-of-the-art results on JMTEB for semantic similarity and search, mapping text to a 1792-dimensional vector space.
  - Downloads: 23,420
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository hosts a medium-sized Japanese GPT-2 model trained by rinna Co., Ltd., and usable via the `transformers` library for causal language modeling.
  - Downloads: 20,747
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - This repository provides a BERT-based model fine-tuned for Japanese sentiment analysis of Amazon product reviews, classifying sentence polarity as positive or negative.
  - Downloads: 19,559
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow are large language models (8B, 70B) continually pre-trained on Llama 3.1, significantly improving Japanese language capabilities alongside retained English proficiency using a 200 billion token corpus.
  - Downloads: 18,145
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jnli) fine-tuned on the JGLUE MARC-ja dataset for natural language inference tasks, as detailed in the book â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€.â€
  - Downloads: 17,325
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - This repository provides a Japanese BERT model pre-trained on Japanese text (Jawiki-20200831) using Unidic-lite word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 16,573
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - This repository provides a Japanese DeBERTa V2 large language model pre-trained on diverse Japanese text corpora using character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 15,689
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14,385
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, trained on 312.5B tokens from Japanese web data, achieving a perplexity of 8.68.
  - Downloads: 14,097
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitionsâ€™ repository hosts Japanese autoregressive language modelsâ€”including sarashina2.2-3b-instruct-v0.1â€”evaluated on Japanese & English benchmarks like Elyza-tasks-100 and MT Bench.
  - Downloads: 13,368
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring installation of `sentence-transformers`, `fugashi`, `sentencepiece`, and `unidic-lite`, and specifying "ã‚¯ã‚¨ãƒª:" or "æ–‡ç« :" prefixes for query/passage input.
  - Downloads: 11,754
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model built on Whisper v2.0, enhanced with speaker diarization, punctuation, and integrated into Hugging Face Transformers (v4.39+).
  - Downloads: 11,488
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering improved performance and efficiency with support for 8192 tokens and a 100K vocabulary, utilizing FlashAttention.
  - Downloads: 11,134
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a multilingual BERT sentence encoder for 109 languages, trained with masked and translation language modeling for tasks like sentence embedding and bi-text retrieval, originally from TensorFlow Hub and now available in PyTorch.
  - Downloads: 10,811
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - This repository provides a Japanese BERT large model pretrained on CC-100 and JA Wiki data using Unidic-lite word-level tokenization and whole word masking.
  - Downloads: 9,494
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - This repository provides a Japanese sentence-transformers model, `sbert-jsnli-luke-japanese-base-lite`, for embedding sentences into 768-dimensional vectors useful for semantic search and clustering, fine-tuned on JSNLI data.
  - Downloads: 9,081
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese is a pre-trained DistilBERT model for Japanese language processing, built by LINE Corporation on 131GB of web text and based on their in-house BERT-base model.
  - Downloads: 8,993
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository hosts a base-sized Japanese RoBERTa model, trained with rinnaâ€™s code, and loadable via the `transformers` library for masked language modeling tasks.
  - Downloads: 8,257
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jsts) fine-tuned on the JSTS dataset for semantic similarity tasks, as demonstrated in the book â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€.â€
  - Downloads: 7,642
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - This repository distributes ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and utilizing GiNZA v5 for enhanced Japanese NLP pipelines.
  - Downloads: 6,790
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Meta's Llama 3, enhanced with Japanese language data and available in 8B and 70B parameter Instruct and Chat versions released July 1, 2024.
  - Downloads: 6,379
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - This repository hosts llm-jp-3-7.2b-instruct3, a 7.2 billion parameter, instruction-tuned Japanese large language model developed by NIIâ€™s Research Center for LLMs, compatible with Hugging Face Transformers and requiring torch>=2.3.0 & transformers>=4.40.
  - Downloads: 6,316
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T is a 3B-parameter Japanese language model built upon StableLM-3B-4E1T, optimized for Japanese language modeling and downstream tasks through continued pretraining.
  - Downloads: 6,209
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri provides Japanese general text embeddings with v3 models (30m-315m parameters, max length 8192) achieving up to 77.24 JMTEB, built using Sentence Transformers, Fugashi, Sentencepiece, and Unidic-lite.
  - Downloads: 5,960
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M is a computationally efficient Japanese BERT model trained on 4.09T tokens, utilizing local and global attention with RoPE for improved long sequence handling and featuring a 102,400 vocabulary.
  - Downloads: 5,679
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1 billion parameter English/Japanese language model utilizing a hybrid Mamba-based architectureâ€”similar to Samba but with added normalization for stable trainingâ€”to achieve efficient performance.
  - Downloads: 5,641
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 5,575
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - This repository hosts a 3.6B parameter Japanese GPT-NeoX model finetuned for instruction-following conversations using translated Anthropic HH and FLAN datasets.
  - Downloads: 5,497
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, offering models for improved Japanese text retrieval and ranking.
  - Downloads: 5,490
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow are 8B/70B large language models continually pre-trained on Llama 3.1, significantly enhancing Japanese language capabilities alongside retained English proficiency using a 200 billion token corpus.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings from Wikipedia to generate contextualized representations of words and entities.
  - Downloads: 5,012
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer modelâ€”an encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 4,890
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - This repository provides a tiny Japanese DeBERTa V2 model, pre-trained on Japanese text data with character-level tokenization, for masked language modeling tasks.
  - Downloads: 4,420
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-14B & 32B Japanese language models, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 4,360
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - This repository provides a GGUF-formatted conversion of the AXCXEPT-phi-4-deepseek-R1K-RL-EZO language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset for Japanese language capabilities and designed for use with llama.cpp.
  - Downloads: 4,108
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository hosts a small Japanese GPT-2 modelâ€”trained by rinnaâ€”and provides code for its use with the `transformers` library.
  - Downloads: 4,060
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - This repository details an experiment applying differences extracted via a chat-vector approach between the suzume-llama-3-8B-japanese and meta-llama/Meta-Llama-3-8B-Instruct models to the larger meta-llama/Meta-Llama-3-70B-Instruct, yielding minimal changes and prompting future work with scaling adjustments.
  - Downloads: 3,937
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - Cyberagentâ€™s llava-calm2-siglip is an experimental vision-language model enabling Japanese-language question answering about images using transformers.
  - Downloads: 3,879
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned variants) developed by NII, utilizing Hugging Face Transformers and requiring specific library versions.
  - Downloads: 3,818
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the deepseek-r1-distill-qwen2.5-bakeneko-32b language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is intended for use with llama.cpp.
  - Downloads: 3,795
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper provides fast, distilled Japanese Automatic Speech Recognition (ASR) modelsâ€”built on OpenAI's Whisper large-v3â€”for improved speed and accuracy using whisper.cpp and stable-ts punctuation pipelines.
  - Downloads: 3,624
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - Rinna's Japanese HuBERT Base model is a 12-layer transformer trained on 19,000 hours of Reazon Japanese speech data, mirroring the original HuBERT Base architecture.
  - Downloads: 3,497
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - Rinnaâ€™s Japanese wav2vec2.0 Base model is a 12-layer transformer trained on roughly 19,000 hours of Japanese speech data, replicating the original architecture and training process.
  - Downloads: 3,476
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - This repository offers a 3.6B parameter Japanese GPT-NeoX language model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 3,388
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - Rinna's japanese-gpt-1b is a 1.3 billion parameter Japanese GPT model for causal language modeling, readily usable with Hugging Face Transformers.
  - Downloads: 3,299
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri provides large Japanese text embeddings via the Sentence Transformers library, requiring prefixes like "ã‚¯ã‚¨ãƒª:" or "æ–‡ç« :" for query and passage texts respectively.
  - Downloads: 3,293
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 is a Japanese language model based on Qwen2.5-7B-Instruct, improved via knowledge distillation from a larger ABEJA model and ChatVector for enhanced instruction-following.
  - Downloads: 3,261
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - ELYZA's Llama-3-ELYZA-JP-8B is a Japanese-enhanced large language model based on Meta Llama 3, provided in a quantized GGUF (Q4_K_M) format for use with llama.cpp.
  - Downloads: 3,215
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large, including a BGE model) with varying layer and hidden size configurations for improved information retrieval.
  - Downloads: 3,105
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data & settings as Japanese Sentence-BERT, achieving comparable or slightly higher accuracy, particularly in qualitative assessments, and requiring SentencePiece for inference.
  - Downloads: 3,019
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides GGUF quantized versions of the 32B rinna/qwen2.5-bakeneko-instruct model, compatible with llama.cpp applications, alongside other quantization formats like AWQ and GPTQ.
  - Downloads: 3,003
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository hosts rinna's extra-small Japanese GPT-2 model for causal language modeling, offering code for use with the `transformers` library.
  - Downloads: 2,581
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M is a Japanese BERT model variant trained on 4.39T tokens, utilizing local and global attention with RoPE for efficient long sequence handling and featuring a 102,400 vocabulary.
  - Downloads: 2,444
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - This repository provides a high-performance Japanese SPLADE v2 model for converting text to sparse vectors, with a WebUI demo and utilizing YAST/YASEM for training and easy inference/token inspection.
  - Downloads: 2,407
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including the `hotchpotch/japanese-bge-reranker-v2-m3-v1` model, with varying layer and hidden sizes for improved sentence ranking performance.
  - Downloads: 2,406
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - This repository provides a Japanese DeBERTa V3 base model pre-trained on the LLM-jp corpus, enabling masked language modeling tasks via the Hugging Face Transformers library.
  - Downloads: 2,321
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository provides a Japanese-fine-tuned version of DeepSeek-R1-Distill-Qwen-14B, enabling it to output thought processes and generated text directly in Japanese.
  - Downloads: 2,277
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) fine-tuned on JA Wiki data using unsupervised SimCSE for semantic similarity and feature extraction.
  - Downloads: 2,248
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese sentence reranking model, implemented with Sentence Transformers, designed to score and reorder sentences based on relevance to a given query.
  - Downloads: 2,230
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF formatted versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, trained with the TFMC/imatrix dataset, for use with llama.cpp.
  - Downloads: 2,212
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Llama 3.3 to significantly enhance Japanese language capabilities while preserving English proficiency, using a 315 billion token corpus.
  - Downloads: 2,209
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - This repository hosts llm-jp-3-1.8b-instruct3, a 1.8 billion parameter Japanese large language model developed by NII, utilizing the Hugging Face Transformers format and requiring PyTorch &gt;=2.3.0 and Transformers &gt;=4.40.
  - Downloads: 2,185
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - This repository provides a fine-tuned BERT-base Japanese language model (llm-book/bert-base-japanese-v3-marc_ja) for sentiment analysis, trained on the JGLUE MARC-ja dataset and detailed in the book â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 2,122
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - Rinna's Japanese HuBERT Large modelâ€”a 24-layer transformer trained on 19,000 hours of Japanese speech dataâ€”replicates the original HuBERT Large architecture for speech recognition.
  - Downloads: 2,060
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering improved performance and efficiency with support for 8192-token sequences and a 100K vocabulary, leveraging FlashAttention.
  - Downloads: 2,042
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing 7B Japanese language model built upon the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 2,029
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Metaâ€™s Llama 3, enhanced with Japanese language data and available in 8B and 70B Instruct and Chat versions released July 1, 2024.
  - Downloads: 2,026
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese sentence embedding model based on RoFormer, optimized for long sequence retrieval tasks with a 1024 token limit and utilizing RoPE, distilled for efficient CPU usage.
  - Downloads: 1,979
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a 13B-parameter Japanese large language model fine-tuned for superior instruction-following, fluency, and contextual understanding.
  - Downloads: 1,950
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - Cyberagentâ€™s DeepSeek-R1-Distill-Qwen-14B-Japanese is a Japanese-language finetuned model based on DeepSeek-R1-Distill-Qwen-14B, designed for use with the Transformers library.
  - Downloads: 1,930
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and readily usable with Hugging Face Transformers.
  - Downloads: 1,900
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base is a Japanese language model based on modernBERT, trained on a 3.4TB corpus with support for 8192 sequence length using the llm-jp-tokenizer.
  - Downloads: 1,860
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with character and word-level tokenization and whole word masking for improved masked language modeling performance.
  - Downloads: 1,847
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri provides pre-trained Japanese text embeddings (v3 models ranging from 30M to 315M parameters) for enhanced semantic understanding and inference, utilizing libraries like Sentence Transformers, Fugashi, and SentencePiece.
  - Downloads: 1,844
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction following.
  - Downloads: 1,841
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 base model pre-trained on large Japanese text corpora and usable for masked language modeling tasks.
  - Downloads: 1,817
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - Youri-7b is a continually pre-trained 7-billion parameter language model based on Llama 2-7b, enhanced with 40B Japanese/English tokens to improve Japanese language performance.
  - Downloads: 1,722
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, featuring extended sequence (8192 tokens) and vocabulary (100K tokens) support with FlashAttention for improved performance and efficiency.
  - Downloads: 1,686
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model, fine-tuned for instruction following using datasets like Dolly-15k, with smaller 7B and fast-tokenizer versions also available.
  - Downloads: 1,663
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,651
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - This repository provides GGUF format conversions of the ELYZA-japanese-Llama-2-7b models, including fast and CodeLlama versions optimized for speed and reduced token cost.
  - Downloads: 1,635
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction following.
  - Downloads: 1,607
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository hosts LINE Corporation's 3.6B parameter Japanese language model for text generation, utilizing Hugging Face Transformers for easy implementation with code examples.
  - Downloads: 1,594
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - ABEJA's repository hosts a large Japanese GPT-2 model for text generation, requiring sentencepiece installation and utilizing the Hugging Face Transformers pipeline.
  - Downloads: 1,587
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository provides a Japanese Sentence BERT base model, pretrained on colorfulscoop/bert-base-ja v1.0 and fine-tuned using the Japanese SNLI dataset for sentence similarity tasks.
  - Downloads: 1,582
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese sentence reranking model based on Sentence Transformers, designed to improve relevance by selecting the best response from a set of candidates, easily usable after installing the `sentence-transformers` library.
  - Downloads: 1,554
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - LINE Corporationâ€™s repository hosts a 3.6B parameter Japanese language model, fine-tuned for improved conversational ability through instruction tuning.
  - Downloads: 1,507
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - KoichiYasuokaâ€™s repository provides a RoBERTa-small model pre-trained on Japanese Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 1,481
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - This repository hosts ABEJA's 2.7B-parameter Japanese GPT-NeoX model, compatible with transformers v4.23+, for text generation tasks.
  - Downloads: 1,454
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B is a continually pre-trained and instruction-tuned 8B parameter language modelâ€”based on Meta Llama 3â€”optimized for improved performance on Japanese and English tasks.
  - Downloads: 1,453
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned versions) developed by NIIâ€™s R&D center, utilizing Hugging Face Transformers and requiring PyTorch >= 2.3.0.
  - Downloads: 1,418
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on a massive Japanese web corpus, Wikipedia, and code to enhance Japanese language capabilities while preserving English proficiency, built upon Meta's Llama 3.1.
  - Downloads: 1,399
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x1.8b-instruct3 Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 1,364
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is a 13B LLaMA-based language model pre-trained on English and Japanese data by Preferred Networks, released under the Apache 2.0 license and usable via the `transformers` pipeline.
  - Downloads: 1,354
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,346
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - This repository details a 717M parameter Japanese character-level GPT-2 language model pre-trained on diverse Japanese text data and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 1,345
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - This repository provides Stability AI's Japanese Stable LM Instruct Gamma 7B, a 7-billion parameter decoder-only language model fine-tuned for instruction following, built upon the Japanese Stable LM Base Gamma 7B.
  - Downloads: 1,332
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - This repository provides a GGUF-formatted conversion of the Japanese DeepSeek-R1-Distill-Qwen-7B model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, for use with llama.cpp.
  - Downloads: 1,302
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model featuring GEGLU activation and optimized dropout settings for improved performance.
  - Downloads: 1,296
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - LINE Corporationâ€™s repository hosts a 1.7B parameter Japanese language model, fine-tuned for instruction following and improved conversational ability.
  - Downloads: 1,280
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - This repository provides a Japanese SimCSE model (pkshatech/simcse-ja-bert-base-clcmlp) for generating sentence embeddings from Japanese text, built on cl-tohoku/bert-base-japanese-v2 and trained with the JSNLI dataset, utilizing fugashi and unidic-lite for tokenization.
  - Downloads: 1,233
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of the Mistral-7B-Instruct-v0.3 model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and runnable with llama.cpp for Japanese language tasks.
  - Downloads: 1,213
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M is a computationally efficient Japanese BERT model trained on 4.39T tokens with RoPE attention, designed for long sequences and featuring a 102,400 vocabulary.
  - Downloads: 1,207
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese and multilingual data, with a chat-optimized, continually-learned fine-tuned version built using the SteerLM technique.
  - Downloads: 1,206
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8B parameter large language model, pre-trained on 1.3T tokens and fine-tuned for dialogue using SFT & DPO, with available quantized versions (AWQ, GPTQ, GGUF).
  - Downloads: 1,198
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a 13 billion parameter Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities.
  - Downloads: 1,197
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - This repository hosts LINE Corporationâ€™s 1.7B parameter Japanese language model for text generation, utilizing the Transformers library and providing example code for implementation.
  - Downloads: 1,191
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b is a Japanese-enhanced Llama 2 model, pretrained for improved language capabilities and designed for instruction-following tasks.
  - Downloads: 1,190
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - This repository provides a Japanese RoBERTa base model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 1,145
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX language model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 1,142
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese and multilingual data, with a fine-tuned chat version (Karakuri LM Chat) employing SteerLM and continual learning techniques.
  - Downloads: 1,135
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is a 13 billion parameter Japanese LLM fine-tuned with instruction data for improved performance, developed by Stockmark Inc. and readily usable with Hugging Face Transformers.
  - Downloads: 1,128
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter language model fine-tuned on Japanese data, built upon Llama-2-70b, designed for strong performance in Japanese language tasks.
  - Downloads: 1,117
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository hosts rinna's 3.6B parameter Japanese GPT-NeoX model, finetuned with a new data split for improved instruction-following conversational ability.
  - Downloads: 1,115
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - Rinnaâ€™s nekomata-14b is a continually pre-trained Qwen-14b modelâ€”enhanced with 66B Japanese/English tokensâ€”that significantly improves Japanese language performance and leverages Qwenâ€™s large, inclusive vocabulary.
  - Downloads: 1,114
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - Stockmarkâ€™s repository hosts a 1.4B parameter GPT-NeoX model pre-trained on a 20B Japanese token corpus, offering language generation capabilities with optimized data types for various GPUs.
  - Downloads: 1,110
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-7B is a 7 billion parameter language model, fine-tuned on diverse Japanese data using an expanded vocabulary, to enhance performance on Japanese language tasks based on Llama-2-7b.
  - Downloads: 1,106
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B is a 7B-parameter language model fine-tuned for instruction following, based on japanese-stablelm-base-beta-7b and trained on datasets like Databricks Dolly-15k, with larger 70B and faster versions also available.
  - Downloads: 1,106
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - CyberAgentâ€™s Mistral-Nemo-Japanese-Instruct-2408 is a continually pre-trained Japanese language model built upon Mistral-Nemo-Instruct-2407, designed for use with the Transformers library.
  - Downloads: 1,106
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, offering a faster, token-cost-reduced Japanese language model alongside related models like CodeLlama variants.
  - Downloads: 1,105
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained for enhanced instruction-following capabilities and designed for fast inference.
  - Downloads: 1,105
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B is a 7B parameter decoder-only language model pre-trained on 1.3T Japanese and English tokens, requiring transformers >= 4.34.1 for usage.
  - Downloads: 1,101
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter large language model pretrained on a 220B Japanese token corpus by Stockmark Inc., with an instruction-tuned version also available and developed with AWS support.
  - Downloads: 1,094
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-7B is a 7 billion parameter language model fine-tuned for instruction following with an expanded Japanese vocabulary, improving performance on Japanese language tasks.
  - Downloads: 1,091
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering extended sequence length (8192 tokens) and a larger 100K vocabulary for improved performance and efficiency.
  - Downloads: 1,087
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, offering fast inference with `torch.float16`.
  - Downloads: 1,082
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - Stockmark-100b is a 100 billion parameter language model pretrained by Stockmark Inc. on a 910 billion token Japanese and English corpus, with an instruction-tuned version (stockmark-100b-instruct-v0.1) available and supported by GENIAC.
  - Downloads: 1,080
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x13b-instruct3 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 1,078
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B is a 7B-parameter decoder-only language model fine-tuned for strong performance on Japanese language tasks, based on Llama-2-7b, with a corresponding instruction-following model also available.
  - Downloads: 1,069
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Nekomata-7b is a continually pre-trained version of Qwen-7b, enhanced with 30B tokens of Japanese and English data to significantly improve performance on Japanese language tasks and leverage Qwen's large vocabulary.
  - Downloads: 1,056
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,040
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B is a 7-billion parameter decoder-only language model pretrained on Japanese data, building upon Mistral-7B-v0.1 to excel in Japanese language modeling and tasks.
  - Downloads: 1,024
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - This repository hosts LLM-jpâ€™s Japanese and English instruction-tuned large language models, including 13B parameter versions like llm-jp-13b-instruct and their various training configurations (DPO, LORA, RLHF).
  - Downloads: 1,022
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.2ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,020
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository hosts a small Japanese GPT-NeoX language model, trained with EleutherAIâ€™s code and compatible with Hugging Faceâ€™s transformers library for causal language modeling.
  - Downloads: 1,012
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the Moonlight-16B-A3B-Instruct language model, trained with imatrix Japanese LLM data and optimized for use with llama.cpp.
  - Downloads: 999
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository hosts the llm-jp-3-13b-instruct3, a 13 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 999
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 982
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository provides GGUF format conversions of the open-calm-7b language model, compatible with llama.cpp, though subject to potential future incompatibility with updates to llama.cpp's GPT-Neox implementation.
  - Downloads: 978
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - This repository provides a GGUF conversion of the AIBunCho-released japanese-novel-gpt-j-6b model, optimized for use with llama.cpp, though compatibility may change with future llama.cpp updates.
  - Downloads: 972
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - This repository provides GGUF-formatted versions of the Vecteus-v1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates usage with llama.cpp for Japanese language processing.
  - Downloads: 971
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - This repository provides a GGUF-formatted conversion of the aixsatoshi Llama-3-8b-Cosmopedia-japanese model, built using TFMC/imatrix-dataset-for-japanese-llm, and links to other related Japanese language models.
  - Downloads: 954
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - This repository provides a GGUF-formatted version of Stability AIâ€™s Japanese-StableLM-2-Instruct 1.6B model, trained with the imatrix dataset, and requires agreement to its terms of use, including membership for commercial applications.
  - Downloads: 938
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese language Llama 2 and CodeLlama 7b models, including standard, fast, and instruct variants.
  - Downloads: 936
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7B-parameter language model pre-trained on Japanese and English data, optimized for Japanese language performance and serving as the base for an instruction-following model.
  - Downloads: 934
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 919
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - Cyberagentâ€™s DeepSeek-R1-Distill-Qwen-32B-Japanese is a Japanese-finetuned, 32B parameter language model built upon DeepSeek-R1-Distill-Qwen, designed for use with the Transformers library.
  - Downloads: 877
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides a quantized, llama.cpp-converted version of the Mistral-Nemo-Japanese-Instruct-2408 model, a Japanese instruction-following language model based on Mistral AI's work.
  - Downloads: 860
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large, including a BGE model) with varying layer and hidden size configurations, offering state-of-the-art performance for Japanese text retrieval.
  - Downloads: 859
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B is a merged 7B Japanese language model evolved from Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004.
  - Downloads: 856
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - This repository hosts weighted and static GGUF quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, offering various quantizations (including IQ1_S at 2.0GB) for efficient usage, with reference to TheBloke's documentation for GGUF file handling.
  - Downloads: 847
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted conversion of the TokyoTech-LLM Llama-3.1-Swallow-8B-Instruct-v0.3 model, trained with imatrix data, for use with llama.cpp.
  - Downloads: 773
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - This repository provides a GGUF-formatted conversion of the r1-1776-distill-llama-70b language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and is intended for use with llama.cpp.
  - Downloads: 754
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused language model based on Qwen2.5-32B, enhanced for instruction following using ChatVector and built with Hugging Face Transformers.
  - Downloads: 732
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF æ¦‚è¦ Aratako/calm3-22b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 731
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese language Llama 2 and CodeLlama 7b models, including a faster variant with reduced token cost.
  - Downloads: 686
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository hosts GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B language model, specifically for Japanese language processing, with links to a potentially better 32B version.
  - Downloads: 685
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - This repository provides a 32K vocabulary T5 transformer model pre-trained on a large corpus of Japanese web text (mC4, wiki40b) with accompanying training code.
  - Downloads: 671
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - This repository provides a Japanese character-level GPT-2 Small (90M) language model pre-trained on diverse Japanese text data for text generation tasks.
  - Downloads: 663
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - This repository hosts a 3B-parameter Japanese language model, fine-tuned for instruction following, based on the StableLM-3B-4E1T base model.
  - Downloads: 646
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B language model, supported by a16z and utilizing hardware from Massed Compute.
  - Downloads: 643
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a 13B parameter, Apache 2.0 licensed, instruct-tuned language model built on PLaMo-13B and fine-tuned with Japanese datasets for 8192 context length.
  - Downloads: 643
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2's Japanese language abilities via continual pre-training on a massive Japanese web corpus, while preserving English proficiency.
  - Downloads: 623
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/calm3-22b-RP-v2 language model, distributed under a CC-BY-NC-SA 4.0 license due to training data including outputs from OpenAI's GPT-4o-mini and Anthropic's Claude 3.5 Sonnet.
  - Downloads: 620
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-based question-answering model pre-trained on the Japanese Aozora corpus for dependency parsing and optimized for handling ambiguous words using masked queries.
  - Downloads: 614
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - This repository provides GGUF-formatted versions of the c4ai-command-r-plus language model, trained on the imatrix dataset, requiring potential file concatenation for larger quantization levels and usage with llama.cpp.
  - Downloads: 613
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 is a 7B parameter language model fine-tuned for instruction following, building upon Japanese-StableLM-Base-Alpha-7B and specializing in Japanese text generation.
  - Downloads: 607
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - This repository provides a T5-base model fine-tuned on the Livedoor News corpus for text summarization, as demonstrated in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 590
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of the Fugaku-LLM-13B-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and requiring acceptance of the terms of use.
  - Downloads: 590
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZAâ€™s Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B-parameter large language model based on Meta Llama 3, provided here in a quantized AutoAWQ format.
  - Downloads: 590
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a non-commercial, 13B parameter, Japanese language model fine-tuned for instruction following, built on PLaMo-13B with an 8192 context length and released under CC-BY-NC-4.0.
  - Downloads: 573
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository hosts llm-jp-3-980m-instruct3, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 569
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow is a series of 8B and 70B large language models continually pre-trained on a massive Japanese web corpus and multilingual data to significantly enhance Japanese language capabilities while preserving English proficiency.
  - Downloads: 547
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-70b-chat-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 545
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of Microsoft's Phi-3-mini-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and offering example usage with llama.cpp.
  - Downloads: 543
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the qwq-bakeneko-32b language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 538
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 70B language model, created with support from a16z and Massed Compute.
  - Downloads: 536
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - This repository hosts llm-jp-3-440m-instruct3, a 440M parameter Japanese large language model from NIIâ€™s LLM-jp-3 series, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 524
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-T2-2B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 516
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 513
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-Preferred-MedSwallow-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 503
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - This repository hosts the llm-jp-3-150m-instruct3, a 150M parameter Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 499
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - This repository hosts llm-jp-3-980m, a 980 million parameter Japanese large language model developed by NII, utilizing Hugging Face Transformers and requiring specific torch, transformers, and tokenizers versions.
  - Downloads: 485
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - This repository provides GGUF format conversions of the llm-jp-3-13b-instruct3 Japanese language model, alongside instructions for use with llama.cpp, noting limitations with custom chat templates.
  - Downloads: 473
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa base model pretrained on Japanese Wikipedia and CC-100, enabling masked language modeling tasks via the `transformers` library.
  - Downloads: 463
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the Gemma-2-Llama-Swallow-9b-it-v0.1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 456
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - This repository provides a Japanese BERT base model fine-tuned on the WRIME dataset for predicting emotion intensities (eight emotions) in Japanese tweets, as detailed in a related research paper.
  - Downloads: 446
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted versions of RakutenAI-2.0-mini-instruct, a Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 445
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - This repository provides a GGUF formatted version of the cyberagent-open-calm-1b language model, intended for use with llama.cpp and potentially subject to compatibility changes with future updates.
  - Downloads: 433
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 431
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èªž SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 426
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides GGUF conversions of Japanese-Starling-ChatV-7B, a 7B Japanese chat model built on chatntq-ja-7b-v1.0 and enhanced with a chat vector derived from Starling-LM-7B-beta.
  - Downloads: 421
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - This repository provides a Japanese BERT model, pretrained on Japanese Wikipedia data, with a small architecture (12 layers, 256 dimensions, 4 attention heads) suitable for finance-related natural language processing.
  - Downloads: 409
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - This repository hosts llm-jp-3-150m, a 150 million parameter large language model developed by NII's R&D Center for Large Language Models, compatible with Hugging Face Transformers using PyTorch.
  - Downloads: 408
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight, transformer-based Japanese language foundation model optimized for resource-constrained environments, serving as a base for instruct models like RakutenAI-2.0-mini-instruct.
  - Downloads: 406
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - This repository provides GGUF quantized versions of Googleâ€™s Gemma-2-2b-jpn-it language model, compatible with tools like llama.cpp, LM Studio, and LLMFarm, built using a process based on npaka's LLM-jp-3.
  - Downloads: 390
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese-language CodeLlama-7b-instruct model, alongside related Japanese Llama-2 models optimized for speed and token cost.
  - Downloads: 378
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - This repository provides a pre-trained Japanese ALBERT model (â€œken11/albert-base-japanese-v1â€) designed for fine-tuning on downstream tasks, specifically addressing potential issues with the `[MASK]` token when using Sentencepiece tokenization.
  - Downloads: 377
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - This repository provides a fine-tuned Japanese BERT model (â€œbert-base-japanese-jsnliâ€) for zero-shot text classification, achieving 92.88% accuracy on the JSNLI dataset.
  - Downloads: 374
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitionsâ€™ repository hosts the sarashina2.2-1b-instruct-v0.1 Japanese language model, evaluated on both Japanese and English tasks alongside other comparable models.
  - Downloads: 368
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2â€™s Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 367
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M is a computationally efficient, Japanese BERT model trained on 4.39T tokens with RoPE attention, designed for long sequences and featuring a 102,400 vocabulary.
  - Downloads: 366
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - This repository provides a Japanese BERT large model pretrained on Jawiki-20200831 using Unidic-lite word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 365
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with further pre-training to enhance its Japanese capabilities, designed for instruction-following tasks.
  - Downloads: 364
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This repository provides a Japanese XLNet language model requiring Mecab and Sentencepiece for tokenization, utilizing NFKD normalization which removes Japanese accent marks.
  - Downloads: 361
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - This repository provides a GGUF-formatted conversion of the karakuri-lm-32b-thinking-2501-exp large language model, trained with imatrix data, for use with llama.cpp.
  - Downloads: 353
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 350
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository provides the llm-jp-3-3.7b-instruct3, a 3.7 billion parameter Japanese large language model developed by NII's R&D center, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 349
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides GGUF quantized versions of the rinna/qwen2.5-bakeneko-32b-instruct-v2 Japanese language model, compatible with llama.cpp applications, including merged reasoning models.
  - Downloads: 344
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository details a Japanese typo detection modelâ€”based on RoBERTaâ€”that identifies and scores the probability of various character-level errors like deletions, insertions, substitutions, and kanji conversion mistakes within text.
  - Downloads: 327
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository hosts Japanese large language models (llm-jp-3, varying in size up to 172B parameters) developed by NII, provided as Hugging Face Transformers checkpoints with specific library requirements.
  - Downloads: 321
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - Luke-Japanese-Large-Lite is a lightweight, pre-trained Japanese language model that generates contextualized representations of words and entities without Wikipedia embeddings.
  - Downloads: 318
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository provides a series of Japanese cross-encoder rerankersâ€”including xsmall, small, base, and large models with varying layer and hidden size configurationsâ€”for improved information retrieval.
  - Downloads: 312
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - This repository provides the ELYZA-japanese-Llama-2-13b-fast-instruct model in GGUF format, optimized for use with LlamaEdge (v0.2.8+) and featuring a 5120 context size with a specific llama-2-chat prompt template.
  - Downloads: 312
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 7B model, created with support from a16z and hardware from Massed Compute.
  - Downloads: 310
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - JMedRoBERTa-base-sentencepiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 302
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese Llama 2 and CodeLlama 7b models, including standard, fast, and instruction-tuned variants.
  - Downloads: 294
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities from text, utilizing Wikipedia data.
  - Downloads: 290
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
  - ABEJA-Qwen2.5-32b-Japanese-v1.0 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct and fine-tuned with SFT and DPO techniques.
  - Downloads: 287
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Base Beta 70B model, created with support from Massed Compute and funding from a16z.
  - Downloads: 285
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - This repository provides a Waseda RoBERTa model finetuned for truthful answer evaluation on the JTruthfulQA dataset.
  - Downloads: 282
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-13b-instruct-v0.1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, alongside links to related model variations.
  - Downloads: 272
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - This repository provides a GGUF-formatted conversion of the cyberagent-open-calm-3b language model, intended for use with llama.cpp and potentially subject to compatibility changes with future updates.
  - Downloads: 269
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - This repository provides a Japanese BERT model, based on BERT small, further pretrained on financial text data derived from Tohoku Universityâ€™s base Japanese model.
  - Downloads: 266
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - This repository provides a Japanese Llama-3-ELYZA-JP-8B model finetuned on a specific dataset using Unsloth and TRL, achieving 2x faster training, and licensed under Apache 2.0.
  - Downloads: 266
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - This repository provides a Japanese-specific DeBERTa V3 model designed for inference without morphological analysis and with improved handling of word boundaries.
  - Downloads: 265
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 259
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporationâ€™s Japanese-large-lm-1.7b-instruction-sft language model, enabling its use with llama.cpp.
  - Downloads: 258
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - This repository provides GGUF-formatted conversions of the Ninja-v1 language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and includes instructions for use with llama.cpp.
  - Downloads: 255
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - This repository provides a Japanese extractive question answering model fine-tuned from rinna/japanese-roberta-base on the JaQuAD dataset, enabling question answering based on Japanese Wikipedia articles.
  - Downloads: 246
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - This repository hosts a 6B-parameter Japanese large language model, â€œwatashiha-gpt-6bâ€, built on GPT-2 and fine-tuned with 6.93 million *okashi* (comedic response) examples, optimized for AWS trn1 instances using the Megatron-LM and Neuron SDK, and released under the Apache 2.0 license.
  - Downloads: 245
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-small-japanese) for improved speech recognition, trained on Japanese datasets and requiring 16kHz audio input.
  - Downloads: 240
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with re-enabled dropout.
  - Downloads: 237
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3.1-13b-instruct4 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp for inference.
  - Downloads: 234
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLS-R-53 model for Japanese speech recognition, trained on public datasets like Common Voice and JSUT, requiring 16kHz sampled input audio.
  - Downloads: 227
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of the qwen2.5-bakeneko-32b-instruct language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is designed for use with llama.cpp.
  - Downloads: 226
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100 for masked language modeling tasks.
  - Downloads: 224
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides a GGUF version of the Llama-3-8B-Japanese-Instruct model, designed for use with LlamaEdge (v0.10.1+) and a specific llama-3-chat prompt template.
  - Downloads: 224
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B is a 14B parameter vision language model developed by NII Japan, requiring Python 3.10.12 and specific library installations including flash-attention.
  - Downloads: 223
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - LLM-jp-3-8x13b-instruct3 is a 13 billion parameter, instruction-tuned Japanese large language model developed by NII's R&D Center for LLMs, available in Hugging Face Transformers format with required PyTorch and Transformers libraries.
  - Downloads: 219
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - This repository hosts the llm-jp-3-8x1.8b-instruct3, an 8x1.8 billion parameter Japanese language model from NII's LLM-jp-3 series, compatible with Hugging Face Transformers and requiring torch>=2.3.0.
  - Downloads: 217
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - This repository provides a GGUF-formatted conversion of the haqishen Llama-3-8B-Japanese-Instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 216
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - This repository provides a Japanese BERT large model pre-trained on CC-100 and JAWIKI with character-level tokenization, Unidic word-level segmentation, and whole word masking for improved language understanding.
  - Downloads: 216
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the RakutenAI-2.0-8x7B-instruct Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 216
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository provides GGUF quantized versions of Stability AI's Japanese-StableLM-3B-4E1T-Instruct model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 214
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B is a Japanese instruction-tuned reasoning model based on rinna/qwen2.5-bakeneko-32b, optimized for superior performance using Chat Vector and ORPO, and available in various quantized formats.
  - Downloads: 214
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - This repository provides GGUF format conversions of the ELYZA-japanese-Llama-2-13b-fast model, a Japanese language model based on Llama 2 and optimized for speed and reduced token cost, alongside related models like CodeLlama versions.
  - Downloads: 213
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is a 47B parameter language model, fine-tuned for dialogue using SFT and DPO, with available AWQ, GPTQ, and GGUF (though GGUF performance is cautioned) quantizations, and requires flash attention for inference.
  - Downloads: 212
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - This repository provides a large Japanese DeBERTa V2 model pre-trained on extensive Japanese text corpora for masked language modeling tasks.
  - Downloads: 212
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - This repository provides a Japanese-optimized, quantized GGUF version of the Gemma-2-2b-it model, leveraging an iMatrix with Japanese keywords and supporting accelerated inference via speculative decoding in llama.cpp.
  - Downloads: 204
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 202
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - This repository provides GGUF-formatted conversions of the Honyaku-13b language model and other models by aixsatoshi and mmnga, built using TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 198
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - This repository provides a Japanese instruction-tuned large language model, based on Meta-Llama-3.1-70B-Instruct, optimized for use with the `transformers` library.
  - Downloads: 197
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-14B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 197
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - This repository provides a GGUF-formatted conversion of the Alfredplpl Llama-3-8B-Instruct-Ja model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 196
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - AMBER is a 132M parameter text embedding model by Retrieva, primarily for Japanese but also supporting English, built upon the modernbert-ja-130m architecture.
  - Downloads: 195
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-70b-instruct-v0.1 model, trained with the TFMC/imatrix-dataset-for-japanese-llm, alongside links to related model variations.
  - Downloads: 194
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with a 128k context window and enhanced long-context memory, including NSFW variants.
  - Downloads: 189
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 189
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - This repository provides a BERT model pretrained on Japanese Wikipedia for dependency parsing and question answering, built upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 187
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - This repository provides statically quantized GGUF versions of the Mistral-Nemo-Japanese-Instruct-2408 model, offering various weighted quantizations for efficient usage, with size and quality notes provided.
  - Downloads: 187
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - KoichiYasuoka's roberta-small-japanese-aozora-char is a pre-trained RoBERTa model for Japanese, utilizing a character tokenizer and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 186
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - This repository provides GGUF-formatted versions of Ryota39's Phi-3-mini-4k-instruct-DPO model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and runnable with llama.cpp.
  - Downloads: 182
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - This repository provides a Japanese medical named entity recognition (NER) model, fine-tuned RoBERTa on MedTxt-CR, to extract entities like diseases, symptoms, drugs, and tests using IOB2 tagging.
  - Downloads: 182
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Meta's Llama 3.3, significantly enhancing Japanese language capabilities alongside retained English proficiency using a 315 billion token corpus.
  - Downloads: 180
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 175
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-7b-instruct-v0.1 model, along with links to other related models and the dataset used for training (TFMC/imatrix-dataset-for-japanese-llm).
  - Downloads: 171
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - This repository provides GGUF conversions of rinna's japanese-gpt-neox-3.6b language model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation.
  - Downloads: 166
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-3.7b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for creation and usage.
  - Downloads: 160
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
  - LLM-jp-3.1-13b-instruct4 is a 13 billion parameter Japanese large language model from NII, improved for instruction following through mid-training compared to LLM-jp-3.
  - Downloads: 158
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - This repository provides a GGUF-formatted version of the EZO-phi-4-v2_900 language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 158
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 157
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - This repository provides a fine-tuned wav2vec 2.0 Large model for Japanese Automatic Speech Recognition (ASR) using the ReazonSpeech v2.0 corpus, accessible via the `transformers` library.
  - Downloads: 154
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - This repository provides a GGUF-formatted version of the Qwen1.5-110B-Chat large language model, utilizing the TFMC/imatrix dataset for Japanese support, and licensed under Tongyi-Qianwen.
  - Downloads: 153
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository provides GGUF versions of Stability AI's Japanese-StableLM-3B-4E1T-Base model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 153
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-32B language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and compatible with llama.cpp.
  - Downloads: 153
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - This repository provides a GGUF-formatted conversion of the DeepSeek-R1-Distill-Qwen-1.5B language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and intended for use with llama.cpp.
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - This repository provides GGUF quantized versions of rinna's Japanese Gemma-2-baku-2b-it model, compatible with tools like llama.cpp, LM Studio, and LLMFarm, built using a process based on npaka's LLM-jp-3.
  - Downloads: 147
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-32B and 14B Japanese language models for efficient inference.
  - Downloads: 145
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1 is a Japanese language model fine-tuned from Google's Gemma-3-4B-it, specializing in multi-turn, prompt-following conversational abilities for AI VTubers with a focus on lightweight performance.
  - Downloads: 141
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - This repository provides a GGUF-formatted version of AXCXEPT's phi-4-open-R1-Distill-EZOv1 model, trained with Japanese LLM data from TFMC/imatrix-dataset, for use with llama.cpp.
  - Downloads: 135
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides quantized GGUF versions of the Llama-3-8B-Japanese-Instruct model, optimized for use with GaiaNet, including a smallest 3.18GB Q2_K model with a 4096 context size.
  - Downloads: 134
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters) developed by NII's R&D center, including both base models and instruct-tuned variants, with support from GENIAC.
  - Downloads: 134
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - This repository details a v1.1 T5 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 131
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - LUKE-Japanese is a pre-trained, lightweight Japanese language model providing contextualized word and entity representations enhanced with knowledge embeddings (excluding Wikipedia in this version).
  - Downloads: 128
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri provides pretrained and fine-tuned Japanese text embedding models, built on ModernBERT-Ja, in varying sizes (30M-70M parameters) for general-purpose use, evaluated by JMTEB scores.
  - Downloads: 127
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - This repository provides a base model card template and details a small, pretrained T5 (Text-to-Text Transfer Transformer) model for Japanese and English natural language processing.
  - Downloads: 126
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - This repository provides a pre-trained Japanese BART base model, `ku-nlp/bart-base-japanese`, for conditional generation tasks, requiring Japanese text to be pre-segmented with Juman++.
  - Downloads: 125
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - This repository provides a GGUF format conversion of the stockmark-gpt-neox-japanese-1.4b language model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation within llama.cpp.
  - Downloads: 124
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - This repository provides a Japanese BigBird base model pretrained on large Japanese text corpora for masked language modeling tasks.
  - Downloads: 123
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - This repository provides a GGUF formatted version of the TokyoTech-LLM Swallow-MS-7b-instruct-v0.1 model, trained with the TFMC/imatrix-dataset-for-japanese-llm dataset, and links to other related GGUF models.
  - Downloads: 123
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - LLM-jp-3-7.2b-instruct provides large language models developed by NII, utilizing the Hugging Face Transformers format and requiring specific library versions for implementation.
  - Downloads: 121
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - KoichiYasuoka's repository provides a RoBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 119
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwm is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and CC-100 data, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 118
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIæ§˜ã® EZO-Common-T2-2B-gemma-2-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 117
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - This repository provides a Japanese question answering model fine-tuned on the JaQuAD dataset, achieving F1 scores of 77.35/78.92 and exact match scores of 61.01/63.38 on development/test sets, built upon the base Japanese BERT model.
  - Downloads: 116
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 116
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2's Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - This repository provides a 32B parameter, 8-bit quantized version of the Qwen2.5-Bakeneko-Instruct model using AutoGPTQ for reduced memory usage and faster inference.
  - Downloads: 113
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma-2's Japanese language skills via continual pre-training on a 200 billion token Japanese web corpus, while preserving English capabilities.
  - Downloads: 110
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository provides GGUF conversions of LINE Corporationâ€™s 1.7B Japanese language model, along with instructions and a conversion script for use with llama.cpp.
  - Downloads: 109
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This repository outlines the terms of use for Fugaku-LLM, a large language model developed collaboratively using the Fugaku supercomputer, permitting commercial and non-commercial use, modification, and redistributionâ€”subject to the agreement detailed within.
  - Downloads: 108
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Asagi-14B is a 14B-parameter Japanese Vision & Language Model trained on a large, diverse datasetâ€”including synthetically generated data from CALM3 and Phi3.5-visionâ€”with openly usable outputs.
  - Downloads: 108
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - This repository provides an 8-layer, 800,000-sentence Japanese language model, derived from intfloat/e5-mistral-7b-instruct and detailed in a linked Japanese article.
  - Downloads: 105
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - This repository provides a GGUF-formatted version of DeepSeek-R1-Distill-Llama-8B, utilizing the TFMC/imatrix dataset for Japanese language modeling and compatible with llama.cpp.
  - Downloads: 104
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
  - LLM-jp-3.1-1.8b-instruct4 is an instruction-tuned, 1.8 billion parameter Japanese large language model from the National Institute of Informatics, improving upon the LLM-jp-3 series with enhanced instruction-following.
  - Downloads: 100
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-1.8b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for creation and usage.
  - Downloads: 100
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 99
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - This repository provides a GGUF-formatted version of the WabiSabi-V1 Japanese language model, built using the TFMC/imatrix-dataset, and designed for use with llama.cpp.
  - Downloads: 99
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - This repository provides a question encoder based on the bert-base-japanese-v3 model, fine-tuned with llm-book/aio-retriever for the BPR document retrieval model detailed in chapter 9 of â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€â€.
  - Downloads: 98
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned Wav2Vec2-large-xlsr-53 model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets with a required 16kHz sampling rate.
  - Downloads: 97
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - KoichiYasuoka/roberta-large-japanese-char-luw-upos is a RoBERTa-large model pre-trained on Japanese Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 95
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the `git_japanese_stablelm_alpha` architecture.
  - Downloads: 91
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted versions of the japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 model, enabling use with tools supporting this quantized format.
  - Downloads: 90
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZAâ€™s Japanese-CodeLlama-7b and Llama-2-7b models, including instruct and fast variations, optimized for performance and reduced token cost.
  - Downloads: 87
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Asagi-8B is a large-scale Japanese Vision & Language Model trained on a diverse, synthesized datasetâ€”primarily using CALM3 and Phi3.5-visionâ€”with openly usable outputs.
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository hosts llm-jp-3-13b-instruct2, a 13 billion parameter Japanese large language model developed by NII's R&D Center, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the cyberagent/Mistral-Nemo-Japanese-Instruct-2408 model, compatible with llama.cpp and runnable via the TensorBlock client.
  - Downloads: 87
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This repository merges intfloatâ€™s e5-mistral-7b-instruct and stabilityaiâ€™s japanese-stablelm-base-gamma-7b models, providing instructions for usage and detailing the necessary steps to overcome class incompatibility during merging.
  - Downloads: 86
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - Ivydataâ€™s whisper-base-japanese is a fine-tuned Whisper model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets, requiring 16kHz sampled input audio.
  - Downloads: 85
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed by Fujitsu, RIKEN, and other institutions on the Fugaku supercomputer, permitting both commercial and non-commercial use, modification, and redistribution.
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - This repository provides a 4-bit quantized version of the llm-jp-3-172b-instruct3 large language model, reducing GPU/memory usage while maintaining performance through BitsAndBytes (fp4) quantization and float16/float32 computation.
  - Downloads: 83
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - This repository provides a pre-trained Japanese character-level GPT-2 Medium model (310M parameters) for text generation, trained on Japanese Wikipedia, CC-100, and OSCAR datasets.
  - Downloads: 82
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - This repository provides a pretrained DeBERTa V2 base model for Japanese language tasks, including masked language modeling, with accompanying pretraining code access.
  - Downloads: 76
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100, supporting sequence lengths up to 512 for masked language modeling tasks.
  - Downloads: 74
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT modelâ€”based on bert-base-japanese-v2â€”pre-trained for Universal Part-of-Speech tagging and dependency parsing on Japanese Wikipedia with long-unit-word support.
  - Downloads: 73
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the japanese-stablelm-instruct-gamma-7b model, based on Mistral 7B, for easier local use.
  - Downloads: 72
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer modelâ€”an encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 72
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository hosts a base-sized Japanese BART model, pre-trained by Stockmark Inc. for text generation and understanding, detailed in the linked article.
  - Downloads: 72
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - This repository provides an ESPnet2 text-to-speech (TTS) model, trained by kan-bayashi using the jsut/tts1 recipe and based on the JSUT dataset.
  - Downloads: 70
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - This repository provides a JaQuAD-fine-tuned RoBERTa base model for Japanese question answering, offering code and a pre-trained checkpoint for use with the `transformers` library.
  - Downloads: 70
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - Rinnaâ€™s Nekomata-7B-instruction model is a GGUF-formatted 7 billion parameter language model optimized for lightweight inference with llama.cpp, with a recommended 4-bit quantization of q4_K_M.
  - Downloads: 68
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese language model, with options tailored for varying VRAM capacities (8GB-16GB).
  - Downloads: 68
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - This repository provides a GPT-2 small model trained on a 540M token Japanese Wikipedia dataset for Japanese language generation.
  - Downloads: 67
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - This repository provides a Japanese BERT modelâ€”specifically a small version with 12 layers and 256 dimensionsâ€”pretrained on a combination of Wikipedia and financial text data.
  - Downloads: 67
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard is a high-performing, 7 billion parameter Japanese language model based on Llama-2-7b, achieving scores exceeding ChatGPT-3.5 on JGLUE benchmarks through additional pre-training and fine-tuning on unique data.
  - Downloads: 66
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a pre-trained Japanese Transformer Encoder built with Megatron-LM, offering improvements over traditional BERT models like PreNorm and featuring a recent bug fix for parameter initialization.
  - Downloads: 66
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - This repository details a 4.11GB quantized version of the ELYZA-japanese-Llama-2-7b-fast-instruct modelâ€”based on Metaâ€™s Llama 2 and optimized for Japanese language tasks with speed improvements, albeit with some performance trade-offs.
  - Downloads: 65
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - This repository provides a QLoRA-finetuned version of cyberagent/calm3-22b-chat, optimized for role-playing using the ChatML format and offering a demo and GGUF conversion.
  - Downloads: 65
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - This repository provides Llama 3.1-70B, fine-tuned for superior Japanese language performance, achieving top open-source results on ElyzaTasks-100 and licensed under the Llama 3.1 Community License.
  - Downloads: 65
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - This repository provides a pre-trained Japanese GPT2 model (`skytnt/gpt2-japanese-lyric-medium`) for generating Japanese lyrics from a title and/or prompt text.
  - Downloads: 63
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This repository provides a ChatML model created by SLERP merging yamatazen/Himeyuri-Magnum-12B with shisa-ai/shisa-v2-mistral-nemo-12b, part of the HMS model family.
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-liteã®é‡ã¿ã®åå‰ã‚’XLMRobertaå½¢å¼ã«ç½®ãæ›ãˆã€XLMRobertaãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸç‰©ã§ã™ã€‚
  - Downloads: 62
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - This repository provides GGUF-formatted versions of Meta-Llama-3-8B-Instruct, utilizing the TFMC/imatrix-dataset-for-japanese-llm for data, and demonstrates usage with llama.cpp.
  - Downloads: 61
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned on the ner-wikipedia-dataset using Kyoto Universityâ€™s BERT Japanese Pretrained model, requiring separate downloads for the tokenizer, Juman++, and pyknp.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - This repository provides a GGUF-formatted conversion of the Stockmark-2-100B-Instruct-beta large language model, trained with imatrix data and usable with llama.cpp for Japanese language tasks like recipe generation.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 58
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - This repository provides a Japanese novel-focused language model, GPT-J-6B, pre-trained on Japanese data and fine-tuned on novel data, runnable on Google Colab with provided installation and loading instructions.
  - Downloads: 57
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference (NLI) model, built on BERT and trained with JSNLI, JNLI, and JSICK datasets, to classify sentence pair relationships as entailment, neutral, or contradiction.
  - Downloads: 57
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a continually pre-trained 32B parameter language model based on Qwen2.5, enhanced for improved performance on Japanese language tasks using an 18B token dataset.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 57
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - This repository provides a 3.89GB AWQ-quantized version of the Japanese instruction-tuned "ELYZA-japanese-Llama-2-7b" model, based on Metaâ€™s Llama 2, optimized for Colab A100 and RTX 3000 series GPUs.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Asagi-4B is a large-scale Japanese Vision & Language Model trained on a diverse, synthetically-generated Japanese dataset, leveraging models like CALM3 and Phi3.5-vision without usage restrictions on generated outputs.
  - Downloads: 56
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Sentence Text Similarity (JSTS) tasks, achieving a Pearson correlation of 0.8971, and utilizing the JGLUE dataset.
  - Downloads: 55
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository provides HuBERT-base model weights trained on JTubeSpeech, an encoder-type transformer designed for speech recognition and embedding audio into latent variables, but not for speech generation.
  - Downloads: 54
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - This repository provides a fine-tuned Qwen3-8B model (GGUF version) specifically for role-playing, utilizing a system prompt to define character settings and dialogue scenarios, as demonstrated with an example using Ollama.
  - Downloads: 54
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - This repository provides a Japanese BERT model (bert-base-japanese-v3) fine-tuned on the JGLUE JCommonsenseQA dataset for multiple-choice question answering, as detailed in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 53
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - This repository provides a Japanese RoBERTa-large model pre-trained on Wikipedia and CC-100 with character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 52
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - This repository hosts a language model merged via the TIES method, building upon shisa-ai/shisa-v2-mistral-nemo-12b with contributions from natong19/Mistral-Nemo-Instruct-2407-abliterated.
  - Downloads: 52
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - This repository hosts weighted and static GGUF quantized versions of the japanese-llama-3-8b-instruct-v2 model, offering various quantized options (including IQ1_S) for efficient use, with references to TheBlokeâ€™s resources for GGUF usage.
  - Downloads: 52
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - This project provides code and models to convert standard Japanese text into â€œOjisanâ€ (middle-aged man) style phrasingâ€”characterized by specific linguistic features like first-person references, katakana particles, and emojisâ€”using Unsloth, LoRA, and GRPO techniques for efficient and high-performance generation.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - This repository hosts a merged 70B Llama 3-based language model (nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1) optimized for Japanese-RP, demonstrated with a conversational example using Gemini-2.0-flash-exp.
  - Downloads: 47
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository provides a fine-tuned luke-japanese-base model for binary sentiment classification (positive/negative) using the JGLUE MARC-ja dataset, achieving 0.9 accuracy.
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - This repository provides a Japanese ELECTRA base modelâ€”pretrained on Japanese Wikipediaâ€”with a 12-layer, 768-dimensional architecture for language understanding tasks.
  - Downloads: 45
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - This repository provides a 32B Japanese language modelâ€”an AWQ-quantized version of DeepSeek-R1-Distill-Qwen-32B, fine-tuned with TFMC/imatrix data for improved performance.
  - Downloads: 45
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
  - This repository details a Japanese language model finetuned from OpenAI's Whisper-large-v3-turbo, achieving state-of-the-art performance on short-form Japanese despite quality issues and hallucinations in long-form content due to progressive unfreezing and a smaller vocabulary.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - This repository provides DataPilot/Llama3.1-ArrowSE-v0.4, a Japanese-enhanced language model fine-tuned from Llama3.1-8B-instruct using Mergekit, designed as a helpful and honest Japanese assistant.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 42
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - This repository details a 1.3B parameter Japanese GPT-2 model (â€œdolly-japanese-gpt-1bâ€) fine-tuned with RLHF on datasets like â€œdatabricks-dolly-15k-jaâ€ and â€œoasst1-89k-jaâ€ for interactive dialogue, requiring 7GB VRAM/RAM, though recent updates show decreased QA accuracy.
  - Downloads: 41
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 is a Japanese language model based on LLaMA, trained on a primarily Wikipedia dataset with 76,000 steps, and designed to run on 24GB VRAM using Flash Attention.
  - Downloads: 41
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a Japanese language model based on a Switch Transformer with a Prefix-LM structure, utilizing a customizable "Spout" vector to influence generation tendency via pre-training or fine-tuning.
  - Downloads: 40
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - Nekomata-7b-gguf provides a GGUF quantized version of the rinna/nekomata-7b model optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 40
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository outlines the terms of use for Fugaku-LLM, a large language model developed collaboratively using the Fugaku supercomputer, permitting commercial and non-commercial use including modification, redistribution, and service implementation.
  - Downloads: 39
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-large model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This repository provides a Japanese+English Sentence-BERT model, pretrained on cl-tohoku/bert-base-japanese-whole-word-masking, achieving improved English STS benchmark accuracy compared to a Japanese-only version, and requiring fugashi and ipadic for inference.
  - Downloads: 37
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - This repository provides a large Japanese BART model pre-trained on Japanese Wikipedia for conditional text generation tasks, utilizing the `transformers` library.
  - Downloads: 37
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository provides a Japanese-adapted version of the Llama 3 8B model, commercially usable under the Llama 3 license, with instructions for quick demos, Colab use, or local implementation via `transformers` and `accelerate`.
  - Downloads: 37
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - ArrowNeo-AME-4x3B-v0.1 is a 4x3B Mixture-of-Experts (MoE) modelâ€”built upon sarashina-2.2-instruct-v0.1 and utilizing Unsloth & synthetic dataâ€”designed for AI virtual YouTubers ("AItuber") with specialized experts for coding, prompt following, and multi-turn conversations, achieving lightweight performance via Mergekit-MoE.
  - Downloads: 37
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - This repository provides a pretrained Japanese GPT-2 model, trained on Japanese Wikipedia & CC-100, for text generation and fine-tuning, requiring pre-tokenization with Juman++.
  - Downloads: 36
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - This repository offers a MIT-licensed, GGUF quantized version of the CyberAgent's DeepSeek-R1-Distill-Qwen-32B-Japanese language model.
  - Downloads: 36
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - Rinnaâ€™s Nekomata-14B-instruction model is provided in GGUF format for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to address potential stability issues.
  - Downloads: 34
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100B Japanese-focused large language model pre-trained on 1.5T tokens (60% English, 30% Japanese, 10% code) and refined with Japanese instruction-following data generated by Qwen2.5-32B-Instruct.
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - This repository provides a t5-base model fine-tuned on the xlsum dataset for Japanese summarization, achieving specified Rouge scores with a learning rate of 0.0001, though further details on intended use, data, and procedure are needed.
  - Downloads: 32
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - This repository provides a fine-tuned Japanese language model, built upon studio-ousia/luke-japanese-large, for automatic detection of defamatory speech categorized into threats, insults, and reputational damage.
  - Downloads: 31
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B is a GGUF conversion of a 7B Japanese language model built by combining ChatNTQ-ja-7b-v1.0 with a modified WizardLM-2-7b, aiming to enhance Japanese language capabilities.
  - Downloads: 31
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - This repository provides a fine-tuned Japanese-StableLM-Base-Alpha-7B model, styled after the character Reimu Hakurei from *Touhou Project*, enabling conversational interactions using a specific prompt format and utilizing 4-bit quantization.
  - Downloads: 31
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - TohokuNLP's BERT-alpha 500M is a Japanese BERT model based on the Llama architecture, enabling long sequence input (up to 8,192 tokens) as an encoder-type language model.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a 370m parameter Japanese chat language model built on the Mamba state-space architecture for efficient, linear-time sequence modeling.
  - Downloads: 31
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - This repository provides a Japanese BERT model pre-trained for Universal Part-Of-Speech tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 30
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This repository provides phi-4-open-R1-Distill-EZOv1, a Japanese-specialized Reasoner model based on Deepseek-R1â€™s Distill methodology, capable of English integration for increased flexibility.
  - Downloads: 30
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, demonstrating improved qualitative and comparable quantitative accuracy, built upon studio-ousia/luke-japanese-base-lite and requiring SentencePiece for inference.
  - Downloads: 30
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA model (ShinoBU) tokenized with SudachiTra/WordPiece, trained on 200M sentences and usable with the Hugging Face Transformers library.
  - Downloads: 30
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - This repository provides a pre-trained Japanese ELECTRA-Small model utilizing subword tokenization from Japanese Wikipedia with MeCab, designed for use with the `transformers` library.
  - Downloads: 29
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts LINE Corporationâ€™s 1.7B parameter, 8-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 29
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - This repository provides a Japanese natural language inference model, built on bert-base-japanese-v3 and trained with SentenceTransformers using the JSNLI dataset, outputting entailment scores for sentence pairs.
  - Downloads: 29
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - This repository provides a GGUF-formatted conversion of the matsuo-lab weblab-10b model, runnable with llama.cpp, using a development branch for faster updates.
  - Downloads: 29
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This repository provides a Japanese BERT-base model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 28
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, offering human-aligned conversational capabilities.
  - Downloads: 28
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository provides AWQ quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 7B, created with support from a16z and hardware from Massed Compute, for efficient and accurate large language model inference.
  - Downloads: 28
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - Sarashina2.2-3b-RP-v0.1 is a 3B parameter language model fine-tuned for role-playing, utilizing a system prompt to define character settings and dialogue scenarios within a fantasy world, and available in GGUF format for use with tools like Ollama.
  - Downloads: 28
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a large 890GB corpus, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in the training data.
  - Downloads: 27
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - This repository provides a Japanese pre-trained ALBERT model (â€œken11/albert-base-japanese-v1-with-japanese-tokenizerâ€) designed for fine-tuning on various downstream Japanese NLP tasks, utilizing the BertJapaneseTokenizer for easier tokenization.
  - Downloads: 27
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - This repository provides a 4-bit MLX quantized version of the DeepSeek-R1-Distill-Qwen-32B-Japanese language model, enabling efficient inference using the `mlx-lm` library.
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - This repository hosts llm-jp-3-980m-instruct2, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡žä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 27
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository provides a Japanese GPT-2 distillation model, trained using rinna/japanese-gpt2-medium as a teacher, achieving a perplexity of around 40 on Wikipedia data, and utilizing HuggingFace Transformers with modifications from rinna's training code.
  - Downloads: 26
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - This repository provides a fine-tuned Japanese GPT-2 model for generating *ES* (entry sheets/applications) based on over 20,000 examples from successful applicants, with a demo web app available and leveraging code from rinnakk/japanese-pretrained-models.
  - Downloads: 26
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - This repository provides japanese-gpt-1b-PII-masking, a 1B Japanese GPT model fine-tuned to mask personal informationâ€”names, birthdays, phone numbers, addresses, etc.â€”within text.
  - Downloads: 26
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training to enhance its Japanese capabilities, designed for assisting with code and general tasks.
  - Downloads: 26
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository provides a Japanese language model, `luke-japanese-wordpiece-base`, built upon studio-ousia/luke-japanese-base and pre-trained on Japanese Wikipedia data, utilizing WordPiece tokenization and improved handling of unknown entities.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUFã¯Japanese-LLaMA-3-8B-Instruct-v2ã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 26
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with an extended 128k context window and improved long-context memory.
  - Downloads: 25
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on a large corpus of Japanese medical academic articles, released under CC BY-NC-SA 4.0.
  - Downloads: 25
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, based on the original ELECTRA architecture (12 layers, 64 dimensions, 1 attention head).
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - This repository provides an uncensored (abliterated) version of vecteus v1, a high-performance Japanese large language model specializing in novel-style text generation and natural language understanding, offering greater freedom in text output while disclaiming responsibility for generated content.
  - Downloads: 25
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository provides the 32B parameter DeepSeek-R1-Distill-Qwen Japanese language model converted to the MLX format for use with the `mlx-lm` library.
  - Downloads: 25
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Asagi-2B is a large-scale Japanese Vision & Language Model trained on a diverse, synthesized datasetâ€”primarily using CALM3 and Phi3.5-visionâ€”with outputs freely usable without restriction.
  - Downloads: 25
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository hosts a fine-tuned Japanese language model (based on sonoisa/sentence-luke-japanese-base-lite) for estimating the aggressiveness of social media comments, achieving a 71.3% F1-score on a manually-labeled dataset and presented at NLP2024.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒžãƒ¼ã‚¸ãªã©ã‚’ç”¨ã„ä½œæˆã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - RoBERTa-long-japanese is a Japanese language model pretrained on 200M sentences, extending the base RoBERTa model with a 1282 maximum position embedding to handle longer inputs after Juman++ and SentencePiece tokenization.
  - Downloads: 24
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - This repository provides a merged, work-in-progress Japanese language model based on Mixtral-8x7B-Instruct-v0.1, featuring vocabulary expansion and continual pre-training, as evaluated on the ABEJA tech blog.
  - Downloads: 24
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - KoichiYasuoka's roberta-large-japanese-aozora is a pre-trained RoBERTa model for Japanese, utilizing the LUW tokenizer and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest â™»
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - This repository provides a Japanese BERT base model and tokenizer trained on the June 2021 Japanese Wikipedia dataset for use in natural language processing tasks.
  - Downloads: 23
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - This repository fine-tunes a Japanese GPT-2 model specifically for generating application essays (ES) for IT industry job seekers, based on code from rinnakk/japanese-pretrained-models.
  - Downloads: 23
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - This repository provides a fine-tuned Japanese GPT-2 model for generating *ES* (rÃ©sumÃ©s/cover letters) using a dataset of 140,000 examples, built upon rinnakk/japanese-pretrained-models and accessible via http://www.eswrite.com.
  - Downloads: 23
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer (Vaporetto + Unigram) requiring a downloaded dictionary file for processing.
  - Downloads: 23
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa-v2-large Japanese model, pretrained for universal dependency parsing and POS-tagging with the `goeswith` subword strategy, requiring `fugashi` for use.
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja is a Japanese language LayoutLM model pretrained for token classification tasks and finetuned from cl-tohoku/bert-base-japanese-v2, released under CC BY-SA 3.0.
  - Downloads: 23
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - This repository provides a 4-bit AutoAWQ quantized version of the phi-4 language model, calibrated with both Japanese and English data for enhanced multilingual performance.
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
  - LLM-jp-3.1-8x13b-instruct4 is an instruction-tuned, 8x13 billion parameter large language model developed by NII, building on the LLM-jp-3 series with improved instruction-following abilities.
  - Downloads: 22
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - KoichiYasuoka/bert-base-japanese-char-extended is a Japanese BERT model pre-trained on Wikipedia, featuring enhanced character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - This repository provides a Japanese T5 language model fine-tuned on a 100GB corpus of Japanese textâ€”including Wikipedia, OSCAR, and CC-100â€”for adapted language modeling (next token prediction).
  - Downloads: 22
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - This repository provides a Japanese BERT-base model with a Juman++ and WordPiece tokenizer, requiring a downloaded dictionary file for use.
  - Downloads: 22
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - REV-Mix is an anime and realistic style diffusion model for Stable Diffusion, inspired by DateMix and RDtMix, optimized with specific sampler, step, and CFG settings and benefiting from optional embeddings.
  - Downloads: 22
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - This repository provides a pre-trained DeBERTa V2 small Japanese language model for masked language modeling tasks, with associated pretraining code available elsewhere.
  - Downloads: 22
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AIâ€™s Japanese StableLM Instruct Beta 70B language model, offering various parameter options for optimized performance.
  - Downloads: 22
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - This repository provides a RoBERTa-large language model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - jp-ModernBert-base-preview is a 150M parameter Japanese language model built by Algomatic, featuring an 8192 context length, trained on ~300B tokens of Japanese data (fineweb2) and utilizing the BertJapaneseTokenizer.
  - Downloads: 22
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM provides open-source, customizable Japanese-to-Chinese translation modelsâ€”fine-tuned on general and ACGN-style dataâ€”for light novels and galgames, released under a non-commercial license.
  - Downloads: 21
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AIâ€™s Japanese StableLM Instruct Gamma 7B language model, offering various parameter configurations for optimized performance.
  - Downloads: 21
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 21
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built upon a base model trained on 42 billion Japanese tokens from Cultura-X.
  - Downloads: 21
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - This repository provides a Japanese instruction-following modelâ€”a fully fine-tuned version of Microsoftâ€™s Phi-3-mini-4k-instruct using the llm-jp/hh-rlhf-12k-ja datasetâ€”designed for multilingual (English reasoning, Japanese response) question answering.
  - Downloads: 21
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained, bilingual Japanese-English language model adapting Llama-2-7b with 42 billion tokens from Cultura-X, achieving state-of-the-art results in perplexity and translation.
  - Downloads: 21
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - This repository provides a 1.5B parameter Japanese GPT2 model, pretrained on Japanese Wikipedia and CC-100, for text generation and fine-tuning, requiring pre-tokenization with Juman++.
  - Downloads: 21
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - This repository provides a BERT model, fine-tuned on Japanese novels, for classifying text (titles & summaries) into genres, built upon cl-tohoku/bert-base-japanese-char-v3.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - KoichiYasuoka/bert-large-japanese-char-extended is a pre-trained Japanese BERT modelâ€”based on bert-large-japanese-charâ€”with extended character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 20
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository provides a medium-sized Japanese GPT-2 model with a BERT-like tokenizer (Unidic) built on PyTorch and Hugging Face Transformers for text generation.
  - Downloads: 20
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model, built with the Heron library, capable of image-based conversation and utilizing the GitGPTNeoX architecture.
  - Downloads: 20
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 1.7B-parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following.
  - Downloads: 20
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Beta 7B language model, offering various parameter permutations for optimized performance.
  - Downloads: 20
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero is a 13B Japanese language model fine-tuned on 15k samples from the Jaster dataset for instruction following.
  - Downloads: 20
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporationâ€™s Japanese-large-lm-3.6b-instruction-sft model, intended for use with llama.cpp, but potentially incompatible with future updates.
  - Downloads: 20
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, featuring 12 layers, 64 hidden dimensions, and 1 attention head.
  - Downloads: 20
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - jp-modernbert-large-preview is a 396M parameter Japanese language modelâ€”built by Algomatic with 100B tokens of training dataâ€”featuring an 8192 context length and compatible with transformers, fugashi, unidic_lite, and FlashAttention.
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - This repository provides GGUF quantized weights for Llama-3.3-Swallow-70B-Instruct-v0.4, fine-tuned with the imatrix Japanese language dataset.
  - Downloads: 20
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository provides a Japanese BERT-large model, fine-tuned on the JGLUE/JCommonsenseQA dataset, for performing CommonsenseQA tasks.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - This repository provides a ctranslate2-compatible dataset converted from AIBunCho's Japanese novel GPT-J-6B model, featuring 8-bit quantization which may reduce accuracyâ€”though quantitative data is currently unavailable.
  - Downloads: 20
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - This Japanese language model, fine-tuned via QLoRA on 216 highly-rated web novels, Aozora Bunko texts, and Wikipedia, generates fiction guided by specified genres, keywords, and prompts, with knowledge up to 2021.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 20
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - JMedRoBERTa-base-manbyo-wordpiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 19
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - This repository provides a pretrained FastText word embedding model for Japanese, including a Google Colaboratory example and installation instructions for necessary dependencies like MeCab and PyTorch.
  - Downloads: 19
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Unigram, requiring a downloaded dictionary for proper functionality.
  - Downloads: 19
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Zenz-v1 is a 90M parameter Japanese GPT-2 language model finetuned for kana-kanji conversion, intended for use with the Zenzai neural conversion system and licensed under CC-BY-SA 4.0.
  - Downloads: 19
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - This repository details a fine-tuned Meta-Llama-3-8B-Instruct model for Japanese conversation, trained on a 49k dataset using h2o-llmstudio with an 8k context length, and usable with the transformers library.
  - Downloads: 19
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model fine-tuned for instruction following, based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 19
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - This repository provides GGUF conversions of the Line Corporation's Japanese-large-lm-3.6b language model, intended for use with llama.cpp and potentially subject to future incompatibility.
  - Downloads: 19
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This repository provides a JAX/Flax-based transformer language model (0.1B parameters) trained on Japanese text, built upon the Flax lm1b example, and includes benchmark scores and FlaxAutoModel support.
  - Downloads: 19
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and WordPiece for enhanced Japanese language processing.
  - Downloads: 19
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - This repository details a fine-tuned Japanese BERT model for multi-class classification of grammar points, trained on dictionary data and LLM-augmented examples for use in language learning.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - Sarashina2.2-3Bx8-moe is a Japanese Mixture of Experts (MoE) language model built upon sbintuitions/sarashina2.2-3b-instruct-v0.1 using mergekit-moe, enabling diverse and high-quality text generation through the integration of eight specialized models.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - This repository provides a 4-bit quantized, Japanese-language fine-tune of the Llama-2-Chat 70B model, trained on the alpaca_ja dataset, requiring adherence to Metaâ€™s Llama license.
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance ã®GGUFç‰ˆ Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - KoichiYasuoka/bert-large-japanese-upos is a BERT model pre-trained on Japanese Wikipedia for Universal Part-Of-Speech tagging and dependency parsing of short-unit words.
  - Downloads: 18
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - This repository provides a pretrained ESPnet2 Text-to-Speech (TTS) model â€“ jsut_vits_accent_with_pause â€“ trained on the JSUT dataset and available via Zenodo.
  - Downloads: 18
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B parameter Japanese GPT2 model finetuned by jweb (based on rinnaâ€™s work) in both PyTorch and Rust formats, requiring the T5Tokenizer for use.
  - Downloads: 18
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - This repository provides a RoBERTa-base Japanese model fine-tuned on the JSNLI dataset for zero-shot text classification, requiring pre-segmented text using Juman++.
  - Downloads: 18
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This repository provides a BART large language model for Japanese, converted from Kyoto Universityâ€™s original release and compatible with the `transformers` library via `BartJapaneseTokenizer`.
  - Downloads: 18
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - This repository provides a Japanese SPLADE model, fine-tuned on the mMARCO dataset from the tohoku-nlp/bert-base-japanese-v2 base model, for semantic text retrieval.
  - Downloads: 18
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 18
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - This repository provides fine-tuned Mistral-7B-based large language models (Ninja-v1, including NSFW and 128k context versions) excelling in both Japanese and English long-context generation.
  - Downloads: 18
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - This repository hosts a LoRA-fine-tuned language model (Ninja-v1-NSFW) for roleplaying, based on Aratako/Ninja-v1-RP, utilizing the Vicuna chat template and trained on Japanese datasets like Rosebleu and LimaRP, with specific prompt formatting and end-of-turn token requirements.
  - Downloads: 18
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - This repository provides a 7B Japanese chat modelâ€”umievo-itr004-7bâ€”created by linearly merging Aratako/Antler-7B-RP-v2, NTQAI/chatntq-ja-7b-v1.0, and TFMC/Japanese-Starling-ChatV-7B with bfloat16 precision and int8 quantization.
  - Downloads: 18
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - This repository provides an early-stage Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, evaluated in an ABEJA tech blog, and built using the Metagton-LM framework.
  - Downloads: 18
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - This repository provides a 7B language modelâ€”fine-tuned for Q&A with context and quantized using AutoGPTQ/AutoAWQâ€”aiming for GPT-3.5-level performance on Japanese Q&A and RAG tasks.
  - Downloads: 18
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - This repository provides a Japanese BERT-based model finetuned for cyberbullying detection, utilizing a unified dataset from BBS comments and Twitter, and licensed under CC BY-SA 4.0.
  - Downloads: 18
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pre-trained on Japanese Wikipedia, intended for text generation or fine-tuning after word segmentation with Juman++.
  - Downloads: 18
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This repository details a 6.8 billion parameter Japanese pre-trained language model built upon EleutherAI's Mesh Transformer JAX, utilizing T5Tokenizer and SentencePiece for tokenization.
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - This repository fine-tunes the wav2vec2-large-xlsr-53 model for live Japanese speech-to-text translation using multiple Japanese datasets like Common Voice and CSS10.
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz-v2.5 is a finetuned GPT-2 language model for Japanese kana-to-kanji conversion, offered in small, medium, and large sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a fine-tuned Mistral-7B-based LLM featuring a 128k context window, high-quality Japanese & English generation, and robust long-context memory, even for NSFW content.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is a 90M parameter Japanese GPT-2 language model, finetuned for high-performance kana-to-kanji conversionâ€”building upon zenz-v1 and utilizing a character/byte-level BPE tokenizerâ€”and released under CC-BY-SA 4.0.
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-large Japanese model, pretrained on Aozora Bunko for dependency parsing and question answering, utilizing UD_Japanese-GSDLUW and designed to handle ambiguous words with [MASK] tokens.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - This repository provides a fine-tuned Japanese LLM based on OpenAI's Whisper, specifically optimized for accurate speech-to-text transcription of Dominion card game terminology, having learned all cards as of December 19, 2023.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - This repository provides a 1.3B parameter Japanese GPT modelâ€”trained on alpaca_ja and GuanacoDatasetâ€”for conversational AI, requiring 7GB VRAM or RAM for operation.
  - Downloads: 17
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese BERT base model specifically for super short unit words (SSUW), requiring full-width conversion and SSUW segmentation as pre-processing steps.
  - Downloads: 17
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - This repository provides an inference model cloned from rinna's â€œjapanese-gpt-1bâ€ and fine-tuned on the â€œdatabricks-dolly-15k-jaâ€ dataset, replicating the functionality of inu-ai/dolly-japanese-gpt-1b with detailed environment setup for local fine-tuning and inference.
  - Downloads: 17
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - This repository provides a Japanese chat modelâ€”a variant of ebisuke/liz-nojaloli-jaâ€”fine-tuned from abeja/gpt-neox-japanese-2.7b for personal study, utilizing a specific input format for conversational turns.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and WordPiece for efficient Japanese text processing within the Transformers library.
  - Downloads: 17
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - This repository provides a Japanese instruction-tuned Llama 2 model (if001/llama2_ja_small) fine-tuned on an instruction dataset using the lit-gpt script.
  - Downloads: 17
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-7B language model, available on Hugging Face.
  - Downloads: 17
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 17
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - This repository provides Japanese-tuned versions of the Meta-Llama-3-8B-Instruct modelâ€”compatible with both transformers and the original llama3 codebaseâ€”finetuned on a 49k Japanese conversation dataset with an 8192 context length.
  - Downloads: 17
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 17
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This repository provides a QLoRA-fine-tuned Llama-2-13b-chat-hf model, trained on a mixed Japanese/Chinese dataset of chat and non-chat samples, with recommended generation parameters and testing script.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - This repository provides a Japanese BERT-base model ("Vaporetto + BPE") and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 17
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 17
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - KoichiYasuoka's roberta-small-japanese-aozora is a pre-trained RoBERTa model for Japanese, trained on Aozora Bunko texts and suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using the Japanese-LUW tokenizer, suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 17
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - This repository provides a fine-tuned DistilHuBERT speech modelâ€”trained 50k steps on a combined Japanese corpus including JVS, Tsukuyomi-Chan, and custom ITA datasetsâ€”requiring acceptance of the JVS corpusâ€™s terms of use.
  - Downloads: 17
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - ModernBERT-large-japanese-aozora is a pre-trained Japanese language model based on the é’ç©ºæ–‡åº« corpus, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia text, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora texts for Universal POS-tagging and dependency parsing of long-unit words.
  - Downloads: 17
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants, with Hugging Face checkpoint formats.
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B is a 7B-parameter Japanese language model, fine-tuned on new datasets and built upon Japanese Stable LM, with some ACG (anime, manga, VN) knowledge for creative content generation.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - This repository provides a Japanese RoBERTa-base model pretrained for universal dependency parsing and POS-tagging using the goeswith subword tokenizer, requiring the `fugashi` library for use with the `transformers` pipeline.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B is a 1 billion parameter language model pre-trained on 100B tokens, utilizing a Differential Transformer architecture with Differential Attention to improve focus and reduce noise, and optimized for efficient training via patch-level training and a faster optimizer.
  - Downloads: 16
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - This repository provides a Japanese BERT large model pretrained on jawiki-20200831 using character and word-level (Unidic 2.1.2) tokenization with whole word masking for improved language understanding.
  - Downloads: 16
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This repository provides a Japanese sentence-T5 model, pre-trained using sonoisa/t5-base-japanese, requiring sentencepiece for inference and achieving comparable accuracy to sonoisa/sentence-bert-base-ja-mean-tokens.
  - Downloads: 16
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporationâ€™s repository hosts a 3.6B parameter, 4-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 16
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - Fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding model, based on BERT, trained on limited data for similarity/entailment and retrieval tasks like JSTS, JSNLI, and MMARCO.
  - Downloads: 16
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - This repository provides a Japanese instruction-following language model, fine-tuned from Meta's Llama 3 8B Instruct using QLoRA on a small dataset, achieving an average score of 3.12 on ELYZA-tasks-100 (Q5_K_M).
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - This repository provides a Japanese-adapted version of Mixtral-8x7B-Instruct-v0.1, enhanced with vocabulary expansion and continued pre-training by ABEJA, built upon the Metagton-LM framework.
  - Downloads: 16
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Base Beta 70B language model with various parameter options for optimized performance.
  - Downloads: 16
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - This repository provides a 417.12M parameter Llama 2 model trained on Japanese text, utilizing the `if001/sentencepiece_ja` tokenizer and demonstrating text generation with a sample prompt.
  - Downloads: 16
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE is a suite of decoder-only Japanese language models by CyberAgent, Inc., fine-tuned with LoRA using PEFT, PyTorch, and Transformers.
  - Downloads: 16
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small, fast, Japanese language LLaMA-based model pre-trained entirely on Japanese text, producing surprisingly coherentâ€”though not always helpfulâ€”responses with a focus on generation quality through careful parameter tuning.
  - Downloads: 16
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - This repository provides a RoBERTa-large Japanese language model pretrained for universal dependency parsing and part-of-speech tagging, utilizing the goeswith subword tokenizer and requiring the fugashi library.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa-based model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependency parsing and part-of-speech tagging, built upon existing Japanese UD datasets and utilizing the goeswith subword approach.
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 16
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - This repository provides ja_ginza_electra, a spaCy v3 package distributing an ELECTRA model pretrained on Japanese mC4 data and finetuned on UD_Japanese, requiring SudachiTra for tokenization via GiNZA v5.
  - Downloads: 16
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - This repository provides a transformer-based Japanese-to-Hebrew translation model (jpn-heb) trained on OPUS data, utilizing SentencePiece preprocessing and including evaluation scores for BLEU and chr-F on the Tatoeba test set.
  - Downloads: 16
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B is a 12B parameter language model merged from Orihime-12B with ForgottenMaid-12B, Francois-PE-V2-Huali-12B, and Ohashi-NeMo-12B using the Model Stock merge method.
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - DeBERTa-large-japanese-wikipedia is a pre-trained Japanese language model based on DeBERTa V2, trained on Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 16
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specialized for kana-to-kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 16
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - This repository provides a Japanese BERT base model pretrained with character-level tokenization and whole word masking, offering an alternative to cl-tohoku/bert-base-japanese-char-v2 without requiring Fugashi or Unidic_lite.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - DeBERTa-base-Japanese model pretrained for Japanese POS-tagging and dependency parsing using goeswith subwords, requiring fugashi for use with the `transformers` pipeline.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese language model, pretrained on the Aozora corpus and fine-tuned for Universal Dependencies (POS-tagging & dependency parsing) using the goeswith subword tokenizer.
  - Downloads: 16
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised Chinese-Japanese masked language model pretraining framework leveraging the Unihan database and a two-stage coarse-to-fine approach to utilize shared character morphology across both languages.
  - Downloads: 16
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - This repository details hyperparameter optimizationâ€”using Optuna with a `bert-base-japanese` model and multilingual sentiment datasetâ€”resulting in optimal settings of cosine learning rate scheduling (2.82e-05), gradient accumulation of 1, and weight decay of 0.00017.
  - Downloads: 16
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - This repository details Japanese sentiment analysis experiments using a BERT base model, the WRIME dataset, Adafactor optimization, and hyperparameter tuning via Optuna, exploring learning rates, batch sizes, and weight decay regularization.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
  - Shirayukihime-12B is a 12B parameter language model merged using the TIES method from shisa-ai/shisa-v2-mistral-nemo-12b, Elizezen/Himeyuri-v0.1-12B, and natong19/Mistral-Nemo-Instruct-2407-abliterated, utilizing bfloat16 precision.
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia, building upon the bert-large-japanese-char-extended base.
  - Downloads: 15
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - This repository provides a fine-tuned T5-base-japanese model for Japanese title generation from input text, pretrained on a 100GB Japanese corpus including Wikipedia and OSCAR data.
  - Downloads: 15
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - This repository provides a fine-tuned Japanese language model based on japanese-novel-gpt-j-6b, enabling conversations with the Touhou Project character Marisa Kirisame, trained with 4-bit quantization.
  - Downloads: 15
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - Rinna/nekomata-14b-gguf provides a GGUF quantized version of the 14 billion parameter rinna/nekomata-14b model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization.
  - Downloads: 15
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-instruct-gamma-7b using Slerp merging across the first 32 layers, based on Mistral-7B as the base model.
  - Downloads: 15
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - This repository reproduces a 7B-parameter Japanese language model, fine-tuned for instruction following using translated Databricks Dolly-15k and other datasets, based on Japanese Stable LM Base Gamma 7B and trained with the notus codebase.
  - Downloads: 15
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B is a Japanese causal language model, fine-tuned with LAPT and randomness, available via PEFT and Transformers for use with 8-bit loading and GPU acceleration.
  - Downloads: 15
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - This repository provides a Japanese instruction-tuned version of Microsoftâ€™s Phi-3-mini-4k model, fine-tuned with the llm-jp/hh-rlhf-12k-ja dataset using full parameter tuning for improved instruction following in Japanese.
  - Downloads: 15
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - This repository provides a Japanese-tuned version of Meta's Llama 3 8B Instruct model, enhanced with ChatVector and QLoRA, achieving an average score of 3.32 on the ELYZA-tasks-100 benchmark (Q5_K_M quantization).
  - Downloads: 15
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - This repository presents Tokara-0.5B-v0.1, a 0.5B parameter language model fine-tuned from Qwen1.5-0.5B with 5B Japanese/English tokens and enhanced with multi-turn chat capabilities via chat vectors, benchmarked on Japanese MT-bench categories.
  - Downloads: 15
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
  - This repository provides a Japanese language model based on Mistral-7B, fine-tuned with LAPT and heuristics, and readily usable with PEFT and Transformers libraries.
  - Downloads: 15
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - This model merges differences between Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1 based on Swallow-MX-8x7b-NVE-v0.1, improving Japanese naturalness and offering top-tier performance for local 32k context LLMs as of March 2024.
  - Downloads: 15
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted weights for ELYZA-japanese-Llama-2-13b-fast-instruct, a Japanese language model based on Llama 2, optimized for use with llama.cpp.
  - Downloads: 15
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - This repository provides a 6.3GB GPTQ-quantized version of the 10 billion parameter, Japanese-centric multilingual Weblab-10b-instruction-sft model, offering faster execution with a slight performance trade-off.
  - Downloads: 15
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This repository details a Japanese GPT-1B-based model fine-tuned for extractive question answering and answer refinement specifically within the gpt-index (v0.2.5) framework, utilizing two prompt templates for enhanced performance.
  - Downloads: 15
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - This repository provides a 1.3B NLLB-200 model fine-tuned for Japanese-to-English translation of the â€œAscendance of a Bookwormâ€ web novel.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - DeBERTa-v2 pre-trained on Japanese Wikipedia and Aozora Bunko texts provides a foundation for fine-tuning various downstream Japanese NLP tasks.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - DeBERTa-large-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - DeBERTa-base-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - DeBERTa-small-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large Japanese text corpora for masked language modeling tasks.
  - Downloads: 15
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - This repository provides a fine-tuned Wav2Vec2 speech recognition model for Japanese accents, achieving a 15.82% Word Error Rate (WER) on 16kHz sampled speech.
  - Downloads: 15
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained on Wikipedia for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - This repository provides a Japanese language model, llm-jp-3-3.7b-instruct, fine-tuned for long-form text generation using the Japanese-LongWriter-3k dataset, with training details including a learning rate of 1e-05 and specified batch sizes.
  - Downloads: 15
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - This repository provides a XLM-RoBERTa-base model fine-tuned on a Japanese mMARCO dataset using an ANCE warmup approach, achieving MRR@100 of 0.242 at 50k steps, with dataset preparation scripts linked.
  - Downloads: 15
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - Bert-base-sudachitra-v11 is a Japanese language model based on SudachiTra, differing from v1.1 by using surface word forms and modified vocabulary handling for improved tokenization.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Rinna's Llama 3 Youko 70B is a continually pre-trained and instruction-tuned version of Meta-Llama-3-70B, enhanced for Japanese language performance, with available 8B parameter quantized versions.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - This repository provides a pretrained Japanese ELECTRA-small model, utilizing Byte-Pair Encoding on Japanese Wikipedia data and requiring proper MeCab dictionary configuration for best results.
  - Downloads: 14
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This repository provides a fine-tuned wav2vec2-xls-r-1b speech recognition modelâ€”trained on the Mozilla Common Voice 8.0 Japanese datasetâ€”achieving a 1.0132 WER and 0.1609 CER with a learning rate of 7.5e-05 and batch size of 32.
  - Downloads: 14
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - This repository provides a Japanese BERT modelâ€”derived from bert-base-japanese-char-extendedâ€”pre-trained for Universal Part-Of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia text, utilizing long-unit-words and FEATS.
  - Downloads: 14
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - This repository provides a Japanese error detection and correction model, a fine-tuned mt5-base trained on 20,000 text pairs, utilizing a â€œcorrection: â€ prefix for text-to-text tasks.
  - Downloads: 14
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - This repository provides a GPT2-based Japanese lyric generation modelâ€”`skytnt/gpt2-japanese-lyric-small`â€”with code examples and a demo website for creating song lyrics.
  - Downloads: 14
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - This repository provides GPT2-base-Japanese-v2, a Japanese language model trained on Wikipedia and web crawl data with a 60,000-token BPE tokenizer, compatible with the `transformers` library for text generation.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-large model pretrained on Japanese é’ç©ºæ–‡åº« texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword tokenizer.
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - LINE Corporationâ€™s repository hosts a 3.6B parameter, 8-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 14
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 14
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
  - This repository provides a 4-bit quantized ExLlamaV2 model of augmxnt/shisa-base-7b-v1, a 7B+8B token model extending Mistral 7B with Japanese and English pre-training data and a 120k token tokenizer.
  - Downloads: 14
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository provides a GPTQ quantized version of ELYZA-japanese-CodeLlama-7b-instruct, calibrated using a 1k sample from Japanese Wikipedia and the ELYZA-tasks-100 dataset.
  - Downloads: 14
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Base Beta 70B language model, optimized for efficiency using hardware from Massed Compute and supported by a grant from a16z.
  - Downloads: 14
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository provides a medium-sized, right-to-left generative Japanese GPT-2 model utilizing a BERT-like tokenizer and requiring PyTorch, fugashi, and Hugging Face Transformers.
  - Downloads: 14
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech-to-text, operating at 16kHz and outputting continuous character sequences without word boundaries.
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository provides a Japanese-novel quality assessment Reward Model, fine-tuned from modernbert-ja-310m, designed for use in reinforcement learning for text generation and predicting user evaluation scores of novel text.
  - Downloads: 14
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - This repository provides a Japanese-enhanced, fine-tuned version of Mistral-Nemo (v0.2) for roleplaying, trained on a diverse dataset and optimized for Japanese output with a recommended temperature of 0.3.
  - Downloads: 14
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - This repository shares a merged Stable Diffusion model (â€œKokuwaâ€) based on KiwiMix and other models, specializing in uniquely stylized, slightly quirky character generation with some seed-dependent instability, and acknowledges its source models with credits.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - This repository provides a DeBERTa(V2) model pre-trained on Japanese é’ç©ºæ–‡åº« text for Universal Part-Of-Speech (UPOS) tagging and dependency parsing.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-large Japanese model, pretrained on the Aozora corpus, for dependency parsing and question answering, particularly suited for long-unit-word handling with the use of [MASK] tokens for disambiguation.
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository provides a Japanese-finetuned version of the unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit model, optimized for Japanese output and reasoning.
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 3.6B-parameter Japanese language model quantized to 4-bit, fine-tuned by LINE Corporation for instruction-following tasks.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - Rinna Co., Ltd.'s repository hosts a 1.3B-parameter Japanese GPT model, requiring T5Tokenizer for use and supporting GPU acceleration with PyTorch.
  - Downloads: 13
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B is a pre-trained Japanese text generation model, similar to GPT2/GPT3, trained on a large Japanese corpus and usable via the `transformers` pipeline.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Byte Pair Encoding (BPE), requiring a downloaded dictionary file for proper loading and processing.
  - Downloads: 13
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - This repository hosts a fork of LINEâ€™s DistilBERT model, pre-trained on a large Japanese web corpus, with updated tokenizer code compatible with transformers >= 4.34.
  - Downloads: 13
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)
  - This repository provides a Japanese-adapted BLOOM-7B language model, fine-tuned with LAPT and randomized weights, accessible via PEFT and Transformers for efficient 8-bit loading and GPU usage.
  - Downloads: 13
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base is a 3 billion parameter language model pre-trained on Japanese and English data, utilizing the RetNet architecture and a retention mechanism for research purposes, released under the MIT license.
  - Downloads: 13
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository likely contains research and analysis data related to the Tsukuba area, potentially involving university students and scientific projects within the Kanto region of Ibaraki prefecture.
  - Downloads: 13
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - This repository provides the Japanese language llm-jp-1.3b-v1.0-aya model, fine-tuned on Cohere's aya dataset, and demonstrates its usage with Hugging Face Transformers.
  - Downloads: 13
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset, offering a non-transformer alternative for sequence modeling.
  - Downloads: 13
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporationâ€™s repository hosts a 1.7 billion parameter, 4-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - This repository provides a Japanese BERT-base tokenizer trained with Nothing + Unigram, requiring a downloaded dictionary file for loading and usage with transformers.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - This repository provides a Japanese BERT-base model and instructions for loading its tokenizer using a downloaded dictionary file for the Nothing + BPE method.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer built with Sudachi and Unigram, requiring a downloaded dictionary file for usage.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 13
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This repository provides a fine-tuned open-calm-7b language model, trained with H2O LLM Studio on a Japanese quiz dataset, and provides instructions for GPU-based inference using the transformers library.
  - Downloads: 13
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 13
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 is a finetuned Japanese GPT-2 model for causal language modeling and text generation, detailed in a linked paper, and readily usable via the `transformers` pipeline with reproducibility via seed setting.
  - Downloads: 13
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - This repository offers a merged language modelâ€”built with mergekit from Aratako/Ninja-v1-RP-WIPâ€”optimized for roleplaying through task vector addition and model stock merging, utilizing the Vicuna chat template and requiring an `eos_token` for multi-turn dialogues.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-v2 model for Japanese language tasks, utilizing the BertJapaneseTokenizer and trained on the Aozora Bunko corpus for applications like POS-tagging and dependency parsing.
  - Downloads: 13
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja is a fine-tuned OpenAI Whisper model for Japanese speech recognition, trained on Common Voice 17.0 with specific hyperparameters like a 1e-05 learning rate and AdamW optimizer.
  - Downloads: 13
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants (Jaster, Dolly, OASST), alongside pre-trained checkpoints.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa(V2) large language model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependencies, specifically POS-tagging and dependency parsing, utilizing the goeswith subword approach.
  - Downloads: 13
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-based model pre-trained on Japanese text (Wikipedia & Aozora Bunko) for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - Isekai-BERT-v1 is a fine-tuned Japanese BERT model (cl-tohoku/bert-base-japanese-v3) with reported loss of 1.9164, but lacking detailed information on data, training, and intended use.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-based question-answering model, pretrained on the Aozora Bunko corpus with UD_Japanese-GSDLUW for dependency parsing and optimized for handling ambiguous words using [MASK] tokens.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora Bunko texts for UPOS-tagged part-of-speech and dependency parsing of long-unit words.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-large model pre-trained on Japanese Wikipedia and Aozora Bunko for universal POS-tagging and dependency parsing with long-unit-word support.
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Instruct Beta 70B, optimized for efficiency using hardware from Massed Compute and supported by a16z grant.
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1ã‚’ huggingface/text-embeddings-inferenceã§å‹•ã‹ã™ãŸã‚ã® fork ã§ã™ã€‚
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause â™»
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - This repository provides a T5-based text-to-text transformer pre-trained on 8K-vocabulary Japanese web text data (mC4 & Wiki40b), with training code available and a 32K version also offered.
  - Downloads: 12
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - This repository provides a Japanese pre-trained MobileBERT modelâ€”a faster alternative to BERTâ€”built for use with the `transformers` library and utilizing the Tohoku University Japanese BERT tokenizer.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab + Unigram, requiring a downloaded dictionary file for processing.
  - Downloads: 12
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 12
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and distributed as a Python package for advanced Japanese natural language processing with GiNZA v5.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0 or later.
  - Downloads: 12
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AIâ€™s Japanese StableLM Instruct Gamma 7B language model, optimized with hardware from Massed Compute and supported by a16z grant.
  - Downloads: 12
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityaiâ€™s Japanese-StableLM-base-gamma-7b using Slerp merging, focusing on layers 0-32 and prioritizing self-attention parameters.
  - Downloads: 12
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - This repository provides a Japanese pre-trained version of the Mixtral 8x7B model, demonstrated with example code using the Hugging Face Transformers library.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 model, currently only in Q4_K_M format.
  - Downloads: 12
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - This repository details fine-tuning of Lineâ€™s â€œjapanese-large-lm-3.6b-instruction-sftâ€ language model on Japanese wikibook data for middle and high school levels, further enhanced with instruction tuning using the sakura dataset.
  - Downloads: 12
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - This repository provides a dynamically generated model card detailing key informationâ€”including developer, funding, license, and languageâ€”for a ðŸ¤— Transformers model hosted on the Hub.
  - Downloads: 12
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - DeBERTa-v3-base-japanese-ud-goeswith is a pretrained Japanese language model for universal dependency parsing and part-of-speech tagging, built upon LLM-jp and utilizing the goeswith subword approach.
  - Downloads: 12
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - This repository provides a Japanese language model based on Mistral-7B, fine-tuned with LAPT and CLP+ techniques, and optimized for Peft usage with 8-bit loading.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0 or later.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, instruction-following Japanese language model fine-tuned from the base Japanese Stable LM Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 12
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - This repository provides a BERT-large model pre-trained on Japanese Wikipedia for dependency parsing and question answering, built upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 model, currently only in Q4_K_M format.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-large Japanese language model, utilizing the BertJapaneseTokenizer and trained on the Aozora Bunko corpus, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - Zenz v2.5 is a Japanese kana-to-kanji conversion model based on the GPT-2 architecture, available in small (91M), medium (310M), and xsmall (26M) sizes, licensed under CC-BY-SA 4.0.
  - Downloads: 12
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - This repository provides a Japanese ELECTRA model, pretrained and finetuned on disaster tweets, for information triage tasks, licensed under CC BY-SA 4.0.
  - Downloads: 12
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - This repository provides a Japanese ELECTRA base model pretrained on Japanese Wikipedia data, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is a full, instruction-following language model based on Meta's Llama-3-8B-Instruct, developed and tested on ConoHa VPS with NVIDIA H100 GPUs.
  - Downloads: 12
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on Japanese Wikipedia and Aozora Bunko texts, specifically for dependency parsing and handling ambiguous words using the [MASK] token.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - This repository provides a Japanese language model exhibiting inconsistent persona traits (shifting gender/personality) but generally cheerful, intended for experimentation and **not** merging, with specific parameters for text generation (max_length 150, top_p 0.8, temperature 0.7).
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - ModernBERT-base-japanese-char is a pre-trained Japanese language model based on BERT, trained on Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æž)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€text-embeddings-inference (TEI) ã§ã€mecab / unidic ãªã©ã‚’ç”¨ã„ãŸæ—¥æœ¬èªžTokenizerã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€dummy ã® tokenizer.json ã‚’ç”¨ã„ã¦ç„¡ç†ã‚„ã‚Šå‹•ã‹ã™ æ–¹æ³•ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - This repository provides a Japanese ELECTRA modelâ€”pretrained on 200M sentences using SudachiTra/WordPiece tokenizationâ€”for use with the `transformers` library.
  - Downloads: 11
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - This repository provides a Japanese-language modelâ€”a fine-tuned mt5-baseâ€”specifically for summarizing patent claims within the pharmaceutical domain.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for universal dependency parsing and part-of-speech tagging, utilizing the goeswith subword approach.
  - Downloads: 11
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - Nagisa-BERT is a BERT model and tokenizer for Japanese, readily usable within the Transformers library via pip installation (Python 3.7+ required) for tasks like masked language modeling.
  - Downloads: 11
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese RoBERTa-base model specifically designed for super short unit word (SSUW) processing, requiring full-width character conversion and prior segmentation (like with KyTea) for masked language modeling tasks.
  - Downloads: 11
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - This repository provides a 130.78M parameter Llama 2 model trained on Japanese text, utilizing the `lit-gpt` training script and compatible with the `transformers` library for text generation.
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and context-based retrieval-augmented generation (RAG) with both GPTQ and AutoAWQ 4-bit quantization, aiming for performance exceeding GPT-3.5.
  - Downloads: 11
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - This repository provides GGUF quantized versions of the Japanese-LLaMA-2-13B language model, available on Hugging Face.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 11
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - This repository details swallow-hermes-st-v1, a Japanese language model created by merging English language model vectors (chat and story) using an evolutionary strategy to create a storytelling LLM with a more natural, less rigidly positive tone than models like GPT-4.
  - Downloads: 11
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA is a Japanese vision-language model built using the Chat Vector method, combining weights from llava-v1.5-7b, Llama-2-7b-hf, and ELYZA-japanese-Llama-2-7b to enable image conversation.
  - Downloads: 11
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - This repository provides a 4-bit GPTQ quantized version of the C3TR-Adapter model for easy English-Japanese and Japanese-English translation, runnable on Colab (with quality improvements on paid tiers).
  - Downloads: 11
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model with GEGLU activation and optimized dropout settings for improved performance.
  - Downloads: 11
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 11
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - This repository provides a Rust implementation of the Tohoku University BERT large Japanese model, enabling its use in Rust projects with instructions for cloning, project setup, and basic usage with the `rust-bert` crate.
  - Downloads: 11
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - This repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of character-level long-unit words.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - This repository provides a Japanese natural language processing pipeline featuring a BERT-based transformer, parser, and NER components, utilizing the UD_Japanese-GSD dataset and licensed under CC BY-SA 4.0.
  - Downloads: 11
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - This repository hosts a Japanese chat model, â€œliz-nojaloli-ja,â€ fine-tuned from rinna/japanese-gpt-neox-3.6b, designed for personal study and featuring a specific conversational style, and requiring a specific input format for consistent output.
  - Downloads: 11
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - This repository provides a small, pre-trained RoBERTa model (â€œroberta-small-hi-charâ€) for Hindi text, utilizing a character-level tokenizer.
  - Downloads: 11
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository provides the DeepSeek-R1-Distill-Qwen-32B-Japanese language model converted to MLX format for use with the `mlx-lm` library.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer leveraging MeCab and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for operation.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository provides a fine-tuned Japanese reward model (based on modernbert-ja-130m) for evaluating the quality of novel text, intended for use with reinforcement learning for text generation, though it may contain biases beyond pure text quality.
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, featuring 12 layers, 256 hidden dimensions, and 4 attention heads.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, optimized for handling long unit words and ambiguous terms via masked input.
  - Downloads: 11
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - This repository provides a Japanese BART-base model converted from Kyoto University's original, compatible with the `transformers` library and requiring the `BartJapaneseTokenizer` for text processing.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-base model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing Japanese RoBERTa and UD_Japanese-GSDLUW resources.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-large Japanese language model pre-trained on Wikipedia and é’ç©ºæ–‡åº« for dependency parsing and question answering, particularly focused on handling long-unit words and ambiguous terms using the [MASK] token.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-base Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, building upon existing char-level and UD Japanese resources.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, featuring 12 layers, 256 hidden dimensions, and 4 attention heads.
  - Downloads: 11
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU is an experimental T5-based machine translation model for Japanese-Ainu language pairs, built upon the sonoisa t5-base-japanese model.
  - Downloads: 11
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - This repository details Japanese sentiment analysis training using cl-tohoku/bert-base, the wrime-sentiment dataset, and Optuna-optimized hyperparametersâ€”cosine learning rate schedule (3.9e-5), batch size of 128, weight decay of 5.2e-5, and 100 epochs with early stopping.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - This repository provides a Japanese BERT-base model (â€œVaporetto + WordPieceâ€) and instructions for loading its associated tokenizer using a downloadable dictionary file.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing the Nothing + WordPiece algorithm, requiring a downloaded dictionary file for proper loading and use.
  - Downloads: 11
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - This repository provides a pretrained Japanese ELECTRA model (ShinoBU) tokenized with SudachiTra/WordPiece, trained on 200M sentences and usable with the Hugging Face Transformers library.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset for conversational ability.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - This repository provides a pretrained VITS prosody model for ESPnet2 TTS, trained on the JSUT dataset by kan-bayashi and imported from Zenodo.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, instruction-following Japanese language model built on Stable LM Base Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent â™»
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chatã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - æ¦‚è¦ GLM-4-9B-Chatã‚’ã€æ—¥æœ¬èªžã®Wikiãƒ‡ãƒ¼ã‚¿ã‚’é¸å®šã—ã€è¿½åŠ å­¦ç¿’ã—ãŸæ—¥æœ¬èªžã«éžå¸¸ã«å¼·ã„ã‚¹ã‚³ã‚¢ã‚’å‡ºã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
### Syntactic Text Processing
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, offering a small, efficient model for Japanese language processing.
  - Downloads: 3,279
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-Japanese-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,004
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - This repository provides the BracingEvoMix_v2 model, licensed under CreativeML Open RAIL-M with additional authorship by sazyou_roukaku, and clarifies that users are solely responsible for generated content adhering to the licenseâ€™s usage restrictions (specifically restriction A prohibiting criminal or specialized uses).
  - Downloads: 2,902
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 2,850
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-NSFW-128k language model, trained on the imatrix dataset and intended for use with llama.cpp for generating Japanese novels.
  - Downloads: 2,352
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B is a suite of 7 billion-parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and accessible via Hugging Face Transformers.
  - Downloads: 2,337
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,049
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,013
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 1,881
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B is a suite of 3 billion-parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and readily usable with transformers for text generation.
  - Downloads: 1,632
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-30B-A3B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,549
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - This repository provides GGUF-quantized versions of the Aratako/Qwen3-8B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,470
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., readily usable via Hugging Face Transformers.
  - Downloads: 1,403
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 is a Japanese/English language model continuously pre-trained from Mixtral-8x7B-Instruct-v0.1, leveraging the same tokenizer and focused on improving Japanese language performance.
  - Downloads: 1,335
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - This model, licensed under CreativeML Open RAIL++-M, generates images with recommended settings (DPM++ 2M SDE karras, 30-40 steps, 1152x896 resolution) but prohibits generation of violent, sexual, or exploitative content, especially involving minors, and requires consent for likenesses of real people.
  - Downloads: 1,251
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7B, 13B, and 70B instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 1,220
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 1,206
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,170
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,132
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., and readily usable via Hugging Face Transformers.
  - Downloads: 1,114
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 1,083
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 1,043
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,042
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 977
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - This repository provides GGUF format conversions of the NSFW Ninja-v1 language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 830
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KillerWhaleã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - This repository provides GGUF formatted versions of rinna's llama-3-youko-8b and other models, built with data from TFMC/imatrix-dataset-for-japanese-llm, and is intended for use with llama.cpp.
  - Downloads: 575
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository provides GGUF format conversions of the llm-jp-3-7.2b-instruct3 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp (excluding custom chat templates/`-cvn`).
  - Downloads: 548
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Jpã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 451
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - This repository provides a Stanza NLP model for Japanese, offering tools for linguistic analysis including syntax and entity recognition, automatically generated via Hugging Face integration.
  - Downloads: 414
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - This repository provides statically quantized versions (GGUF format) of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, offering various quantization levels (e.g., Q2_K) for different size/performance trade-offs.
  - Downloads: 396
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªžæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - Wanabi-24B is a Japanese large language model fine-tuned for novel writing assistance, based on Mistral-Small-24B, and currently available in GGUF format with multiple quantization levels, undergoing continuous development and training.
  - Downloads: 348
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Commonã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 312
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-128k language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 302
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - This repository presents evaluation results for several Japanese SPLADE models (including variations like v2 and v3) on MIRACL and JQaRA datasets, reporting metrics such as nDCG, Recall, and MRR for information retrieval performance.
  - Downloads: 283
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 277
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores archived and experimental Stable Diffusion 1.5 merge material (v1745 + littleMonsters_anime) designed to heavily deform and simplify outputs when merged with other models, creating a unique "lametta"-style aesthetic.
  - Downloads: 263
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Large-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 249
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - This repository provides a faster, CTranslate2-converted Japanese speech recognition model based on OpenAI's Whisper-large-v2, optimized for use with the `faster-whisper` library.
  - Downloads: 233
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - This repository provides a GGUF formatted version of the matsuo-lab weblab-10b-instruction-sft model, compatible with llama.cpp for local inference and utilizing the mmnga-dev branch.
  - Downloads: 214
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-A-v1-7B base model, utilizing models like Shisa, Gamma, and Mistral 7B, and is designed for use with llama.cpp.
  - Downloads: 199
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - Wanabi-24B is a preview of a 24B parameter large language model fine-tuned for Japanese novel writing assistance, built on Mistral-Small-24B, and excelling at tasks like idea generation and continuation, though currently limited to a 1500-step training and GGUF format.
  - Downloads: 197
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - This repository provides GGUF-formatted versions of RakutenAI-7B, a base language model, for use with llama.cpp and similar applications.
  - Downloads: 193
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - This repository provides GGUF-formatted versions of the shisa-7b-v1 language model, demonstrated with examples using llama.cpp for tasks like Japanese question answering and translation related to PokÃ©mon.
  - Downloads: 141
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-breadcrumbsã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIæ§˜ã® Llama-3.1-8B-EZO-1.1-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Oumuamua-7b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-v1-7B base model, a merge of models like Shisa Gamma and WizardMath, for use with llama.cpp.
  - Downloads: 97
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides statically quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, offering various quantizations (Q2_K, Q3_K_S) for different size/quality trade-offs.
  - Downloads: 91
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/Ninja-v1-RP-expressive language model, requiring users to check the original model for licensing details.
  - Downloads: 86
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 81
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on chatntq-ja-7b-v1.0, derived from Mistral-7B-v0.1, with details and GGUF versions available.
  - Downloads: 78
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/Ninja-v1-RP language model, with licensing details remaining with the original model.
  - Downloads: 77
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha is a vision-language model generating Japanese descriptions from images and optional text inputs, built on Llama and utilizing BLIP image processing.
  - Downloads: 74
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - This repository provides a Japanese speech recognition model fine-tuned from OpenAI's whisper-large-v2 using 5000 training steps on the Japanese CommonVoice v11 dataset, achieving a 0.7449 Word Error Rate.
  - Downloads: 73
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 70
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository releases a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS-large-v1, featuring a custom tokenizer and currently in beta.
  - Downloads: 69
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - This repository provides a GGUF-formatted, 7B parameter version of the Deepreneur Blue Lizard language model, compatible with llama.cpp and licensed under the Llama 2 license.
  - Downloads: 65
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - This repository provides static quantized versions of the Japanese-LLaMA-3-8B-instruct-v2 model in GGUF format, offering various quants (including Q2_K) for use with tools like llama.cpp.
  - Downloads: 41
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 39
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - This repository provides GGUF-formatted versions of the Tora-7B-v0.2 language model, including quantized models with iMatrix applied using the c4_en_ja_imatrix.txt text dataset.
  - Downloads: 36
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - ã¯ã˜ã‚ã« ãªã‚“ã‹æ—¥æœ¬èªžãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚
  - Downloads: 35
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps speech recognition model to the CTranslate2 format for faster inference using CTranslate2 or projects like faster-whisper.
  - Downloads: 33
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIæ§˜ã® Llama-3-EZO-8b-Common-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - This repository provides a GGUF-formatted version of the Tanuki-ZeRo base language model, optimized for use with llama.cpp for tasks like natural language processing.
  - Downloads: 30
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository provides a Japanese novel quality reward model, fine-tuned from sbintuitions/sarashina2.1-1b, designed for evaluating and improving generated novel text via regression-based prediction of user ratings, while acknowledging potential biases beyond text quality.
  - Downloads: 30
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - This repository provides a GGUF conversion of the ChatNTQ-JA-7b-v1.0, a Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b (based on Mistral 7B).
  - Downloads: 29
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - Shisa-base-7b-v1 is a 7B language model built upon Mistral 7B, enhanced with 8B Japanese tokens from MADLAD-400 and a 120k extended tokenizer for improved Japanese language efficiency (~2.3 characters/token).
  - Downloads: 23
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 22
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAEã®å†…è‡“ã¯ãªã„ãžï¼ã¨è¨€ã‚ã›ãªã„ãžï¼ï¼ï¼ï¼
  - Downloads: 21
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - This repository provides Mixtral-8x7B-v0.1-japanese, a Japanese language model based on Mixtral-8x7B-v0.1 and further pre-trained with expanded Japanese vocabulary.
  - Downloads: 20
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: æ—¥æœ¬èªžã§è³ªå•ã™ã‚‹ã¨ã€æ—¥æœ¬èªžã§å›žç­”ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚
  - Downloads: 20
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - This repository provides a model for generating article bodies from titles, as detailed in the linked Qiita article.
  - Downloads: 19
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - This repository details Tokara-0.5B-v0.1, a Qwen1.5-0.5B model further pre-trained on 5B Japanese-English tokens to improve stable Japanese output, evidenced by benchmark scores on jsquad, jcommonsenseqa, and jnli.
  - Downloads: 18
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - This repository implements Named Entity Recognition (NER) using the modernBERT Japanese language model (sbintuitions/modernbert-ja-130m) with a custom tokenizer and defined label set for person, organization, location, facility, product, and event entities.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - This repository showcases Japanese text generation examples using the Phos 7B model, demonstrating its ability to create narrative prose with emotional depth, as seen in this plea for mercy and assistance.
  - Downloads: 17
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - This repository presents a Japanese-enhanced 8B language model, merging OpenBioLLM-8B (biology/medicine expertise) with Llama-3-youko-8b-instruct-chatvector, potentially exhibiting detailed medical knowledge but also occasional hallucinations and relaxed content restrictions.
  - Downloads: 15
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - This repository provides GGUF-converted and K-quantized versions of the Tora-7B-v0.1 language model, enhanced with iMatrix using TFMC's c4_en_ja_imatrix.txt for improved performance.
  - Downloads: 15
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B is a Japanese causal language model, built on PEFT and incorporating LAPT + heuristics, available for use with transformers and optimized for 8-bit loading.
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - This repository hosts a merged language modelâ€”built from Aratako/Ninja-v1-RP and a derivative of Elizezen/Antler-7Bâ€”designed for expressive roleplaying using the Vicuna chat template and requiring an `eos_token` at the end of each assistant turn.
  - Downloads: 14
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - This model adapts Vecteus to be compatible with LLava by integrating the EvoVLM-JP-v1-7B (shisa-gamma-7b-v1) recipe, maintaining expressiveness while enabling LLava usage.
  - Downloads: 14
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B is a 7B-parameter Japanese chat model based on Mistral-7B and quantized to 6-bit, with details and GGUF versions available in the repository.
  - Downloads: 14
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - This repository implements a quote-based reasoning model for natural language inference.
  - Downloads: 13
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - This repository provides a Japanese language model example utilizing AutoTokenizer, AutoModelForCausalLM, and Unifine formatting for in-context and instruction learning.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - This repository provides a Japanese SentencePiece tokenizer with a 52000 vocabularyâ€”specifically trained for the AI Novelist SuperTrin and Damsel 20B models to enhance creative writing.
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - Suzume-POC is a commercially usable, Japanese-adapted base model derived from Google's Gemma-2B, optimized for smaller devices despite potential instruction tuning challenges.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - â—†ArcanaMix äºŒæ¬¡å…ƒã‚¤ãƒ©ã‚¹ãƒˆã‚’ä¸­å¿ƒã«ã€ã‹ã‚ã„ã„ã‚¤ãƒ©ã‚¹ãƒˆãŒå‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€‚
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - This repository provides a Japanese Bloom language model with 10,000 vocabulary size, 12 layers, and 8 attention heads.
  - Downloads: 12
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - This repository implements a reinforcement learning approach to generate more character-consistent conversational responses.
  - Downloads: 12
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - Sarashina2.2-3Bx4-moe is a 12B parameter Mixture of Experts (MoE) model created by merging one base model with three â€œsbintuitions/sarashina2.2-3b-instruct-v0.1â€ expert models to enhance performance and generate high-quality responses.
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - This model is a QLoRA-finetuned version of sbintuitions/sarashina2.2-3b-instruct-v0.1, enhanced with Japanese-Pythonic-FunctionCall and Kendamarron/jimba-instruction-all to enable calling Python functions via a specific system tool format.
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This model fine-tuned from SakanaAI/TinySwallow-1.5B-Instruct generates numerous, concise, bullet-point slides (max 15 characters per point) in the style of the Takahashi Method for impactful presentations.
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - DataPilotâ€™s Arrival-32B-Instruct-v0.4 is a Japanese-enhanced, instruction-tuned language model finetuned from Abeja's Qwen2.5-32B base (using negative chat vectors due to base model unavailability) and incorporating DeepSeek-R1-Distill-Qwen-32B-Japanese.
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6Bæ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­å¤§æ¨¡åž‹ï¼Œæœ¬é¡¹ç›®ä¸ºChatGLM3-6BåŠ å…¥æ—¥æ–‡èƒ½åŠ›ã€‚
  - Downloads: 12
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository provides the AfterRealXL_beta2 modelâ€”licensed under CreativeML Open RAIL++-Mâ€”along with merged checkpoints, requiring adherence to the license terms which prohibit specific uses like illegal activities or medical imaging, with no further responsibility assumed by the author.
  - Downloads: 11
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares experimental, minimally-adjusted merged modelsâ€”originally from the spekulatius projectâ€”built upon lametta_v1921 and other sources like vorpal_v1 and various anime/toon styles, offering unique variations not found in the lametta repository.
  - Downloads: 11
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - YACIS-ELECTRA-Small is a Japanese ELECTRA model pretrained on a 5.6 billion-word blog corpus (YACIS) using MeCab, WordPiece tokenization, and a 12-layer, 128-dimension architecture with a 32,000 token vocabulary.
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details â€»å¥½å¥‡å¿ƒã‹ã‚‰ç”Ÿã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2ã®ãƒžã‚¤ãƒŠãƒ¼ãƒã‚§ãƒ³ã‚¸ç‰ˆã§ã™ã€‚
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is an English-to-Japanese translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via Hugging Face pipelines.
  - Downloads: 75,316
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT is a Japanese-to-English translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via Hugging Face pipelines and evaluated on the Tatoeba dataset.
  - Downloads: 56,221
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - Shisa-gamma-7b-v1 is a Japanese language model fine-tuned from Stable LM Base Gamma 7B, sharing promising results on JA MT-Bench.
  - Downloads: 25,006
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªžã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 10,197
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T multilingual corpus with support for Chinese, English, Japanese, and Korean, and featuring demos, benchmarks, and technical documentation.
  - Downloads: 5,396
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - Ultralytics YOLOv11 is a state-of-the-art, fast and flexible model for object detection, tracking, segmentation, classification, and pose estimation, building upon previous YOLO versions.
  - Downloads: 4,902
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 4,219
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-8B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,805
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX transformer model, trained on a large corpus including Japanese CC-100, C4, and The Pile, and linked to the MiniGPT4 series on Hugging Face.
  - Downloads: 2,754
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a 2B parameter small language model delivering near-7B quality for Japanese-English/English-Japanese translation with a compact ~2GB file size for fast execution.
  - Downloads: 2,738
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - This repository provides a GGUF-formatted version of the Aya-23-8B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm dataset and runnable with llama.cpp.
  - Downloads: 2,719
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL is a 260M parameter translator model, finetuned from sbintuitions/modernbert-ja-130m and Dart v3, that converts Japanese and English into Danbooru tags, licensed under Apache-2.0 and usable with ComfyUI.
  - Downloads: 1,934
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,653
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-inclusive, 4-bit quantized version of the Gemma 3B modelâ€”compatible with the latest llama.cpp and utilizing an imatrix for enhanced Japanese language processing and image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 1,587
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - This repository provides a Japanese-to-Korean translation model leveraging BERT for Japanese encoding and KOGPT2 for Korean decoding, with a Hugging Face Spaces demo available.
  - Downloads: 1,586
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FinguAI-Chat-v1 is a multilingual AI model designed to improve English, Korean, and Japanese language skills through a curriculum focused on finance, investment, and legal frameworks within global markets.
  - Downloads: 916
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 is a family of open-weight, bilingual (Japanese/English) chat models developed by Shisa.AI, focusing on improved Japanese language performance with retained English capabilities through increased Japanese pre-training and tokenizer efficiency.
  - Downloads: 886
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - This repository provides GGUF-formatted conversions of the multilingual Suzume-Llama-3-8B model, utilizing the TFMC/imatrix dataset, alongside other related LLM models from lightblue and mmnga.
  - Downloads: 685
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 667
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained from scratch by OrionStarAI on a 2.5T corpus, with accompanying demos, benchmarks, and documentation.
  - Downloads: 663
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - This repository provides a 4-bit quantized version of the Gemma 3B model, specifically optimized for Japanese language processing using an imatrix dataset, and runnable with the latest llama.cpp including image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 596
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - This repository provides GGUF-formatted conversions of the lightblue-suzume-llama-3-8B Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm, alongside links to other related lightblue/mmnga models.
  - Downloads: 574
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - This repository provides GGUF versions of the Mistral-nemo-ja-rp-v0.2 Japanese language model, referencing the original model for details.
  - Downloads: 486
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªžã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 479
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - This repository provides a 1.3B NLLB model fine-tuned for Japanese to English light novel translation, capable of processing up to 512 tokens using the `transformers` library.
  - Downloads: 427
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - This repository provides a GGUF-formatted version of the pfnet-nekomata-14b-pfn-qfin-inst-merge language model, trained with TFMC/imatrix data and licensed under Tongyi-Qianwen, for use with llama.cpp.
  - Downloads: 404
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleæ§˜ã® google/gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 333
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT-BT-ja-en is an openly licensed Japanese-to-English translation model built and fine-tuned from scratch using only CC0, CC BY/SA licensed data and Wikipedia back-translation with the ElanMT base models.
  - Downloads: 287
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - This repository details llm-jp-clip-vit-large-patch14, a 467M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset, enabling zero-shot image classification with `open_clip_torch`.
  - Downloads: 277
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi-mini-12B-GGUF is a Japanese large language model fine-tuned for novel writing assistance, offering comparable functionality to wanabi-24B with improved accessibility and performance via high-quality data and GGUF quantization for GPUs with 8GB+ VRAM.
  - Downloads: 230
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - This model is based on CreativeML Open RAIL-M, adding â€œsazyou_roukakuâ€ as an additional author, and users agree to the original licenseâ€”including restrictions against illegal or specialized usesâ€”by downloading/using it.
  - Downloads: 222
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 215
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - QuinceMix â€œDefactaâ€ is a merged Stable Diffusion model excelling in backgrounds and effects, recommended for use with DDIM/DPM++ SDE Karras samplers, and optimized for detailed imagery with prompts including quality and lighting tags.
  - Downloads: 194
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository provides GGUF-converted versions of rinnaâ€™s japanese-gpt-neox-3.6b-instruction-ppo model, intended for use with llama.cpp, but potentially incompatible upon official GPT-Neox implementation.
  - Downloads: 156
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - This repository provides a VAE-integrated model, SakuraMixSeries, prioritizing both background and character quality, licensed under a modified CreativeML OpenRAIL-M allowing commercial use and modification with certain conditions.
  - Downloads: 146
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved with increased Japanese pre-training and enhanced performance for Japanese language tasks.
  - Downloads: 137
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin language model, trained on the TFMC/imatrix dataset and licensed under Tongyi-Qianwen, with usage examples leveraging llama.cpp.
  - Downloads: 128
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT-BT-en-ja is an English-to-Japanese translation model built from openly licensed data and Wikipedia back-translation, fine-tuned from the ElanMT-base-en-ja foundation model.
  - Downloads: 112
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 105
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 89
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 83
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 82
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the Karasu-Mixtral-8x22B-v0.1 language model and related Lightblue/mmnga models, trained with the TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 81
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source, multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and technical documentation.
  - Downloads: 77
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models developed by Shisa.AI, boasting improved Japanese language capabilities through increased pre-training and tokenizer efficiency.
  - Downloads: 74
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medLLM is a trilingual (English, Japanese, Chinese) biomedical large language model built on Llama-3-8B, trained via continued pre-training and supervised fine-tuning for enhanced domain-specific and multilingual capabilities.
  - Downloads: 70
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - ã€ŒLLM-jp-3 172Bã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172Bã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 66
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source multilingual large language model series by OrionStarAI, trained on 2.5T tokens of diverse languages including Chinese, English, and Japanese, with associated demos, benchmarks, and technical documentation.
  - Downloads: 64
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 63
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a Japanese/English bilingual chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 62
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - This repository provides a Japanese-adapted refiner model for Stable Diffusion XL 1.0, finetuning only the OpenCLIP-ViT/G or CLIP-ViT/L text encoders using Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer to enable Japanese text prompting.
  - Downloads: 59
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 59
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instructã‚’CoTãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸreasoningãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 54
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 45
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and technical documentation.
  - Downloads: 41
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - This repository details llm-jp-clip-vit-base-patch16, a 248M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset, enabling zero-shot image classification with `open_clip_torch`.
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 36
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a DPO-trained, Japanese-language conversational AI model based on Qwen2.5-7B-Instruct, demonstrating improved performance on multilingual chat benchmarks and suitable for general use.
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix æ¦‚è¦ / Overview Yaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 32
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 30
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - This repository hosts a Japanese large language model, built by continuing pre-training Llama2-13b and fine-tuning on *okashi* (pun/gag) data, expanding its vocabulary to 45,046 tokens and leveraging AWS Trainium for training.
  - Downloads: 27
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - This project provides a Japanese-to-English machine translation model, fine-tuned for Weiss Schwarz (WS) trading card game text, deployable locally or via a Gradio app on Hugging Face Spaces.
  - Downloads: 26
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - This repository provides the Japanese-Alpaca-2-13B language model in GGUF format, sourced from Hugging Face.
  - Downloads: 25
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling is a 7B multilingual language model continually pretrained on Korean, English, Chinese, Japanese, and a 500-language corpus, built upon the original Gemma model.
  - Downloads: 24
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B is a Japanese-English bilingual LLMâ€”built on LLaMA 2 and trained with the LEIA techniqueâ€”that improves cross-lingual transfer and achieves strong performance on Japanese question answering benchmarks.
  - Downloads: 24
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæžœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T multilingual corpus, offering English, Chinese, Japanese, and Korean support with demos, benchmarks, and a technical report.
  - Downloads: 23
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source, multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and documentation.
  - Downloads: 22
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for improved Japanese language performance.
  - Downloads: 22
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - This repository provides a Japanese-input-compatible version of the SDXL 1.0 base model, achieved by fine-tuning only the OpenCLIP/CLIP-ViT text encoders using Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 22
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 is a 70B language model currently underperforming compared to its predecessor, Swallow, potentially due to identified bugs related to repetition penalty and temperature settings, despite showing performance improvements with parameter adjustments.
  - Downloads: 20
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - This repository hosts a fine-tuned MPT-7B base model (Jumtra/mpt-7b-base) evaluated on a 100QA dataset, requiring `trust_remote_code=True` due to its custom architecture and featuring training efficiencies like FlashAttention.
  - Downloads: 20
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 20
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source multilingual large language model series developed by OrionStarAI, trained on a 2.5T multilingual corpus with support for Chinese, English, Japanese, and Korean, and includes demos, benchmarks, and a tech report.
  - Downloads: 19
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for improved Japanese language performance.
  - Downloads: 19
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba is a multilingual NLI & zero-shot classification model based on XLM-RoBERTa, served via TensorFlow, and trained on GLUE, CLUE, JGLUE, KLUE, and private datasets.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - This repository provides base and instruction-tuned large language models (LLMs) for Japanese, including full and LoRA versions of Japanese-LLaMA-2-13B and Japanese-Alpaca-2-13B.
  - Downloads: 18
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository provides a quantized EXL2 version of a merged Qwen-14B model, specifically designed for translating Japanese game scripts into fluent Chinese using provided character and historical context.
  - Downloads: 18
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 18
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model with an extended 8192 context length achieved through RoPE positional interpolation fine-tuning.
  - Downloads: 17
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - This repository hosts Swallow-MoE-2x13B-v0.1, a merged Mixture-of-Experts model built upon the Llama-2-based Swallow-13b-instruct-hf and Superswallow-13b-v0.2, inheriting both Llama 2 and potentially AI2 ImpACT licenses.
  - Downloads: 17
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - This repository provides a LoRA-fine-tuned Llama 3 Youko 8B model for English-to-Japanese translation, achieving a COMET score of 0.9126 and BLEU of 35.2 on the Flores-200 dataset.
  - Downloads: 17
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is a Japanese-English bilingual LLM built on LLaMA 2, enhanced with the LEIA training technique to improve cross-lingual transfer and performance on Japanese question-answering tasks.
  - Downloads: 17
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides a transformer-based Japanese-to-Malay machine translation model, trained on OPUS data with normalization and SentencePiece, requiring a language ID token for input.
  - Downloads: 17
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a translation model built with Marian-NMT and transformers, translating from German, English, Spanish, French, Italian, Russian, and Ukrainian to Japanese, utilizing sentencepiece and readily usable via a Hugging Face pipeline.
  - Downloads: 16
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 16
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, and Japanese, with demos, benchmarks, and technical documentation available.
  - Downloads: 16
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This repository provides a fine-tuned translation modelâ€”based on Helsinki-NLP/opus-mt-ja-enâ€”for converting Japanese text to English using the bsd_ja_en dataset.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - This repository provides a Japanese language model built by merging and applying Mixture of Experts (MoE) to the elyza/ELYZA-japanese-Llama-2-7b-fast and instruction-tuned versions, inheriting the Llama 2 license.
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository hosts a Mixture-of-Experts (MoE) modelâ€”ELYZA-japanese-Llama-2-MoE-2x7Bâ€”created by merging and instruction-tuning the ELYZA-japanese-Llama-2-7B and ELYZA-japanese-Llama-2-7b-instruct models, licensed under Llama 2 Community License.
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 15
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - ByT5-small-ain-jpn-mt is a pretrained ByT5-small model fine-tuned for Ainu-to-Japanese machine translation using web-crawled bilingual data.
  - Downloads: 15
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B provides both a foundational 7B language model and a LoRA-adapted version for Japanese language processing.
  - Downloads: 14
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - This repository details the karakuri-midrose-mg model.
  - Downloads: 14
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 is a family of bilingual (Japanese/English) general-purpose chat models by Shisa.AI, improved for superior Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 14
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - This repository provides a Japanese mT5-based doc2query model for improving information retrieval by generating synonymous queries to expand documents and enhance BM25-based search relevance.
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - This repository provides a Japanese language modelâ€”ELYZA-japanese-Llama-2-MoE-2x13Bâ€”created by merging and applying Mixture of Experts (MoE) to the elyza/ELYZA-japanese-Llama-2-13B and instruction-tuned versions, inheriting the Llama 2 license.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime ã¯æ—¥æœ¬èªžã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - This repository provides a T5 model fine-tuned on the friendly_JA corpus to simplify Japanese translation for English speakers by prioritizing Latin/English-derived *katakana* over Sino-Japanese vocabulary.
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - This repository provides full and LoRA models for Japanese-Alpaca-2-13B, a Japanese instruction-following model built upon the Japanese-LLaMA-2-13B base model.
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹HODACHI/Llama-3.1-70B-EZO-1.1-itã®ggufç‰ˆã§ã™ã€‚
  - Downloads: 12
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - This repository provides a compiled version of the Watashiha-Llama-2-13B-Ogiri-sft model optimized for AWS Inf2 instances using Neuron, requiring an Inf2.xlarge instance with at least 256GB storage and the Deep Learning AMI Neuron PyTorch 1.13.
  - Downloads: 11
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - This repository provides a work-in-progress Japanese-to-English translation model built on tinyllama, designed for long-context inputs (500-1000 tokens) and requiring deterministic inference settings.
  - Downloads: 11
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - This repository details the karakuri-MS-01 model.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR is a Vision Encoder Decoder-based optical character recognition system specializing in high-quality Japanese text extraction from manga, handling diverse layouts, fonts, and image qualities.
  - Downloads: 387,456
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - This repository provides a GGUF-formatted, Japanese-optimized slice of the DeepSeek-V3 model, meticulously rebuilt with frequently used Mixture of Experts layers and trained on the imatrix dataset, prioritizing Japanese language tasks over code generation.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - Rinna's Japanese CLIP model, `japanese-clip-vit-b-16`, enables contrastive image and text pre-training for Japanese language tasks via a pip-installable PyTorch package.
  - Downloads: 25,454
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet-tdt_ctc-0.6b-ja is a 600M parameter Japanese Automatic Speech Recognition (ASR) model developed by NVIDIA NeMo, utilizing a Hybrid FastConformer TDT-CTC architecture to transcribe speech with punctuation.
  - Downloads: 8,824
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - This repository provides a Japanese CLIP model, trained on a billion web-collected image-text pairs, for zero-shot image classification and multimodal retrieval tasks.
  - Downloads: 8,274
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - This repository provides a 619M parameter, subword-based RNN-T ASR modelâ€”built with a Longformer-enhanced Conformer architectureâ€”for efficient, long-form Japanese audio transcription using the ReazonSpeech v2.0 corpus.
  - Downloads: 6,291
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is a Japanese ASR model building on kotoba-tech/kotoba-whisper-v2.0, enhanced with integrated punctuation and postprocessing pipelines developed collaboratively by Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,956
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the Umievo-itr012-Gleipnir-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp.
  - Downloads: 2,518
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B is a Japanese and English vision-language model built on the NVILA-Lite architecture, utilizing Qwen2.5-1.5B-Instruct and a paligemma-siglip vision encoder, requiring specific transformer, accelerate, and opencv-python installations.
  - Downloads: 2,184
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing Qwen2.5-14B-Instruct and a paligemma-siglip vision encoder, with specified transformer dependency requirements.
  - Downloads: 1,593
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - Karamaru is a conversational AI model by Sakana AI fine-tuned on Edo-period Japanese textâ€”including both human and AI-transcribed historical documentsâ€”to generate responses in that style.
  - Downloads: 1,058
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a multilingual speech foundation model offering ASR, LID, SER, and AED capabilities, with pre-trained models available on ModelScope and Hugging Face, and online demos for testing.
  - Downloads: 996
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - NABLA-VL is a Japanese vision-language model by NABLAS that processes single/multiple images and videos to understand and generate text for multimodal tasks.
  - Downloads: 913
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - This repository provides a Japanese CLIP model for encoding text and images, enabling tasks like similarity calculation, embedding, and multimodal search, with accompanying documentation and example code.
  - Downloads: 890
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is a Japanese ASR model building upon kotoba-whisper-v1.0 with integrated postprocessingâ€”including automatic punctuationâ€”developed jointly by Asahi Ushio and Kotoba Technologies.
  - Downloads: 841
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - This repository releases a Japanese Contrastive Language-Image Pretrained (CLIP) modelâ€”`japanese-clip-vit-b-32-roberta-base`â€”for tasks like zero-shot classification and text-image retrieval.
  - Downloads: 619
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - This repository provides a GGUF formatted version of the DataPilot-ArrowPro-7B-KUJIRA language model, trained with TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp.
  - Downloads: 555
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the ELYZA-Shortcut-1.0-Qwen-7B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 490
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing a Qwen2.5-0.5B-Instruct LLM and paligemma-siglip-so400m-patch14-448 vision encoder, requiring transformers 4.45.0 or later.
  - Downloads: 460
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-performing Japanese Large Vision Language Model, built on Sarashina2-7B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 439
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - This repository provides a fine-tuned Whisper large-v3 model for Japanese speech recognition, trained on the Common Voice 16.1 dataset with 4000 steps, achieving a loss of 0.4057 and reported WER on an evaluation set.
  - Downloads: 436
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - This repository provides a GGUF-formatted version of the DataPilot-ArrowPro-7B-RobinHood language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 404
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a high-performing Japanese Large Vision Language Model built on Sarashina2-13B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 316
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - This repository provides a GGUF-formatted version of cyberagent's Mistral-Nemo-Japanese-Instruct-2408 language model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 312
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut, a base-sized model fine-tuned on a synthetic visual novel dataset, performs optical character recognition (OCR) and dialogue extraction from images, as demonstrated in the provided Colab notebook and sample outputs.
  - Downloads: 303
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B is a text-to-speech model built on sarashina2.2â€‘0.5bâ€‘instructâ€‘v0.1 and XCodec2, offering fine-grained voice control via both control and speech prompts, similar to Parler-TTS, and based on the Llama architecture.
  - Downloads: 300
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - This repository provides a GGUF-formatted version of the stockmark-100b language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 190
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - This repository provides a Japanese text-to-speech model, SpeechT5 fine-tuned on the JVS dataset with 100 speakers, utilizing 16-dimensional speaker embeddings for voice independence.
  - Downloads: 182
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing a siglip2 vision encoder and Qwen2.5-32B-Instruct LLM, with specific compatibility noted for Transformers versions 4.45.0, 4.46.0, and 4.49.0.
  - Downloads: 181
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - This repository provides a GGUF-formatted version of the QwQ-32B language model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 156
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a multilingual text-to-speech model trained on 300k hours of English, Chinese, and Japanese audio, with a demo available at Fish Audio.
  - Downloads: 151
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 æ˜Žç¤ºçš„ãªè¨±è«¾ã‚’å¾—ãŸã‚ªãƒ—ãƒˆã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªž/è‹±èªžãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«CLIP (Contrastive Language-Image Pre-training)ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 149
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - Hakuhodo Technologiesâ€™ Japanese CLIP ViT-H/14 model enables zero-shot image classification and multimodal tasks by aligning Japanese text and images in a shared embedding space, licensed under CC BY-NC-SA 4.0.
  - Downloads: 128
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, enabling unified Japanese text and image embeddings under a CC BY-NC-SA 4.0 license.
  - Downloads: 126
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - This repository provides a GGUF version of the Ocuteus model, optimized for use with Koboldcpp, and recommends lowering image resolution due to token limits, with a suggested context size of 16384.
  - Downloads: 117
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input audio.
  - Downloads: 99
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - This repository provides optical character recognition (OCR) specifically trained for Japanese text, particularly focusing on recognizing characters within Japanese manga.
  - Downloads: 98
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - This repository provides a fine-tuned Japanese hubert-base ASR model, trained on common_voice_11_0, that predicts only Hiragana with a reported WER of 0.614952.
  - Downloads: 67
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - This repository provides GGUF-formatted, K-quantized language models converted from Local-Novel-LLM-project, enhanced with iMatrix using the TFMC c4_en_ja_imatrix.txt dataset.
  - Downloads: 54
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - This repository provides GGUF-formatted and K-quantized versions of the Japanese-Chat-Umievo-itr004-7b language model, utilizing iMatrix with the c4_en_ja_imatrix.txt text for improved performance.
  - Downloads: 50
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool built on a Vision Encoder Decoder framework, specializing in accurately extracting text from manga, including vertical text, furigana, and varied fonts/image quality.
  - Downloads: 49
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input.
  - Downloads: 46
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a Japanese vision-language model built by fine-tuning llm-jp/llm-jp-1.3b-v1.0 with the LLaVA method on datasets including LLaVA-CC3M-Pretrain and LLaVA-Instruct-150K-JA for image-based conversation.
  - Downloads: 42
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - This repository provides a fine-tuned XLSR-53 model for Japanese two-speaker speech diarization, specifically trained on the CallHome phone-call dataset, with example code for direct usage.
  - Downloads: 42
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max is a 1.3B parameter Japanese vision-language model built on LLaVA architecture, utilizing a ConvNeXt Large vision encoder and trained on a custom Japanese dataset with 1280x1280 resolution and 1024 token context length, licensed under Apache 2.0.
  - Downloads: 40
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - This repository provides a fine-tuned wav2vec2-base model for Japanese Automatic Speech Recognition (ASR), specifically predicting Hiragana, trained on the common_voice_11_0 dataset and achieving a WER of 1.0 at step 1000.
  - Downloads: 39
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository provides a CTranslate2-formatted version of the vumichien/whisper-large-v2-jp speech recognition model, optimized for faster transcription with CTranslate2 and projects like faster-whisper.
  - Downloads: 37
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - This repository hosts Canary-TTS-150M, a Japanese TTS model built on llm-jp/llm-jp-3-150m-instruct3 and XCodec2, offering fine-grained voice control via prompt-based adjustments similar to Parler-TTS.
  - Downloads: 36
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - This repository provides a Japanese language Stable Diffusion model for generating PokÃ©mon images from text prompts, trained using the diffusers library and licensed under CreativeML OpenRAIL-M.
  - Downloads: 34
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 33
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository converts the whisper-large-v2-mix-jp speech recognition model to the CTranslate2 format for faster, optimized inference with tools like faster-whisper.
  - Downloads: 31
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - This repository provides GGUF quantized versions of the ArrowPro-7B-KUJIRA language model, including K-quantized models enhanced with iMatrix using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 30
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR is a fine-tuned Whisper-large-v3 model for accurate Japanese speech recognition, including non-speech sound detection and improved punctuation, requiring specific post-processing for optimal performance.
  - Downloads: 28
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B parameter, end-to-end Transformer model for fluent Japanese text-to-speech generation and one-shot voice cloning, built upon the metavoice framework.
  - Downloads: 27
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - ReazonSpeech-ESPnet-Next provides cutting-edge Japanese Automatic Speech Recognition (ASR) models and datasets, openly maintained by the ReazonSpeech team with community feedback integration.
  - Downloads: 26
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - Rinnaâ€™s Japanese data2vec Audio Base model is a 12-layer transformer trained on ~19,000 hours of Japanese audio, mirroring the original data2vec architecture and training methodology.
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªžã«å¯¾å¿œã—ã¦ã„ã‚‹Llama-3ãƒ™ãƒ¼ã‚¹ã®ï¼”ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒžãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 25
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR is a Japanese text recognition tool using a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga â€“ handling vertical text, furigana, varied fonts, and low-quality images.
  - Downloads: 23
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - This repository provides a fine-tuned Whisper Large V3 model for accurate Japanese speech transcription into Katakana with pitch accent annotation, trained on Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 23
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M speech recognition model for Japanese Hiragana, achieving 9.34% CER on Common Voice Japanese data with 16kHz audio input and continuous (non-word-separated) sentence output.
  - Downloads: 20
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on ~60 hours of combined Japanese speech datasets (Common Voice, JUST, JSSS, CSS10) for speech recognition research, achieving benchmark WER results on Common Voice.
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO is a Japanese-specific Stable Diffusion model fine-tuned for photorealistic text-to-image generation using the ðŸ¤— Diffusers library.
  - Downloads: 19
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Hakuhodo Technologiesâ€™ Japanese CLIP ViT-H/14 model enables zero-shot image classification and other multimodal tasks by aligning Japanese text and images in a shared embedding space, released under CC BY-NC-SA 4.0.
  - Downloads: 18
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 â™»
  - Downloads: 17
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - This repository provides a Japanese text-to-speech (TTS) model, â€œAmitaro,â€ finetuned from Plachtaaâ€™s VITS using free voice data, offering 600-epoch training and sample usage via Lycoris53.
  - Downloads: 16
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - This repository showcases a demo of Heron BLIP Japanese StableLM Base 7B, a vision-language model built with the Heron library that enables conversational interaction about images.
  - Downloads: 16
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - This repository provides a pre-trained ESPnet model for Japanese automatic speech recognition, trained on 15,000 hours of ReazonSpeech data and requiring 16kHz audio input.
  - Downloads: 16
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 â™»
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 â™»
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa æ¦‚è¦ tokyotech-llm/Swallow-7b-hfã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ä»¥ä¸‹ã®4ãƒ¢ãƒ‡ãƒ«ã‚’gate_mode=randomã§MoEã—ã€ãã®å¾ŒLISAã¨ã„ã†æ‰‹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned, real-time Japanese speech recognition model based on OpenAI's Whisper-tiny, trained on the Common Voice dataset with a WER of 225.23 and loss of 0.5491.
  - Downloads: 14
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - This repository provides fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition models for specific languages, trained on datasets like Common Voice, requiring 16kHz sampled input.
  - Downloads: 14
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This repository provides a Style Bert VITS2 voice clone capable of text-to-speech generation in English, Japanese, and Chinese, featuring a young, neutral voice suitable for diverse applications like virtual characters.
  - Downloads: 14
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - This repository provides a Japanese VITS-TTS voice model finetuned on Sakura Mikoâ€™s voice data, offering a freely usable model for non-commercial purposes adhering to Cover Corporationâ€™s secondary creation guidelines.
  - Downloads: 14
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for Japanese audio transcription directly into Hiragana, achieving a Character Error Rate (CER) of 0.2227 on a Common Voice dataset after converting all text to Hiragana with pykakasi and tokenizing with fugashi.
  - Downloads: 13
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - This repository provides a free, commercially-usable voice generation modelâ€”a â€œcoolâ€ version of RikkaBotanâ€”specializing in gentle, childish voices ideal for reading text, with variants for emotionality, English, ASMR, and Chinese speech.
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool leveraging a Vision Encoder Decoder framework, specifically designed for high-quality OCR of mangaâ€”handling vertical/horizontal text, furigana, varied fonts, and low-quality images.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave â™»
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - â– endlessMixã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦ æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Defactaã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸéšŽå±¤ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned WAV2Vec2-xls-r-300m model for Japanese speech recognition, trained on the Common Voice 8.0 dataset with Kanji-to-Hiragana conversion and evaluated using Character Error Rate (CER).
  - Downloads: 12
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository provides a free, commercially usable ASMR voice modelâ€”a childlike and gentle version of RikkaBotanâ€”with variations for emotional, English, Chinese, and logical speech styles.
  - Downloads: 11
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the Heron library, capable of image-based conversation and utilizing LlamaTokenizer.
  - Downloads: 11
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - This repository provides a Japanese VL-T5 model, pretrained on a Japanese corpus for unifying vision-and-language tasks via text generation, with inference examples available.
  - Downloads: 11
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a Japanese-specific, fine-tuned version of OpenAI's Whisper-tiny model, trained on the Common Voice dataset, for real-time Japanese speech recognition with a reported WER of 301.625840.
  - Downloads: 11
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA is a fine-tuned speech recognition model based on the SVJ Japanese dataset and Common Voice 11.0, achieving a 17.7261 Character Error Rate on the evaluation set.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave â™»
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTæ§˜ã® AXCXEPT/EZO-gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - This repository provides a Japanese T5 model pretrained on a 100GB corpus of Japanese textâ€”including Wikipedia, OSCAR, and CC-100â€”requiring fine-tuning for specific tasks and mindful use due to potential biases in generated text.
  - Downloads: 9,091
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.3-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 4,835
- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and post-trained models alongside a pairwise evaluation model.
  - Downloads: 3,639
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - This repository hosts the v2 update of the chilled_remix and reversemix models, licensed under CreativeML Open RAIL-M with added authorship by sazyou_roukaku, and clarifies no liability for generated content beyond license restrictions.
  - Downloads: 2,578
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository provides a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS-Mini, utilizing a custom tokenizer incompatible with the original Parler-TTS.
  - Downloads: 2,351
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitionsâ€™ repository hosts sarashina2.2-0.5b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English tasks alongside other models like Qwen and RakutenAI.
  - Downloads: 2,280
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - This repository provides a Stable Diffusion model (BraV6, XXMix_9, Soda Mix) licensed under CreativeML Open RAIL-M, prohibiting its use for generating violent, sexually explicit, or exploitative content, especially involving minors, and restricts public sharing of images resembling real individuals without consent.
  - Downloads: 1,813
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) v1.1 model pretrained on a 100GB corpus of Japanese text data, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 1,149
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-7b-Japanese-v0.1 large language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 1,123
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-ELYZA-JP-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-35Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 993
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 848
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides a GGUF-formatted version of the qwen2.5-bakeneko-32b-instruct-v2 language model, trained with the imatrix dataset and intended for use with llama.cpp.
  - Downloads: 841
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-2-2b-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 788
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/gemma-3-4b-it-RP-v0.1 Italian language model, inheriting the Gemma Terms of Use and Prohibited Use Policy.
  - Downloads: 782
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠåˆ†ã»ã©ãŒæ—¥æœ¬èªžãªã®ã§magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªžã«ã¯å¼·ã„ã¯ãšï¼Ÿ
  - Downloads: 727
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - This repository provides a fine-tuned mt5-small model for Japanese summarization, specifically trained on BBC news articles to generate summaries from provided news stories.
  - Downloads: 722
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen3-30B-A3B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is designed for use with llama.cpp.
  - Downloads: 678
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - This repository provides a quantized, GGUF version of Qwen/Qwen2.5-3B-Instruct, optimized with a Japanese-focused importance matrix (iMatrix) to enable accurate summarization of extremely long texts exceeding 32K tokens.
  - Downloads: 655
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - This repository provides a GGUF-formatted version of the Cogito-v1-preview-Qwen-32B language model, trained with imatrix data and compatible with llama.cpp for local inference.
  - Downloads: 571
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-70b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 546
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - This repository provides a GGUF-formatted version of Microsoftâ€™s Phi-3-medium-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 545
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - This repository details a Japanese question generation model fine-tuned on a cleaned, translated SQuAD 1.1 dataset, using Japanese T5 to generate questions from given answers and context.
  - Downloads: 533
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual provides fast, distilled Whisper models for Japanese & English speech recognition and translation, built on OpenAI's large-v3 and developed by Asahi Ushio & Kotoba Technologies.
  - Downloads: 527
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹datagemma-rag-27b-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 424
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a Japanese/English instruction-tuning dataset (Sarashina2.2) built from fineweb-edu and fineweb-2, aiming for higher accuracy than existing imatrix datasets, and is designed for use with Ollama (specifically, the `hf.co/neody/sarashina2.2-3b-instruct-v0.1-gguf:Q5_K_M` model).
  - Downloads: 404
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaæ§˜ã® rinna/gemma-2-baku-2b-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 296
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) model, fine-tuned from rinna/japanese-hubert-large on reazonspeech and common_voice datasets, specifically for Hiragana prediction, inspired by vumichienâ€™s training approach.
  - Downloads: 245
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - This repository provides a GGUF-formatted conversion of the QwQ-32B-Preview language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 244
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository provides GGUF quantized versions of a VNTL LLaMA 3 8B QLoRA merge, featuring a new chat mode optimized for Japanese grammar and including translation prompt examples.
  - Downloads: 241
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-mini-instruct model, enabling its use with tools like llama.cpp and text-generation-webui.
  - Downloads: 202
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Qwen2.5-72B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 188
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality speech generation with a custom, incompatible tokenizer and is currently in beta.
  - Downloads: 182
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 170
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the sarashina2.2-3b-instruct-v0.1 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 133
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Humanities-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 133
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-32b-Japanese-v0.1 large language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 132
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªžåŒ»ç™‚LLM MedLlama3-JP ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama3ã®ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸï¼”ç¨®é¡žã®LLMã‹ã‚‰æˆã‚‹ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 122
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - This repository provides a T5 model pretrained on a balanced 500GB English-Japanese corpus, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 85
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and evaluation models for enhanced translation performance.
  - Downloads: 75
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Vecteus-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 68
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository provides GGUF quantized versions of the VNTL Gemma 2 27B model, featuring a new chat mode optimized for Japanese grammar and including translation prompt examples.
  - Downloads: 59
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Kage-v0.1-2x7B is a merged 2x7B Japanese text generation model, enhanced with Ninja-v1 and employing a Vicuna prompt format for improved performance.
  - Downloads: 56
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides statically quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model in GGUF format, offering various quantization levels for different size/quality trade-offs.
  - Downloads: 56
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/c4ai-command-r-v01-japanese-instruct model for efficient inference, with licensing details available in the original model repository.
  - Downloads: 55
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - This repository provides a model for automatically generating titles from article content, as detailed in the linked Qiita article.
  - Downloads: 54
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - This repository provides a Japanese ByT5 modelâ€”a tokenizer-free Text-to-Text Transfer Transformerâ€”pretrained on a 100GB corpus including Wikipedia, OSCAR, and CC-100, requiring fine-tuning for specific tasks and acknowledging potential biases in generated text.
  - Downloads: 54
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - This repository provides a HubERT-based Japanese Automatic Speech Recognition (ASR) model, fine-tuned on the uniTKU dataset to predict Hiragana with reported WER scores as low as 0.337.
  - Downloads: 50
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and post-trained models alongside a pairwise evaluation model.
  - Downloads: 50
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - This repository provides a GGUF-formatted version of the Sarashina 2.1-1B-SFT Japanese language model, trained with TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 47
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 is a Japanese T5 model fine-tuned on the ATOMIC dataset for text-to-text generation, offering a pipeline for predicting subsequent events.
  - Downloads: 41
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime ã¯æ—¥æœ¬èªžã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 40
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - This repository details EZO2.5-gemma-3-12b-it, a Japanese language model improving upon the base Gemma modelâ€™s performance on MT Bench and Elyza Tasks100 through a novel, low-cost training method combining GRPO/PPO concepts with the proprietary â€œEZOâ€ technique.
  - Downloads: 34
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7Bã‚’ä¼šè©±ã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 30
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - This repository details a fine-tuned, Japanese audio transcription model (based on distil-whisper/distil-large-v2) specifically optimized for visual novel audio, part of a unified WaifuAssistant demo.
  - Downloads: 28
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - This repository details a modified CreativeML OpenRAIL-M license allowing commercial use, sale, and merging of the model and its generated images, even with modified permissions for derivative works.
  - Downloads: 27
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints ã‚’ optimum ç”¨ã« ONNX ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - Saikyou Shield 30M is a lightweight (30M parameter) Japanese text classification model designed to identify *all* prompts as dangerous, including jailbreaks and prompt injections, for ultimate safetyâ€”released as an April Fool's joke.
  - Downloads: 20
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - This repository provides a finetuned GPT-2 XL model (COMET-GPT2) for text generation, trained on the ATOMIC dataset using causal language modeling, and includes usage examples with the `transformers` pipeline.
  - Downloads: 18
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a 6 billion parameter Japanese language model finetuned from EleutherAIâ€™s GPT-J 6B specifically for generating Japanese web novels, utilizing RoPE embeddings and a 50,400 vocabulary.
  - Downloads: 17
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on machine-translated feedback datasets, based on the STF Japanese Stable LM Instruct Gamma 7B.
  - Downloads: 17
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - This repository provides a fully instruction-tuned 1.7 billion parameter Japanese language model, built upon the line-corporation/japanese-large-lm-1.7b base model.
  - Downloads: 17
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository provides a 32B Japanese language modelâ€”merged from pre-trained models using mergekitâ€”optimized for code generation, exemplified by a FizzBuzz program, and configured with specific temperature and token parameters.
  - Downloads: 17
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - This repository hosts an alpha version of a Japanese-language assistant AI, fine-tuned from calm2-7b-chat, designed to continue provided text based on approximately 150M novel tokens, and usable with TextGen-WebUI.
  - Downloads: 16
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - Tokara-0.5B-v0.1 is a Japanese instruction-tuned language modelâ€”based on Qwen1.5-0.5B and further trained on 5B Japanese/English tokensâ€”capable of multi-turn conversations, though repetition may require penalty adjustments.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on a machine-translated Ultrafeedback dataset, based on the STF Japanese Stable LM Instruct Gamma 7B.
  - Downloads: 15
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - Gendec is a machine learning framework for gender detection from Japanese names (provided in ROMAJI) detailed in the ISDA'23 paper and implemented in this repository.
  - Downloads: 15
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository hosts a fine-tuned version of MosaicML's MPT-7B-instruct, evaluated on the Jumtra/test_data_100QA dataset and requiring `trust_remote_code=True` due to its custom MPT architecture with features like FlashAttention.
  - Downloads: 14
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri is a Japanese, vision-language model fine-tuned with LLaVA on STAIR Captions and Japanese Visual Genome VQA data for image-based witty responses, utilizing a CLIP-ViT-B-32 vision encoder under the LLAMA 2 Community License.
  - Downloads: 14
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 is a Japanese language model based on DeepSeek-V3, reconfigured with a refined 64-expert Mixture of Experts (MoE) architecture per layer for improved stability and performance on frequent Japanese examples.
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - This repository details a modified CreativeML OpenRAIL-M license permitting commercial use, royalty-free merging, and sales of the model and its derivatives, even without attribution.
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2ã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 v2 is a finetuned large-version GPT-2 model, built on the ATOMIC dataset using causal language modeling, and readily usable for reproducible text generation via the `transformers` pipeline.
  - Downloads: 12
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - This repository details the karakuri-midroze-CV model.
  - Downloads: 12
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This repository provides a QLoRA-finetuned LLaMA2-7B model, trained on the Guanaco dataset with 49,000 chat samples, exhibiting improved performance in Chinese and Japanese, and testable via the included `test.py` script.
  - Downloads: 11
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - This repository provides a Japanese-language Gemma model fine-tuned with a female/â€œdaughter-likeâ€ tone, intended for experimentation but **not** recommended for merging, using specific hyperparameters for generation.
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Natural Language Interfaces
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumerã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Reflection-Llama-3.1-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,323
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-ArrowSE-8B-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 700
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling natural, real-time overlapping speech and turn-takingâ€”this repository provides the trained models and interaction methods.
  - Downloads: 646
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotæ§˜ã® Llama3-ArrowSE-8B-v0.3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 449
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - This repository provides static quantization of the DeepSeek-R1-Distill-Qwen-7B-Japanese model in GGUF format, with imatrix quants potentially available upon request and usage guidance referencing TheBloke's resources.
  - Downloads: 375
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers statically quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese language model in GGUF format, with community requests welcome for imatrix quants not currently provided.
  - Downloads: 309
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmæ§˜ã® Llama-3-Swallow-8B-Instruct-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the DDQA dataset, achieving an exact match accuracy of 0.845933.
  - Downloads: 103
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - This repository demonstrates solving simple arithmetic problems using GRPO learning, featuring a specific prompt format with `<think>`/`<answer>` blocks for reasoning and solutions, and dynamically generated training data.
  - Downloads: 102
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - This repository provides a question-answering model fine-tuned from luke-japanese-large-lite using the DDQA dataset, achieving 86.3% exact match accuracy.
  - Downloads: 95
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for Question-Answering tasks using the DDQA dataset, compatible with transformers, PyTorch, and SentencePiece.
  - Downloads: 92
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B ã®GGUFé‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 84
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - This repository provides GGUF-formatted conversions of the ArrowPro-7B-RobinHood model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt text dataset.
  - Downloads: 55
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with potential for unnatural responses and limited instruction-following.
  - Downloads: 31
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 is a merged language model optimized for helpful, harmless responses, enhanced role-playing (particularly as a Japanese speaker), and improved multi-turn conversation performance.
  - Downloads: 30
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - This repository provides a small Japanese DialoGPT model trained on dialogue extracted from Aozora Bunko, a collection of public domain Japanese literature.
  - Downloads: 27
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - Karasu-LoRA-JP-QA-Chat is a LoRA-tuned Japanese Q&A modelâ€”based on a merged Karasu variantâ€”optimized for RAG systems using a native Japanese question-answer dataset.
  - Downloads: 27
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - This repository hosts Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, a Japanese language model extending Mixtral-8x7B to 32K context length with improved instruction following by merging differences from both Instruct and base models.
  - Downloads: 18
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - This repository hosts a Japanese instruction-tuned language model (c4ai-command-r-v01) further refined with ichikara-instruction, trained using LoRA on Runpod with an A6000 GPU and evaluated on datasets like jsquad and jcommonsenseqa.
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - Animagineç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ãŸVAEå†…è”µãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the JSQuAD dataset, achieving an F1 score of 0.876 and exact match of 0.758.
  - Downloads: 16
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 is a question-answering model built on japanese-stablelm-instruct-gamma-7b, designed to help users learn Japanese in English using a specific prompt format and requiring Transformers 4.34.0+.
  - Downloads: 16
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - Lightblue's QLoRA finetune specializes in Japanese closed-question answering, trained on a combined dataset of SNOW TyDiQA (Ja), and XLSUM (Ja) using OpenOrca's 13B model.
  - Downloads: 16
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - This repository details a permissively licensed (MIT) Japanese causal language model, finetuned from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese for conversational use, but trained on a limited dataset potentially impacting performance.
  - Downloads: 16
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This repository provides a Japanese chatbot model finetuned on the Yuyuyui scenario corpus, utilizing rinna/japanese-gpt2-medium to generate responses based on character-prefixed utterance sequences.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository provides a Japanese DeBERTa-v2-tiny model fine-tuned on the DDQA dataset for Question-Answering tasks, compatible with transformers and PyTorch.
  - Downloads: 13
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - This repository provides a Qwen2.5-7B-Instruct fine-tuned language model designed to generate chain-of-thought reasoning from given questions and answers, formatted with specific XML-like tags for input and output.
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯tokyotech-llm/Swallow-MS-7b-instruct-v0.1ã®tokenizer.chat_templateã‚’ä»¥ä¸‹ã«å¤‰æ›´ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - This repository hosts a Japanese instruction-tuned model, tiny_mixtral_ja, trained on a dataset and available on Hugging Face.
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ä¸Šè¨˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚¢ãƒ€ãƒ«ãƒˆç”¨èªžã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct ðŸš¨ This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model, fine-tuned from `cl-tohoku/bert-base-japanese-v3` using the `llm-book/ner-wikipedia-dataset`, as detailed in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 81,562
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - This repository provides a Japanese Named Entity Recognition (NER) model based on BERT, capable of extracting eight entity typesâ€”including person, organization, location, and product namesâ€”from text.
  - Downloads: 2,171
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository offers public access to files and content upon acceptance of specified conditions.
  - Downloads: 649
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from luke-japanese-base using a Wikipedia dataset, achieving a reported F1-score of 0.77 for organizational names.
  - Downloads: 424
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This repository provides a pre-trained Japanese medical named entity recognition model, prediction script, and normalization method outputting XML-formatted tags based on the MedTxt-CR-JA dataset.
  - Downloads: 275
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This repository provides a Japanese medical named entity recognition model, runnable via `predict.py`, for identifying disease and medication entities within text.
  - Downloads: 257
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - Kurumi_flux_lora_v1.0 is a non-commercial LoRA model based on flux.1-dev, optimized for realistic, beautiful girl depictions, with usage restrictions including prohibited violent content and retraining, requiring model name attribution when sharing.
  - Downloads: 218
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune, using an expanded VNTL dataset, to enhance English translation of Japanese visual novels with improved accuracy and stability.
  - Downloads: 217
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - This repository provides a fine-tuned Japanese BERT model (tohoku-nlp/bert-base-japanese-v3) for Named Entity Recognition (NER) using a Wikipedia-derived dataset from Stockmark Inc.
  - Downloads: 155
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - This repository provides weighted/imatrix quantized versions of the Japanese-Starling-ChatV-7B model in GGUF format, offering various quantization levels (like i1-IQ1_S at 1.7GB) for optimized performance.
  - Downloads: 154
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - This repository details a binary classification model (ID 59362) trained with AutoNLP for Japanese sentiment analysis, achieving 95.27% accuracy, and accessible via a Hugging Face API endpoint.
  - Downloads: 153
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - This repository provides a fine-tuned BERT-large-Japanese-v2 model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.862.
  - Downloads: 151
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tuneâ€”trained on an expanded VNTL datasetâ€”to enhance English translation of Japanese visual novels, offering improved accuracy and stability over previous versions without chat mode.
  - Downloads: 121
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository provides fastText classifiersâ€”trained on Wikipedia and LLM dataâ€”to assess the educational value of Japanese web pages under a CC BY-SA 4.0 license.
  - Downloads: 108
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - This repository provides a Japanese language model, fine-tuned from `studio-ousia/luke-japanese-large-lite`, that scores short texts for sexual content on a 0-1 scale to aid in content moderation.
  - Downloads: 59
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - `ja_core_news_lg` is a CPU-optimized spaCy 3.7+ pipeline for Japanese natural language processing, featuring tok2vec, morphological analysis, parsing, sentence segmentation, named entity recognition, and attribute rules, trained on UD Japanese GSD v2.8 data with 300-dimensional vectors.
  - Downloads: 57
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model built by fine-tuning a BERT-base model with a CRF layer on the NER-wikipedia-dataset, as detailed in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 55
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - This repository provides a Japanese Language Proficiency Test (JLPT) level classifier finetuned from cl-tohoku-bert-japanese-v3, achieving high precision, recall, and F1-scores (up to 0.95) on a ~5000 sentence dataset.
  - Downloads: 36
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - This repository provides a Fully Convolutional Neural Network (FCN) model trained for classifying images of 49 Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - WRIME is a dataset of 20K weakly-supervised region identification annotations for improving object detection and segmentation models.
  - Downloads: 21
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for Japanese Named Entity Recognition (NER) using a Wikipedia-based dataset.
  - Downloads: 19
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - This repository provides a model for automatically generating article titles from text content, as detailed in the linked Qiita article.
  - Downloads: 18
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This repository provides a named entity recognition model and script (`predict.py`) for Japanese medical documents, identifying entities like diseases, treatments, and temporal expressions.
  - Downloads: 17
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model trained on the 49000 chat & 280000 non-chat Guanaco dataset, exhibiting improved Chinese and Japanese performance and testable via the provided script.
  - Downloads: 16
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository provides a DeBERTa-v2-large-japanese model fine-tuned for Japanese Named Entity Recognition (NER) using a Wikipedia dataset, requiring transformers, PyTorch, SentencePiece, and Juman++.
  - Downloads: 16
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - This repository details a binary classification model (ID: 59363) trained with AutoNLP for Japanese sentiment analysis, achieving 97% accuracy, precision, recall, and F1-score, and accessible via a Hugging Face API.
  - Downloads: 14
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - This repository provides a fine-tuned luke-japanese-large model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.845.
  - Downloads: 13
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This repository provides a language model trained on 2022 Japanese parliamentary proceedings, built as a multi-GPU/node training example for the #ABCILLM hackathon, and uses data from the National Diet Library API.
  - Downloads: 13
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This So-vits-svc 4.0 model creates natural-sounding, approachable female vocals â€“ synthesized from the authorâ€™s voice and expanded with ElevenLabs â€“ and includes necessary checkpoints & notebooks for inference/training, acknowledging potential pronunciation quirks and recommending peaceful use.
  - Downloads: 13
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This repository provides a baseline transformer model trained and evaluated on the awesome-japanese-nlp-classification-dataset, achieving 97% accuracy and offering simple classification usage with the `transformers` library.
  - Downloads: 12
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - `ja_core_news_trf` is a spaCy 3.7.x Japanese language model leveraging a `cl-tohoku/bert-base-japanese-char-v2` transformer for NLP tasks including tokenization, parsing, and named entity recognition.
  - Downloads: 11
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - `ja_core_news_md` is a CPU-optimized spaCy 3.7+ model for Japanese NLP, featuring tok2vec, morphological analysis, parsing, sentence detection, named entity recognition, and attribute rules, trained on UD Japanese GSD v2.8 data with 200-dimensional vectors.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Aratako/Japanese-Novel-Reward-TinySwallow-1.5B is a fine-tuned reward model based on SakanaAI/TinySwallow-1.5B, designed to predict user evaluation scores for Japanese novel text, enabling quality assessment for reinforcement learning of text generation models.
  - Downloads: 11
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - This repository provides a 4-bit quantized Llama-2-70b-chat model fine-tuned on the Japanese instruction dataset â€œizumi-lab/llm-japanese-datasetâ€ for improved performance in Japanese language tasks.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ About this model.
  - Downloads: 816
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - This repository requires agreement to a license and privacy policy from Stability AI before access.
  - Downloads: 355
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository provides a private demonstration project.
  - Downloads: 304
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 198
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - This repository details a fine-tuned Twitter/twhin-bert-large model for classifying Japanese social media comments as non-offensive, gray-area, or offensive, achieving a macro-averaged F1-score of 64.8% and 66.1% accuracy.
  - Downloads: 109
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - This repository details a fine-tuned language model (based on studio-ousia/luke-japanese-large-lite) for Japanese social media comment offensiveness detection, achieving a macro F1-score of 64.0% and 65.0% accuracy on a manually labeled dataset.
  - Downloads: 106
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Twitter/twhin-bert-base model for classifying Japanese social media comments based on offensiveness (Not Offensive, Gray-area, Offensive) achieving a macro-averaged F1-score of 64.7% and accuracy of 65.6%.
  - Downloads: 106
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 81
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 70
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - This repository provides a Japanese language ELECTRA Small model finetuned for cyberbullying detection using data from online forums and Twitter, built upon a YACIS-pretrained foundation.
  - Downloads: 64
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model with a CreativeML OpenRAIL-M license, granting usage rights while prohibiting illegal/harmful outputs and establishing user accountability for generated content.
  - Downloads: 59
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - This commercially-usable Stable Diffusion model, licensed under CreativeML Open RAIL++-M, generates images with hires support but prohibits depictions of violence, explicit content, minors, or likenesses of real people without consentâ€”please use #tsubaki_mix when sharing.
  - Downloads: 46
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 30
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged modelâ€”similar to HimawariMixâ€”focused on strong backgrounds and detail, tuned with ideas from â€œRiga,â€ and includes a standard VAE, but prohibits commercial use, resale, illegal outputs, and modified permissions upon sharing.
  - Downloads: 22
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA model finetuned for cyberbullying detection using a combined dataset of online comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 15
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - This repository provides instruction-tuned language models for Japanese (llm-jp).
  - Downloads: 14
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA Base model finetuned for cyberbullying detection using a combined dataset of BBS and Twitter comments, licensed under CC BY-SA 4.0.
  - Downloads: 12
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Reasoning
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-QwQ32b-Reasoning-Japanese-v1.0 large language model, utilizing the TFMC/imatrix dataset and compatible with llama.cpp for inference.
  - Downloads: 1,112
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹mathstral-7B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,067
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 is a Japanese reasoning model built upon ABEJA-Qwen2.5-32b-Japanese-v0.1, enhanced with Qwen/QwQ-32Bâ€™s chat vectors and further trained to generate responses prefaced by a `<think>`-delimited thought process.
  - Downloads: 604
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B is a Japanese reasoning model built upon Qwen/Qwen2.5-32B-Instruct and enhanced with imitation learning using Monte Carlo Tree Search-generated Chains of Thought data.
  - Downloads: 564
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - This repository provides a GGUF converted version of Microsoftâ€™s Phi-4-reasoning-plus model, utilizing the TFMC/imatrix-dataset-for-japanese-llm for data, and is intended for use with llama.cpp.
  - Downloads: 529
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - This repository provides a Japanese-specific fine-tune of Deepseek-R1, addressing inconsistencies in language output and improving performance for Japanese prompts.
  - Downloads: 395
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - This repository provides a GGUF formatted version of YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1, a Japanese language model for mathematical tasks, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 255
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is a transparently-developed, merged Stable Diffusion-based model focused on generating anime-style illustrations of solo, cute girls, with a strong emphasis on lineart and moderate NSFW/tag support, built from fine-tuned models and LoRA distilled from nijijourney and user-generated art.
  - Downloads: 229
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository provides a fine-tuned version of the luke-japanese-large language model, achieving 83.82% accuracy on the JCommonsenseQA dataset for Japanese multiple-choice question answering.
  - Downloads: 99
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference model, trained on JGLUE-JNLI and JSICK datasets, that classifies sentence pair relationships as contradiction, entailment, or neutral using SentenceTransformers.
  - Downloads: 93
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - This repository provides a luke-japanese-base model fine-tuned on the JGLUE JNLI dataset for natural language inference, achieving 89.77% accuracy in determining relationships (entailment, neutral, contradiction) between sentences.
  - Downloads: 56
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-Japanese model for CommonsenseQA tasks, trained on the JGLUE/JCommonsenseQA dataset.
  - Downloads: 20
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - This model merges reasoning capabilities extracted from DeepSeek-R1-Distill-Llama-8B into the Japanese Llama-3.1-Swallow-8B-v0.2, enhancing its reasoning performance.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman for morphological analysis.
  - Downloads: 16
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 15
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a Japanese language model finetuned from COMET on TimeATOMIC data using causal language modeling, detailed in a LREC-COLING2024 paper, and preprocessed with Juman++ and SentencePiece.
  - Downloads: 14
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - This repository hosts Polyglot-math-4x7b-24b, a multilingual Mixture of Experts modelâ€”merging Chinese, Japanese, and English capabilitiesâ€”fine-tuned on GSM8k with a 20GB VRAM footprint, and provides inference code.
  - Downloads: 14
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - This repository provides a highly accurate (80.07%) Japanese language model fine-tuned from luke-japanese-base on the JGLUE JCommonsenseQA dataset for multiple-choice question answering.
  - Downloads: 11
### Sentiment Analysis
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model, trained on the chABSA dataset, achieving 1.0 accuracy and F1 score.
  - Downloads: 4,898
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This repository provides a Japanese emotion analysis model, fine-tuned from Luke-japanese-large-lite using the wrime dataset, to detect eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, and trustâ€”within text.
  - Downloads: 1,726
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - This repository provides a Japanese BERT Base model finetuned for both sentiment analysis and automatic irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 994
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model finetuned from scratch using the Japanese Sentiment Polarity Dictionary dataset and based on the jarvisx17/japanese-sentiment-analysis pretrained model.
  - Downloads: 635
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - This repository provides a fine-tuned Japanese BERT model for sentiment analysis of Twitter data, trained on the JTS1k dataset with negative, neutral, and positive classifications.
  - Downloads: 395
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This repository provides a Japanese BERT-based model fine-tuned for emotion detection and classification across 10 categoriesâ€”amaze, anger, dislike, excite, fear, joy, like, relief, sad, and shameâ€”using a 1,000-sentence blog post dataset.
  - Downloads: 286
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - This repository provides a GGUF-formatted version of the umyuki Japanese-Chat-Umievo-itr001-7b language model, created using the TFMC/imatrix-dataset, and runnable with llama.cpp.
  - Downloads: 257
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - This repository provides a Japanese BERT Base model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 67
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - This repository provides a Japanese BERT-based model, trained on financial data, for sentiment analysis of financial news, classifying text as positive, negative, or neutral.
  - Downloads: 36
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - This repository provides a Japanese ELECTRA-based model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 23
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - This repository provides a fine-tuned language model (calm-2-7b-chat) using the Tsukuyomi corpus for conversational AI, subject to the specified Tsukuyomi character and AI development plan licenses.
  - Downloads: 22
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - This repository provides a sentiment analysis model trained on Japanese stock comments to classify opinions as bullish or bearish, aiding investors and analysts.
  - Downloads: 22
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository provides an ELECTRA-based Japanese language model finetuned for irony detection using sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 16
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - This repository provides a Japanese BERT model pretrained on a Twitter corpus, optimized for social media tasks like sentiment analysis and defamation detection, and used as a base for further finetuned models.
  - Downloads: 16
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese is a finetuned language model for detecting irony and sarcasm in Japanese tweets, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - This repository provides a free, commercially usable voice generation model specializing in cute, gentle speech, with variations available for different tones, languages, and speaking styles (cool, English, ASMR, Chinese).
  - Downloads: 13
### Responsible NLP
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªžç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªžãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a Japanese speech recognition model fine-tuned on a large anime voice acting dataset, excelling in that domain but also offering unique performance on other audio, and should be used *without* an initial prompt to avoid hallucinations.
  - Downloads: 2,343
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - This Stable Diffusion model focuses on generating low-ratio, young female characters with emphasized eye highlights, requiring careful age adjustments and potentially limited compatibility with LoRAs, and benefits from short prompts and DPM++ 2M Karras sampling.
  - Downloads: 506
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - This repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime models (with specific ratios noted in filenames) for use in Colab WebUI, addressing potential oversaturation when merging with realistic models and providing download instructions.
  - Downloads: 333
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - Suzume_mix_v1.0 is a non-commercial, merged Stable Diffusion model based on flux1-dev, designed to soften facial features and intended for personal use with proper attribution, while prohibiting retraining with generated images.
  - Downloads: 149
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 41
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - This repository hosts Llama-3-Umievo-Shizuko-sqlcoder-2x8B, a Mixture of Experts (MoE) language model created with MergeKit, combining Japanese language skills with SQL generation capabilities via fine-tuning on SQL datasets.
  - Downloads: 24
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - Sarashina 2.2-3B, a Japanese language model fine-tuned for instruction following, is built upon the sbintuitions base model and trained with the imatrix dataset.
  - Downloads: 23
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 merges DreamShaper 3.3 with WaifuDiffusion/Stable Diffusion VAEs to improve color vibrancy and expressive range, creating highly detailed, realistic imagesâ€”though it may contain elements of NAI and Insta-style models.
  - Downloads: 17
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - ã‚·ã‚µãƒ èªžã«ã‚ˆã‚‹èª¬æ˜Ž ã‚¢ã‚¤ãƒŒèªžã¨æ—¥æœ¬èªžã®åŒæ–¹å‘æ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - This repository details a merged Stable Diffusion model (MoeDiffusion + HassanBlend + VMix03) optimized for generating black-haired ponytail hairstyles, potentially exhibiting some instruction-following issues due to its base models, and is primarily SFW-focused.
  - Downloads: 15
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªžç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªžãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 13
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - YaguruMagiku 0.6, a merged Stable Diffusion model based on AbyssOrangeMix2, aims for consistent black-haired ponytail depictions while potentially exhibiting some NAI leak characteristics and benefiting from simultaneous 4-image generation and a good VAE.
  - Downloads: 12
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜Ž(English explanation is below.
  - Downloads: 11
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performance, CPU-runnable Japanese text embedding model optimized for semantic similarity and retrieval tasks, achieving top results on benchmarks like MIRACL.
  - Downloads: 15,538
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - This repository distributes LoRA models created by Hotaru Jujo, released under a dual license of MIT or CreativeML Open RAIL-M, with no usage restrictions but social media mention appreciated.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT v1 is a Japanese document retrieval model based on ColBERT, achieving near-multilingual performance despite evaluation on out-of-domain datasets, and surpassing previous Japanese models.
  - Downloads: 607
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - This repository provides a series of small, fast Japanese reranker models (v2) varying in size and speed, offering performance scores and GPU processing times for each configuration.
  - Downloads: 429
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - This repository provides a series of fast and very small Japanese reranker models (v2) with varying sizes and speeds, including both standard and cross-encoder architectures, evaluated on score and GPU processing time.
  - Downloads: 105
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - This repository provides a passage encoder for the BPR document retrieval model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of â€œå¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€.â€
  - Downloads: 39
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã§tohoku-nlp/bert-base-japanese-v3ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - This repository provides a Japanese NLP modelâ€”finetuned from `tohoku-nlp/bert-base-japanese-v2`â€”for extracting key entities (location, type, season, ingredient) from cooking-related search queries.
  - Downloads: 11
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªžãƒ‡ãƒ¼ã‚¿ã§pkshatech/GLuCoSE-base-jaã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 is a 7-billion parameter Japanese language model fine-tuned for instruction following, achieving a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 18
## ðŸ§  Datasets

This list is sorted by downloads as of June 03, 2025.
535 datasets are listed.

### Information Extraction & Text Mining
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset provides news articles from the Livedoor News corpus, used in the book *Introduction to Large Language Models*, licensed under CC BY-ND 2.1 JP for Named Entity Recognition (NER) tasks.
  - Downloads: 6,807
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a multilingual sentence dataset enabling translation tasks via customizable language pair and version selection using the `load_dataset` function.
  - Downloads: 2,308
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - FineWeb2 Edu Japanese is a high-quality, 89.3 billion token dataset of 120 million educational Japanese texts, with provided subsets for sampling and shorter-text analysis.
  - Downloads: 1,731
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - This dataset contains 612M Japanese news article tokens from July-October 2024, extracted from Common Crawl using Uzushio and filtered with pipeline_03a.conf, intended for use with llm-jp/llm-jp-13b-v1.0.
  - Downloads: 1,408
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - WRIME is a dataset for emotional intensity estimation, featuring both self-reported and reader-annotated intensity levels from 50 participants' social media posts.
  - Downloads: 642
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a processed, machine learning-ready dataset of public-domain Japanese texts sourced from Aozora Bunko, with code for reproduction available via the Aozora Bunko extractor.
  - Downloads: 425
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository provides a sharded, Japanese-language subset of the large CC100 dataset in Parquet format.
  - Downloads: 330
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - JSICK is a Japanese natural language inference and semantic textual similarity dataset created by translating the English SICK dataset, designed for researching multilingual compositional inference and model stress-testing.
  - Downloads: 322
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This dataset provides 8.75K comprehensive records of Japanese laws â€“ including number, title, effective date, and full text â€“ sourced from e-Gov and deduplicated as of August 1, 2023.
  - Downloads: 301
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - This dataset contains 5,000 annotated Japanese tweets (Feb-Jun 2022) for detecting online defamation, labeling both the target of abuse and the type of abusive content.
  - Downloads: 229
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository provides version 2.0 of a Japanese named entity recognition dataset created by Stockmark, based on Wikipedia, and used in the book *Introduction to Large Language Models*, licensed under CC-BY-SA 3.0.
  - Downloads: 222
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This repository provides a filtered Japanese summarization dataset, XL-Sum, processed with PaLM 2 filters to reduce 15-gram overlap, resulting in 4215 training, 758 validation, and 766 test examples.
  - Downloads: 195
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - This dataset provides images for evaluating Japanese image classification models across four tasks, including 101 types of Japanese food, licensed under CC-BY-4.0.
  - Downloads: 170
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository provides a Japanese-only subset of the wiki40b dataset, consisting of three parquet files generated via a provided Python script.
  - Downloads: 170
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - This Japanese dataset provides three-line summaries and indexing of thousands of mycological taxonomy papers from the Daikinrin website, maintained by Atsushi Nakajima.
  - Downloads: 159
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - This repository provides a dataset card for â€œjapanese_alpaca_data,â€ built upon the japanese-alpaca-lora project and requiring further information.
  - Downloads: 150
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - This dataset classifies GitHub repository descriptions as relevant (1) or not relevant (0) to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing.
  - Downloads: 150
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - This dataset provides Japanese named entity recognition (NER) data extracted from Wikipedia, licensed under CC-BY-SA 3.0 and developed by Stockmark Inc.
  - Downloads: 142
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - This repository provides a Parquet dataset of Japanese Wikipedia articles extracted on January 1, 2023, generated using the `datasets` library.
  - Downloads: 141
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This dataset provides named entity recognition (NER) labels for Wikinews articles in Japanese, featuring 8 entity types and licensed under CC BY 2.5, serving as a test set for large language model experimentationâ€”specifically for the book *Introduction to Large Language Models*.
  - Downloads: 135
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - This dataset extracts September and October news from the CC-news-2024-July-October-cleaned corpus, adjusted to approximately 1000 tokens for efficient learning with the llm-jp/llm-jp-3-13b tokenizer, assuming an output token limit of 1024.
  - Downloads: 122
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - This Japanese dataset compiles manually extracted comparative diagnostic charactersâ€”shared or differing traitsâ€”from thousands of mycological taxonomy papers summarized on the Daikinrin website.
  - Downloads: 112
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - This Japanese dataset provides multi-label annotations of NLP research fields for GitHub repositories, utilizing repository information like descriptions, READMEs, and images for training multi-label classification models.
  - Downloads: 98
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - This dataset, zenz-v2.5, comprises 190M pairs of kana-kanji conversion data (â€œleft context-input-outputâ€) for training conditional language models, including zenz-v2.5 models of varying sizes, and includes the AJIMEE-Bench evaluation benchmark.
  - Downloads: 92
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - This dataset contains cleaned Japanese news data from September and October 2024, preprocessed with dates prepended for continued pre-training with a target sequence length of ~1000 tokens using the llm-jp/llm-jp-3-13b tokenizer.
  - Downloads: 90
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This repository provides a dataset of excerpts from Japanese securities reports (æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸) submitted to EDINET from 2014-2022, including company information, document details, and reporting periods.
  - Downloads: 87
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a list of 100 common Japanese stopwords sourced from CC-100 and Wikipedia, designed for use with the nagisa Japanese text analysis library, and accessible via provided code requiring the Hugging Face `datasets` library.
  - Downloads: 80
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository provides a voice label dataset for Nene Kusakabe (Machico) from *Project Sekai: Colorful Stage! feat. Hatsune Miku*, currently containing a partial collection of her in-game vocals with plans for future completion and standardization, and links to a larger dataset for all characters via QQ group 691795641.
  - Downloads: 78
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR is a 209K-instance dataset combining annotations from Manga109 and a manga onomatopoeia collection, designed for narrative text recognition in diverse manga styles and layouts.
  - Downloads: 78
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA is a Hugging Face mirror of the AWS Open Data Registry dataset for Japanese image captioning and visual question answering tasks.
  - Downloads: 78
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - This repository provides a dataset subject to a LICENSE agreement that users must acknowledge and review before use.
  - Downloads: 73
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
  - This repository provides counts of nouns (204,661) and verbs (16,454) extracted from a 400,000-sentence Japanese Wikipedia dataset (izumi-lab/wikipedia-ja-20230720), totaling 221,115 tokens.
  - Downloads: 72
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - This dataset provides a filtered 10 billion Japanese token corpus from CommonCrawl, processed to remove sensitive personal information using rule-based and machine learning techniques, licensed under CC terms.
  - Downloads: 68
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - This repository provides a JSONL version of the Dolly-15k-ja dataset, formatted for use with the SFTTrainer, and licensed under CC BY SA 3.0.
  - Downloads: 67
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench is a 102-question benchmark dataset, featuring 21 Japanese images categorized by detail, conversation, and complexity, designed to evaluate Vision-Language Models across seven subcategories like anime, culture, and landscape.
  - Downloads: 67
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - This repository provides a dataset of cooking recipe search queries with labeled entities (area, type, season, ingredients) and includes code for fine-tuning language models and building a related application.
  - Downloads: 64
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - This repository provides a comprehensive JSON anime dataset with metadata and links to popular anime platforms like MAL, AniList, and Kitsu, sourced from the Manami Project.
  - Downloads: 64
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - This project automatically generates question-answer pairs from Japanese Wikipedia articles using a 5-bit quantized Mixtral 8x22b model on the TSUBAME4.0 supercomputer, requiring potential filtering due to possible hallucinations.
  - Downloads: 58
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - J-NER is a Japanese named entity recognition dataset containing 1,570 examples of 157 entity types â€“ sourced from Wikipedia and designed for LLM training â€“ with five positive and negative examples per entity.
  - Downloads: 50
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
  - This repository provides a Japanese-English datasetâ€”built from sources like Aozora Bunko and Wikipediaâ€”for knowledge distillation using the llama-imatrix method to improve LLM performance.
  - Downloads: 48
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - This repository provides a large dataset of Japanese web novels (80B+ characters, 55B+ tokens) intended solely for machine learning research, requiring justification for access.
  - Downloads: 48
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - This repository provides a cleaned, UTF-8 encoded dataset of over 7,000 human-generated Japanese movie/TV show subtitles from OpenSubtitles, formatted as Parquet files with text, timing, metadata, and Open Assistant-compliant data.
  - Downloads: 47
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains extracted sections from 2024 Japanese securities reports (æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸) filed via EDINET, including company information, dates, and identifiers like EDINET code and JCN (æ³•äººç•ªå·).
  - Downloads: 47
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - This repository provides a Japanese Reinforcement Learning from Human Feedback (RLHF) dataset reformatted as a binary classification task (chosen/rejected) using synthetically generated text from Phi-3-medium.
  - Downloads: 46
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - This repository provides a dataset of inspiring anime quotes, including character attribution, sourced from Anime Motivation for analysis and enjoyment.
  - Downloads: 43
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - This repository provides a Parquet-formatted dataset of lyrics from diverse anime songs for use by researchers and enthusiasts.
  - Downloads: 41
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - This repository provides a Hugging Face-compatible version of the Kyoto University Japanese Wikipedia input error dataset (v2) licensed under CC-BY-SA 3.0, originally created by the Language Media Research Lab.
  - Downloads: 41
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - This dataset provides 53,640 annotated Japanese tweets (January-June 2020) related to COVID-19, intended for text classification tasks and requiring access to original tweets via the Twitter API.
  - Downloads: 40
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - This repository provides the English/Japanese dataset utilized to train the shisa-7b-v1 language model, with further details available in that modelâ€™s documentation.
  - Downloads: 39
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - Giellm is a Japanese general information extraction dataset built from the Livedoor News corpus and detailed in the linked arXiv paper, utilizing mutual reinforcement learning.
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 37
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Jibiki.fr provides a collaboratively built, large-coverage French-Japanese dictionary and aligned bilingual corpus sourced from multiple dictionaries (Cesselin, Raguet-Martin, JMdict) and Wikipedia, currently containing over 154,000 entries.
  - Downloads: 36
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - JParaCrawl is a large, publicly available English-Japanese parallel corpus built by web crawling and automatic sentence alignment, accessible via the `datasets` library.
  - Downloads: 35
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a Japanese language benchmark dataset evaluating long-context LLM performance on extractive QA and abstractive summarization tasks, built with web documents and GPT-4/Claude-generated question-answer pairs.
  - Downloads: 35
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - This repository provides a Japanese text generation dataset, expanding Cosmopedia to 100k entries with contributions from kunishou, including translated promptsâ€”available on Hugging Face Datasets.
  - Downloads: 33
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - This repository provides scripts for downloading, parsing, and preprocessing the en-ja-alignæ—¥è‹±å¯¾è¨³æ–‡ dataset (Uchiyama et al., 2003) without redistributing the data itself, utilizing libraries like `datasets`, `bs4`, and `lxml`.
  - Downloads: 33
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 32
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is a JSONL dataset derived from the English Touhou Wiki, designed for unsupervised text generation and potentially text classification, containing primarily English with some Japanese content.
  - Downloads: 31
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - This dataset provides RAW text from `fineweb-2-edu-japanese` with Unicode normalization (NFKC) and noise inference using `fineweb-2-japanese-text-cleaner`, including `noise_spans` indicating noisy text segments (length â‰¥ 4) identified with a threshold of 0.7, licensed under ODC-By.
  - Downloads: 30
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - This repository provides a work-in-progress Japanese-English dataset sourced from Ruby/Rails documentation, including synthetically generated data, for machine translation or related tasks.
  - Downloads: 30
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-following dataset containing 6,259 human-annotated input-target pairs for use in tasks like instruction tuning and language modeling.
  - Downloads: 28
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides a JSONL dataset of validated linguistic minimal pairs for benchmarking Japanese language models, as described in Someya and Oseki (2023).
  - Downloads: 28
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This synthetic question-answering dataset, built from the sakura_japanese_dataset, was generated using Nurture-intelligence/Gemma-2-108B-DPO-v0.1 with Pro-type inference and is subject to both its original data license and the Gemma Terms of Use.
  - Downloads: 26
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - This dataset provides furigana annotations derived from bibliographic data of the National Diet Library, available as a downloadable ZIP file on GitHub and the NDL Lab website.
  - Downloads: 24
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - This repository provides the dataset and code for the SLG framework, a sentence-to-label generation approach for multi-task Japanese sentence classification and named entity recognition, detailed in the linked paper and arXiv preprint.
  - Downloads: 22
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences is a Japanese Wikipedia-based dataset of cleaned sentences, each labeled with its article and section title, licensed under CC BY-SA 4.0 and GFDL.
  - Downloads: 21
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - This dataset contains approximately 7,000 rows of disease-symptom relationships extracted from Japanese medical journal articles (2003-2023), including disease names, findings, supporting text, and article metadata in JSON-Lines format.
  - Downloads: 20
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository provides a Japanese subset of the NTX dataset converted to the Aya instruction format, released under CC-BY-SA 4.0, and linked to the full instruction-formatted dataset and related research.
  - Downloads: 20
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This AutoTrain-processed dataset, intended for the â€œtam_jpâ€ project, provides Japanese (ja) language data instances with â€œcontextâ€ fields for potential training applications.
  - Downloads: 20
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - This dataset provides English-Japanese paragraph pairs with labels indicating whether they describe the same or different entities, extending existing PubChem & Wikipedia classification resources for multilingual analysis.
  - Downloads: 18
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - This repository provides a clustered dataset, sourced from Japan's PMDA website, for training and evaluating embedding models, featuring text data (â€œgeneric nameâ€ + â€œdefinitionâ€) labeled with â€œclassification codesâ€ and split into train/test sets with preserved label proportions.
  - Downloads: 17
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - This repository provides a manually translated, machine learning-friendly English-Japanese passage-level parallel dataset sourced from English Wikipedia introductions, created to avoid restrictive licenses common in existing corpora and leveraging permissive LLMs like CALM3-22B-Chat and Qwen 2.5 for reference.
  - Downloads: 17
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - This dataset contains crawled data from the â€œKawaryu Toshu Marusenâ€ Japanese haiku submission website, comprising 5346 submissions across 376 prompts, structured for text-to-text tasks and intended for use within the YANS hackathon.
  - Downloads: 16
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset provides 600 Japanese sentences from Wikipedia articles about racehorses, labeled with nine named entity typesâ€”including a newly added "racehorse name"â€”for use in named entity recognition tasks, acknowledging potential data inconsistencies due to Wikipedia/DBpedia link issues and lacking negative examples.
  - Downloads: 15
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - This repository provides a working sample of Parquet files extracted from the January 2023 Japanese Wikipedia dataset, specifically articles categorized under â€œCategory:æŠ•è³‡â€ (Investment).
  - Downloads: 15
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - This repository provides a clustered dataset for training and evaluating embedding models, derived from Japanese customs advance ruling data (item classification) with labels representing HS code sections, split into train/test sets while maintaining label proportions.
  - Downloads: 14
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully is a commercially-usable dataset in Japanese and other languages designed to improve LLM safety, but its use is restricted to safety enhancement and prohibits redistribution, while allowing derivative data creation with proper attribution and adherence to its terms of use, acknowledging it contains potentially harmful content and includes a disclaimer of liability.
  - Downloads: 14
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M is a corpus of 3.5 million unique Japanese light novel character names from syosetu.com, designed for culturally aware NLP applications like name generation and NER model training.
  - Downloads: 13
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - This dataset contains 221 haiku poems from the Itoen New Haiku Grand Prix, including author/judge comments, translations, language tags, and image URLs, structured with metadata like contest details and award information.
  - Downloads: 13
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - This repository provides a Japanese text dataset ("Washi") created via DSIR sampling from CulturaX, focusing on documents similar to XLSum and Aozora Bunko for improved language model performance.
  - Downloads: 13
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - This dataset is a working sample of Japanese Wikipedia articles related to the "Nikkei 225" stock market index, extracted from the range3/wikipedia-ja-20230101 dataset and saved in Parquet format.
  - Downloads: 12
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’UVRã‚’ä½¿ç”¨ã—ã¦BGMã‚„ãƒŽã‚¤ã‚ºé™¤åŽ»ã—ãŸã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tags (average 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 5,514
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese-anime-speech is an audio-text dataset of transcribed dialogue from visual novels, created to improve automatic speech recognition accuracy for anime and similar Japanese media.
  - Downloads: 2,903
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST is a dataset of 70,000 handwritten Japanese characters (hiragana, katakana, and digits) for image classification tasks.
  - Downloads: 2,756
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - MOMIJI is a 56M-document, large-scale Japanese image-text dataset extracted from Common Crawl, intended for training vision-language models.
  - Downloads: 2,400
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 is a 292,637-clip audio-text dataset from visual novels created to improve automatic speech recognition accuracy, and is distinct from version 1.
  - Downloads: 2,181
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - ReazonSpeech is a freely available, 35,000+ hour Japanese audio dataset in FLAC format for Automatic Speech Recognition (ASR) research, governed by Japanese copyright law (Article 30-4).
  - Downloads: 653
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark designed to rigorously evaluate Large Multimodal Model (LMM) performance by addressing cultural dependencies present in existing benchmarks through expert-created, culture-agnostic questions.
  - Downloads: 430
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - This repository provides a CC-0 licensed dataset of AI-generated anime images with Japanese captions (including translations) created for ethical machine learning, featuring image, original prompt, and automatically generated/translated captions.
  - Downloads: 404
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 provides a large, curated dataset of over 240,000 animation clipsâ€”primarily Japanese animeâ€”aimed at addressing the growing need for animation data in AI and generative video model research.
  - Downloads: 331
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a 1.2 million+ image MIT-licensed anime illustration dataset with diverse, high-quality images and accompanying tags, sourced from keyframes, manga, and artbooks.
  - Downloads: 295
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This repository provides a transcribed dataset of voice lines from the game *Umamusume Pretty Derby*, totaling 77 characters and over 10,000 seconds of audio across multiple characters.
  - Downloads: 278
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tags (averaging 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 243
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - This dataset provides a diverse collection of 2020s photos of Japanâ€”covering landscapes, culture, and daily lifeâ€”intended for AI training.
  - Downloads: 235
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample subset of the Japanese Visual Genome VQA dataset, used to evaluate EvoVLM-JP-v1-7B and available under a Creative Commons license.
  - Downloads: 232
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - This dataset provides approximately 39 million Japanese characters of high-quality text extracted from 1,924 academic papers (including NLP2024) and 360 journal articles under CC-BY licenses, intended for pre-training language models and RAG applications.
  - Downloads: 222
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset provides a clarified version of the Japanese-Heron-Bench, offering image-context-question pairs for evaluating vision-language models in Japanese.
  - Downloads: 220
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This project creates a publicly available dataset of voice data from VTuber Sakura Miko (hololive production) for use in speech recognition and other applications, adhering to their secondary creation guidelines and retaining all copyright with Cover Corporation.
  - Downloads: 192
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - This repository provides Japanese ASR transcriptions generated using Whisper, specifically the reazon_speech_all dataset, excluding the original audio files.
  - Downloads: 173
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - AIxBlock provides a Japanese audio dataset of synthetic human-machine conversations simulating call center interactions, licensed under Creative Commons Non-Commercial 4.0.
  - Downloads: 169
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja is a synthetic Japanese multi-turn dialogue dataset created by translating DailyTalk with Gemma-3-27B and synthesizing speech using Zyphra/Zonos-v0.1-transformer with varied voices.
  - Downloads: 155
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - This repository provides a comprehensive 86,000+ sample Japanese voice datasetâ€”combining sources like StoryTTS and Genshin Impact voicesâ€”for speech recognition, text-to-speech, and machine learning applications.
  - Downloads: 129
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from Project Sekai for research use with So-vits-svc 4.0, licensed under CC-BY-NC 4.0 with rights reserved by SEGA and voice actors.
  - Downloads: 123
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - This repository provides the Japanese audio portion of the CallHome corpus, featuring phone conversations with 120 US-based participants, and requires proper citation according to TalkBank rules.
  - Downloads: 119
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - This repository provides a split dataset derived from the joujiboi/japanese-anime-speech-v2 corpus for speech research and applications.
  - Downloads: 110
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - This dataset pairs images of PDF pages (resized to 896px, 700px, or 588px) with OCR-processed text (using NDLOCR, potentially containing "ã€“" for failed reads) and questions generated by Qwen/Qwen2.5-14B-Instruct to train image retrieval models.
  - Downloads: 82
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - This dataset provides approximately 1000 Japanese roleplay instructions created by applying the Magpie technique to Nvidiaâ€™s Nemotron-4-340B-Instruct model, built using DeepInfra, and may contain low-quality records due to minimal post-filtering.
  - Downloads: 71
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This repository provides a Japanese text corpus generated by Phi-3 from randomly sampled data, built upon the OpenMathInstruct-1-1.8m-ja code and partially utilizing the TSUBAME4.0 supercomputer for computation.
  - Downloads: 70
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay dataset (based on DeepSeek-V3-0324) with system messages and formatting, released under the MIT license.
  - Downloads: 68
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from Project Sekai, intended for research use with So-vits-svc 4.0, under a CC-BY-NC 4.0 license with copyright retained by SEGA and voice actors.
  - Downloads: 67
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - This repository provides a large speech dataset (445,793 .wav files, 577+ hours) created using VOICEVOX, leveraging text corpora like ITA, Tsukuyomi-chan, and ROHAN.
  - Downloads: 66
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This synthetic dataset, built using images from ThePioneer/japanese-photos, was generated with Qwen2-VL-7B-Instruct and Qwen2.5-32B-Instruct-AWQ models.
  - Downloads: 61
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - This repository provides translated speech instructionsâ€”sourced from the Common Voice dataset across 120 languagesâ€”into English, Arabic, Japanese, Mandarin, and French, ideal for finetuning Speech LLMs.
  - Downloads: 59
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This dataset contains crawled dataâ€”including images and textâ€”from the â€œHome Mate Senryu Grand Prizeâ€ photo-prompted senryu (short poem) contest, comprising 435 prompts and 1767 responses, intended for use in the YANS hackathon.
  - Downloads: 56
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - This dataset provides annotated speech data with furigana, derived from Aozora Bunko and SAPIE audio daisy data, comprising over 3.3 million cleaned entries with kanji.
  - Downloads: 55
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA is a 41,895-sample synthetic VQA dataset for manga understanding, generated using GPT-4o and images from Manga109, licensed under CC BY 4.0 and OpenAI terms.
  - Downloads: 53
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This dataset contains Japanese *senryu* (short, witty poems) sourced from two websites, featuring 70 image-to-text and 30 text-to-text prompts with two curated responses each, for tasks involving generating poems from given prompts.
  - Downloads: 50
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - This repository provides data and instructions to reproduce the FrÃ©chet Video Distance (FVD) evaluation for AIdeaLab VideoJP, including necessary files and a step-by-step guide for setup and execution.
  - Downloads: 49
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - This repository provides the CABank Japanese Sakura Corpus, a dataset of 31 participantsâ€™ audio recordings from Japan, requiring citation and adherence to TalkBank usage rules (doi:10.21415/T5M90R).
  - Downloads: 49
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - This repository provides a Hugging Face Datasets version of the Tanaka Corpus, preprocessed for easy use in natural language processing tasks with Japanese and English sentence pairs.
  - Downloads: 43
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS scores and transcriptions) for reazon-research/reazonspeech-v2, saved as a JSON file and visualized with histograms, utilizing resources provided by AiHUB.
  - Downloads: 43
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - This repository provides a Japanese translation of the HelpSteer dataset for experimenting with SteerLM, a technique to customize LLMs during inference, leveraging NVIDIA's NeMo Aligner.
  - Downloads: 40
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - This dataset provides English and Japanese captions generated by BLIP for PokÃ©mon images from the FastGAN dataset, used for training PokÃ©mon text-to-image models.
  - Downloads: 40
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - This dataset provides CC0-licensed images of places in Japan for training text-to-image or other machine learning models without copyright concerns.
  - Downloads: 37
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This dataset facilitates evaluation of large language models on three comedic response generation tasks â€“ text-to-text, image-to-text, and text-image-to-text â€“ using paired prompts and images.
  - Downloads: 36
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - This dataset filters high-quality recordsâ€”totaling 5,475 with Apache 2.0, CC-BY-SA-3.0, and MIT licensesâ€”from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja, specifically evaluated for performance on JGLUE tasks (JcommonsenseQA, MARC-ja, JSQuAD).
  - Downloads: 35
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS values) of the Common Voice Corpus 17.0, stored as a JSON file with counts of audio files exceeding specific MOS thresholds (1, 2, and 3), utilizing resources provided by AiHUB.
  - Downloads: 32
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - This dataset provides high-quality images and videos of diverse Japanese subjectsâ€”landscapes, culture, and daily lifeâ€”captured in the 2020s for AI training.
  - Downloads: 32
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This repository provides a 66.4-hour Japanese voice dataset of 30,800 recordings from *Fate/Grand Order* charactersâ€”each with a single voice actorâ€”suitable for ASR/ASV model training and evaluation.
  - Downloads: 32
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus provides a dataset of 96kHz/16bit Japanese speech recordings, both raw and cleaned, spoken by a virtual character, with accompanying transcripts and metadata under a CC BY 4.0 license.
  - Downloads: 31
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - This repository provides Japanese MS MARCO data with hard negatives mined through normalization, filtering, and selection of queries and documents, alongside SPLADE model training and comparison with mMARCO for information retrieval.
  - Downloads: 29
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage is a Japanese QA dataset derived from JDocQAâ€™s test split, consisting of single-image questions and answers extracted from PDF files converted to 200dpi PNG images, designed for practicality with a reduced size.
  - Downloads: 29
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - This repository provides a 32K Japanese instruction dataset, Rakuten-Alpaca-Data-32K, automatically generated using RakutenAI-7B-chat and inspired by Stanford Alpaca, with seed data from `seed_tasks_japanese.jsonl`, and recommends filtering due to potential quality issues.
  - Downloads: 28
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - LLaVA JP Instruct 108K is a 108,000-example datasetâ€”formatted for LLaVA instruction tuningâ€”created from Japanese Visual Genome VQA and docci_ja data under an Apache 2.0 license.
  - Downloads: 27
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - This dataset provides 6315 converted 1024x1024 PNG images of Kanji characters, paired with their textual definitions, derived from the original KanjiVG SVG files.
  - Downloads: 25
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - This repository provides a dataset of Japanese music analyzed for emotion using Music2Emotion, featuring JSONL files with video metadata and predicted moods, valence, and arousal scores.
  - Downloads: 23
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - This dataset provides CC0-licensed images of Japanese scenery for training text-to-image models and other machine learning applications, serving as a template for new dataset creation.
  - Downloads: 22
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella is a Japanese a cappella vocal ensemble corpus featuring scores and isolated audio tracks of six-part arrangements of public domain children's songs.
  - Downloads: 22
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This dataset, derived from CLoT-Oogiri-Go, provides Japanese humorous responses for three tasks â€“ text-to-text, image-to-text, and text-image-to-text â€“ sourced from the Bokete website, totaling 100 examples.
  - Downloads: 22
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This dataset contains 100 Japanese *senryu* (short poems) â€“ 70 image-to-text and 30 text-to-text â€“ crawled from photo and online *senryu* websites, intended for evaluating generative models through a leaderboard and human evaluation.
  - Downloads: 21
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This dataset provides 300k training and 30k testing examples of noisy spans within Japanese FineWeb2 data, identified by an LLM (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) and formatted as JSON to pinpoint non-content text like navigation, ads, and timestamps.
  - Downloads: 20
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Japanese LLaVA Instruct 150K is a Japanese-translated version of the original LLaVA Visual Instruct 150K dataset, intended for visual instruction tuning in Japanese, released under a CC BY-NC-4.0 license.
  - Downloads: 18
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository provides a dataset of voice embeddings for Japanese parliament members, generated using speechbrain/spkrec-ecapa-voxceleb, intended for speaker separation and analysis of parliamentary recordings and speeches.
  - Downloads: 17
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - This dataset clusters business description text from EDINET filings, labeled with industry codes, and split into train/test sets for embedding model training and evaluation.
  - Downloads: 16
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - This dataset provides quiz data licensed under CC-BY-SA-4.0, extracted from the JAQKET dataset used in the AI-ÅŒ competition, accessible via the `datasets` library.
  - Downloads: 14
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - This dataset provides 2.5M+ cleaned and verified Japanese speech entries with furigana, derived from the Aozora library and SAPIE audio data, improving upon the original flawed Whisper-transcribed corpus used for training models like FLFL.
  - Downloads: 13
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - This repository provides a clustered dataset of 6,127 Japanese legal documents (from e-Gov XML data, specifically "Heisei" and "Reiwa" eras) with text from law titles and main provisions, labeled according to legal classifications and split into train/test sets while preserving label proportions.
  - Downloads: 12
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - This dataset provides Japanese audio and transcriptions of veterinary medicine terminologyâ€”covering drugs, diseases, and symptomsâ€”for training speech recognition or natural language processing models.
  - Downloads: 11
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - This dataset provides multilingual Amazon product reviews (English, Japanese, German, French, Chinese, Spanish) collected between 2015-2019 for text classification, but is currently inaccessible due to provider decisions.
  - Downloads: 2,233
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - This repository ranks Large Language Models (LLMs) by their performance translating Japanese visual novels to English, offering a comparison to established translation tools with preliminary, evolving results.
  - Downloads: 670
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - This repository provides a Japanese translation of the research-safe English subset of the ReLAION-5B dataset, created using the Gemma-2-9b-it LLM and the vLLM inference library via the text2dataset tool.
  - Downloads: 511
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªžã‹ã‚‰è‹±èªžã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 365
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - JHumanEval is a human-revised, Japanese translation of the HumanEval code generation benchmark, designed to evaluate the code-solving capabilities of Japanese Large Language Models, intentionally retaining original errors for realistic assessment.
  - Downloads: 312
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - This repository provides Japanese queries from the MMarco dataset with hard negatives retrieved using multilingual e5 and BM25 models for information retrieval research.
  - Downloads: 272
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - This repository provides the filtered training and development datasets (train_w_filtering & dev) from JSNLI Version 1.1, used in the book â€œIntroduction to Large Language Models,â€ licensed under CC BY-SA 4.0.
  - Downloads: 252
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - This dataset provides Japanese-to-English translations of the kaken subset from the llm-jp-corpus-v3, created using Qwen/Qwen2.5-32B-Instruct and released as an open, parallel corpus under the CC-BY 4.0 license.
  - Downloads: 201
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS is a 30-hour, 10,000-sample paired dataset of Japanese speech and Simplified Chinese text designed for speech translation and multilingual speech understanding research, released under a CC B license.
  - Downloads: 167
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - This dataset provides a cleaned Japanese language corpus derived from OSCAR-2301, containing 94 million words (181.2 GB) after removing unsuccessfully cleaned metadata files.
  - Downloads: 160
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository hosts oasst2-33k-ja, a Japanese instruction tuning dataset translated from an English oasst2 subset by LLM-jp using DeepL, built upon kunishou/oasst2-135k-ja.
  - Downloads: 133
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja is a 12,000-entry Japanese human preference dataset, translated from hh-rlhf using DeepL, comprising random samples from four groups designed for training helpful and harmless LLMs.
  - Downloads: 131
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This repository provides a Japanese-English parallel text dataset for translation, licensed under Apache 2.0 with potential source-specific restrictions, including metadata indicating fanfiction origins.
  - Downloads: 121
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLMã®ãŸã‚ã®æ—¥æœ¬èªžã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸ å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€ æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªžå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›žå¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
  - Downloads: 117
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - This repository provides a Japanese translation of a cleaned subset of the bluemoon-fandom-1-1-rp dataset, utilizing the command-r-08-2024 model via the openrouter API for faster, resource-efficient, and uncensored NSFW translation.
  - Downloads: 108
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - This repository provides a Japanese translation of the MS MARCO dataset, generated using google/madlad400-3b-mt and structured like the original, though translation quality is limited and comparison with higher-quality multilingual datasets like mMARCO is recommended.
  - Downloads: 106
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - This repository provides a Japanese translation of the GSM8K reasoning dataset, with answers extracted from translated descriptions, utilizing a quantized language model and acknowledging potential data inconsistencies.
  - Downloads: 101
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - This repository provides a hand-crafted Japanese dataset (â€œliz-nojaloli-jaâ€) for preparing training data, potentially for Reinforcement Learning from Human Feedback (RLHF).
  - Downloads: 100
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This repository provides a sentence-aligned Japanese-English dataset of web novel chapters, designed for document translation with accompanying metadata like series titles and alignment scores.
  - Downloads: 96
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - This repository provides a deduplicated and randomized dataset of English-Japanese sentence pairs sourced from Tatoeba, suitable for machine translation or language learning.
  - Downloads: 90
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - This repository provides a 3.3 million row Vietnamese-Japanese parallel corpus for machine translation and natural language processing tasks.
  - Downloads: 77
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This repository provides a JSON conversion of the NilanE/ParallelFiction-Ja_En-100k datasetâ€”sentence-aligned Japanese/English web novel chaptersâ€”formatted for use in training translation models with text-generation-webui.
  - Downloads: 74
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl provides a BeIR-formatted version of the Japanese miracl dataset for streamlined evaluation with the mteb framework.
  - Downloads: 66
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - This repository provides paired Japanese and Vietnamese sentences for machine translation and language learning purposes.
  - Downloads: 65
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - This dataset provides the Japanese-English parallel corpus extracted from the Asian Language Treebank (ALT) project, sourced from the Hugging Face `alt` dataset, and cited in Riza et al. (2016).
  - Downloads: 64
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - This repository provides a filtered English-Japanese parallel corpus derived from Wikidata descriptions, formatted as a JSONL file optimized for training translation models with Hugging Face Transformers.
  - Downloads: 58
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA is a 30-hour, 10,000-sample dataset of paired Japanese speech and Simplified Chinese text for training and evaluating speech translation and multilingual speech understanding models.
  - Downloads: 56
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - This repository provides a quality-filtered, 1 million row subset of the JParaCrawl v3 English-Japanese parallel corpus, addressing issues with alignment and completeness in the original dataset.
  - Downloads: 55
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset provides corrections and translations for the Japanese portion of the multilingual LLAVA-Bench-in-the-wild benchmark, building upon the original liuhaotian/llava-bench-in-the-wild dataset.
  - Downloads: 55
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - This repository provides a free 850,000 sentence sample of a larger paid English-Japanese parallel corpus covering diverse fields and suitable for machine translation and text analysis.
  - Downloads: 54
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - This repository provides a question-answering dataset generated from the Japanese wiki40b corpus.
  - Downloads: 53
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual form understanding benchmark dataset with human-labeled key-value pairs across 7 languages, designed for form analysis and information extraction research.
  - Downloads: 50
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - This repository provides a 20,000-record Japanese-English translation dataset generated using the Magpie method applied to nvidia/Nemotron-4-340B-Instruct, along with the code used for its creation, noting the dataset lacks post-generation filtering and may contain low-quality entries.
  - Downloads: 49
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Japanese LLaVA Pretrain is a Japanese-translated version of the original LLaVA dataset, intended for similar applications in Japanese language processing and requiring adherence to the licenses of CC-3M and BLIP.
  - Downloads: 46
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP is a Japanese-language benchmark dataset of 100 quiz questionsâ€”focused on five popular anime seriesâ€”designed to evaluate the anime knowledge and understanding of Large Language Models.
  - Downloads: 45
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a manually created, high-quality Japanese dataset of 100 Chain-of-Thought (CoT) examples, available in both connected and separated CoT/output formats, with the connected version offering more natural transitions.
  - Downloads: 44
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - This dataset provides long-form instruction data based on the Aozora Bunko corpus, primarily designed to challenge question answering performance on lengthy texts without filtering for correctness, acknowledging the difficulty and model-dependency of such tasks.
  - Downloads: 42
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers a Japanese translation of the MBPP dataset, created using LLM-jp and DeepL, with authorship by Han, Otake, Ozaki, and Miyao.
  - Downloads: 40
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This repository provides a Japanese RLHF datasetâ€”a modified version of kunishou/hh-rlhf-49k-jaâ€”excluding examples with next-generation translation.
  - Downloads: 38
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - This repository provides a Japanese translation of the MMLU dataset, generated using GPT-3.5-turbo, for use in multilingual research like MultilingualSIFT.
  - Downloads: 36
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - LIMA-JA is a Japanese translation and lightly adjusted version of Metaâ€™s LIMA dataset, accessible via the `datasets` library for use in training and evaluating language models.
  - Downloads: 35
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - LLM-jp's Synthetic-JP-EN-Coding-Dataset is an instruction tuning datasetâ€”a subset of Aratakoâ€™s 801k datasetâ€”created through a Japanese collaborative project, with inquiries directed to llm-jp@nii.ac.jp and authored by Hirokazu Kiyomaru and Takashi Kodama.
  - Downloads: 35
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This repository provides the Japanese language portion of the Guanaco dataset, similar to related datasets like alpaca-guanaco-japanese-gpt-1b.
  - Downloads: 35
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - JSynFlow is a 10,000-entry Japanese flowchart VQA dataset generated using Meta's Llama 3.1 405B, containing PNG images and JSON data representing workflow procedures across various professions for research purposes.
  - Downloads: 33
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This repository provides a Japanese translation of the ViQuAE dataset, focusing on translating the original answers into Japanese while leaving the answers themselves untranslated.
  - Downloads: 33
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - This dataset provides summaries of long texts sourced from the cleaned Aozora Bunko corpus, licensed under CC BY 4.0.
  - Downloads: 29
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - This repository provides a Japanese translation of the FED dataset using the Google Cloud Translate API v2, noting potential inconsistencies in certain dimensions due to machine translation impacts on annotation alignment.
  - Downloads: 28
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - This repository provides a sample of a 9.83 million sentence pair Chinese-Japanese parallel corpus, covering diverse fields and preprocessed for machine translation and text analysis.
  - Downloads: 27
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - This repository provides a Japanese translation of MT-Bench, corrected by Inflection AI, with some questions sourced from Stability AIâ€™s Japanese MT-Bench.
  - Downloads: 26
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This repository provides a multilingual translation dataset of Korean, Chinese, and Japanese text originally from OpenOrca, aligned by ID and prioritizing the most similar translations using embedding similarity (BAAI/BGE-m3).
  - Downloads: 24
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - This repository provides the Tanaka-corpus, a publicly available Japanese-English parallel corpus originally compiled by Professor Yasuhito Tanaka and detailed in his 2001 Pacling paper.
  - Downloads: 24
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - This repository provides a Japanese translation of the SciQ dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed under CC-BY-NC 3.0.
  - Downloads: 24
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - This repository provides a Faiss index and associated data for efficient similarity search on Japanese Wikipedia paragraphs vectorized using the intfloat/multilingual-e5-base model.
  - Downloads: 22
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - This repository provides a ~2.46M row, train-only Japanese instruction dataset in Aya format, converted from v1.0.0 and licensed under CC-BY-SA 4.0, for LLM development and tuning.
  - Downloads: 19
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - This repository provides a sample of a 101,702-entry Japanese pronunciation dictionary created by linguists, intended for research and development of Japanese Automatic Speech Recognition (ASR) technology, with a link to the full paid dataset.
  - Downloads: 18
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - OPUS-MIT-5M is a balanced multilingual image translation dataset comprising 5 million sentence pairs from the OPUS corpus across 20 languages.
  - Downloads: 18
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This repository provides a Japanese-English sentence-aligned web novel dataset, formatted for Alpaca and chunked to 4096 tokens for use with the augmxnt/shisa-base-7b-v1 model.
  - Downloads: 18
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository provides 1k responses generated using DeepSeek-R1-Distill-Llama-8B, fine-tuned on aya-ja-evol-instruct-calm3-dpo-masked, with 8-bit quantization, intended for reference or pre-processing despite potential inaccuracies and incomplete token generation.
  - Downloads: 18
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - This dataset provides Japanese-English translation pairs from the TALPCo corpus, converted to Hugging Face format with whitespace normalization, and licensed under CC-BY 4.0.
  - Downloads: 16
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - This repository provides a Python function, `prompt`, for rigorously evaluating the accuracy and quality of Japanese-to-English translations based on strict criteria including completeness, accuracy, grammar, and overall quality of both languages.
  - Downloads: 15
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - This dataset provides 50,000 English sentences extracted from the larger 801k Synthetic Japanese-English Coding Dataset, referencing the original dataset for details and considerations.
  - Downloads: 14
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - This repository provides the Japanese translation of the PIQA dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed identically to the original PIQA.
  - Downloads: 13
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository provides the TaCo dataset used for researching improved cross-lingual transfer and chain-of-thought prompting in low-resource language LLMs, as detailed in the Upadhayay & Behzadan ICLR 2024 paper.
  - Downloads: 13
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - This repository provides a 380,000-sentence sample of a larger, paid Japanese-English parallel corpus suitable for machine translation, text analysis, and NLU tasks, excluding sensitive content.
  - Downloads: 11
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides the TaCo dataset for cross-lingual instruction tuning, featuring instruction-input-output examples in multiple languages as detailed in the associated research paper.
  - Downloads: 11
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - This repository provides Japanese-English translation resources licensed under CC-BY 4.0.
  - Downloads: 11
### Semantic Text Processing
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - JGLUE is a novel, natively-Japanese NLU benchmark dataset created by Yahoo Japan and Waseda University to evaluate and advance general language understanding abilities in Japanese.
  - Downloads: 1,726
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a Japanese biomedical LLM benchmark dataset with a provided evaluation framework (med-eval) designed to assess performance and encourage contributions.
  - Downloads: 1,542
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a Japanese text embedding benchmark comprising 6 tasks and 24 datasets for evaluating model performance on tasks like news classification and intent recognition.
  - Downloads: 942
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth provides Japanese/English synthetic conversation datasets derived from LMSYS-Chat-1M, used for post-training Llama-3.1-Swallow and Gemma-2 models.
  - Downloads: 933
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - This repository provides Japanese Wikipedia embeddings and a FAISS index for RAG applications, including demos, conversion scripts, and evaluations of various Japanese embeddings like OpenAI's `text-embedding-3-small` under mixed licenses (CC-BY-SA-4.0 and OpenAI for specific files).
  - Downloads: 808
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - This repository provides a Japanese instruction-following (chat) dataset for fine-tuning LLMsâ€”like those initially trained on Englishâ€”using methods such as LoRA, with recent updates addressing licensing and data quality issues including removal of Alpaca and cleaning of Wikipedia/ALT data.
  - Downloads: 553
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - This repository merges and extracts lines of 256 characters or less from the neody/c4-ja-cleaned, neody/cc100-ja-cleaned, and neody/oscar-ja-cleaned Japanese datasets.
  - Downloads: 225
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - This dataset provides simple configuration data for Zunda Mon, intended for verifying character LLM functionality, and includes formats for LLM-jp and ChatGPT, with licensing details from (ãšãƒ»Ï‰ãƒ»ãã‚‡).
  - Downloads: 211
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - This repository provides a Japanese Wikipedia sentence dataset used in the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 177
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - This repository provides a Windows executable for running the Japanese GPT-2 language model (ggml-japanese-gpt2) with pre-trained bin and SentencePiece model files, though the xsmall model currently has issues.
  - Downloads: 167
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - This repository provides a 200GB subset of the multilingual FineWeb2-HQ dataset, specifically containing Japanese text data split into 14 JSONL chunk files.
  - Downloads: 156
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - This repository provides a Japanese chat dataset, derived from izumi-lab/llm-japanese-dataset, for fine-tuning large language modelsâ€”particularly for instruction-following and chat tasks using methods like LoRA.
  - Downloads: 147
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - This repository provides a clean, 5.1 million sentence Japanese dataset with context, suitable for unsupervised semantic similarity learning and related tasks.
  - Downloads: 105
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - This dataset provides ~39.6k formatted Japanese roleplay conversations generated with gpt-4o-mini, including system messages, licensed under CC-BY-NC-SA 4.0, and prohibits use for developing competing OpenAI models.
  - Downloads: 96
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - SimpleStories is a diverse, annotated dataset of short stories generated by gpt-4o-mini (and soon other models), available in English and Japanese, and built as an improvement upon TinyStories, designed for easy NLP data filtering.
  - Downloads: 90
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - This repository provides a processed version of the ShareGPT52K dataset, converting conversations to Markdown, detecting language, and handling CJK/Simplified Chinese text conversions using various open-source tools.
  - Downloads: 68
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„19800ä»¶ã®æ—¥æœ¬èªžãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åŽéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 63
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - This repository provides a Japanese fake news dataset converted for Hugging Face datasets, including text context, labels identifying truthfulness (real, partial_gpt2, full_gpt2), and character counts for real and fake portions.
  - Downloads: 62
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese temporal inference datasetâ€”including training, test, and template dataâ€”designed to evaluate the generalization capacity of language models on temporal reasoning.
  - Downloads: 62
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository hosts databricks-dolly-15k-ja, a Japanese translation of the dolly-15k instruction tuning dataset created collaboratively by LLM-jp.
  - Downloads: 55
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This dataset provides BERT-based binary passage embeddings for the â€œAI Kingâ€ competition passages, originally from llm-book/aio-passages, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 53
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - This dataset provides 190k synthetic Japanese preference rankings generated using five open-source models (including Tanuki, Calm3, and Qwen) and judged by Qwen/Qwen2.5-72B, designed to mitigate positional bias.
  - Downloads: 51
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - This repository provides a continuously updated, GPT-4 generated, Japanese question-answering dataset for fine-tuning non-English open-source models, created with minimal verification beyond semantic similarity filtering.
  - Downloads: 49
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - This repository provides the Japanese Wikipedia paragraphs dataset used in the book â€œIntroduction to Large Language Models,â€ sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 48
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - This dataset consists of bullet-point lists generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and DeepSeek-R1-Distill-Qwen-32B-Japanese, licensed under CC-BY-SA 4.0.
  - Downloads: 42
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - This repository provides a 69k Japanese-English coding dialogue dataset generated using models like Nemotron, Phi-3, Mixtral, and Calm3 via the Magpie method, including the generation code, but without post-filtering for quality.
  - Downloads: 42
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - Shisa-base-7b-v1 is pre-trained on this dataset of primarily Japanese/English MADLAD-400 tokens sampled using DSIR.
  - Downloads: 42
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - llm-jp-instructions is a manually created Japanese instruction dataset, available via the `datasets` library with train, development, and test splits in v1.0.
  - Downloads: 42
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG is a 20,000-record benchmark for evaluating the Japanese Retrieval-Augmented Generation (RAG) performance of Large Language Models (LLMs), utilizing (Context, Question, Groundtruth Answer) data triplets.
  - Downloads: 42
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - This dataset records PokÃ©mon VGC Regulation F battle team selections gathered from YouTube streams, including data from the author (trainer_id 13), and was presented at a remote PokÃ©mon study group in May 2024.
  - Downloads: 36
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - This dataset consists of conversational data generated by GPT-3.5-Turbo based on the July 2023 Japanese Wikipedia dataset, and is not licensed for commercial use.
  - Downloads: 35
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - Kanalizer is a dataset repository for a library that predicts pronunciation from English words, with related code at VOICEVOX/kanalizer and trained models at VOICEVOX/kanalizer-model.
  - Downloads: 33
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese synthetic dataset of high-quality prompts and AI outputs generated using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 33
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - This dataset, created by the University of Tokyo's Matsuo & Iwasawa Lab LLM course, contains human-crafted inputs and outputs from two language models (watashiha-gpt-6b & Watashiha-Llama-2-13B-Ogiri-sft) for supervised fine-tuning (SFT) exercises, intended for educational and research use.
  - Downloads: 32
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - This dataset provides cleaned Japanese example sentences demonstrating various grammatical patternsâ€”including politeness, negation, desire, progression, and moreâ€”generated using the calm3-22b language model.
  - Downloads: 32
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - This repository provides document-level combined versions of the cc100/cc100-ja Japanese text dataset, originally line-split, while maintaining the original cc100 license.
  - Downloads: 31
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, and built as an improvement upon TinyStories, intended for NLP tasks.
  - Downloads: 30
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - This dataset provides a ShareGPT-formatted, Japanese language version of the OpenAssistant/oasst2 135k dataset, optimized for multi-turn conversation fine-tuning with significant computational resource requirements.
  - Downloads: 29
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - This repository provides a Japanese translation of the Databricks Dolly 15k dataset, enhanced with BERTScore-calculated translation quality scores to identify and address issues like inconsistent or inaccurate translations.
  - Downloads: 29
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - This dataset is a Japanese role-playing learning setâ€”a translation of the Bluemoon_Top50MB dataset using karakuri-lm-8x7b-chat-v0.1-awq, processed with 3-shot prompting and limited to 8000 tokens, with some records removed due to incomplete translation or LLM repetition, and containing inaccurate token count columns.
  - Downloads: 28
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One aimed at enhancing realism, complexity in paintings, and simplifying anime-style illustration generation via placement in the Stable Diffusion WebUI embeddings folder.
  - Downloads: 28
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese coding dialogue dataset generated using the Magpie technique applied to Nvidia's Nemotron-4-340B-Instruct, along with the code used for its creation, noting potential quality variations due to minimal post-filtering.
  - Downloads: 26
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - DataPilot converted the kinokokoro/ichikara-instruction-003 Japanese instruction dataset into the widely-used ShareGPT format, providing JSON Lines data optimized for training conversational large language models (LLMs).
  - Downloads: 26
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - This repository provides a Japanese translation of the HelpSteer2 dataset, used for training and aligning large language models with NVIDIAâ€™s SteerLM and Nemotron-4-430B-Reward.
  - Downloads: 25
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - This dataset converts the Open-Platypus-Japanese-masked formatted data into OpenAI message format, referencing the original datasetâ€™s details for content and usage notes.
  - Downloads: 25
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - This dataset provides educational scores (0-4) for approximately 310,000 Japanese texts from the fineweb-2 dataset, scored using the Deepseek API and a method inspired by the FineWeb-Edu classifier to assess educational value.
  - Downloads: 24
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - YokaiEval is a Japanese dataset of 810 multiple-choice questions designed to evaluate Large Language Models' knowledge of Japanese folklore, specifically focusing on *yokai* (supernatural creatures) across six knowledge categories.
  - Downloads: 23
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - This repository provides a model finetuned on both the kenkensz9/1242tw2 dataset and a custom collection of 330 personality-driven tweets, designed to generate tweets and assign scores ranging from 10 (excellent) to 8 (okay).
  - Downloads: 21
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - This dataset combines human-written (OSCAR) and LLM-generated (GPT-3.5 Turbo) Japanese text to evaluate the performance of LLMs in detecting machine-generated text.
  - Downloads: 21
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - This repository provides an experimental dataset of 50 queries and evaluationsâ€”generated with ChatGPT-4o focusing on five perspectives related to patent attorney servicesâ€”with manually created answers for 10 excluded queries sourced from open patent databases.
  - Downloads: 21
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using the Mistral Small 3.1 24B Instruct model, formatted as JSONL for supervised fine-tuning.
  - Downloads: 20
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - This repository provides human-generated responses to the ELYZA-tasks-100 benchmark for Japanese LLM evaluation, achieving an average GPT-4o score of 3.69 and outperforming Claude 3.5 Sonnet at 4.42, assessed using automated scoring based on gpt4-autoeval.
  - Downloads: 20
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 20
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - This dataset provides cleaned Japanese example sentences demonstrating various grammatical patternsâ€”including politeness, negation, desire, progression, and moreâ€”generated using the calm3-22b language model.
  - Downloads: 18
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository provides a 60k Japanese instruction dataset, Self-Instruct-Qwen2.5-72B-Instruct-60k, generated using Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8 and Self-Instruct techniques, licensed under Apache 2.0 with Qwen License considerations.
  - Downloads: 18
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - This repository provides a filtered and modified Japanese/Chinese parallel dataset from WikiMatrix v1, improved by regex filtering, semantic similarity scoring (LaBSE, threshold 0.6), and Traditional to Simplified Chinese conversion using zhconv.
  - Downloads: 16
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - This repository provides manually created variations of tasksâ€”presenting conversion candidates like IME and correcting bracket matchingâ€”developed to address weaknesses in a model created by @pokutuna for a University of Tokyo competition on large language model applications.
  - Downloads: 16
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This repository provides a Japanese translation of Databricksâ€™ Dolly project, licensed under CC BY-SA 3.0, utilizing and building upon data from sources like Wikipedia under the same license.
  - Downloads: 14
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset provides detailed, natural language overviews of VTubersâ€”including their character, activities, collaborations, and styleâ€”collected using GPT-4o Search Preview and costing $27.04 for 36,276 tokens.
  - Downloads: 13
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - This repository provides a 26.5k Japanese instruction dataset created by applying Magpie methodology, clustering instruction embeddings, and then evolving the selected instructions using Qwen2.5-72B, licensed under Apache 2.0 with considerations for the Qwen license.
  - Downloads: 13
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - LLM-jp Corpus v3 (excluding Wikipedia due to its CC-BY-SA license) provides a mirrored dataset of Japanese text for large language model training and research.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªžãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - This repository explores prompt extraction using the Magpie method and the rinna/llama-3-youko-8b language model, based on prompts from a research paper, despite the model not being instruction-tuned.
  - Downloads: 11
### Natural Language Interfaces
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda provides 40 Japanese-language questionsâ€”covering history, society, government, and geographyâ€”to benchmark and rank the performance of Japanese Large Language Models.
  - Downloads: 646
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset designed to evaluate the effectiveness of Retrieval-Augmented Generation (RAG) for improving LLM accuracy by utilizing external knowledge retrieval.
  - Downloads: 474
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - This dataset provides 20,000 synthetic Japanese roleplay conversations (10-20 turns each) with detailed metadata like genre, tags, and character/scene settings, formatted for use with language models via OpenAI messages.
  - Downloads: 298
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696 question-answer pair dataset for Japanese machine reading comprehension, built from Wikipedia and designed as a Japanese analogue to SQuAD, achieving strong performance with fine-tuned BERT-Japanese models.
  - Downloads: 297
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - This repository provides the Japanese question-answering benchmark dataset used for evaluating large language models, specifically within the book *Large Language Models Introduction II*, and compatible with the llm-jp-eval framework, released under the Apache 2.0 license.
  - Downloads: 246
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This repository provides the Japanese subset of the LAION-2B-Multi research-safe dataset, offering a safe and accessible resource for multilingual image-text research.
  - Downloads: 230
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - JAQKET is a Japanese open-domain question answering dataset, built from Wikipedia articles, designed to facilitate research in question answering and machine reading comprehension with multiple-choice questions (v1.0 & v2.0).
  - Downloads: 191
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The Business Scene Dialogue (BSD) dataset is a Japanese-English parallel corpus of written business conversations created through scenario writing and translation, offering a balanced representation of both languages.
  - Downloads: 129
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - AIxBlock provides a quality-assured dataset of 500k Japanese sentences for applications like speech data generation and Natural Language Processing (NLP).
  - Downloads: 123
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a Japanese dialogue corpus of approximately 14,000 conversations, including speaker personas and personality traits, intended for research with ethical considerations regarding privacy and impersonation.
  - Downloads: 106
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository provides a Japanese dialogue summarization dataset created by translating the English DialogSum and CSDS datasets.
  - Downloads: 105
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIçŽ‹ (AIO) is a Japanese quiz datasetâ€”specifically, the version 2.0 validation setâ€”enhanced with manually verified answers, featuring fields like question ID, competition name, timestamp, and original question text.
  - Downloads: 104
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval benchmark dataset of 5,000 questions paired with 500,000 web page titles/summaries sourced from Hatena Bookmark, designed to evaluate search systems responding to natural language queries.
  - Downloads: 104
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository provides a Japanese translation of the everyday-conversations-llama3.1-2k dataset, formatted as topic-based conversation pairs generated using DeepL and licensed under Apache 2.0.
  - Downloads: 103
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - This repository provides a multi-turn Q&A dataset automatically generated using Mixtral-8x22B with data sourced from oasst2, Databricks-Dolly, and other openly licensed datasets (CC0, CC-BY-SA 3.0, CC-BY 4.0).
  - Downloads: 100
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository provides the Kyoto University's Japanese Vicuna QA Benchmark, an 80-question dataset across 10 categories for evaluating Japanese LLM responses without references, licensed under Apache 2.0.
  - Downloads: 93
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This repository provides 3,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential unfiltered, unusual dialogue.
  - Downloads: 86
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a challenging Japanese visual question answering benchmark, built on a dataset of 101 Japanese food images with multiple-choice questions designed to test vision-language models.
  - Downloads: 77
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - This dataset is a manually cleaned, Japanese translation of the WikiHow-NFQA question-answering dataset.
  - Downloads: 68
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a 4K+ question-answering dataset of past Japanese National Pharmacist Exam questions (2012-2024) with answers, commentaries, and image data, including a performance leaderboard for models like GPT-4o and MedGemma.
  - Downloads: 66
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - This repository provides a 100k Japanese question answering and reasoning dataset with answer candidates generated and evaluated using Qwen models, licensed under Apache 2.0.
  - Downloads: 63
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - LLM-jp Chatbot Arena Conversations Dataset provides ~1,000 pairwise human-preference conversationsâ€”primarily in Japaneseâ€”collected from head-to-head model comparisons during a January-February 2025 trial, including transcripts, votes, and metadata.
  - Downloads: 51
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - JEMHopQA is a Japanese multi-hop question answering dataset with explainable derivation steps, focusing on compositional and comparison questions requiring information from linked Wikipedia articles, licensed under CC BY-SA 4.0.
  - Downloads: 49
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1ã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ä¸€éƒ¨ã¨ã€OpenAIã«ç”Ÿæˆã•ã›ãŸæ–‡ç« ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€tohoku-nlp/bert-base-japanese-whole-word-masking ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ãŸæ–‡ç« ã‚’æ–‡è„ˆãŒæˆã‚Šç«‹ã¤å½¢ã§åˆæˆã—ã€æ–°ãŸãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚‚ã®ã€‚
  - Downloads: 49
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA is a benchmark dataset of 526 manually created question-answer pairs evaluating manga understanding, built using images from the Manga109 collection.
  - Downloads: 48
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - This dataset provides 11,808 multi-turn conversational instructions about Japanese photos, generated using GPT-4o via the Azure OpenAI API, sourced from the â€œjapanese-photosâ€ Hugging Face dataset.
  - Downloads: 42
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - This dataset contains high-quality, Qwen2.5-72B-Instruct generated answers derived from an 8B model's excellent-rated inputs, licensed under Apache 2.0 but subject to Qwen License restrictions for model training.
  - Downloads: 41
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - This dataset comprises dialogue extracted from Japanese public-domain books in Aozora Bunko, created by identifying and grouping consecutive lines enclosed in quotation marks, with reproduction code provided.
  - Downloads: 40
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - This repository provides Japanese QA datasets derived from Stack Overflow, featuring processed question-answer pairs with markdown-formatted text, code blocks, and base64 image replacement, available in both default and simplified subsets.
  - Downloads: 39
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - mc4 is a massive, multilingual, cleaned Common Crawl corpus of text data designed for training and evaluating large language models across 101 languages.
  - Downloads: 38
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - This repository provides a multi-turn Q&A dataset automatically generated using Calm3-22b from open data sources like oasst2, databricks-dolly, and minnade, with licensing including Apache 2.0, CC-BY-SA-3.0, CC-BY-4.0, and CC0.
  - Downloads: 38
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - This repository provides a Japanese translation of the Open-Orca dataset, currently about 20% complete, and is available for commercial use.
  - Downloads: 38
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - This repository provides a filtered and original Japanese dialogue corpus extracted from role-playing forum conversations, excluding threads with single or minimal contributors or very short posts.
  - Downloads: 37
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository provides the 1.56B token text datasetâ€”sourced from publicly available Japanese datasets like accommodation dialog and movie recommendationsâ€”used for pre-training the AKU-d_ms-0.5B-chat-v0.1 model, with each datasetâ€™s license and processing scripts detailed within.
  - Downloads: 36
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench is a Japanese multimodal QA benchmark for geometry problems, featuring questions paired with context, text, images, and exact answers for evaluating AI reasoning.
  - Downloads: 35
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a 4,246-dialogue Japanese dataset for multi-domain task-oriented dialogue research, featuring Wizard-of-Oz collection, six domains, and annotations for dialogue state tracking and goal-oriented conversation.
  - Downloads: 33
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This repository provides a growing, manually-created dataset designed for training Japanese chatbots.
  - Downloads: 33
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - This dataset removes prompts matching those from the chatbot-arena-ja-calm2-7b-chat dataset.
  - Downloads: 32
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - This repository provides a commercially-usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of the Orion-14B Models Community License and acknowledging compute resources from ABCI.
  - Downloads: 29
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - This dataset provides Japanese question-answer pairs derived from Japanese Stack Exchange data, processed with markdown formatting, base64 image replacement, and includes question/answer IDs and popularity indicators for both default and simplified subsets.
  - Downloads: 28
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset offers 60,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using Llama2Pro8B, with automated screening but potential for unusual dialogue.
  - Downloads: 27
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 is a Japanese (ja-JP) question answering dataset derived from shunk031/JDocQA, containing 200 single-image questions with text, IDs, answers, and answer types, optimized for reduced size and faster processing.
  - Downloads: 27
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - This repository provides Japanese single-turn conversation data generated using the Swallow-MX-8x7b model, based on translated user prompts from the Chatbot Arena Conversations JA dataset (CC-BY 4.0 license).
  - Downloads: 26
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese instruction tuning dataset generated by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, alongside the code used for its creation, noting the dataset lacks post-filtering and may contain low-quality examples.
  - Downloads: 26
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - Tengentoppa is a large Japanese instruction-following dataset created by merging 16 diverse datasets in JSON format for supervised finetuning, including sources like Hachi-Alpaca and Chatbot Arena.
  - Downloads: 26
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - This repository provides a commercially-usable, multi-turn conversation dataset generated from Japanese Wikipedia using the Orion14B-Chat model, requiring careful review of the Orion Series Models Community License Agreement and acknowledging computing resources from ABCI.
  - Downloads: 26
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This repository provides a sample of the Nexdata Japanese Conversational Speech dataset, featuring ~1000 speakers engaged in natural, manually-transcribed dialogues on diverse topics recorded via mobile phone.
  - Downloads: 26
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This repository provides a 3,000-conversation dataset generated from Japanese Wikipedia using llama2Pro8B, licensed for commercial use, and automatically screened with potential for some unusual dialogue.
  - Downloads: 25
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - This dataset contains approximately 1,000 synthetic Japanese roleplay dialogues, each with 10 turns, generated using Nvidia's Nemotron-4-340B-Instruct and the Magpie method, but may contain low-quality records and exhibit tendencies to prematurely end longer conversations.
  - Downloads: 24
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - This dataset contains approximately 39,600 synthetic Japanese role-playing conversations generated with gpt-4o-mini, each with 5-10 turns and detailed metadata including genre, tags, and character/setting information for system message creation and model training.
  - Downloads: 24
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset contains over 80,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using llama2Pro8B, automatically screened but potentially including unusual dialogue.
  - Downloads: 23
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - This repository provides a filtered subset of the Aozora Bunko clean text dataset, specifically including texts written in modern orthography (æ–°å­—æ–°ä»®å).
  - Downloads: 23
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - This dataset is a Japanese translation of the WikiHowNFQA dataset, providing question-answer pairs sourced from WikiHow articles.
  - Downloads: 22
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - This dataset is a Japanese translation of the â€œdatabricks-dolly-15kâ€ dataset, modified with â€œnya!â€ appended to each sentence using ArrowPro-7B-KUJIRA, and inherits the original datasetâ€™s license.
  - Downloads: 22
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - This repository provides a Japanese multi-turn conversation dataset, generated with Qarasu14B from Wikipedia data for non-commercial use and compatible with Axolotl, leveraging the Tsuginosuke AI Super Computer.
  - Downloads: 21
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - This dataset comprises human-checked and corrected instructions used with an open-source LLM, generating outputs via Swallow-MX, though the outputs themselves haven't been verified for accuracy and originated from the LOCAL AI HACKATHON #000.
  - Downloads: 21
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - This dataset provides AI-generated subtitles (with potential Turkish/Japanese errors) specifically for chatbot training, and is not intended for translation purposes.
  - Downloads: 20
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW is a synthetic Japanese role-playing benchmark dataset with detailed character, setting, and dialogue attributes, designed to evaluate LLM role-playing capabilities and built using Claude 3.5 Sonnet outputs.
  - Downloads: 20
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - This repository provides a Japanese-only subset of the OpenAssistant Conversations Dataset, formatted as paired human-assistant messages, potentially lacking full conversational context per line.
  - Downloads: 19
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - This dataset provides Japanese example sentences created with the `if001/elementray_m calm3-22b` model, covering various grammatical patterns including polite forms, negation, desires, and more, with cleaned, failed generations.
  - Downloads: 14
### Text Generation
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - This repository provides a clustered Japanese text corpus of approximately 10,000 texts, created by cleaning web corpora like mc4-ja and applying unsupervised learning, intended for information analysis purposes with attention to file formats (some are Parquet) and download instructions via Git LFS.
  - Downloads: 808
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese language benchmark, built from translated MMLU questions and culturally-specific queries, designed to evaluate large language model performance in Japanese understanding.
  - Downloads: 679
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual NL-to-code benchmark comprising 945 samples and 1,707 test cases across English, Spanish, Japanese, and Russian, designed for evaluating code generation models.
  - Downloads: 351
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard evaluates the performance of Japanese Retrieval-Augmented Generation (RAG) solutions across five industries, providing datasets and a comprehensive assessment of Parser, Retrieval, and Generation components â€“ a currently unique resource for Japanese RAG evaluation.
  - Downloads: 336
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This dataset from YANS-official provides crawled data from the Bokete joke-posting site, featuring image-to-text, text-to-text, and text-image-to-text tasksâ€”totaling 500 images and 2455 responsesâ€”as part of the CLoT-Oogiri-Go dataset explored in a CVPR2024 project.
  - Downloads: 230
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ567077ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - This repository extracts text snippets of 256 characters or less from the cleaned OSCAR Japanese dataset (neody/oscar-ja-cleaned).
  - Downloads: 114
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15 & T23 is a 50,000-sentence simplified Japanese corpus with English translations, designed for text simplification and Japanese-English translation using a 2,000-word vocabulary.
  - Downloads: 69
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct is a 5.2K instruction dataset for code-related tasks â€“ including code generation, behavior check, and bug fixing â€“ built from commercially licensed and permissioned programming learning content, primarily in Japanese (translated from English).
  - Downloads: 69
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - This 801k synthetic dataset contains Japanese & English code instruction-following data generated with Evol-Instruct, built upon the Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k base and created using models like Nemotron, Phi-3, Mixtral, and Calm3, with added evolution history information.
  - Downloads: 68
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - This repository provides a templated dataset of ~40 Japanese downstream tasksâ€”with up to 20,000 samples per taskâ€”intended for instruction tuning LLMs using natively Japanese, non-translated data, offering both 0-shot and few-shot examples.
  - Downloads: 67
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository provides a Japanese preference dataset for Reinforcement Learning from Human Feedback (RLHF), created by scoring and ranking responses generated with Llama-Gemma and Qwen models based on a Self-Instruct dataset, and is subject to Meta Llama 3.1, Gemma, and Qwen licenses.
  - Downloads: 62
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - This dataset provides approximately 3000 Japanese childrenâ€™s stories generated by GPT-4o-mini, utilizing only simple vocabulary and based on the methodology detailed in the linked research paper.
  - Downloads: 55
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This dataset provides Japanese humorous response data from the Bokete website, including text-to-text, image-to-text, and text-image-to-text tasks, derived from the CLoT-Oogiri-Go dataset and intended for creative text generation research.
  - Downloads: 54
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - This dataset converts a subset of the LLM-JP-Corpus v3 (kaken) into Hugging Face format, adding article titles sourced from original URLs where available, all licensed under CC-BY 4.0.
  - Downloads: 47
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - This Japanese preference dataset, built upon llm-jp/magpie-sft-v1.0, uses Aratako/Llama-Gemma-2-27b-SFT-trial1 to regenerate responses and employs google/gemma-2-27b-it for judging preferred answers, inheriting licenses from META LLAMA 3.1, Gemma Terms of Use, and Qwen while requiring attribution for models trained with it.
  - Downloads: 46
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - This dataset contains Japanese question-answer pairs generated using a language model (DeepSeek-R1-Distill-Qwen-32B-Japanese) based on Japanese Wikipedia content, licensed under CC-BY-SA 4.0.
  - Downloads: 43
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªžæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
  - Downloads: 37
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends the CommonCatalog CC-BY dataset with English and Japanese captions generated by Phi-3 models, enabling commercial use under the CC BY license and easy streaming via a CSV file.
  - Downloads: 36
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for Supervised Fine-Tuning.
  - Downloads: 33
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - This repository presents the results of speech quality analysis using ReazonSpeech-v2 data with WADA SNR, saved as a JSON file containing filename, SNR values, and transcriptions, with 1,208,360 data points exceeding an SNR of 100, and supported by AiHUB's computing resources.
  - Downloads: 31
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This dataset is a Japanese translation of the Open_o1_sft_Pro dataset using Qwen2.5-14B-Instruct, formatted with user/assistant conversations and licensed according to the original dataset.
  - Downloads: 31
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - This dataset provides 3,907 three-line summaries of Livedoor News articles, formatted with prompts for Llama v2 and recommending the addition of "[R_START]" and "[R_END]" as special tokens for training.
  - Downloads: 30
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - This dataset converts level-2 filtered data from the llm-jp-corpus-v3 (WARP HTML) into Hugging Face format, adding article titles sourced from original URLs, and is licensed under CC-BY 4.0.
  - Downloads: 30
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - This Japanese dataset provides 10K-100K questions and answers labeled with â€œevilâ€ and â€œjusticeâ€, generated using anthracite-org/magnum-v4-12b, for both classification and generation tasks, and is licensed under Apache-2.0.
  - Downloads: 29
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - This repository provides automatically generated Japanese Q&A data sourced from both team-created data and Common Crawl, utilizing MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, and employing random text extraction to minimize reliance on original sourcesâ€”cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 28
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - This repository provides automatically generated Q&A data sourced from Common Crawl, utilizing Mixtral-8x22B-Instruct-v0.1-GGUF, with randomized text excerpts to reduce reliance on the original source, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 28
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset provides Japanese translations of English quotes from the original Hugging Face dataset, generated using the llm-jp/llm-jp-3-3.7b-instruct model and licensed under CC BY 4.0.
  - Downloads: 28
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - This repository provides a Japanese RLHF datasetâ€”a reformatted version of open_preference_v0.1â€”for reward model training, using 0 for rejected and 1 for chosen sentences, though the text quality is somewhat limited due to synthetic and translated content.
  - Downloads: 28
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset (BCP-47 ja-JP) designed to advance research in multimodal ad text generation models.
  - Downloads: 27
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - This repository provides a prototype dataset, generated by Deepseek-V3-0324, designed to improve adherence to constrained system prompts, with code and data released under the MIT license.
  - Downloads: 26
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository provides a Japanese preference dataset for SimPO training, created by scoring and ranking responses generated with Llama-Gemma and Qwen models based on a synthetic instruction dataset, and licensed under META LLAMA 3.1, Gemma Terms of Use, and Qwen.
  - Downloads: 23
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset provides Japanese translations of the in-foxhound instruction dataset, originally from glaive-ai, focusing on investment, Berkshire Hathaway, and Warren Buffett.
  - Downloads: 22
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - This repository provides a Japanese-English glossary of generative AI terminology, intended as a reference to improve translation quality with models like GPT-4, though accuracy isn't guaranteed.
  - Downloads: 20
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This repository provides a Japanese translation of the nlvr (Natural Language for Visual Reasoning) dataset, originally developed by lil-lab for multimodal reasoning tasks.
  - Downloads: 20
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - This repository provides a Japanese dataset generated by ELYZA-japanese-Llama-2 for evaluating AI-generated text detection, particularly with self-instruct methods, sourced from the GPT-4-Self-Instruct-Japanese dataset.
  - Downloads: 20
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This dataset facilitates evaluation of large language models for *okashi* (pun/gag) generation through two tasks: generating text responses to text prompts and generating text captions from images, featuring data for both task types and associated IDs/filenames.
  - Downloads: 20
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This dataset provides question data generated using Qwen/Qwen2.5-32B-Instruct with ollama, intended for building thinking models, and is licensed under Apache 2.0 *only* for the questions, not the answers.
  - Downloads: 19
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - This repository provides a Japanese dataset generated by Qwen/Qwen1.5-14B for evaluating AI text detection and self-instruct methods, sourced from GPT-4-Self-Instruct-Japanese instructions.
  - Downloads: 18
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset combines null-instruct-ja and DeepSeek-v2.5 (q4) models, generated using ollama and 7 A5000 GPUs in 2 hours 7 minutes, and is licensed under the DeepSeek license.
  - Downloads: 16
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - This repository provides Japanese translations of the aimo-validation-aime dataset, generated using ChatGPT-4o, with a unique 0-30 index differing from the standard AIME24 format.
  - Downloads: 15
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository provides a dataset of all *Keitai Okiri* (mobile comedy challenge) questions and responses from NHK broadcasts, scraped from a Hatenablog archive and structured into columns including question ID, episode, question text, and a list of responses with their IDs.
  - Downloads: 13
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - This repository provides 1k generated responses from DeepSeek-R1-Distill-Qwen-32B, fine-tuned on aya-ja-evol-instruct-calm3-dpo-masked, using 8-bit quantization, primarily for reference or pre-processing despite limited practical use and issues with `<think>` token generation.
  - Downloads: 13
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - This repository provides a synthetic instruction dataset, Jimba-Wiki-Instruction-Calm3, created using the Calm3-22B-Chat model and Japanese Wikipedia text, designed for reduced hallucination but requiring caution due to potential unfiltered outputs.
  - Downloads: 11
### Syntactic Text Processing
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully is a commercially-usable dataset of potentially harmful promptsâ€”in Japanese and other languagesâ€”intended solely for improving LLM safety, with restrictions on redistribution but allowance for derivative work acknowledging its use.
  - Downloads: 249
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - This dataset provides human-annotated preferencesâ€”including intensity scores from -3 to 3â€”for paired question-response sets in Japanese, covering diverse topics like general knowledge, history, medicine, coding, and creative tasks, but exhibits limited overall and per-genre variation.
  - Downloads: 232
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - This repository provides Japanese datasets transformed for easy SentenceTransformers training, particularly for contrastive learning, sourced and filtered from several Hugging Face datasets using re-ranking scores to create (anchor, positive/negative) pairs.
  - Downloads: 167
- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
  - This dataset aims to develop author identification models by learning writing styleâ€”not specific worksâ€”from cleaned Japanese text paragraphs (100-400 characters) authored by a list of 17 prominent writers, ensuring unique paragraphs and minimal label bias across train/validation splits.
  - Downloads: 164
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - This dataset contains 2139 human-evaluated question-answer pairs and associated ratings from LLMChat, a system used to benchmark 13 LLMsâ€”including Tanuki and Llama-3â€”via pairwise comparison, operating from August 19-25, 2024.
  - Downloads: 128
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - This repository provides Japanese text data extracted from PDF documents sourced from Common Crawl.
  - Downloads: 101
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - This repository provides a dataset of approximately 150,000 paired Danbooru and Japanese tags, filtered using fastText and the Calm3 LLM to improve accuracy and ensure at least one associated Japanese translation exists.
  - Downloads: 98
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k is a 97,269-record Japanese dialogue dataset created by applying the Magpie method to the Tanuki-8B-dpo-v1.0 model, potentially containing low-quality records due to minimal post-filtering.
  - Downloads: 87
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This repository integrates data from erai-raws and MyAnimeList for 2056 anime, providing IDs and resources like RSS feeds, publication dates, and links to various anime platforms (AniDB, Kitsu, LiveChart, MAL).
  - Downloads: 82
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 69
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - This dataset provides approximately 2800 high-quality images (beauty score 87+) of a generated â€œartificial girlfriendâ€ â€“ versions 2.1 & 2.6 â€“ designed to address portrait rights issues in realistic AI models, with over 1000 images scoring 90+ for beauty.
  - Downloads: 67
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki is a text dataset extracted from a Japanese Wikipedia HTML dump (January 1, 2024), offering clean, structurally-preserved text with paragraph information and accompanying preprocessing scripts for NLP tasks.
  - Downloads: 62
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - This repository provides a cleaned dataset of 950 Japanese math problemsâ€”translated from MetaMathQA using RekaAI/reka-flash-3â€”with formatting issues removed, requiring further output validation.
  - Downloads: 61
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - This repository provides a multilingual dataset released under the MIT license.
  - Downloads: 58
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - This dataset annotates the Aratako/Magpie-Tanuki-8B-97k datasetâ€”created by applying the Magpie method to Tanuki-8Bâ€”with difficulty, quality, and category labels using cyberagent/calm3-22b-chat, based on detailed annotation prompts.
  - Downloads: 54
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - This repository provides a 26,728-dataset subset of annotated Japanese instruction dataâ€”filtered for high quality and easeâ€”specifically for fine-tuning small Japanese chat LLMs (excluding coding) using the Qwen-2.5-turbo model, covering information seeking, reasoning, planning, and editing tasks.
  - Downloads: 52
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - Alpaca-jp-python is a Japanese synthetic dataset, curated by HachiML under Apache 2.0, generated using the Stanford Alpaca methodology with mistralai/Mixtral-8x22B-Instruct-v0.1 and refined versions are available via the `datasets` library.
  - Downloads: 44
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - This repository provides a CBZ-formatted manga dataset from Nhentai, including adult content and metadata, for research in image analysis, text recognition, and related fields.
  - Downloads: 35
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - This repository provides 300,000 Japanese image-caption pairs generated from alfredplpl/commoncatalog-cc-by-ja, with images resized to within 512px.
  - Downloads: 34
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - This repository provides a validated dataset of furigana (Japanese pronunciation guide) derived from the National Diet Library's bibliographic data, with 5064 inconsistencies corrected.
  - Downloads: 32
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja is a 525k instruction-tuning dataset created by automatically translating the open-source, high-quality ApolloCorpus multilingual medical dataset into Japanese, primarily for LLM use with caution due to potential translation errors.
  - Downloads: 32
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - This dataset provides furigana annotations derived from Aozora Bunko and SAPIE braille data, with 307 validation-corrected mismatches identified in the original corpus examples like incorrect readings in texts by Nakajima Atsushi and Hori Tatsuo.
  - Downloads: 30
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - This repository provides a Japanese question-answer dataset of approximately 1,300 pairs manually created for Databricks, sourced from blogs, FAQs, and Qitta articles by Databricks employees.
  - Downloads: 24
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - ã‚‹ã‚Šã®ã‚¹ãƒ†ãƒƒã‚«ãƒ¼ is a collection of fun stickers featuring the character Ruri.
  - Downloads: 22
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)
  - This dataset provides high-quality, human-verified synthetic data for fine-tuning reasoning models, featuring question-answer pairs with explicit &lt;think&gt;...&lt;/think&gt; tagged thought processes detailing the reasoning steps.
  - Downloads: 22
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - This repository likely contains resources related to the popular Japanese manga/anime character Chiikawa, potentially fan-created content or tools based on the series.
  - Downloads: 22
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Japanese LLaVA v1.5 Instruct 620K is a 620K image-text dataset translated to Japanese from LLaVA v1.5, intended for visual instruction tuning in Japanese language models under a CC BY-NC-4.0 license.
  - Downloads: 22
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This repository provides a dataset of fungal traitsâ€”morphological and ecological characteristics extracted from fungal descriptions using natural language processingâ€”currently for casual use only, as academic use is not permitted at this time.
  - Downloads: 22
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - This repository scrapes metadata (excluding full text) from the Dengeki Bunko novecomi website.
  - Downloads: 21
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - This repository provides automatically generated question-answer pairs sourced from various CC-BY/Apache-2.0 licensed datasets, modified with random text excerpts to reduce similarity to original sources, and requiring potential cleaning due to possible unnatural phrasing.
  - Downloads: 20
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - This repository provides a preprocessed dataset of 50,000 Delite posts by data creator t_w, optimized for embedding learning, with usage limited to learningâ€”redistribution is prohibited under Japanese law.
  - Downloads: 19
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - This dataset contains freely reusable quiz questions sourced from Quiz Forest as of August 5, 2024, suitable for RAG, document search, and other applications, with a permissive license allowing commercial and widespread distribution while requesting respectful usage.
  - Downloads: 19
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - This repository provides a 3-turn multi-turn instruction dataset generated with Qwen/Qwen2.5-32B-Instruct-AWQ, containing mixed English and Chinese data based on instructions from Aratako/Magpie-Tanuki-8B-annotated-96k.
  - Downloads: 19
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - This repository provides a preprocessed dataset of 50,000 Delite posts by data creator t_w, improved for embedding training and with structural changes, usable for learning but not redistribution under Japanese law.
  - Downloads: 19
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - This repository provides a modified parsing and chunking method for Wikipedia data, built upon a fork of singletongue/wikipedia-utils and utilizing data crawled between December 5-8, 2023.
  - Downloads: 18
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSECãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ likely provides resources and information related to the Japan System Evaluation Committee (JSEC), potentially including security evaluations and certifications.
  - Downloads: 18
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - This repository provides a 280-image dataset of female illustrations generated with NijiJourney v5, intended for LoRA model training and merging, with included tags and a disclaimer regarding copyrighted characters and responsible use.
  - Downloads: 18
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - This repository provides voice data for Chiaki Nanami from the *Danganronpa* series.
  - Downloads: 17
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthetic datasetâ€”created with the Evol-Instruction method and mistralai/Mixtral-8x22B-Instruct-v0.1â€”based on Stanford Alpaca's seed tasks, licensed under Apache 2.0, and accessible via Deepinfra.
  - Downloads: 16
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data is provided for unspecified use cases.
  - Downloads: 15
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - âš 
  - Downloads: 13
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, created with resources borrowed from IPA (Information-technology Promotion Agency, Japan) and intended solely for research purposes.
  - Downloads: 12
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - This dataset contains freely reusable quiz questions sourced from Quiz Works as of August 4-5, 2024, suitable for building RAG or document retrieval systems.
  - Downloads: 11
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, formatted as JSON and licensed under Apache-2.0, intended for training and evaluating large language models.
  - Downloads: 11
### Responsible NLP
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 891
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 824
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository provides a Japanese-only dataset extracted from Common Crawl using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 457
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset consists of manually extracted, high-quality Japanese Q&A pairs from Japanese government websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models.
  - Downloads: 390
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository provides the Japanese Web Corpus 2010, a dataset for research use limited by copyright law, with automatically added punctuation using morphological analysis and includes conversion scripts.
  - Downloads: 254
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This dataset provides filtered Japanese Wikipedia typo dataâ€”specifically kanji conversion errorsâ€”split into pre- and post-error text segments for use with Hugging Face models.
  - Downloads: 178
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - This dataset consists of question-answer pairs generated by an LLM based on paraphrased text from Japanese Wikipedia, and is released under a CC-BY-SA 4.0 license.
  - Downloads: 161
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - This repository provides texts regenerated with Phi-3 from large (tens of GB) datasets sourced from Wikibooks, Wikipedia, Cosmopedia, and legal precedents, potentially requiring Git LFS for full access due to size limitations with the `datasets` library, and utilizing computational resources from the TSUBAME4.0 supercomputer.
  - Downloads: 122
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - This repository provides a deduplicated and preprocessed query-passage dataset (mqa) with cleaned text and NFKC normalization, where passage IDs correspond to indices within the `collection` data.
  - Downloads: 88
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - This repository provides a large (tens of GB) Japanese-English parallel corpus generated by re-generating randomly sampled Japanese text from Wikibooks, Wikipedia, and code using Phi-3, potentially requiring Git LFS for full download due to its size.
  - Downloads: 76
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - This repository provides Japanese prompts from the GuanacoDataset, identified and extracted using language detection.
  - Downloads: 72
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - This dataset contains Japanese translations of English Wikipedia text generated by DeepSeek-R1-Distill-Qwen-32B, including input/raw output and licensed under CC-BY-SA 4.0.
  - Downloads: 56
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - This dataset converts the oasst1-89k-ja Japanese dataset into a ShareGPT-formatted, multi-turn conversation format suitable for fine-tuning large language models, requiring significant computational resources.
  - Downloads: 47
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - This repository provides automatically generated multi-turn dialogue data created with Calm3-22B-chat, based on randomly selected text from the Aozora Bunko library, specifically using a lightly cleaned version of *I Am a Cat* as the source.
  - Downloads: 39
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst2 dataset, automatically translated with DeepL and formatted for instruction/output-based fine-tuning of large language models, with provided code for conversion.
  - Downloads: 34
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty or liability, and ensure legal compliance in their use and distribution.
  - Downloads: 33
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - This repository provides data based on Japan Post's English, Chinese, and HS code translations for international mailing contents, as detailed on their website.
  - Downloads: 24
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This repository provides a Japanese-only subset of the Open Assistant dataset, sourced from the timdettmers/openassistant-guanaco Hugging Face dataset.
  - Downloads: 23
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - This repository provides a deduplicated and preprocessed version of the mmarco query-passage dataset, with IDs referencing indices within the `collection` subset for direct data access, adhering to the original dataset's license.
  - Downloads: 23
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - This dataset contains Japanese sentences generated by Phi-3 based on ConceptNet(5.7) triples, aiming to express relationships between subjects, relations, and objects.
  - Downloads: 22
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset comprises 1243 carefully selected tweets (May 2022 - May 2024) showcasing uniquely expressive and world-building language, intended for fine-tuning language models, particularly for character-driven tweet generation with fixed system prompts and inputs.
  - Downloads: 21
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - This repository provides automatically generated Japanese question-answer pairs for Retrieval-Augmented Generation (RAG) training, sourced from Wikibooks, Wikipedia, and case law data, intended for pre-training rather than instruction tuning, with some computations utilizing the TSUBAME4.0 supercomputer.
  - Downloads: 19
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - This dataset provides 20,000 automatically generated Japanese instruction-following examples, created with the LLM-JP 3.13B Instruct modelâ€”including Chain-of-Thought reasoning in some casesâ€”formatted as JSON for LLM training and evaluation.
  - Downloads: 19
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - This repository provides a low-quality dataset of 1002 examples for enabling Python function calling in chat LLMs, generated with Qwen2.5 and Phi-4, and containing potential issues with empty/Chinese tools and repetitive/low-quality responses.
  - Downloads: 18
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty or liability, and ensure compliance with applicable laws when using and sharing the content.
  - Downloads: 16
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset contains cleaned lines from the anime â€œMy Next Life as a Villainess,â€ featuring dialogue primarily from Lay (User) and Claire (Assistant) characters.
  - Downloads: 15
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset provides question-answer pairs regarding characters from the *Touhou Project*'s Tokama Club, structured as a CSV for use in chatbots, Q&A systems, and machine learning training.
  - Downloads: 14
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - This repository provides publicly released models and datasets under a license agreement requiring non-commercial use, disclaimer of warranty, and adherence to all applicable laws by both the user and any recipients.
  - Downloads: 14
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - This repository provides publicly released models and datasets under a license agreement prohibiting enjoyment of expressed ideas/emotions and disclaiming any warranty or liability for their use, requiring lawful compliance from both users and shared parties.
  - Downloads: 14
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use, disclaiming warranties, and requiring adherence to legal compliance by both users and any third parties they share the content with.
  - Downloads: 13
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This repository provides a 16,000-sample Japanese instruction-following dataset, automatically generated using the Qwen2.5 72B model, containing instructions, reasoning steps, initial responses, and refined answers in JSONL format for LLM training and evaluation.
  - Downloads: 13
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use, disclaiming warranties, and requiring adherence to all applicable laws by both the downloader and any recipients.
  - Downloads: 12
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, acknowledge no warranty or liability, and ensure legal compliance in their use and distribution.
  - Downloads: 11
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - This repository provides publicly available models and datasets under a license requiring users to agree to terms prohibiting enjoyment of expressed ideas/emotions, acknowledging no warranty or liability from the publisher, and ensuring compliance with laws and license terms by all users and shared parties.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository provides a Japanese-only dataset extracted from Common Crawl using cc-downloader-rs, created with resources from IPA (Information-promotion Agency) and intended for research purposes only due to copyright restrictions.
  - Downloads: 11
### Reasoning
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a high-quality, assured-correctness Japanese mathematical datasetâ€”synthesized via translation and reasoning from English datasets like PRM800K and GSM8Kâ€”designed for chain-of-thought reasoning tasks.
  - Downloads: 175
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using the Qwen3-235B-A22B model, formatted as JSONL for instruction tuning.
  - Downloads: 141
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - JSNLI is a Japanese natural language inference (NLI) dataset, a translation of the SNLI benchmark, provided in TSV format with morphologically analyzed text using JUMAN++.
  - Downloads: 119
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - This dataset, abc-multiple-choice, is a Japanese multiple-choice question answering dataset based on questions from the â€œabcâ€ quiz competition, intended for research purposes only and described in a NLP2024 publication.
  - Downloads: 114
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset is a small, high-quality Japanese dataset licensed for commercial use, featuring commonsense and mathematical question-answering data from multiple sources including MU-NLPC/Calc-ape210k.
  - Downloads: 108
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - JSQuAD is a Japanese reading comprehension dataset, based on SQuAD 1.1 and utilizing the 20211101 Japanese Wikipedia dump, released under a Creative Commons Attribution Share Alike 4.0 license.
  - Downloads: 105
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a human-crafted Japanese dataset of multi-turn conversations and passages for training and evaluating logical reasoning capabilities, demonstrated with Qwen2.5-7B models on Japanese MT-Bench.
  - Downloads: 99
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - This repository provides a 100k Japanese question answering and reasoning dataset, evaluated using Qwen/Qwen3-14B, and acknowledges support from the TPU Research Cloud program.
  - Downloads: 85
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - JCommonsenseQA is a Japanese multiple-choice question answering dataset, based on CommonsenseQA and ConceptNet, designed to evaluate commonsense reasoning abilities and is licensed under CC BY-SA 4.0.
  - Downloads: 81
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - This repository provides a commercially-usable, 1.8 million instruction-tuning datasetâ€”a Japanese translation of OpenMathInstruct-1â€”containing math questions from GSM8K & MATH benchmarks paired with Mixtral-8x7B-generated solutions validated for accuracy, and licensed under NVIDIA's permissive terms.
  - Downloads: 71
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - This repository provides a 17k synthetic instruction dataset for Japanese mathematical problems, generated using Magpie with the rinna/qwen2.5-bakeneko-32b-instruct model and filtered for consistent answers from two different system prompts â€“ one for logical assistance and another for Python-based problem-solving.
  - Downloads: 61
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - æ—¥æœ¬èªžæŒ‡ç¤ºãƒ»æŽ¨è«–ãƒ»å›žç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªžç‰ˆã®æŒ‡ç¤ºãƒ»æŽ¨è«–ãƒ»å›žç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 59
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100kã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 51
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - OpenO1-SFT (Japanese Translation) is a 77,312-sample dataset of Japanese-translated Chain of Thought reasoning examples, created using Gemma-2-27b-it, for fine-tuning language models.
  - Downloads: 46
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - This dataset contains Japanese question-answer pairs, reasoning traces, and corresponding text excerpts generated by DeepSeek-R1, based on the fineweb2-edu-japanese dataset, and licensed under ODC-By.
  - Downloads: 42
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository provides a high-information-density, multi-turn Japanese conversation dataset, synthetically created from the cosmopedia dataset, focusing on reasoning, knowledge, and conversational exchanges.
  - Downloads: 40
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - This repository offers a 50k-subset of the NuminaMath CoT dataset, enhanced to promote reflective, multistep reasoning in Japanese language models by encouraging repeated self-evaluation of solution steps.
  - Downloads: 37
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - This repository provides a 125,000-sample Japanese instruction-following dataset, automatically generated using the Qwen2.5-32B-instruct model with multi-persona diversification and Chain-of-Thought reasoning, formatted as JSONL under the Apache-2.0 license.
  - Downloads: 36
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - This dataset contains synthetically generated math instruction data created with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, filtered for consistent answers from two system prompts â€“ one for logical/math assistance and another for Python-based problem-solving.
  - Downloads: 32
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - This repository provides a high-quality, 1800-example Japanese instruction-following dataset, featuring instructions, reasoning processes, and answers, generated using the Qwen/Qwen2.5-32B-Instruct model.
  - Downloads: 31
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - This repository provides a 100k question-answering dataset in Japanese, generated using Gemini Pro 2.5, and licensed under the Open Data Commons Attribution license.
  - Downloads: 28
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suite, based on and extending the FraCaS dataset, for evaluating recognizing textual entailment (RTE) through premise-hypothesis pairs labeled with entailment, neutral, or contradiction judgements.
  - Downloads: 26
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - This dataset filters the magpie-reasoning-llama-nemotron-70b-100k dataset to exclude examples without â€œæ”¹è‰¯â€ in the refined_answer column and converts the results into OpenAI message format.
  - Downloads: 25
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - JCQ is a Japanese NLP dataset of 700 questions (100 per task) designed to evaluate creativity, inspired by the Torrance Test of Creative Thinking and published in the NLP2025 paper, covering tasks like unusual uses, consequences, and hypothetical scenarios.
  - Downloads: 22
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench is a Japanese reasoning benchmark utilizing challenging 2023 Kyoto University math entrance exam questions to evaluate the problem-solving abilities of Large Language Models.
  - Downloads: 21
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - This repository provides a 100k-sample Japanese translation of the NuminaMath CoT dataset, featuring 860k math problems with Chain of Thought solutions, translated using google/gemma-2-27b-it.
  - Downloads: 21
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE/JNLI is a Japanese Natural Language Inference (NLI) dataset for evaluating relationships â€“ entailment, contradiction, or neutral â€“ between premise and hypothesis sentences.
  - Downloads: 20
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - This dataset contains 200 simplified instruction prompts derived from Kendamarron/jimba-instuction-1k-beta, created to replicate the â€œWizard LMâ€ In-depth evolving process as a result of the LOCAL AI HACKATHON #000 collaboration.
  - Downloads: 16
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa is a 210-question, multi-turn Japanese language benchmark evaluating reasoning across seven domainsâ€”math, writing, coding, understanding, grammar, culture, and general logicâ€”with 30 questions per category.
  - Downloads: 15
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja ã®question_jaã‚’ã‚‚ã¨ã«phi-3-mediumã«ã‚ˆã‚Šãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžã‚’ç”¨ã„ãªã„å½¢å¼ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
### Responsible & Trustworthy NLP
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec is a machine learning framework and dataset for gender detection from Japanese names, detailed in a research paper accepted at ISDA'23, intended for research purposes only.
  - Downloads: 105
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - JaNLI is a Japanese adversarial Natural Language Inference (NLI) dataset, modeled after HANS, designed to challenge models with nuanced Japanese linguistics and reveal their weaknesses.
  - Downloads: 92
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - This repository releases a 42k Japanese-English parallel datasetâ€”a subset of Swallow-Magpie-Ultra-v0.1â€”designed for instruction tuning of Llama-3.1-Swallow language models.
  - Downloads: 75
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset is a Japanese language dataset for evaluating and mitigating toxicity in large language models, accessible via the linked GitLab repository.
  - Downloads: 60
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA is a 1402-question Japanese adversarial dataset for red teaming, designed to evaluate LLM vulnerability to generating harmful responses, and contains potentially offensive content.
  - Downloads: 43
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - This repository provides a dataset of Japanese physician licensing exam questions (NMLE, 110th-117th exams) for model evaluation, RAG, and overview purposes, licensed under CC-BY-NC-ND 4.0 (non-commercial use only).
  - Downloads: 30
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - This repository contains pairwise comparison data evaluating responses from two LLMChat models using various other models, created to verify agreement between human and automated (open LLM) evaluations, and licensed under the terms of team-hatakeyama-phase2/LLMChat.
  - Downloads: 20
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository provides a dataset detailing game states with unit-level informationâ€”including IDs, classes, teams, locations, and visibilityâ€”across various game snapshots and timestamps.
  - Downloads: 17
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - This dataset provides a 10,341-hour sample of unsupervised Japanese speech across 28 domains, offering quality-tested data for improved AI performance while prioritizing user privacy and legal compliance.
  - Downloads: 14
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - This repository provides access to scripts and instructions for generating a private, non-public dataset to prevent unintentional inclusion in LLM training.
  - Downloads: 11
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - Cauldron-JA is a Japanese vision-language dataset created by translating the Idefics2 fine-tuning dataset, The Cauldron, excluding OCR, coding, and graph-related subsets to preserve data consistency.
  - Downloads: 47,285
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This repository provides a Japanese translation of the databricks-dolly-15k dataset, licensed under CC-BY-SA-3.0.
  - Downloads: 666
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst1 dataset, including a flag indicating translation failures where â€œtextâ€ and â€œtext_enâ€ are identical, with recent updates involving manual correction of code-related translation errors and a chat-formatted version.
  - Downloads: 87
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka is a dataset of Japanese â€œkaidanâ€ (ghost stories) linked to the Hyakumonogatari ritual, offering a resource for exploring Japanese folklore and supernatural tales.
  - Downloads: 68
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository provides ~40 natively-Japanese, open-source datasets for downstream tasks and LLM instruction finetuning, avoiding machine-translated data.
  - Downloads: 57
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN is a deduplicated, Japanese-translated version of the CT-RATE datasetâ€”containing chest CT scans and radiology reportsâ€”designed to support the development of Japanese medical AI.
  - Downloads: 45
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This repository provides a 69K Japanese translation of the databricks-dolly-15k dataset, licensed under CC BY SA 3.0, for ja-en translation tasks.
  - Downloads: 34
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - Hoshikuzu/Japanese-Law-Translation is a 260k sentence Japanese-English parallel corpus of legal texts sourced from japaneselawtranslation.go.jp, accessible via the `datasets` library.
  - Downloads: 33
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - This repository provides a Japanese-Korean paired text dataset from Helsinki-NLP/Tatoeba-Challenge for training translation models, explicitly prohibiting commercial use.
  - Downloads: 26
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated Japanese datasetâ€”derived from a subset of ultrachat_200k with 6,537 training and 995 test samplesâ€”using DeepSeek-R1-Distill-Qwen-32B-Japanese, though some 'id' columns are currently missing.
  - Downloads: 13
### Information Retrieval
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - This repository provides reranking scores for existing Japanese search and QA datasets, using five multilingual/Japanese rerankers to score positive and negative example relevance, including average scores for each reranker.
  - Downloads: 485
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - This repository provides AutoWikiQA, the largest freely available Japanese question-answering dataset generated from Wikipedia text using Swallow-MX, notable for its diverse question-answer format and intended for QA model training and RAG development.
  - Downloads: 252
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This repository provides the passage dataset used in the book *Introduction to Large Language Models* and sourced from the cl-tohoku/quiz-datasets repository, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 85
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - This dataset provides QA pairs for training document retrieval models, sourced from cl-tohoku/quiz-datasets and incorporating quiz questions with varying licenses including CC BY-SA and GFDL, as used in the book â€œIntroduction to Large Language Models.â€
  - Downloads: 70
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset provides automatically translated Japanese versions of records 20,000 to 100,000 from the cosmopedia-100k dataset, excluding records with translation errors.
  - Downloads: 54
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - This dataset provides Japanese question-answering pairs with human-retrieved Wikipedia articles, detailing the data collection process and structure for research purposes.
  - Downloads: 46
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
  - JCrRAG is a 20,000-record Japanese benchmark evaluating the contextual relevance and information handling capabilities of Retrieval-Augmented Generation (RAG) systems, focusing on fixed context scenarios with multi-level complexity.
  - Downloads: 37
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - This repository provides 1 million synthetic, long-context, multi-turn chat entriesâ€”derived from rewritten web textâ€”for continued pre-training and research into LLM data/internet culture, building upon the Refined-Anime-Text thematic subset.
  - Downloads: 26
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - This dataset provides JSONL-formatted metadata for YouTube channels, labeled as either VTuber (1) or non-VTuber (0), to facilitate text classification model training and evaluation, primarily in Japanese with potential multilingual content and an imbalanced class distribution.
  - Downloads: 24
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - Jawiki-20220404-c400 is a Japanese Wikipedia passage dataset (â‰¤400 characters) used for question answering benchmarks, like those in the AIçŽ‹ competition.
  - Downloads: 14
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - This dataset archives 11 years of historical comments from the now-discontinued "Niconico Realtime Commentary" service, preserving a valuable record of user interactions before its 2020 renewal.
  - Downloads: 470,788
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100 is a 100-example Japanese instruction-tuning evaluation dataset featuring complex tasksâ€”including summarization, reasoning, and creative generationâ€”with annotated evaluation criteria for assessing helpful and polite AI assistant performance.
  - Downloads: 2,958
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - This dataset provides a binary positive/negative sentiment classification version of the WRIME Japanese sentiment analysis dataset, designed for use with the "Large Language Model Introduction" bookâ€™s sample code.
  - Downloads: 467
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - This repository provides the Sayoko TTS corpus, an 81-year-old female Japanese voice dataset with both noisy (wav_noise) and denoised (wav) audio files, alongside phoneme labels for speech synthesis tasks, downloadable from Google Drive or Hugging Face Hub.
  - Downloads: 157
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
  - This repository generates and filters Japanese sentences using the phi4 model, covering 53 grammatical patterns with a 2364-vocabulary base, focusing on diverse sentence structures like polite forms, desires, permissions, and more.
  - Downloads: 131
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - This repository provides a manually checked and corrected Japanese instruction dataset generated from the outputs of the calm2-7b-chat model, detailed in the linked Zenn article.
  - Downloads: 26
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - This dataset adapts the databricks-dolly-15k instruction-following dataset to mimic the emotionless speaking style of Yuki Nagato from "The Melancholy of Haruhi Suzumiya" through linguistic adjustments.
  - Downloads: 24
### Linguistics & Cognitive NLP
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - NewsQ is a free, Japanese question-answering benchmark for current events, distributed via Hugging Face with access granted upon agreement to terms and provision of user information for verification and communication.
  - Downloads: 41
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - This repository provides refined English translations of helpful, chosen texts from the `helpful-base` dataset within the `hh-rlhf` project, improved by filtering and correcting outputs from the fuguMT translation model.
  - Downloads: 21
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - NewsQ is a free, Japanese question-answering benchmark for current events, distributed via Hugging Face upon agreement to its terms of use and privacy policy, requiring a form submission for access.
  - Downloads: 21
