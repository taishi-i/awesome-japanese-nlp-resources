# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 459 models and 101 datasets are listed.

# Contents

 * [Models](#models)
 * [Datasets](#datasets)

## Models

This list is sorted by downloads as of May 13, 2024.

 * ğŸ“¥ 26775140 [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - BERT base Japanese (IPA dictionary)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
 * ğŸ“¥ 1213634 [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - This is a Japanese sentence-BERT model. æ—¥æœ¬èªç”¨Sentence-BERTãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³2ï¼‰ã§ã™ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ³1ã‚ˆã‚Šã‚‚è‰¯ã„ãƒ­ã‚¹é–¢æ•°ã§ã‚ã‚‹MultipleNegativesRankingLossã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸæ”¹è‰¯ç‰ˆã§ã™ã€‚æ‰‹å…ƒã®éå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³1ã‚ˆã‚Šã‚‚1.5ã€œ2ãƒã‚¤ãƒ³ãƒˆã»ã©ç²¾åº¦ãŒé«˜ã„çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦cl-tohoku/bert-base-japanese-whole-word-maskingã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚å¾“ã£ã¦ã€æ¨è«–ã®å®Ÿè¡Œã«ã¯fugashiã¨ipadicãŒå¿…è¦ã§ã™ï¼ˆpip install fugashi ipadicï¼‰ã€‚æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è§£èª¬https://qiita.com/sonoisa/items/1df94d0a98cd4f209051ãƒ¢ãƒ‡ãƒ«åã‚’"sonoisa/sentence-bert-base-ja-mean-tokens-v2"ã«æ›¸ãæ›ãˆã‚Œã°ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ãŸæŒ™å‹•ã«ãªã‚Šã¾ã™ã€‚ä½¿ã„æ–¹from transformers import BertJapaneseTokenizer, BertModelimport to
 * ğŸ“¥ 1046750 [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - xlm-roberta-ner-japanese(Japanese caption : æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.
 * ğŸ“¥ 516521 [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :)
 * ğŸ“¥ 509124 [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - shisa-gamma-7b-v1For more information see our main Shisa 7B modelWe applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest. Check out our JA MT-Bench results.
 * ğŸ“¥ 420870 [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - This is a Japanese sentence-LUKE model. æ—¥æœ¬èªç”¨Sentence-LUKEãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ—¥æœ¬èªSentence-BERTãƒ¢ãƒ‡ãƒ«ã¨åŒä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨­å®šã§å­¦ç¿’ã—ã¾ã—ãŸã€‚æ‰‹å…ƒã®éå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€æ—¥æœ¬èªSentence-BERTãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦å®šé‡çš„ãªç²¾åº¦ãŒåŒç­‰ã€œ0.5ptç¨‹åº¦é«˜ãã€å®šæ€§çš„ãªç²¾åº¦ã¯æœ¬ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒé«˜ã„çµæœã§ã—ãŸã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦studio-ousia/luke-japanese-base-liteã‚’åˆ©ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚æ¨è«–ã®å®Ÿè¡Œã«ã¯SentencePieceãŒå¿…è¦ã§ã™ï¼ˆpip install sentencepieceï¼‰ã€‚ä½¿ã„æ–¹from transformers import MLukeTokenizer, LukeModelimport torchclass SentenceLukeJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)self
 * ğŸ“¥ 185829 [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - BERT base Japanese (IPA dictionary, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
 * ğŸ“¥ 125132 [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework.
 * ğŸ“¥ 123052 [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfr
 * ğŸ“¥ 110673 [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * ğŸ“¥ 104772 [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - BERT base Japanese (character tokenization)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.
 * ğŸ“¥ 77775 [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¯ãƒãŒæµ·è¾ºã«è¡Œã£ã¦ã‚¢ã‚¶ãƒ©ã‚·ã¨å‹é”ã«ãªã‚Šã€æœ€çµ‚çš„ã«ã¯å®¶ã«å¸°ã‚‹ã¨ã„ã†ãƒ—ãƒ­ãƒƒãƒˆã®çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * ğŸ“¥ 60368 [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - This is a Japanese sentence-BERT model. æ—¥æœ¬èªç”¨Sentence-BERTãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³1ï¼‰ã§ã™ã€‚â€»: ç²¾åº¦ãŒ1.5ãƒã‚¤ãƒ³ãƒˆã»ã©å‘ä¸Šã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³2ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚ã‚Šã¾ã™ã€‚è§£èª¬https://qiita.com/sonoisa/items/1df94d0a98cd4f209051ä½¿ã„æ–¹from transformers import BertJapaneseTokenizer, BertModelimport torchclass SentenceBertJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)self.model = BertModel.from_pretrained(model_name_or_path)self.model.eval()if device is None:device = "cuda" if torch.cuda.is_ava
 * ğŸ“¥ 59719 [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - Sentence BERT base Japanese modelThis repository contains a Sentence BERT base model for Japanese. Pretrained modelThis model utilizes a Japanese BERT model colorfulscoop/bert-base-ja v1.0 released under Creative Commons Attribution-ShareAlike 3.0 as a pretrained model.
 * ğŸ“¥ 48568 [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’llm-book/ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«ã®ãƒ—ãƒ­é‡çƒé¸æ‰‹"# textä¸­ã®å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºpprint(ner_pipeline(text)) #
 * ğŸ“¥ 36492 [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - Model Card for Japanese DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-base-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-base-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶ è¨€èª
 * ğŸ“¥ 35226 [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã—ã‹ã—è¿½åŠ è‘—ä½œè€…ã¨ã—ã¦é–åŸéƒéƒ­ã®åå‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 27874 [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE) - LaBSEModel descriptionLanguage-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling.
 * ğŸ“¥ 25938 [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * ğŸ“¥ 25928 [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japaneseæ—¥æœ¬èªã®README/Japanese READMEGLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE. In order to create a general-purpose, user-friendly Japanese text embedding model, GLuCoSE has been trained on a mix of web data and various datasets associated with natural language inference and search.
 * ğŸ“¥ 17520 [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium) - OpenCALM-MediumModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 14498 [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 13867 [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - japanese-roberta-baseThis repository provides a base-sized Japanese RoBERTa model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * ğŸ“¥ 13658 [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * ğŸ“¥ 13281 [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 12699 [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - rinna/japanese-clip-vit-b-16This is a Japanese CLIP (Contrastive Language-Image Pre-Training) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models. How to use the modelInstall package$ pip install git+https://github.com/rinnakk/japanese-clip.gitRunimport ioimport requestsfrom PIL import Imageimport torchimport japanese_clip as ja_clipdevice = "cuda" if torch.cuda.is_available() else "cpu"model, preprocess = ja_clip.load("rinna/japanese-clip-vit-b-16", cache_dir="/tmp/japan
 * ğŸ“¥ 12559 [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix) - ã€å‘ŠçŸ¥ã€‘chilled_remixåŠã³reversemixã¯2023å¹´5æœˆ21æ—¥ã«Versionå¤‰æ›´ã‚’è¡Œã„ã€v2ã¸ç§»è¡Œã„ãŸã—ã¾ã—ãŸã€‚ ä¼´ã„v1ã¯å‰Šé™¤è‡´ã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 11143 [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - reazonspeech-nemo-v2reazonspeech-nemo-v2 is an automatic speech recognition model trainedon ReazonSpeech v2.0 corpus. This model supports inference of long-form Japanese audio clips up toseveral hours.
 * ğŸ“¥ 10859 [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 9977 [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¯ãƒãŒæµ·è¾ºã«è¡Œã£ã¦ã‚¢ã‚¶ãƒ©ã‚·ã¨å‹é”ã«ãªã‚Šã€æœ€çµ‚çš„ã«ã¯å®¶ã«å¸°ã‚‹ã¨ã„ã†ãƒ—ãƒ­ãƒƒãƒˆã®çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * ğŸ“¥ 9885 [Lasorco/lametta](https://huggingface.co/Lasorco/lametta) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ï¼Ÿ å€‹äººçš„ãªæ™®æ®µé£ã„ã®ãŸã‚ã«ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€ç™–ãŒå¼·ã„ã¨æ€ã„ã¾ã™ã€‚
 * ğŸ“¥ 9256 [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - OpenCALM-3BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-3b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-3b")inputs = tokenizer("AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * ğŸ“¥ 9129 [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ä»•äº‹ã®ç†±æ„ã‚’å–ã‚Šæˆ»ã™ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’5ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-13b-instruct"tokenizer = AutoTokenizer.from_pretrained(model_name
 * ğŸ“¥ 9111 [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000) - alabnii/jmedroberta-base-sentencepiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "æ‰æœ¬æµ·äºº and å£¹å²å¤ªä¸€ and çŸ¥ç”°æ‚ ç”Ÿ and é‡‘æ²¢è¼ä¸€ and ç›¸æ¾¤å½°å­",title =     "J{M}ed{R}o{BERT}a: æ—¥æœ¬èªã®åŒ»å­¦è«–æ–‡ã«ã‚‚ã¨ã¥ã„ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡",booktitle = "è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š",y
 * ğŸ“¥ 7983 [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - OpenCALM-7BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-7b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-7b")inputs = tokenizer("AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * ğŸ“¥ 7919 [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - sbert-jsnli-luke-japanese-base-liteThis is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The base model is studio-ousia/luke-japanese-base-lite and was trained 1 epoch with shunk031/jsnli.
 * ğŸ“¥ 7884 [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
 * ğŸ“¥ 7812 [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * ğŸ“¥ 6638 [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - gpt-neox-japanese-2.7bThe open PR is merged on 2022/9/14.You can use this model with v4.23 and higher versions of transformers as follows,pip install transformersThis repository provides a 2.7B-parameter Japanese GPT-NeoX-based model. The model was trained by ABEJA, IncHow to useWhen using pipeline for text generation.from transformers import pipelinegenerator
 * ğŸ“¥ 6491 [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - Japanese Stable LM Base Gamma 7BModel DescriptionThis is a 7B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * ğŸ“¥ 6162 [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-7b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-
 * ğŸ“¥ 6087 [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - ELYZA-japanese-Llama-2-13b-fast-instructModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ä»•äº‹ã®ç†±æ„ã‚’å–ã‚Šæˆ»ã™ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’5ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-13b-fast-instruct"tokenizer = AutoTokenizer.from_pr
 * ğŸ“¥ 6051 [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - umiyuki-Japanese-Chat-Umievo-itr001-7b-ggufumiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Japanese-Chat-Umievo-itr001-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'umiyuki-Japanese-Chat-Umievo-itr001-7b-Q4_0.gguf' -p "[INST] ä»Šæ™©ã®å¤•é£Ÿã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦
 * ğŸ“¥ 5823 [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese) - This is a Japanese sentence-LUKE model. æ—¥æœ¬èªç”¨Sentence-LUKEãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ—¥æœ¬èªSentence-BERTãƒ¢ãƒ‡ãƒ«ã¨åŒä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨­å®šã§å­¦ç¿’ã—ã¾ã—ãŸã€‚æ‰‹å…ƒã®éå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€æ—¥æœ¬èªSentence-BERTãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦å®šé‡çš„ãªç²¾åº¦ãŒåŒç­‰ã€œ0.5ptç¨‹åº¦é«˜ãã€å®šæ€§çš„ãªç²¾åº¦ã¯æœ¬ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒé«˜ã„çµæœã§ã—ãŸã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦studio-ousia/luke-japanese-base-liteã‚’åˆ©ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚æ¨è«–ã®å®Ÿè¡Œã«ã¯SentencePieceãŒå¿…è¦ã§ã™ï¼ˆpip install sentencepieceï¼‰ã€‚ä½¿ã„æ–¹from transformers import MLukeTokenizer, LukeModelimport torchclass SentenceLukeJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)self
 * ğŸ“¥ 5546 [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - Kotoba-WhisperKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteenAsahi Ushio and Kotoba Technologies. Following the original work of distil-whisper (Robust Knowledge Distillation via Large-Scale Pseudo Labelling),we employ OpenAI's Whisper large-v3 as the teacher model, and the student model consists the full encoder of theteacher large-v3 model and the decoder with two layers initialized from the first and last layer of the large-v3 model.
 * ğŸ“¥ 5175 [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - Japanese Stable LM Instruct Gamma 7BModel DescriptionThis is a 7B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese Stable LM Base Gamma 7B.If you are in search of a smaller model, please check Japanese StableLM-3B-4E1T Instruct.
 * ğŸ“¥ 5171 [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)This is an ELECTRA model pretrained on approximately 200M Japanese sentences extracted from the mC4 and finetuned by spaCy v3 on UD_Japanese_BCCWJ r2.8.The base pretrain model is megagonlabs/transformers-ud-japanese-electra-base-discrimininator. The entire spaCy v3 model is distributed as a python package named ja_ginza_electra from PyPI along with GiNZA v5 which provides some custom pipeline components to recognize the Japanese b
 * ğŸ“¥ 4985 [mmnga/gemma-7b-it-gguf](https://huggingface.co/mmnga/gemma-7b-it-gguf) - gemma-7b-it-ggufgoogleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-7b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ç¾åœ¨é‡å­åŒ–ã•ã‚ŒãŸå‡ºåŠ›ãŒä¸å®‰å®šãªå•é¡ŒãŒã‚ã‚‹ã‚‰ã—ãQ8_0ã‚’æ¨å¥¨ã—ã¾ã™ã€‚gemma : token_embd.weight ãƒ†ãƒ³ã‚½ãƒ«ã« Q8_0 ã‚’ä½¿ç”¨ã—ã¾ã™ #5650Licencegemma-terms-of-use åˆ©ç”¨è¦ç´„ã‚’ã”åˆ©ç”¨å‰ã«å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'gemma-7b-it-q4_0.gguf' -p "&lt;start_of_turn&gt;user\næ—¥æœ¬ã®æ–‡åŒ–ã‚’ï¼‘ï¼å€‹æ•™ãˆã¦ã€‚&lt;end_of_t
 * ğŸ“¥ 4965 [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 4925 [rinna/youri-7b](https://huggingface.co/rinna/youri-7b) - rinna/youri-7bOverviewWe conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * ğŸ“¥ 4801 [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM,
 * ğŸ“¥ 4737 [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 4704 [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf) - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufaixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8b-Cosmopedia-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufmmnga/aixsatoshi-Honyaku-7b-v2-ggufmmnga/aixsatoshi-Honyaku-Multi-Translator-Swallow-ms7b-ggufmmnga/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-ggufmmnga/aixsatoshi-Mixtral-8x7B-ja-sft-ChatbotArenaJAcalm2-bnb4bitmmnga/aixsatoshi-calm2-7b-chat-7b-moe-ggufUsagegit c
 * ğŸ“¥ 4679 [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 4609 [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - lightblue-suzume-llama-3-8B-japanese-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-ggufmmnga/lightblue-suzume-llama-3-8B-multilingual-ggufmmnga/lightblue-suzume-llama-3-8B-japanese-ggufmmnga/lightblue-ao-karasu-72B-ggufmmnga/lightblue-karasu-1.1B-ggufmmnga/lightblue-karasu-7B-chat-plus-unleashed-ggufmmnga/lightblue-qarasu-14B-chat-plus-unleashed-ggufUsagegit clone https://github.com
 * ğŸ“¥ 4589 [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - haqishen-Llama-3-8B-Japanese-Instruct-ggufhaqishenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Japanese-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/haqishen-Llama-3-8B-Japanese-Instruct-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * ğŸ“¥ 4582 [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese-char-wwm')m
 * ğŸ“¥ 4535 [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 4488 [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - ELYZA-japanese-Llama-2-13b-fastModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM,
 * ğŸ“¥ 4444 [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT) - ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªç‰ˆã¯ã¾ã ä½œæˆä¸­ã§ã™ã€‚ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚IntroDetailed report in the arXiv ReportIf you just want to check out how to use the model, please check out the Usage section below!Welcome to JaColBERT version 1, the initial release of JaColBERT, a Japanese-only document retrieval model based on ColBERT.It outperforms previous common Japanese models used for document retrieval, and gets close to the performance of multilingual models, despite the evaluation datasets being out-of-domain for our models but in-domain for multi
 * ğŸ“¥ 4399 [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - BERT base Japanese (character tokenization, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.
 * ğŸ“¥ 4316 [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¯ãƒãŒæµ·è¾ºã«è¡Œã£ã¦ã‚¢ã‚¶ãƒ©ã‚·ã¨å‹é”ã«ãªã‚Šã€æœ€çµ‚çš„ã«ã¯å®¶ã«å¸°ã‚‹ã¨ã„ã†ãƒ—ãƒ­ãƒƒãƒˆã®çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * ğŸ“¥ 4296 [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - japanese-gpt2-smallThis repository provides a small-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * ğŸ“¥ 4115 [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos) - bert-base-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-char-extended. Every short-unit-word is tagged by UPOS (Universal Part-Of-Speech).How to Useimport torchfrom transformers import AutoTokenizer,AutoModelForTokenClassificationtokenizer=AutoTokenizer.from_pretrained("KoichiYasuoka/bert-base-japanese-upos")model=AutoModelForTokenClassification.from_pretrained("KoichiYasuoka/bert-base-japane
 * ğŸ“¥ 4090 [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. æ¬¡ã®æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆç´„100GBï¼‰ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸT5 (Text-to-Text Transfer Transformer) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 3879 [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - OpenCALM-LargeModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 3762 [Helsinki-NLP/opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - en-jasource group: Englishtarget group: JapaneseOPUS readme: eng-jpnmodel: transformer-alignsource language(s): engtarget language(s): jpnmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus+bt-2021-04-10.ziptest set translations: opus+bt-2021-04-10.test.txttest set scores: opus+bt-2021-04-10.eval.txtBenchmarkstestsetBLEUchr-F#sent#wordsBPTatoeba-test.eng-jpn15.20.25810000992061.000System Info:hf_name: en-jasource_languages: engtarget_languages
 * ğŸ“¥ 3759 [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1) - FINGU-AI/FinguAI-Chat-v1OverviewThe FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks. It aims to enhance language proficiency while providing insights into global finance markets and regulatory landscapes.
 * ğŸ“¥ 3720 [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK) - ğŸˆ FlexDreamHKFlexDreamHKã¯ãƒªãƒ¼ã‚¯ã•ã‚ŒãŸNovelAIãƒ¢ãƒ‡ãƒ«ã®å…¥ã£ã¦ã„ãªã„ã€ã‚ã‚‹ã„ã¯ãã®ãƒªã‚¹ã‚¯ã‚’å¯èƒ½ãªé™ã‚Šä½ãã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ ãƒ¢ãƒ‡ãƒ«åã¯ãƒãƒ¼ã‚¸ã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ãŸã¡ã«æ•¬æ„ã‚’è¡¨ã—ã€ä¸»è¦ãªãƒ¢ãƒ‡ãƒ«åã‚’çµ„ã¿åˆã‚ã›ã¦å‘½åã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 3582 [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) - bilingual-gpt-neox-4bOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters. LibraryThe model was trained using code based on EleutherAI/gpt-neox.
 * ğŸ“¥ 3507 [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base) - rinna/japanese-hubert-baseOverviewThis is a Japanese HuBERT Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Base model, which contains 12 transformer layers with 12 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * ğŸ“¥ 3472 [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - Japanese SimCSE (BERT-base)æ—¥æœ¬èªã®README/Japanese READMEsummarymodel name: pkshatech/simcse-ja-bert-base-clcmlpThis is a Japanese SimCSE model. You can easily extract sentence embedding representations from Japanese sentences.
 * ğŸ“¥ 3388 [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - shisa-base-7b-v1shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training. Japanese tokens were sourced from MADLAD-400, using DSIR, along with 10% English tokens sampled from a mix of MADLAD-400 EN and various open datasources added in to prevent catastrophic forgetting.
 * ğŸ“¥ 3350 [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - LINE DistilBERT JapaneseThis is a DistilBERT model pre-trained on 131 GB of Japanese web text.
 * ğŸ“¥ 3220 [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - Shisa 7BShisa 7B (shisa-7b-v1) is a bilingual Japanese and English (JA/EN) general-purpose chat model that aims to achieve strong Japanese language performance while retaining robust English capabilities, using a synthetic-data driven approach. This model is based on Mistral 7B with a custom JA-optimized extended tokenizer that is &gt;2X more efficient in Japanese than Mistral's original tokenizer.
 * ğŸ“¥ 3117 [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b) - CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets. Variant: CyberAgentLM2-ChatRequirementstransformers &gt;= 4.34.1accelerateUsageimport transformersfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamerassert transformers.__version__ &gt;= "4.34.1"model = AutoModelForCausalLM.from_pretrained("cyberagent/calm2-7b", device_map="auto", torch_dtype="auto")tokenizer = Au
 * ğŸ“¥ 3094 [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 3081 [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - Model Card for Japanese DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶ è¨€èª
 * ğŸ“¥ 3058 [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese-char-wwm
 * ğŸ“¥ 3028 [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k) - bilingual-gpt-neox-4b-8kOverviewNotice: This model requires transformers&gt;=4.31.0 to work properly. This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * ğŸ“¥ 2867 [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - japanese-gpt2-xsmallThis repository provides an extra-small-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * ğŸ“¥ 2783 [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) - japanese-gpt-neox-3.6bOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. LibraryThe model was trained using code based on EleutherAI/gpt-neox.
 * ğŸ“¥ 2674 [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct) - c4ai-command-r-v01-japanese-instructGGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF versionæ¦‚è¦CohereForAI/c4ai-command-r-v01ã‚’ã€ichikara-instructionã‚’ä½¿ã£ã¦è¿½åŠ ã§æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å­¦ç¿’ã®è¨­å®šRunpodã§GPUã‚µãƒ¼ãƒã‚’å€Ÿã‚Šã€A6000x4ã§å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚ä¸»ãªå­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚lora_r: 64lisa_alpha: 128lora_dropout: 0.05lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]learning_rate: 2e-5num_train_epochs: 10epochsbatch_size: 50max_seq_length: 2048è©•ä¾¡jsquad(jsquad-1.1-0.3, 2-shots)ã€jcommonsenseqa(jcommonsenseqa-1.1-0
 * ğŸ“¥ 2665 [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * ğŸ“¥ 2661 [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - japanese-gpt2-mediumThis repository provides a medium-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * ğŸ“¥ 2658 [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator) - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) - MIYAGINOThis is an ELECTRA model pretrained on approximately 200M Japanese sentences.
 * ğŸ“¥ 2599 [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora. KARAKURI LM Chat is a fine-tuned version of KARAKURI LM, which was trained on a mixture of publicly available and closed datasets using the SteerLM technique.
 * ğŸ“¥ 2525 [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf) - ELYZA-japanese-Llama-2-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruc
 * ğŸ“¥ 2510 [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo) - bilingual-gpt-neox-4b-instruction-ppoOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters. The model is based on rinna/bilingual-gpt-neox-4b-instruction-sft and has been aligned to serve as an instruction-following conversational agent.
 * ğŸ“¥ 2394 [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - Japanese-StableLM-Instruct-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data. This model is also available in a smaller 7b version, or a smaller and faster version with a specialized tokenizer.
 * ğŸ“¥ 2312 [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - hotchpotch/japanese-reranker-cross-encoder-large-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfro
 * ğŸ“¥ 2283 [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-gg
 * ğŸ“¥ 2238 [SakanaAI/EvoLLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B) - ğŸŸ EvoLLM-JP-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦ TwitterEvoLLM-JP-v1-7B is an experimental general-purpose Japanese LLM.
 * ğŸ“¥ 2160 [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) - japanese-gpt-neox-3.6b-instruction-ppoOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b-instruction-sft-v2 and has been aligned to serve as an instruction-following conversational agent.
 * ğŸ“¥ 2154 [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr) - hubert-base-asrThis model is a fine-tuned version of rinna/japanese-hubert-base on the common_voice_11_0 dataset for ASR tasks. This model can only predict Hiragana.
 * ğŸ“¥ 2153 [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF) - Local-Novel-LLM-projectæ§˜ã® Assistance ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚ Ké‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã‚‚iMatrixé©ç”¨ã—ã¦ã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 2104 [SakanaAI/EvoVLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B) - ğŸŸ EvoVLM-JP-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦ TwitterEvoVLM-JP-v1-7B is an experimental general-purpose Japanese VLM.This model was created using the Evolutionary Model Merge method.
 * ğŸ“¥ 2070 [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-novel-gpt-j-6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚æ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake
 * ğŸ“¥ 2031 [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF) - Japanese-Starling-ChatV-7B-GGUFGGUF conversion of "Japanese-Starling-ChatV-7B""Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.I applied the chat vector acquired by subtracting the weights of Mistral-7B-v0.1 from the weights of "Starling-LM-7B-beta" to this model. ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯chatntq-ja-7b-v1.0ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚é«˜æ€§èƒ½ã®è‹±èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Starling-LM-7B-betaã®é‡ã¿ã‹ã‚‰Mistral-7B-v0.1ã®é‡ã¿ã‚’å·®ã—å¼•ãã“ã¨ã§å¾—ãŸchat vectorã‚’é©ç”¨ã—ã¦ã„ã¾ã™ï¼ˆãƒ–ãƒ­ã‚°è¨˜äº‹ï¼‰ã€‚PerformanceModel(Q8_0 quan
 * ğŸ“¥ 2019 [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - wav2vec2-base-asrThis model is a fine-tuned version of rinna/japanese-wav2vec2-base on the common_voice_11_0 dataset for ASR tasks. This model can only predict Hiragana.
 * ğŸ“¥ 2007 [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr) - hubert-large-asrThis model is a fine-tuned version of rinna/japanese-hubert-large ASR. Initially fine-tuned on the reazonspeech(small) dataset, it was subsequently further fine-tuned on the common_voice_11_0 dataset for ASR tasks.
 * ğŸ“¥ 1905 [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - Japanese-StableLM-Base-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. For an instruction-following model, check Japanese-StableLM-Instruct-Beta-70B.
 * ğŸ“¥ 1813 [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - OpenCALM-1BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-1b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-1b")inputs = tokenizer("AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * ğŸ“¥ 1780 [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - rinna/japanese-cloob-vit-b-16This is a Japanese CLOOB (Contrastive Leave One Out Boost) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models. How to use the modelInstall package$ pip install git+https://github.com/rinnakk/japanese-clip.gitRunimport ioimport requestsfrom PIL import Imageimport torchimport japanese_clip as ja_clipdevice = "cuda" if torch.cuda.is_available() else "cpu"model, preprocess = ja_clip.load("rinna/japanese-cloob-vit-b-16", device=device)tokenizer =
 * ğŸ“¥ 1753 [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - stockmark/stockmark-13bStockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.
 * ğŸ“¥ 1749 [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - japanese-sentiment-analysisThis model was trained from scratch on the chABSA dataset. It achieves the following results on the evaluation set:Loss: 0.0001Accuracy: 1.0F1: 1.0Model descriptionModel Train for Japanese sentence sentiments.
 * ğŸ“¥ 1696 [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ï¼˜ã¤ã®æ„Ÿæƒ…ï¼ˆå–œã³ã€æ‚²ã—ã¿ã€æœŸå¾…ã€é©šãã€æ€’ã‚Šã€æã‚Œã€å«Œæ‚ªã€ä¿¡é ¼ï¼‰ã®å†…ã€ã©ã®æ„Ÿæƒ…ãŒæ–‡ç« ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã‹åˆ†æã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯wrimeãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆhttps://huggingface.co/datasets/shunk031/wrimeï¼‰ã‚’ç”¨ã„ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚This model is based on Luke-japanese-large-liteThis model is fine-tuned model which besed on studio-ousia/Luke-japanese-large-lite. This could be able to analyze which emotions (joy or sadness or anticipation or surprise or anger or fear or disdust or trust ) are included.
 * ğŸ“¥ 1689 [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 1591 [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - stockmark-gpt-neox-japanese-1.4b-ggufstockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gpt-neox-japanese-1.4bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚æ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake
 * ğŸ“¥ 1573 [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2) - Japanese-StableLM-Instruct-Alpha-7B-v2"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets. UsageFirst install additional dependencies in requirements.txt:pip install sentencepiece einopsThen start generating text with japanese-stablelm-instruct-alpha-7
 * ğŸ“¥ 1551 [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct) - Japanese StableLM-3B-4E1T InstructModel DescriptionThis is a 3B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese StableLM-3B-4E1T Base. If you are in search of a larger model, please check Japanese Stable LM Instruct Gamma 7B.Usageimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("stabilityai/japanese-stablelm-3b-4e1t-instruct")model = AutoModelForCausalLM.from_p
 * ğŸ“¥ 1549 [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old) - oldï¼Ÿ ä¸€åº¦ä¸Šã’ãŸãƒ¢ãƒ‡ãƒ«ã¯ä½•ã‚‰ã‹ã®å•é¡ŒãŒãªã„é™ã‚Šã„ã¤ã§ã‚‚èª°ã§ã‚‚ä½¿ãˆã‚‹ã¹ãã¨æ€ã†ã®ã§ã“ã“ã¯ãã‚“ãªä¿ç®¡åº«ã§ã™æ—§ãƒ¢ãƒ‡ãƒ«ã€æ²¡ãƒ¢ãƒ‡ãƒ«ã€ãƒ¡ã‚¤ãƒ³å¤–ã®ãŠéŠã³é›‘ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ãŒé›‘ã«ãŠã„ã¦ã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 1542 [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - OpenCALM-SmallModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 1527 [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - japanese-gpt-1bThis repository provides a 1.3B-parameter Japanese GPT model. The model was trained by rinna Co., Ltd.How to use the modelimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("rinna/japanese-gpt-1b", use_fast=False)model = AutoModelForCausalLM.from_pretrained("rinna/japanese-gpt-1b")if torch.cuda.is_available():model = model.to("cuda")text = "è¥¿ç”°å¹¾å¤šéƒã¯ã€"token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors="pt")wi
 * ğŸ“¥ 1453 [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - BERTã«ã‚ˆã‚‹æ—¥æœ¬èªå›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«BertForTokenClassificationã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã®æ–‡ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ã€‚æŠ½å‡ºã•ã‚Œã‚‹å›ºæœ‰è¡¨ç¾ã®ã‚¿ã‚¤ãƒ—ã¯ã€ä»¥ä¸‹ã®8ç¨®é¡ã§ã™ã€‚äººåæ³•äººåï¼ˆæ³•äººã¾ãŸã¯æ³•äººã«é¡ã™ã‚‹çµ„ç¹”ï¼‰æ”¿æ²»çš„çµ„ç¹”åï¼ˆæ”¿æ²»çš„çµ„ç¹”åã€æ”¿å…šåã€æ”¿åºœçµ„ç¹”åã€è¡Œæ”¿çµ„ç¹”åã€è»éšŠåã€å›½éš›çµ„ç¹”åï¼‰ãã®ä»–ã®çµ„ç¹”å	ï¼ˆç«¶æŠ€çµ„ç¹”åã€å…¬æ¼”çµ„ç¹”åã€ãã®ä»–ï¼‰åœ°åæ–½è¨­åè£½å“åï¼ˆå•†å“åã€ç•ªçµ„åã€æ˜ ç”»åã€æ›¸ç±åã€æ­Œåã€ãƒ–ãƒ©ãƒ³ãƒ‰åç­‰ï¼‰ã‚¤ãƒ™ãƒ³ãƒˆåä½¿ç”¨æ–¹æ³•å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆtransformersã€unidic_liteã€fugashiï¼‰ã‚’pipãªã©ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã€ä¸‹è¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã™ã€‚from transformers import BertJapaneseTokenizer, BertForTokenClassificationfrom transformers import pipelinemodel = BertForTokenClassification.from_pretrained("jurabi/bert-ner-japanese")tokenizer = BertJapaneseTokeniz
 * ğŸ“¥ 1427 [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * ğŸ“¥ 1400 [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning. This model is a quantized(miniaturized to 4.11GB) version of the original model(13.69GB).Model DetailsQuantization reduces the amount of memory required and improves execution speed, but unfortunately performance deteriorates.
 * ğŸ“¥ 1332 [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - bert-finetuned-japanese-sentimentThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset. Model descriptionModel Train for amazon reviews Japanese sentence sentiments.
 * ğŸ“¥ 1332 [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft) - japanese-large-lm-3.6b-instruction-sftThis repository provides a 3.6B parameters Japanese language model, fine-tuned and trained by LINE Corporation. For Japaneseè©³ç´°ãªèª¬æ˜ã‚„å®Ÿé¨“ã«é–¢ã—ã¦ã¯ã€ŒInstruction Tuningã«ã‚ˆã‚Šå¯¾è©±æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸ3.6Bæ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã™ã€ã‚’ã”è¦§ãã ã•ã„ã€‚How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinetokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-3.6b-instruction-sft", use_fast=False)model = AutoModelForCausalLM.from_pretrained("line-corporation/japanese
 * ğŸ“¥ 1331 [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora. KARAKURI LM Chat is a fine-tuned version of KARAKURI LM, which was trained on a mixture of publicly available and closed datasets using the SteerLM technique.
 * ğŸ“¥ 1259 [KBlueLeaf/guanaco-7b-leh-v2](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2) - Guanaco-leh-V2: A Multilingual Instruction-Following Language Model Based on LLaMA 7BThis model is trained with guanaco-lora with lora + embed_tokens
 * ğŸ“¥ 1214 [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 1175 [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - nlp-waseda/roberta-large-japanese-seq512Model descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")se
 * ğŸ“¥ 1165 [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - bert-base-japanese-v3-marc_jaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®MARC-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinetext_classification_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-marc_ja")print(text_classification_pipeline("ä¸–ç•Œã«ã¯è¨€è‘‰ãŒã‚ã‹ã‚‰ãªãã¦ã‚‚æ„Ÿå‹•ã™ã‚‹éŸ³æ¥½ãŒã‚ã‚‹ã€‚")[0])# {'label': 'positive', 'score': 0.9993619322776794}ãƒ©ã‚¤ã‚»ãƒ³ã‚¹Apache License 2.0
 * ğŸ“¥ 1142 [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b) - Japanese-StableLM-Instruct-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-base-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * ğŸ“¥ 1142 [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 1127 [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 1115 [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF) - Japanese-WizardLM2-ChatV-7B-GGUFGGUF conversion of "Japanese-WizardLM2-ChatV-7B"This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.We aimed to add the high performance of WizardLM-2 to the Japanese language capability of ChatNTQ.ã“ã®ãƒ¢ãƒ‡ãƒ«ã€Japanese-WizardLM2-ChatV-7Bã¯ã€â€chatntq-ja-7b-v1.0â€ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€"WizardLM-2-7b"ã‹ã‚‰"Mistral-7B-v0.1"ã‚’å·®ã—å¼•ã„ã¦ä½œã£ãŸChatVectorã‚’1.0å€ã§è¶³ã—ã¾ã—ãŸã€‚ChatNTQã®æ—¥æœ¬èªèƒ½åŠ›ã«WizardLM
 * ğŸ“¥ 1095 [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1) - Swallow-MX-8x7b-NVE-v0.1Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data. Model DetailsModel type: Please refer to Mixtral technical report for details on the model architecture.
 * ğŸ“¥ 1049 [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) - japanese-gpt-neox-3.6b-instruction-sftOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b and has been finetuned to serve as an instruction-following conversational agent.
 * ğŸ“¥ 1039 [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) - Japanese-StableLM-Base-Alpha-7B"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance. For an instruction-following model, check Japanese-StableLM-Instruct-Alpha-7B and get access by accepting the terms and conditions.
 * ğŸ“¥ 1037 [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - japanese-gpt-neox-smallThis repository provides a small-sized Japanese GPT-NeoX model. The model was trained using code based on EleutherAI/gpt-neox.
 * ğŸ“¥ 1029 [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha [Kaggle] [X] [LinkedIn]This is a meta-llama/Meta-Llama-3-8B-Instruct model that finetuned on Japanese conversation dataset.
 * ğŸ“¥ 1021 [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional) - Additional pretrained BERT base Japanese financeThis is a BERT model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 1020 [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese) - gpt2-large-japaneseThis repository provides a large sized Japanese GPT-2 model. The model was trained by ABEJA, IncHow to useFirst, install sentencepiece.
 * ğŸ“¥ 1012 [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1) - albert-base-japanese-v1æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«Sentencepieceã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ãã®ã¾ã¾ã§ã¯[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚ã¨ã«ä½™è¨ˆãªãƒˆãƒ¼ã‚¯ãƒ³ãŒæ··å…¥ã™ã‚‹å•é¡ŒãŒã‚ã‚‹ã®ã§ã€åˆ©ç”¨ã™ã‚‹éš›ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™for PyTorchfrom transformers import (AlbertForMaskedLM, AlbertTokenizerFast)import torchtokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")model = AlbertForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1")text = "å¤§å­¦ã§[MASK]ã®ç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™"tokenized_t
 * ğŸ“¥ 1006 [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf) - ELYZA-japanese-Llama-2-7b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-c
 * ğŸ“¥ 1004 [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct) - PLaMo-13B-InstructModel DescriptionPLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model. PLaMo-13B-Instruct is fine-tuned using multiple publicly available Japanese datasets.
 * ğŸ“¥ 1003 [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese) - nlp-waseda/bigbird-base-japaneseModel descriptionThis is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/bigbird-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/bigbird-base-japanese")sentence = '[MASK] å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’
 * ğŸ“¥ 992 [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)CoolJapanDiffusion 2.1.1ã¨WaifuDiffusion 1.4 anime epoch2ã®ãƒãƒ¼ã‚¸ã€‚ æ¯”ç‡ã¯ckptãƒ•ã‚¡ã‚¤ãƒ«åã®è¨˜è¼‰ã®é€šã‚Šã€‚
 * ğŸ“¥ 941 [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b) - stockmark/gpt-neox-japanese-1.4bThis repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens. This model is developed by Stockmark Inc.
 * ğŸ“¥ 938 [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B) - Japanese-Starling-ChatV-7Bã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.è©³ç´°ã¨GGUFç‰ˆã¯ã“ã¡ã‚‰ã€‚Details and GGUFs are here.
 * ğŸ“¥ 934 [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©ã«ã¤ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ç¤ºã—ã€è§£èª¬ã—ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-CodeLlama-7b-instruct"tokenizer = AutoTokenizer.from_pretrained
 * ğŸ“¥ 924 [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - japanese-large-lm-3.6bThis repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation. Tech Blog explains details.
 * ğŸ“¥ 922 [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã—ã‹ã—è¿½åŠ è‘—ä½œè€…ã¨ã—ã¦ä½åŸéƒç”»ã®åå‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 921 [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0) - ChatNTQ JA 7B V1.0Model DescriptionThis is a 7B-parameter decoder-only Japanese language model fine-tuned on our instruction-following datasets, built on top of the base model Japanese Stable LM Base Gamma 7B.PerformanceFor our final model, we've used Stability AI Japan's Japanese MT-Bench as a more representative test of our model's capabilities.
 * ğŸ“¥ 908 [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 904 [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr) - uniTKU-hubert-japanese-asrThis model was fine-tuned on a dataset provided by uniTKU, and it has maintained the original performance metrics on the common_voice_11_0 dataset. This model can only predict Hiragana.
 * ğŸ“¥ 899 [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large) - luke-japanese-largeluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * ğŸ“¥ 878 [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * ğŸ“¥ 856 [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b) - rinna/nekomata-14bOverviewWe conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * ğŸ“¥ 851 [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - Llama 3 Youko 8B (rinna/llama-3-youko-8b)OverviewWe conduct continual pre-training of meta-llama/Meta-Llama-3-8B on 22B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * ğŸ“¥ 835 [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b) - rinna/nekomata-7bOverviewWe conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * ğŸ“¥ 822 [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf) - ELYZA-japanese-CodeLlama-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-ins
 * ğŸ“¥ 811 [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos) - roberta-small-japanese-luw-uposModel DescriptionThis is a RoBERTa model pre-trained on é’ç©ºæ–‡åº« texts for POS-tagging and dependency-parsing, derived from roberta-small-japanese-aozora.
 * ğŸ“¥ 805 [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF) - c4ai-command-r-v01-japanese-instruct-GGUFæ¦‚è¦Aratako/c4ai-command-r-v01-japanese-instructã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
 * ğŸ“¥ 795 [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc) - PLaMo-13B-Instruct-NCModel DescriptionPLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model. PLaMo-13B-Instruct-NC is fine-tuned using multiple publicly available Japanese datasets.
 * ğŸ“¥ 783 [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base) - Japanese StableLM-3B-4E1T BaseModel DescriptionThis is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance. We conducted continued pretraining using Japanese data on the English language model, StableLM-3B-4E1T, to transfer the model's knowledge and capabilities to Japanese.
 * ğŸ“¥ 767 [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b) - Japanese-StableLM-Base-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. For an instruction-following model, check Japanese-StableLM-Instruct-Beta-7B. The base and instruct models are also available in
 * ğŸ“¥ 764 [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 751 [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli) - bert-base-japanese-v3-jnliã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(è‡ªç„¶è¨€èªæ¨è«–)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®MARC-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinenli_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jnli")text = "äºŒäººã®ç”·æ€§ãŒã‚¸ã‚§ãƒƒãƒˆæ©Ÿã‚’è¦‹ã¦ã„ã¾ã™"entailment_text = "ã‚¸ã‚§ãƒƒãƒˆæ©Ÿã‚’è¦‹ã¦ã„ã‚‹äººãŒäºŒäººã„ã¾ã™"# textã¨entailment_textã®è«–ç†é–¢ä¿‚ã‚’äºˆæ¸¬print(nli_pipeline({"text": text, "text_pair": entailment_text}))# {'label': 'enta
 * ğŸ“¥ 721 [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - c4ai-command-r-plus-ggufCohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹c4ai-command-r-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚åˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦q6_kã‚„q8_0ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µã‚¤ã‚ºãŒå¤§ããåˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ã®ã§çµåˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚cat c4ai-command-r-plus-Q5_K_M.gguf.* &gt; c4ai-command-r-plus-Q5_K_M.ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'c4ai-command-r-plus-Q4_0.gguf' -p "&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™Command-Rã§ã™&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_T
 * ğŸ“¥ 703 [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite) - luke-japaneseluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * ğŸ“¥ 687 [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft) - japanese-large-lm-1.7b-instruction-sftThis repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation. For Japaneseè©³ç´°ãªèª¬æ˜ã‚„å®Ÿé¨“ã«é–¢ã—ã¦ã¯ã€ŒInstruction Tuningã«ã‚ˆã‚Šå¯¾è©±æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸ3.6Bæ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã™ã€ã‚’ã”è¦§ãã ã•ã„ã€‚How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinemodel = AutoModelForCausalLM.from_pretrained("line-corporation/japanese-large-lm-1.7b-instruction-sft")tokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-1.7b-i
 * ğŸ“¥ 659 [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - bert-base-japanese-v3-jstsã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®JSTSãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 658 [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 654 [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) - Model Card for Tanrei/GPTSAN-japaneseGeneral-purpose Swich transformer based Japanese language modelGPTSAN has some unique features. It has a model structure of Prefix-LM.
 * ğŸ“¥ 652 [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - nlp-waseda/roberta-base-japaneseModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-base-japanese")sentence = 'æ—©ç¨²ç”° å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’ [MASK] ã™ã‚‹ ã€‚' # input should be segm
 * ğŸ“¥ 643 [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * ğŸ“¥ 637 [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b) - japanese-large-lm-1.7bThis repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation. Tech Blog explains details.
 * ğŸ“¥ 611 [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - Evaluation on MIRACL japaneseThese models don't train on the MIRACL training data. ModelnDCG@10Recall@1000Recall@5Recall@30BM250.3690.931--splade-japanese0.4050.9310.4060.663splade-japanese-efficient0.4080.9540.4190.718splade-japanese-v20.5800.9670.6290.844splade-japanese-v2-doc0.4780.9300.5140.759splade-japanese-v30.6040.9790.6470.877*'splade-japanese-v2-doc' model does not require query encoder during inference.
 * ğŸ“¥ 605 [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf) - ELYZA-japanese-Llama-2-13b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japa
 * ğŸ“¥ 588 [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf) - rinna/nekomata-14b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b-instruction. It can be used with llama.cpp for lightweight inference.
 * ğŸ“¥ 585 [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf) - ELYZA-japanese-Llama-2-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k
 * ğŸ“¥ 582 [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * ğŸ“¥ 579 [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base) - Original ModelOptical character recognition for Japanese text, with the main focus being Japanese manga.
 * ğŸ“¥ 543 [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf) - rinna/japanese-gpt-neox-3.6b-instruction-pporinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6b-instruction-ppoã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰mmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.com/mmnga/llama.cp
 * ğŸ“¥ 542 [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf) - rinna/japanese-gpt-neox-3.6brinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰mmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake -j./main -
 * ğŸ“¥ 536 [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese) - COMET-T5 jaFinetuned T5 on ATOMIC ja using a text-to-text language modeling objective. It was introduced in this paper.
 * ğŸ“¥ 536 [mmnga/gemma-1.1-7b-it-gguf](https://huggingface.co/mmnga/gemma-1.1-7b-it-gguf) - gemma-1.1-7b-it-ggufgoogleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-1.1-7b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚Licencegemma-terms-of-use åˆ©ç”¨è¦ç´„ã‚’ã”åˆ©ç”¨å‰ã«å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'gemma-1.1-7b-it-q4_0.gguf' -p "&lt;start_of_turn&gt;user\næ—¥æœ¬ã®æ–‡åŒ–ã‚’ï¼‘ï¼å€‹æ•™ãˆã¦ã€‚&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n" -n 128
 * ğŸ“¥ 525 [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - hotchpotch/japanese-reranker-cross-encoder-small-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfro
 * ğŸ“¥ 505 [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2) - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * ğŸ“¥ 481 [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 479 [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2) - japanese-gpt-neox-3.6b-instruction-sft-v2OverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b and has been finetuned to serve as an instruction-following conversational agent.
 * ğŸ“¥ 475 [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base) - rinna/japanese-wav2vec2-baseOverviewThis is a Japanese wav2vec 2.0 Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original wav2vec 2.0 Base model, which contains 12 transformer layers with 12 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * ğŸ“¥ 457 [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF) - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUFOriginal Modelelyza/ELYZA-japanese-Llama-2-13b-fast-instructRun with LlamaEdgeLlamaEdge version: v0.2.8 and abovePrompt templatePrompt type: llama-2-chatPrompt string&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{{ system_prompt }}&lt;&lt;/SYS&gt;&gt;{{ user_msg_1 }} [/INST] {{ model_answer_1 }} &lt;/s&gt;&lt;s&gt;[INST] {{ user_msg_2 }}Context size: 5120Run as LlamaEdge servicewasmedge --dir .:.
 * ğŸ“¥ 456 [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 450 [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1) - Kotoba-Whisper-v1.1Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, withadditional postprocessing stacks integrated as pipeline. The new features includes(i) improved timestamp achieved by stable-ts and (ii) adding punctuation with punctuators.
 * ğŸ“¥ 438 [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese) - mt5_summarize_japanese(Japanese caption : æ—¥æœ¬èªã®è¦ç´„ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization. This model is fine-tuned on BBC news articles (XL-Sum Japanese dataset), in which the first sentence (headline sentence) is used for summary and others are used for article.
 * ğŸ“¥ 431 [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 431 [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for VecTeus-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the f
 * ğŸ“¥ 426 [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) - bert-base-japanese-v3-unsup-simcse-jawikiã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬8ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3 ã‚’ llm-book/jawiki-sentences ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from torch.nn.functional import cosine_similarityfrom transformers import pipelinesim_enc_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-unsup-simcse-jawiki", task="feature-extraction")text = "å·ã¹ã‚Šã§ã‚µãƒ¼ãƒ•ãƒœãƒ¼ãƒ‰ã‚’æŒã£ãŸäººãŸã¡ãŒã„ã¾ã™"sim_text = "ã‚µãƒ¼ãƒ•ã‚¡ãƒ¼ãŸã¡ãŒå·ã¹ã‚Šã«
 * ğŸ“¥ 424 [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web) - t5-base-japanese-web (with Byte-fallback, 32K)Descriptionmegagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts. Training codes are available on GitHub.
 * ğŸ“¥ 415 [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - Model Card for Japanese DeBERTa V3 baseModel descriptionThis is a Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v3-base-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v3-base-japanese')sentences =
 * ğŸ“¥ 414 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-instruct - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-instructStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models. The model will crash immediately if -ngl is lar
 * ğŸ“¥ 414 [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * ğŸ“¥ 413 [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja) - æ—¥æœ¬èªå‘ã‘ Llama 3 8Bã¯ã˜ã‚ã«ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯Llama 3ã‚’æ—¥æœ¬èªåŒ–ã—ã‚ˆã†ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚ 4/23ã«æ›´æ–°ã—ãŸãŸã‚ã€æ–°ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ã‚ªã‚¹ã‚¹ãƒ¡ã—ã¾ã™ã€‚
 * ğŸ“¥ 408 [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 403 [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf) - ELYZA-japanese-CodeLlama-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPT
 * ğŸ“¥ 402 [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube) - hubert-base-jtubeThis repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.Scroll down for the model usageFAQQ.  ä½•ã‚’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼Ÿ
 * ğŸ“¥ 401 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new format introduced
 * ğŸ“¥ 397 [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char) - Model Card for Japanese character-level GPT-2 SmallModel descriptionThis is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-small-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * ğŸ“¥ 390 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-base - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-baseStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models. The model will crash immediately if -ngl is larger than
 * ğŸ“¥ 375 [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows:!pip install mecab-python3!pip install unidic-lite!pip install pykakasi!python
 * ğŸ“¥ 367 [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - hotchpotch/japanese-bge-reranker-v2-m3-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfrom sentence
 * ğŸ“¥ 363 [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1) - fio-base-japanese-v0.1æ—¥æœ¬èªç‰ˆã¯è¿‘æ—¥å…¬é–‹äºˆå®šã§ã™ï¼ˆæ—¥æœ¬èªã‚’å‹‰å¼·ä¸­ãªã®ã§ã€é–“é•ã„ã¯ã”å®¹èµ¦ãã ã•ã„ï¼ï¼‰fio-base-japanese-v0.1 is a proof of concept, and the first release of the Fio family of Japanese embeddings. It is based on cl-tohoku/bert-base-japanese-v3 and trained on limited volumes of data on a single GPU.For more information, please refer to my notes on Fio.
 * ğŸ“¥ 359 [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf) - SakanaAI-EvoLLM-JP-A-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-A-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ ã“ã¡ã‚‰ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 354 [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese) - roberta_qa_japanese(Japanese caption : æ—¥æœ¬èªã® (æŠ½å‡ºå‹) è³ªå•å¿œç­”ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of rinna/japanese-roberta-base (pre-trained RoBERTa model provided by rinna Co., Ltd.) trained for extractive question answering. The model is fine-tuned on JaQuAD dataset provided by Skelter Labs, in which data is collected from Japanese Wikipedia articles and annotated by a human.
 * ğŸ“¥ 348 [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick) - Japanese Natural Language Inference ModelThis model was trained using SentenceTransformers Cross-Encoder class, gradient accumulation PR, and the code from CyberAgentAILab/japanese-nli-model. Training DataThe model was trained on the JGLUE-JNLI and JSICK datasets.
 * ğŸ“¥ 344 [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’é‹è»¢ãƒ‰ãƒ¡ã‚¤ãƒ³QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆDDQAï¼‰ï¼ˆã€€https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasetsã€€ï¼‰ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚Question-Answeringã‚¿ã‚¹ã‚¯ï¼ˆSQuADï¼‰ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Question-Answering which is based on luke-japanese-base-liteThis model is fine-tuned by using DDQA dataset. You could use this model for Question-Answering tasks.ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ accuracy of model'em(å³å¯†ä¸€è‡´)': 0.8459330143540
 * ğŸ“¥ 343 [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 341 [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese) - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 333 [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 331 [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head) - bert-base-japanese-wikipedia-ud-headModel DescriptionThis is a BERT model pretrained on Japanese Wikipedia texts for dependency-parsing (head-detection on long-unit-words) as question-answering, derived from bert-base-japanese-char-extended and UD_Japanese-GSDLUW.
 * ğŸ“¥ 329 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1) - Heron GIT Japanese StableLM Base 7BModel DetailsHeron GIT Japanese StableLM
 * ğŸ“¥ 329 [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf) - rinna/nekomata-14b-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b. It can be used with llama.cpp for lightweight inference.
 * ğŸ“¥ 328 [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - Model Card for Japanese DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-large-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶
 * ğŸ“¥ 327 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new f
 * ğŸ“¥ 317 [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf) - line-corporation/japanese-large-lm-3.6b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.
 * ğŸ“¥ 315 [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese) - XLNet-japaneseModel descriptionThis model require Mecab and senetencepiece with XLNetTokenizer. See details https://qiita.com/mkt3/items/4d0ae36f3f212aee8002This model uses NFKD as the normalization method for character encoding.
 * ğŸ“¥ 314 [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * ğŸ“¥ 310 [webbigdata/C3TR-Adapter](https://huggingface.co/webbigdata/C3TR-Adapter) - ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰(Model Card for Model ID)C3TR-Adapterã¯GoogleãŒç™ºè¡¨ã—ãŸLLMã§ã‚ã‚‹gemma-7bã®æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹QLoRA Adapterã§ã™ã€‚C3TR-Adapter is a QLoRA Adapter that improves the Japanese-English and English-Japanese translation performance of gemma-7b released by Google. ãƒ¢ãƒ‡ãƒ«è©³ç´°(Model Details)C3TR-Adapterã¯ç¿»è¨³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¤šè¨€èªç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Googleã®Madlad400ã‚„metaã®Seamless m4t v2 largeã€ALMA-Ja-V2 (ç§é”ã®ä»¥å‰ã®llama 2ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«)ã‚ˆã‚Šã‚‚å¤§å¹…ã«å„ªã‚ŒãŸæ—¥è‹±ãƒ»æ—¥è‹±ç¿»è¨³æ€§èƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚Benchmarks show significantly better English-Japanese and Japanese-English translation performance than Google's
 * ğŸ“¥ 306 [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus) - llm-book/t5-base-long-livedoor-news-corpusã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬7ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹è¦ç´„ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ retrieva-jp/t5-base-longã‚’llm-book/livedoor-news-corpusã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 303 [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en) - WS TCG Card Text TranslatorA Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en. Hugging Face Space DemoCheck out the demo at https://huggingface.co/spaces/eepj/wstcg-mt.DatasetOfficial WS Card ListJapanese-English parallel card text comprising 6000+ card text retrieved from the offical card list.
 * ğŸ“¥ 297 [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) - nlp-waseda/roberta-large-japaneseModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese")sentence = 'æ—©ç¨²ç”° å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’ [MASK] ã™ã‚‹ ã€‚' # input should be
 * ğŸ“¥ 293 [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja) - Stanza model for Japanese (ja)Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.
 * ğŸ“¥ 288 [SakanaAI/EvoLLM-JP-v1-10B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B) - ğŸŸ EvoLLM-JP-v1-10BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦ TwitterEvoLLM-JP-v1-10B is an experimental general-purpose Japanese LLM.This model was created using the Evolutionary Model Merge method.
 * ğŸ“¥ 286 [SakanaAI/EvoLLM-JP-A-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B) - ğŸŸ EvoLLM-JP-A-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦ TwitterEvoLLM-JP-A-v1-7B is an experimental general-purpose Japanese LLM.This model was created using the Evolutionary Model Merge method.
 * ğŸ“¥ 283 [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps) - whisper-large-v2-japanese-5k-stepsThis model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11).. It achieves the following results on the evaluation set:Loss: 0.4200Wer: 0.7449Model descriptionThis model is finetuned for 5000 steps for research purposes which means that the transcriptions might not be that satisfactory for users.
 * ğŸ“¥ 280 [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf) - line-corporation/japanese-large-lm-1.7b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆline-gpt2_convert-hf-to-gguf.pyUsage (è©¦ç”¨)git clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m
 * ğŸ“¥ 276 [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf) - line-corporation/japanese-large-lm-1.7bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆline-gpt2_convert-hf-to-gguf.pyUsagegit clone --branch mmnga-dev-merge https://github.com/mmnga/llama.cpp.gitcd llama.cppmake -j./main -m 'line-corp-japanese-large-lm-1.7b-q4_0.gguf'
 * ğŸ“¥ 270 [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head) - deberta-base-japanese-aozora-ud-headModel DescriptionThis is a DeBERTa(V2) model pretrained on é’ç©ºæ–‡åº« for dependency-parsing (head-detection on long-unit-words) as question-answering, derived from deberta-base-japanese-aozora and UD_Japanese-GSDLUW.
 * ğŸ“¥ 266 [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char) - Model Card for Japanese character-level GPT-2 LargeModel descriptionThis is a Japanese character-level GPT-2 Large (717M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-large-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * ğŸ“¥ 260 [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b) - Japanese-StableLM-Base-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. Compared to the standard base model, this model uses a tokenizer with an expanded vocabulary derived from Japane
 * ğŸ“¥ 257 [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf) - shisa-7b-v1-ggufaugmxntã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹shisa-7b-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j.
 * ğŸ“¥ 245 [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b) - Japanese-StableLM-Instruct-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data. Compared to the standard base model, this model uses a tokenizer with an expanded vocabulary derived from Japanese data.
 * ğŸ“¥ 235 [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 229 [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF) - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
 * ğŸ“¥ 228 [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking) - japanese-gpt-1b-PII-maskingModel Descriptionjapanese-gpt-1b-PII-masking ã¯ã€ æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿1B GPTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ—¥æœ¬èªã®æ–‡ç« ã‹ã‚‰å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ å€‹äººæƒ…å ±ã¯ä»¥ä¸‹ã®å¯¾å¿œé–¢ä¿‚ã§ãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚
 * ğŸ“¥ 227 [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on luke-japanese-baseThis model is fine-tuned by using Wikipedia dataset. You could use this model for NER tasks.ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ accuracy of modelprecisionrecallf1-scoresupportãã®ä»–ã®çµ„ç¹”å0.760.770.77238ã‚¤ãƒ™ãƒ³ãƒˆå0.830.90
 * ğŸ“¥ 223 [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 215 [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf) - SakanaAI-EvoLLM-JP-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ ã“ã¡ã‚‰ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 214 [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. æ¬¡ã®æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆç´„100GBï¼‰ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸT5 (Text-to-Text Transfer Transformer) v1.1ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 211 [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese) - Wav2Vec2-Large-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data. This model is a model trained on public data.
 * ğŸ“¥ 206 [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis) - bert-japanese_finetuned-sentiment-analysisThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset. Pre-trained modeljarvisx17/japanese-sentiment-analysisLink : https://huggingface.co/jarvisx17/japanese-sentiment-analysisTraining DataThe model was trained on Japanese Sentiment Polarity Dictionary dataset.link :
 * ğŸ“¥ 203 [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1) - æ—¥æœ¬èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ«This is a CLIP text/image encoder model for Japanese. è‹±èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä¸€ç¨®ã®è’¸ç•™ã‚’ç”¨ã„ã¦æ—¥æœ¬èªåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 203 [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf) - Deepreneur-blue-lizard-ggufDeepreneurã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹blue-lizardã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¯7Bã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 202 [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite) - luke-japanese-large-liteluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * ğŸ“¥ 202 [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 201 [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 200 [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 199 [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion) - One more step before getting this model. This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
 * ğŸ“¥ 190 [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next) - reazonspeech-espnet-nextReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team. We maintain this repository because we want to make our latest researchresults readily available, and try to incorporate feedback from communityas quickly as possible.
 * ğŸ“¥ 186 [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 185 [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1) - hotchpotch/japanese-reranker-cross-encoder-base-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfrom
 * ğŸ“¥ 183 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’é‹è»¢ãƒ‰ãƒ¡ã‚¤ãƒ³QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆDDQAï¼‰ï¼ˆã€€https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasetsã€€ï¼‰ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚Question-Answeringã‚¿ã‚¹ã‚¯ï¼ˆSQuADï¼‰ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Question-Answering which is based on deberta-v2-base-japaneseThis model is fine-tuned by using DDQA dataset. You could use this model for Question-Answering tasks.
 * ğŸ“¥ 173 [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf) - line-corporation/japanese-large-lm-3.6bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.
 * ğŸ“¥ 171 [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 166 [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base) - luke-japaneseluke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities. LUKE treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.
 * ğŸ“¥ 165 [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JSTS(æ–‡ç« ã®é¡ä¼¼åº¦è¨ˆç®—)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’yahoo japan/JGLUEã®JSTS( https://github.com/yahoojapan/JGLUE )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 161 [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hackathon. We would like to take this opportunity to
 * ğŸ“¥ 161 [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")sentence
 * ğŸ“¥ 161 [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ã‚„Instaç³»ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒãƒ»Instaç³»ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚ ã‚ˆã£ã¦ãƒãƒ¼ã‚¸è€…ã®ç›®çš„ã§ã‚ã‚‹é»’é«ªãƒãƒ‹ãƒ†ä»¥å¤–ã®å‹•ä½œã¯èˆˆå‘³ãªã„ã—ã€çŸ¥ã‚‰ãªã„ã€‚
 * ğŸ“¥ 160 [rinna/nue-asr](https://huggingface.co/rinna/nue-asr) - rinna/nue-asrOverview[Paper][GitHub]We propose a novel end-to-end speech recognition model, Nue ASR, which integrates pre-trained speech and language models. The name Nue comes from the Japanese word (éµº/ã¬ãˆ/Nue), one of the Japanese legendary creatures (å¦–æ€ª/ã‚ˆã†ã‹ã„/YÅkai).This model provides end-to-end Japanese speech recognition with recognition accuracy comparable to the recent ASR models.
 * ğŸ“¥ 155 [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf) - Japanese-LLaMA-2-13B-GGUFJapanese-LLaMA-2-13B-GGUFã¯Japanese-LLaMA-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«URLï¼šhttps://huggingface.co/owner203/japanese-llama-2-13b
 * ğŸ“¥ 155 [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF) - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUFThis model was converted to GGUF format from inu-ai/alpaca-guanaco-japanese-gpt-1b using llama.cpp via the ggml.ai's GGUF-my-repo space. Refer to the original model card for more details on the model.
 * ğŸ“¥ 153 [sappho192/jesc-ja-en-translator](https://huggingface.co/sappho192/jesc-ja-en-translator) - Japanese to English translatorJapanese to English translator model based on EncoderDecoderModel(bert-japanese+GPT2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/jesc-ja-en-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferenceimport transformersimport torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "openai-community/gpt2"src_tokenizer = transformers. BertJapaneseTokenizer.from_pretrained(encoder_model_name)trg_tokenizer
 * ğŸ“¥ 153 [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix) - Yaki-Dofu-Mixæ¦‚è¦ / OverviewYaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ / Yaki-Dofu-Mix is a merge model that specializes in an anime-like painting style. VAEãªã—ã§ã‚‚é®®ã‚„ã‹ãªè‰²åˆã„ã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚
 * ğŸ“¥ 145 [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard) - Deepreneur-blue-lizardModel DescriptionDeepreneur-blue-lizardã¯ã€Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ 70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨éå¸¸ã«è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšã€JGLUEï¼ˆæ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã‚’ç”¨ã„ãŸè©•ä¾¡ã§ã¯ã€ChatGPT-3.5ã‚’è¶…ãˆã‚‹ã‚¹ã‚³ã‚¢ãŒç®—å‡ºã•ã‚Œã¦ãŠã‚Šã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯æœ€é«˜æ€§èƒ½ã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 138 [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf) - rinna/nekomata-7b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b-instruction. It can be used with llama.cpp for lightweight inference.
 * ğŸ“¥ 138 [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * ğŸ“¥ 137 [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation) - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
 * ğŸ“¥ 137 [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - DeBERTa V2 base JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 136 [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 134 [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b) - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
 * ğŸ“¥ 130 [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0) - LLaVA-JP Model CardModel detailModel type:LLaVA-JP is a vision-language model that can converse about input images. This model was trained by fine-tuning  llm-jp/llm-jp-1.3b-v1.0 using LLaVA method.
 * ğŸ“¥ 128 [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 127 [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) - DeBERTa V2 small JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 121 [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small) - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics. You can try it on my website https://lyric.fab.moe/How to useimport torchfrom transformers import T5Tokenizer, GPT2LMHeadModeltokenizer = T5Tokenizer.from_pretrained("skytnt/gpt2-japanese-lyric-small")model = GPT2LMHeadModel.from_pretrained("skytnt/gpt2-japanese-lyric-small")def gen_lyric(prompt_text: str):prompt_text = "&lt;s&gt;" + prompt_text.replace("\n", "\\n ")prompt_tokens = tokenizer.tokenize(prompt_text)prompt_t
 * ğŸ“¥ 120 [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion) - SFCOCO Stable Diffusion Model CardSFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input. This model was fine-tuned by using a powerful Japanese-specific latent text-to-image diffusion model, Japanese Stable Diffusion.
 * ğŸ“¥ 120 [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1-128kThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja-128k has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hack
 * ğŸ“¥ 119 [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - Model Card for Japanese BART baseModel descriptionThis is a Japanese BART base model pre-trained on Japanese Wikipedia.
 * ğŸ“¥ 116 [Local-Novel-LLM-project/Ninja-v1-GGUF](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-GGUF) - Ninja-v1 ã®GGUFç‰ˆOur Models for GGUFVecteus-GGUFNinja-v1-GGUFNinja-v1-NSFW-GGUFNinja-v1-128k-GGUFNinja-v1-NSFW-128k-GGUF
 * ğŸ“¥ 116 [aerner/lm-v2](https://huggingface.co/aerner/lm-v2) - Aerner LM-v2äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã§ã™ã€‚ LLaMAãƒ™ãƒ¼ã‚¹ã§ã€24GBã®VRAMã§äº‹å‰å­¦ç¿’ã§ãã‚‹è¦æ¨¡ã«å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 115 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1) - Heron BLIP Japanese StableLM Base 7B v1Model DetailsHeron BLIP Japanese StableLM
 * ğŸ“¥ 115 [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base) - SpiralAI Spiral-RetNet-3b-baseWe have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621) architecture model 3b using a mixed dataset of Japanese and English. This model is released primarily for the basic research of "retention mechanism".
 * ğŸ“¥ 114 [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc) - ã¯ã˜ã‚ã«Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å°å‹ãªã®ã§ã‚¹ãƒãƒ›ã‚„å®¶é›»ãªã©ã«å‘ã„ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€Instruction tuningãŒå›°é›£ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚Colabã§è©¦ã™mmngaã•ã‚“ãŒä½œã£ãŸè»½é‡ç‰ˆã‚’Colabã§è©¦ã™Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("alfredplpl/suzume-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/suzume-poc")input_text = """äººå·¥çŸ¥èƒ½ã¨ã¯"""input_ids = tokenizer(input_text, return_tensors="pt")outputs = model.generate(**input_ids,max_new_tokens=64)print(tokenizer.decode(outputs[0])
 * ğŸ“¥ 112 [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on daigo's BERT Base for Japanese sentiment analysis, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * ğŸ“¥ 108 [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct) - Stockmark-13b-instructStockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM. This model is developed by Stockmark Inc.
 * ğŸ“¥ 107 [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b) - æ›´æ–°å±¥æ­´2023å¹´5æœˆ7æ—¥ã€Œoasst1-89k-jaã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ã¾ã—ãŸã€‚ 1024ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã§ãã¾ã™ã€‚
 * ğŸ“¥ 106 [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - Model Card for Model IDå®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ / This is an experimental model.lightblue/suzume-llama-3-8B-japaneseã¨ã€meta-llama/Meta-Llama-3-8B-Instructã®å·®åˆ†ã‚’chat-vectorã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æŠ½å‡ºã—ã€meta-llama/Meta-Llama-3-70B-Instructã«é©ç”¨ã—ã¾ã—ãŸçµæœå·®åˆ†ãŒå°ã•ã„ã®ã‹ã‚ã¾ã‚Šå¤‰åŒ–ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸä»Šå¾Œã¯å€ç‡ãªã©ä»˜ä¸ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™.æ‰‹é †/procedurechat_vector.ipynbjameta-llama/Meta-Llama-3-8B-Instructã¨lightblue/suzume-llama-3-8B-japaneseã®å·®åˆ†ã‚’ä½œæˆshapeãŒç•°ãªã‚‹ã®ã§ã€å·®åˆ†ã‚’meta-llama/Meta-Llama-3-70B-Instructç”¨ã«ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‰ã‹ã‚‰ 8-layerã€æœ€å¾Œã‹ã‚‰8-layerã¯ãã®ã¾ã¾é©ç”¨ä¸­é–“layerã‚’å¼•ãå»¶ã°ã—ã¦é©ç”¨enCreate the difference between meta-llama/Meta
 * ğŸ“¥ 100 [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius) - spekulatiusãƒãƒ¼ã‚¸ã—ã¦ã„ã‚‹ã¨ãŸã¾ã«å‡ºã¦ãã‚‹ã€Œç›®çš„ã®æ„å›³ã¨ã¯é•ã†ã®ã ã‘ã©ãªã‚“ã ã‹æ¶ˆã™ã«ã¯ã‚‚ã£ãŸã„ãªã„ãƒ¢ãƒ‡ãƒ«ã€ã‚’ãŠã™ãåˆ†ã‘ã™ã‚‹ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ å…¬é–‹ã™ã‚‹ä»¥ä¸Šæœ€ä½é™èª¿æ•´ã—ã¦å‡ºãã†ã¨ã¯æ€ã„ã¾ã™ãŒã‚ã¾ã‚ŠæœŸå¾…ã¯ã—ãªã„ã§ãã ã•ã„ã€‚
 * ğŸ“¥ 100 [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base) - recruit-jp/japanese-clip-vit-b-32-roberta-baseOverviewDeveloped by: Recruit Co., Ltd.Model type: Contrastive Language-Image Pretrained ModelLanguage(s): JapaneseLICENSE: CC-BY-4.0More details are described in our tech blog post.æ—¥æœ¬èªCLIPå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¬é–‹Model DetailsThis model is a Japanese CLIP.
 * ğŸ“¥ 100 [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection) - Riga_collectionã¨ã¯ï¼Ÿ Riga_collectionã¯æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™HimawariMixã¨åŒã˜ã§èƒŒæ™¯ã‚„ç´°éƒ¨ã®è¡¨ç¾ãŒå¼·ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã‚¤ãƒ¡ãƒ¼ã‚¸çš„ã«ã¯HimawariMixã‚’ã‚ŠãŒã•ã‚“ã®ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢ã‚’å…ƒã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã§ã™ï¼
 * ğŸ“¥ 99 [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa) - Kokuwalamettaã®æ”¹è‰¯ã§ãƒãƒ¼ã‚¸ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«æ¢ã—ã‚’ã—ã¦ã„ãŸã‚‰KiwiMixã¨ã„ã†é¢ç™½ãã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚ LoRAã§ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ãŸã‚‚ã®ãŒãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã‚ˆã†ã§ã™ãŒã€ãã‚Œã«ã‚ˆã£ã¦ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒæ˜ç¢ºã«ãªã£ãŸã®ã§æ—©é€Ÿä½¿ã‚ã›ã¦ã„ãŸã ãã“ã¨ã«ã—ãŸã®ãŒã“ã‚Œã§ã™ã€‚
 * ğŸ“¥ 97 [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char) - Model Card for Japanese character-level GPT-2 MediumModel descriptionThis is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-medium-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&
 * ğŸ“¥ 97 [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2) - japanese-sexual-moderation-v2ã¯ã€studio-ousia/luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚çŸ­æ–‡ãŒæ€§çš„ã‹ã©ã†ã‹ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚regressionã§å­¦ç¿’ã—ã¦ãŠã‚Šã€å‡ºåŠ›ã™ã‚‹ã‚¹ã‚³ã‚¢ã¯ãŠãŠã‚€ã­0-1ã®ç¯„å›²ã‚’å–ã‚Šã¾ã™ãŒè² ã®å€¤ã‚„1ã‚’è¶…ãˆã‚‹å€¤ãŒå‡ºã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚é•·ã„æ–‡ç« ã¯å­¦ç¿’ã—ã¦ãŠã‚‰ãšã€å…¥åŠ›ã¯æ”¹è¡Œå˜ä½ã§åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚0.0-0.2: å…¨ãæ€§çš„ã§ã¯ãªã„0.2-0.4: ã»ã¨ã‚“ã©æ€§çš„ãªå†…å®¹ã‚’å«ã¾ãªã„0.4-0.6: æ€§çš„ãªå†…å®¹ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹0.6-0.8: æ€§çš„ãªå†…å®¹ã‚’å«ã‚“ã§ã„ã‚‹0.8-1.0: éå¸¸ã«æ€§çš„ãªå†…å®¹ã§ã‚ã‚‹Usageimport torchfrom transformers import AutoModelForSequenceClassification, AutoTokenizermodel_id = "oshizo/japanese-sexual-moderation-v2"tokenizer = AutoTokenizer.from_pretrained(model_id)model = Auto
 * ğŸ“¥ 96 [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL) - ã“ã¡ã‚‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã€civitaiã«ã¦å…ˆã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚ AfterRealXL_beta2(https://civitai.com/models/150212/afterrealxl)
 * ğŸ“¥ 94 [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja) - MobileBERT æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çˆ†èª•ï¼ï¼ AIé–¢ä¿‚ã®ä»•äº‹ã‚’ã—ã¦ã„ã‚‹æ«»æœ¬ã§ã™ã€‚
 * ğŸ“¥ 93 [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 92 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2) - Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Swallow-MX-8x7b-NVE-v0.1 + 0.8*(Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1)aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®èªå½™ãŒãŠã‹ã—ã„å ´åˆã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªãŒã‚ˆã‚Šè‡ªç„¶ã«ãªã‚Šã¾ã™context size 32k tokenä½¿ç”¨å¯èƒ½ãªæ—¥æœ¬èªå¯¾å¿œãƒ­ãƒ¼ã‚«ãƒ«ç”¨LLMã¨ã—ã¦ã¯2024å¹´3æœˆæ™‚ç‚¹ã§ã¯æœ€é«˜ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½ã§ã™
 * ğŸ“¥ 88 [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts) - SpeechT5 (TTS task) for JapaneseSpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech) on JVS.This model utilizes the JVS dataset which encompasses 100 speakers. From this dataset, speaker embeddings were crafted, segregating them based on male and female voice types, and producing a unique speaker embedding vector.
 * ğŸ“¥ 84 [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1) - Kotoba-Speech-v0.1Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model. It supports the following properties:Fluent text-to-speech generation in JapaneseOne-shot voice cloning through speech promptUsagePlesae check out our HF Spaces demo.
 * ğŸ“¥ 84 [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3ã€‚ WaifuDiffusionã®vaeã¨ã€StableDiffusionã®vaeã‚’bake inã—ã¦ã€MoeDiffusionç³»ã®ç™ºè‰²ã®å¼±ã•ã‚’æ”¹å–„ã€‚
 * ğŸ“¥ 83 [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚ ã‚ˆã£ã¦ãƒãƒ¼ã‚¸è€…ã®ç›®çš„ã§ã‚ã‚‹é»’é«ªãƒãƒ‹ãƒ†ä»¥å¤–ã®å‹•ä½œã¯èˆˆå‘³ãªã„ã—ã€çŸ¥ã‚‰ãªã„ã€‚
 * ğŸ“¥ 83 [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b) - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦AWSã®trn1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦é–‹ç™ºã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚äº‹å‰å­¦ç¿’å¾Œã«å¤§å–œåˆ©ãƒ‡ãƒ¼ã‚¿ã§Fine-tuningã—ã¦ã„ã¾ã™ã€‚Architecture: GPT2Vocab size: 44880Model size: 6B paramsLicense: Apache License 2.0Library: aws-neuron-reference-for-megatron-lmå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚ãã®éš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯477å„„ãƒˆãƒ¼ã‚¯ãƒ³ã§ã—ãŸã€‚C4ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿CC-100ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿OSCARã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿Wikipediaã®æ—¥æœ¬èªãƒ€ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿Fine-tuningã¯ã€693ä¸‡ä»¶ã®å¤§å–œåˆ©ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¡Œã„ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "watashiha/watashiha-gpt-6b"tokenizer = AutoTokenizer.from_pretrained(model_name, u
 * ğŸ“¥ 81 [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel) - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset )ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒãƒ™ãƒ«é¢¨ç”»åƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§naver-clova-ix/donut-baseã‚’è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ ä½¿ã„æ–¹ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯sample_predictions_colab.ipynbã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
 * ğŸ“¥ 80 [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - Japanese to Korean translatorJapanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/aihub-ja-ko-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferencefrom transformers import(EncoderDecoderModel,PreTrainedTokenizerFast,BertJapaneseTokenizer,)import torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "skt/kogpt2-base-v2"src_tokenizer = BertJapaneseTokenizer.from_pre
 * ğŸ“¥ 79 [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha) - Japanese InstructBLIP AlphaModel DetailsJapanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions. UsageFirst install additional dependencies in requirements.txt:pip install sentencepiece einopsimport torchfrom transformers import LlamaTokenizer, AutoModelForVision2Seq, BlipImageProcessorfrom PIL import Imageimport requests# helper function to format input promptsdef build_prompt(pr
 * ğŸ“¥ 79 [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese) - wav2vec2-live-japanesehttps://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using thecommon_voiceJSUTCSS10TEDxJP-10KJVSJSSSInference#usageimport torchimport torchaudiofrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processormodel = Wav2Vec2ForCTC.from_pretrained("ttop324/wav2vec2-live-japanese")processor = Wav2Vec2Processor.from_pretrained("ttop324/wav2vec2-live-japanese")test_dataset = load_dataset("commo
 * ğŸ“¥ 79 [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor) - Model Card for Model IDFine tunned ASR model from distil-whisper/distil-large-v2.This model aimed to transcribe japanese audio especially visual novel. Model DetailsModel DescriptionThis is the model card of a ğŸ¤— transformers model that has been pushed on the Hub.
 * ğŸ“¥ 78 [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot) - yuyuyui-chatbotThis model is based on rinna/japanese-gpt2-medium and finetuned on Yuyuyui scenario corpus. UsageThe model takes a sequence of utterances (context) to generate a subsequent utterance (response).
 * ğŸ“¥ 77 [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese) - Mixtral-8x7B-Instruct-v0.1-japaneseMixtral-8x7B-Instruct-v0.1-japaneseã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ABEJAã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸMetagton-LMã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯ã“ã¡ã‚‰ã§ã™ã€‚ä½¿ã„æ–¹import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "abeja/Mixtral-8x7B-Instruct-v0.1-japanese"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,use_cache=True,device_map="auto",)model.eval()input_text = """#
 * ğŸ“¥ 73 [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf) - Japanese-Alpaca-2-13B-GGUFJapanese-Alpaca-2-13B-GGUFã¯Japanese-Alpaca-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«URLï¼šhttps://huggingface.co/owner203/japanese-alpaca-2-13b
 * ğŸ“¥ 73 [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece) - alabnii/jmedroberta-base-manbyo-wordpieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "æ‰æœ¬æµ·äºº and å£¹å²å¤ªä¸€ and çŸ¥ç”°æ‚ ç”Ÿ and é‡‘æ²¢è¼ä¸€ and ç›¸æ¾¤å½°å­",title =     "J{M}ed{R}o{BERT}a: æ—¥æœ¬èªã®åŒ»å­¦è«–æ–‡ã«ã‚‚ã¨ã¥ã„ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡",booktitle = "è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š",year =   
 * ğŸ“¥ 72 [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English. It is trained using direct preference optimization on top the base model SambaLingo-Japanese-Base.
 * ğŸ“¥ 72 [alfredplpl/gemma-2b-it-ja-poc-3](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-3) - ã¯ã˜ã‚ã«ãªã‚“ã‹æ—¥æœ¬èªãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚å°å‹ãªã®ã§ã‚¹ãƒãƒ›ã‚„å®¶é›»ãªã©ã«å‘ã„ã¦ã„ã¾ã™ã€‚Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchfrom peft import PeftModel# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™tokenizer = AutoTokenizer.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = AutoModelForCausalLM.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = PeftModel.from_pretrained(model = model, model_id = "alfredplpl/gemma-2b-it-ja-poc-3")# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™prompt="""ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‹±èªã¯å–‹ã‚‰ãšã€æ—¥æœ¬èªã ã‘å–‹ã£ã¦ãã ã•ã„ã€‚&lt;start_of_turn&gt;us
 * ğŸ“¥ 70 [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony) - BERT Base Japanese for IronyThis is a BERT Base model for sentiment analysis in Japanese additionally finetuned for automatic irony detection. The model was based on bert-base-japanese-sentiment, and later finetuned on a dataset containing ironic and sarcastic tweets.
 * ğŸ“¥ 69 [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer) - albert-base-japanese-v1-with-japaneseæ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«BertJapaneseTokenizerã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™albert-base-japanese-v1ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒæ¥½ã«ãªã£ã¦ã„ã¾ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskfor PyTorchfrom transformers import (AutoModelForMaskedLM, AutoTokenizer)tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")model = AutoModelForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")tex
 * ğŸ“¥ 67 [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens) - This is a Japanese+English sentence-BERT model. æ—¥æœ¬èª+è‹±èªç”¨Sentence-BERTãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æ—¥æœ¬èªã®ã¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”ã¹ã¦ã€æ‰‹å…ƒã®éå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æ—¥æœ¬èªã®ç²¾åº¦ãŒ0.8ptä½ãã€è‹±èªSTSbenchmarkã§ã¯ç²¾åº¦ãŒ8.3pté«˜ã„ï¼ˆCosine-Similarity SpearmanãŒ79.11%ï¼‰çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦cl-tohoku/bert-base-japanese-whole-word-maskingã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚æ¨è«–ã®å®Ÿè¡Œã«ã¯fugashiã¨ipadicãŒå¿…è¦ã§ã™ï¼ˆpip install fugashi ipadicï¼‰ã€‚æ—¥æœ¬èªã®ã¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è§£èª¬https://qiita.com/sonoisa/items/1df94d0a98cd4f209051ãƒ¢ãƒ‡ãƒ«åã‚’"sonoisa/sentence-bert-base-ja-en-mean-tokens"ã«æ›¸ãæ›ãˆã‚Œã°ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ãŸæŒ™å‹•ã«ãªã‚Šã¾ã™ã€‚ä½¿ã„æ–¹from transformers import BertJapaneseTokenizer, BertModeli
 * ğŸ“¥ 65 [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf) - rinna/nekomata-7b-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b. It can be used with llama.cpp for lightweight inference.
 * ğŸ“¥ 65 [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese) - nlp-waseda/gpt2-small-japaneseThis model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task. Note that the texts should be segmented into words using Juman++ in advance.
 * ğŸ“¥ 64 [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã§åˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚2021å¹´ã«ä½œã£ãŸå°èª¬ç”¨è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Model DetailsGPT-J-6Bã‚’TPUã§2é€±é–“æ—¥æœ¬èªtokenizerã‚’ç”¨ã„ã¦æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã—ã€ãã®å¾Œ2é€±é–“å°èª¬ãƒ‡ãƒ¼ã‚¿ã§è»¢ç§»å­¦ç¿’ã—ãŸã‚‚ã®ã§ã™ã€‚UsesGoogle colabã®T4 High-RAMã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚pip install transformers sentencepiece acceleratefrom transformers import GPTJForCausalLM, AlbertTokenizerimport torchtokenizer = AlbertTokenizer.from_pretrained('AIBunCho/japanese-novel-gpt-j-6b', keep_accents=True, remove_space=False)model = GPTJForCausalLM.from_pretrained("AIBunCho/japanese-novel-gpt-j-6b", torch_
 * ğŸ“¥ 62 [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b) - ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ calm-2-7b-chat ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ ä¸‹è¨˜ã«è¨˜è¼‰ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ç¯„å›²å†…ã§ã”è‡ªç”±ã«åˆ©ç”¨ã„ãŸã ã‘ã¾ã™ã€‚
 * ğŸ“¥ 61 [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens) - This is a Japanese sentence-T5 model. æ—¥æœ¬èªç”¨Sentence-T5ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦sonoisa/t5-base-japaneseã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚æ¨è«–ã®å®Ÿè¡Œã«ã¯sentencepieceãŒå¿…è¦ã§ã™ï¼ˆpip install sentencepieceï¼‰ã€‚æ‰‹å…ƒã®éå…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ç²¾åº¦ã¯sonoisa/sentence-bert-base-ja-mean-tokensã¨åŒç¨‹åº¦ã§ã™ã€‚ä½¿ã„æ–¹from transformers import T5Tokenizer, T5Modelimport torchclass SentenceT5:def __init__(self, model_name_or_path, device=None):self.tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, is_fast=False)self.model = T5Model.from_pretrained(model_name_or_path).encoderself.model.eval()if
 * ğŸ“¥ 61 [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - This is a model for named entity recognition of Japanese medical documents. IntroductionThis repository contains the base model and a support predict script for using the model and providing a XML tagged text output.
 * ğŸ“¥ 60 [Helsinki-NLP/opus-mt-ja-it](https://huggingface.co/Helsinki-NLP/opus-mt-ja-it) - jpn-itasource group: Japanesetarget group: ItalianOPUS readme: jpn-itamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): itamodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus-2020-06-17.ziptest set translations: opus-2020-06-17.test.txttest set scores: opus-2020-06-17.eval.txtBenchmarkstestsetBLEUchr-FTatoeba-test.jpn.ita22.80.460System Info:hf_name: jpn-itasource_languages: jpntarg
 * ğŸ“¥ 59 [mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis) - Sentiment Analysis in Japanese - PhÃ¢n tÃ­ch cáº£m xÃºc trong tiáº¿ng Nháº­tBert phÃ¢n tÃ­ch cáº£m xÃºcModel descriptionMÃ´ hÃ¬nh cÃ³ tÃ¡c dá»¥ng xÃ¡c Ä‘á»‹nh cáº£m xÃºc cá»§a Ä‘oáº¡n vÄƒn. Sá»­ dá»¥ng nhÃ£n: "positive", "negative"VÃ­ dá»¥:ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­negative: 6.001393558108248e-05positive: 0.999940037727356ä»Šæ—¥ã®é£Ÿã¹ç‰©ã¯ã¨ã¦ã‚‚ã¤ã¾ã‚‰ãªã„negative: 0.9999252557754517positive: 7.470489799743518e-05Base modelMÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘áº¡o táº¡o dá»±a trÃªn cÆ¡ sá»Ÿ cá»§a model Base JapaneseTraining dataMÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o dá»±a trÃªn dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p bá»Ÿi TAKAHIRO KUBO (https://www.kaggle.
 * ğŸ“¥ 57 [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine) - Model Card for Model IDæ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã‹ã‚‰ã€æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚ã‚‹å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™Model DetailsModel Descriptionä¾‹ãˆã°ã€ã€Œæ±äº¬ã®è‚‰æ–™ç†ã§ã€æ˜¥ã«é£Ÿã¹ã‚‰ã‚Œã‚‹ã€é¶è‚‰ã‚’ä½¿ã£ãŸæ–™ç†ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã€Œæ±äº¬ã€€â†’ã€€éƒ½é“åºœçœŒ/åœ°æ–¹(AREA)ã€ã€€ã€Œè‚‰æ–™ç†ã€€â†’ã€€ç¨®é¡(TYPE)ã€ã€€ã€Œæ˜¥ã€€â†’ã€€å­£ç¯€(SZN)ã€ã€€ã€Œé¶è‚‰ã€€â†’ã€€é£Ÿæ(INGR)ã€ã®ã‚ˆã†ã«ã€å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™æŠ½å‡ºå¯¾è±¡ã¯ã€AREAã€TYPEã€SZNã€INGRã®ï¼”ã¤ã§ã™Language(s) (NLP): æ—¥æœ¬èªLicense: mitFinetuned from model: tohoku-nlp/bert-base-japanese-v2Model SourcesRepository: wolf4032/nlp-token-classificationãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã‚¢ãƒ—ãƒªã®ä½œæˆã«ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ãŒæ²è¼‰ã•ã‚Œã¦ã„ã¾ã™Paper: [More Information Needed]Demo: wolf4032/japanese-token-classification-s
 * ğŸ“¥ 56 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-base-gamma-7bğŸ§© Configurationslices:- sources:- model: mistralai/Mistral-7B-Instruct-v0.1layer_range:
 * ğŸ“¥ 56 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-instruct-gamma-7bğŸ§© Configurationslices:- sources:- model: mistralai/Mistral-7B-Instruct-v0.1layer_range:
 * ğŸ“¥ 56 [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English. It is trained using direct preference optimization on top the base model SambaLingo-Japanese-Base.
 * ğŸ“¥ 55 [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - WRIME-fine-tuned BERT base JapaneseThis model is a Japanese BERTBASE fine-tuned on the WRIME dataset. It was trained as part of the paper "Emotion Analysis of Writers and Readers of Japanese Tweets on Vaccinations".
 * ğŸ“¥ 55 [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - BERT base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer BERT base Japanese for details about the pre-training model. The codes for the fine-tuning are available at SkelterLabsInc/JaQuADEvaluation resultsOn the development set.{"f1": 77.35, "exact_match": 61.01}On the test set.{"f1": 78.92, "exact_match": 63.38}Usagefrom transformers import AutoModelForQuestionAnswering, AutoTokenizerquestion = 'ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ€ãƒ¼ãƒ»ã‚°ãƒ©ãƒãƒ ãƒ»ãƒ™ãƒ«ã¯ã€ã©ã“ã§ç”Ÿã¾ã‚ŒãŸã®?'context = 'ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ€ãƒ¼ãƒ»ã‚°ãƒ©ãƒãƒ ãƒ»ãƒ™ãƒ«ã¯ã€ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰ç”Ÿ
 * ğŸ“¥ 52 [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo) - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instructå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-jaå­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«import torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",trust_remote_code=True,)model = AutoModelForCausalLM.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",device_map="auto",torch_dtype='auto',trust_remote_code=True,)text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚æ±äº¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n&lt;|end|&gt;\n&lt;|assistant
 * ğŸ“¥ 52 [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese) - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)This is the longer input version of RoBERTa Japanese model pretrained on approximately 200 M Japanese sentences.max_position_embeddings has been increased to 1282, allowing it to handle much longer inputs than the basic RoBERTa model.
 * ğŸ“¥ 52 [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese) - è‹±èª+æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus. æ¬¡ã®æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆç´„500GBï¼‰ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸT5 (Text-to-Text Transfer Transformer) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 52 [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework.
 * ğŸ“¥ 51 [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja) - GPT-2 small Japanese modelThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset. Training dataJapanese Wikipedia dataset as of Aug20, 2021 released under Creative Commons Attribution-ShareAlike 3.0 is used for both tokenizer and GPT-2 model.
 * ğŸ“¥ 50 [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-baseFine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly as follows.from transformers import WhisperForConditionalGeneration, WhisperProcessorfrom datasets import load_datasetimport librosaimport torchLANG_ID
 * ğŸ“¥ 49 [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000) - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "æ‰æœ¬æµ·äºº and å£¹å²å¤ªä¸€ and çŸ¥ç”°æ‚ ç”Ÿ and é‡‘æ²¢è¼ä¸€ and ç›¸æ¾¤å½°å­",title =     "J{M}ed{R}o{BERT}a: æ—¥æœ¬èªã®åŒ»å­¦è«–æ–‡ã«ã‚‚ã¨ã¥ã„ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡",booktitle = "è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š
 * ğŸ“¥ 45 [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos) - bert-base-japanese-unidic-luw-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-v2.
 * ğŸ“¥ 44 [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese) - COMET-GPT2 jaFinetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective. It was introduced in this paper.
 * ğŸ“¥ 43 [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L) - Model Card for Model IDThis is a small T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese and English corpus. This modelcard aims to be a base template for new models.
 * ğŸ“¥ 42 [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps) - whisper-large-v3-japanese-4k-stepsThis model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset. I followed a post by Sanchit Gandhi, https://huggingface.co/blog/fine-tune-whisperIt took 24 hours using an A100 on Google Colab to complete 4000 steps using the Common Voice 16.1 dataset.
 * ğŸ“¥ 42 [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¯ãƒãŒæµ·è¾ºã«è¡Œã£ã¦ã‚¢ã‚¶ãƒ©ã‚·ã¨å‹é”ã«ãªã‚Šã€æœ€çµ‚çš„ã«ã¯å®¶ã«å¸°ã‚‹ã¨ã„ã†ãƒ—ãƒ­ãƒƒãƒˆã®çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * ğŸ“¥ 41 [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k) - t5-base-japanese-web-8k (with Byte-fallback, 8K)Descriptionmegagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts. Training codes are available on GitHub.
 * ğŸ“¥ 41 [alfredplpl/gemma-2b-it-ja-poc](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc) - Noteã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¦ãƒã‚°ã£ã¦ã„ã‚‹ãŸã‚ã€ã“ã¡ã‚‰ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚Google ColabUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torch# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™tokenizer = AutoTokenizer.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™prompt="""ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‹±èªã¯å–‹ã‚‰ãšã€æ—¥æœ¬èªã ã‘å–‹ã£ã¦ãã ã•ã„ã€‚&lt;start_of_turn&gt;useräººç”Ÿã§å¤§åˆ‡ãªã“ã¨ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ&lt;end_of_turn&gt;&lt;start_of_turn&gt;model"""# æ¨è«–ã®å®Ÿè¡Œinput_ids = tokenizer(prompt, return_tensors="pt").to(model.device
 * ğŸ“¥ 40 [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese) - æ—¥æœ¬èªByT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. æ¬¡ã®æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆç´„100GBï¼‰ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 40 [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli) - bert-base-japanese-jsnliThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset. It achieves the following results on the evaluation set:Loss: 0.2085Accuracy: 0.9288How to use the modelSimple zero-shot classification pipelinefrom transformers import pipelineclassifier = pipeline("zero-shot-classification", model="Formzu/bert-base-japanese-jsnli")sequence_to_classify = "ã„ã¤ã‹ä¸–ç•Œã‚’è¦‹ã‚‹ã€‚"candidate_labels =
 * ğŸ“¥ 40 [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363) - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59363Validation MetricsLoss: 0.12651239335536957Accuracy: 0.9532079853817648Precision: 0.9729688278823665Recall: 0.9744633462616643AUC: 0.9717333684823413F1: 0.9737155136027014UsageYou can use cURL to access this model:$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-59363Or Python A
 * ğŸ“¥ 39 [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base) - studio-ousia/luke-japanese-baseã«å¯¾ã—ã¦æ¬¡ã®å¤‰æ›´ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’RoBERTaã‹ã‚‰æ—¥æœ¬èªBERTã«åˆ‡ã‚Šæ›¿ãˆã€ãã‚Œã«ä¼´ã£ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒSentencepieceã‹ã‚‰WordPieceã«ãªã‚Šã¾ã—ãŸ2023å¹´7æœˆ1æ—¥æ™‚ç‚¹ã®æ—¥æœ¬èªWikipediaã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã‚’ãŠã“ãªã„ã¾ã—ãŸ[UNK] (unknown) ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸè©³ç´°ã¯ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„ã€‚ä½¿ç”¨æ–¹æ³•from transformers import AutoTokenizer, AutoModel# æœ¬ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€trust_remote_code=True ã®æŒ‡å®šãŒå¿…è¦ã§ã™tokenizer = AutoTokenizer.from_pretrained("uzabase/luke-japanese-wordpiece-base", trust_remote_code=True)model = AutoModel.from_pretrained("uzabase/luke-japanese-wordpiece-base")æ›´æ–°æƒ…å ±2023/11
 * ğŸ“¥ 38 [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta) - æ—¥æœ¬èªåŒ»ç™‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«æ¦‚è¦ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶å®¤ã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹MedTxt-CRã‚’ç”¨ã„ã¦ã€alabniiã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹RoBERTaã‚’fine-tuningã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ æ–‡ä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®ã‚¿ã‚°ã‚’IOB2å½¢å¼ã§ä»˜ä¸ã—ã¾ã™ (Positiveã‚„Negativeãªã©ã®äº‹å®Ÿæ€§ãŒåŒæ™‚ã«ä»˜ä¸ã•ã‚Œã‚‹ã‚¿ã‚°ã‚‚ã‚ã‚Šã¾ã™)ã€‚
 * ğŸ“¥ 37 [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base) - recruit-jp/japanese-typo-detector-roberta-baseãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦æ—¥æœ¬èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨å„æ–‡å­—ã”ã¨ã«èª¤å­—è„±å­—ã§ã‚ã‚‹ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¾ã™å„ãƒ©ãƒ™ãƒ«ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™idlabelmeaning0OKèª¤å­—ãªã—1deletion1æ–‡å­—ã®æŠœã‘2insertion_aä½™åˆ†ãª1æ–‡å­—ã®æŒ¿å…¥3insertion_bç›´å‰ã®æ–‡å­—åˆ—ã¨ä¸€è‡´ã™ã‚‹ï¼’æ–‡å­—ä»¥ä¸Šã®ä½™åˆ†ãªæ–‡å­—ã®æŒ¿å…¥4kanji-conversion_aåŒä¸€ã®èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰5kanji-conversion_bè¿‘ã„èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰6substitution1æ–‡å­—ã®å…¥ã‚Œæ›¿ãˆ7transpositionéš£æ¥ã™ã‚‹ï¼’æ–‡å­—é–“ã®è»¢ç½®8othersãã®ä»–ã®å…¥åŠ›èª¤ã‚Šèª¤ã‚Šç¨®é¡ã®è©³ç´°ã«ã¤ã„ã¦ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…ƒè«–æ–‡ã‚’ã”å‚ç…§ãã ã•ã„æ—¥æœ¬èª Wikipedia ã®ç·¨é›†å±¥æ­´ã«åŸºã¥ã å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ãã®ä»–ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯å½“ç¤¾ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„èª¤å­—è„±å­—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Hubã«å…¬é–‹ã—ã¾ã—ãŸ (Recruit Data Blog)å­¦ç¿’ãƒ‡ãƒ¼ã‚¿äº¬éƒ½å¤§å­¦å¤§å­¦é™¢æƒ…å ±å­¦ç ”ç©¶ç§‘çŸ¥èƒ½æƒ…
 * ğŸ“¥ 36 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k) - Heron BLIP Japanese StableLM Base 7B llava-620kModel DetailsHeron BLIP Japanese StableLM
 * ğŸ“¥ 36 [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large) - rinna/japanese-hubert-largeOverviewThis is a Japanese HuBERT Large model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Large model, which contains 24 transformer layers with 16 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * ğŸ“¥ 36 [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model) - Model overviewThis model is the baseline model for awesome-japanese-nlp-classification-dataset. It was trained on this dataset, saved using the development data, and evaluated using the test data.
 * ğŸ“¥ 36 [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm) - ku-nlp/roberta-large-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/roberta-large-japanese-char-wwm')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/r
 * ğŸ“¥ 36 [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps) - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.Usage:Install pip install faster-whisper (Check faster-whisper for detailed instructions.)from faster_whisper import WhisperModelmodel = WhisperModel('zh-plus/faster-whisper-large-v2-japanese-5k-steps', device="cuda", compute_type="float16")segments, info = model.transcribe("audio.mp3", beam_size=5)print("Detected language '%s' with probability %f" % (info.language, info.language_probability))for segment in segments:print("[%.2fs -
 * ğŸ“¥ 35 [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja) - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™samplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)prompt = "ãã‚Œã¯ä¹æœˆåˆæ—¬ã®ã‚ã‚‹è’¸ã—æš‘ã„æ™©ã®ã“ã¨ã§ã‚ã£ãŸã€‚ç§ã¯ã€ï¼¤å‚ã®"inputs = tokenizer(prompt, return_tensors="pt")generate_ids = model.generate(inputs.input_ids,max_length=30,top_k=30,top_p=0.95,temperature=0.6,repetition_penalty=1.2,do_sample=True,)tokenizer.decode(gen
 * ğŸ“¥ 34 [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1) - swallow-hermes-st-v1ç‰©èªä½œæˆã«å¼·ã‚ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºæ¥ãªã„ã‹ã¨è€ƒãˆã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ Chat Vectorã¨EvoLLMã®æ‰‹æ³•ã«å½±éŸ¿ã‚’å—ã‘ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 34 [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese) - Model Card for Japanese BART largeModel descriptionThis is a Japanese BART large model pre-trained on Japanese Wikipedia. How to useYou can use this model as follows:from transformers import AutoTokenizer, MBartForConditionalGenerationtokenizer = AutoTokenizer.from_pretrained('ku-nlp/bart-large-japanese')model = MBartForConditionalGeneration.from_pretrained('ku-nlp/bart-large-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç†
 * ğŸ“¥ 33 [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix) - â—†REV-Mix"ãƒ¬ãƒœãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ DateMixã‚„RDtMixã‚’æ„è­˜ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 32 [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator) - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Our pretraining process employs subword units derived from the Japanese Wikipedia, using the Byte-Pair Encoding method and building on an initial tokenization with mecab-ipadic-NEologd.
 * ğŸ“¥ 32 [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner) - bert-japanese-nerã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã‚¿ã‚¹ã‚¯ã‚’ç›®çš„ã¨ã—ã¦ã€äº¬éƒ½å¤§å­¦ é»’æ©‹ãƒ»è¤šãƒ»æ‘è„‡ç ”ç©¶å®¤ãŒå…¬é–‹ã—ã¦ã„ã‚‹BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ãŒå…¬é–‹ã—ã¦ã„ã‚‹ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚ How to useã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Tokenizerã«ä¸Šè¿°ã®äº¬éƒ½å¤§å­¦BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã®Tokenizerã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
 * ğŸ“¥ 32 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct) - æ›´æ–°æƒ…å ±æ—¥æœ¬èªæ©Ÿèƒ½ã¨instructãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã—ãŸver.2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ãƒ¢ãƒ‡ãƒ«æ¦‚è¦Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ Swallow-MX-8x7b-NVE-v0.1 + Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1Swallow-MX-8x7b-NVE-v0.1ã¯ã€ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆé•·4096ã¾ã§ã®æ—¥æœ¬èªç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€è‹±èªãƒ¢ãƒ‡ãƒ«ã®Instructãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã§ã€æµæš¢ãªæ—¥æœ¬èªæ©Ÿèƒ½ã‚’ç¶­æŒã—ã¦ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆé•·ã‚’32Kã¾ã§æ‹¡å¤§ã€Instructæ©Ÿèƒ½ã‚’å¤§å¹…ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 31 [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt) - æ—¥æœ¬èªT5 Prefix Language ModelThis is a T5 (Text-to-Text Transfer Transformer) Adapted Language Model fine-tuned on Japanese corpus.
 * ğŸ“¥ 31 [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ Blogè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"text = "ã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©ã«ã¤ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ç¤ºã—ã€è§£èª¬ã—ã¦ãã ã•ã„ã€‚"model_name = "elyza/ELYZA-japanese-CodeLlama-7b-instruct"tokenizer = AutoTokenizer.from_pretrained
 * ğŸ“¥ 31 [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese) - nlp-waseda/gpt2-xl-japaneseThis is Japanese GPT2 with approximatelyã€€1.5B parameters pretrained on Japanese Wikipedia and CC-100The model architecture of the model are based on Radford+ 2019.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task. Note that the texts should be segmented into words using Juman++ in advance.
 * ğŸ“¥ 30 [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft) - The English document is here. ãƒ¢ãƒ‡ãƒ«æ¦‚è¦Llama2-13bã«æ—¥æœ¬èªèªå½™ã‚’è¿½åŠ ã—ã¦ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 30 [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 29 [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã®å‡ºåŠ›å±¤ã«CRFå±¤ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ãƒ‡ãƒ«ã‚’llm-book/ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«ã®ãƒ—ãƒ­é‡çƒé¸æ‰‹"# textä¸­ã®å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºpprint(ner_pipe
 * ğŸ“¥ 29 [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * ğŸ“¥ 29 [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering) - RoBERTa base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model. The codes for the fine-tuning are available on this notebookUsagefrom transformers import AutoModelForQuestionAnswering, AutoTokenizerquestion = 'ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ€ãƒ¼ãƒ»ã‚°ãƒ©ãƒãƒ ãƒ»ãƒ™ãƒ«ã¯ã€ã©ã“ã§ç”Ÿã¾ã‚ŒãŸã®?'context = 'ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ€ãƒ¼ãƒ»ã‚°ãƒ©ãƒãƒ ãƒ»ãƒ™ãƒ«ã¯ã€ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰ç”Ÿã¾ã‚Œã®ç§‘å­¦è€…ã€ç™ºæ˜å®¶ã€å·¥å­¦è€…ã§ã‚ã‚‹ã€‚ä¸–ç•Œåˆã®&gt;å®Ÿç”¨çš„é›»è©±ã®ç™ºæ˜ã§çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚'model = AutoModelForQuestionAnswering.from_pretrained('ybelkada/japanese-roberta-quest
 * ğŸ“¥ 29 [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation) - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
 * ğŸ“¥ 28 [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa) - bert-base-japanese-v3-jcommonsenseqaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(å¤šè‚¢é¸æŠå¼è³ªå•å¿œç­”)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®JCommonsenseQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 28 [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese) - bart-base-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University. Both the encoder and decoder outputs are identical to the original Fairseq model.
 * ğŸ“¥ 26 [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B) - GPT-Neo 1.3B pre-trained model for JapaneseModel DescriptionGPT2/GPT3 like model trained on Japanese.corpus. Training datacc100 jaoscar jawikipedia jaHow to usefrom transformers import pipeline&gt;&gt;&gt; generator = pipeline('text-generation', model='yellowback/gpt-neo-japanese-1.3B')&gt;&gt;&gt; generator("ã“ã‚“ã°ã‚“ã¯ã€å¾³å·å®¶åº·ã§ã™ã€‚", do_sample=True, max_length=50, num_return_sequences=3)[{'generated_text': 'ã“ã‚“ã°ã‚“ã¯ã€å¾³å·å®¶åº·ã§ã™ã€‚ ä¸–ã®ä¸­ã‚’è¦‹æ¸¡ã—ã¦ã¿ã¦ã‚‚ã€æ®‹å¿µãªã“ã¨ã ã‘ã‚Œã©ã‚‚ã€ã¾ãã‚Œã‚‚ãªãã€Œä¸–ã®ãªã‹...\n5æœˆã«ãªã‚Šã¾ã—ãŸã€å®¶åº·ã§ã™ã€‚ ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¦ã‚£ãƒ¼ã‚¯ã‚‚çµ‚ã£ã¦ã—ã¾ã„ã€ä¸–é–“ã§ã¯'},{'generated_text':
 * ğŸ“¥ 26 [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base) - SambaLingo-Japanese-BaseSambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset. This model reports state of the art evaluation results in perplexity and FLORES-200 translation.
 * ğŸ“¥ 25 [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€MARC-ja(positive or negativeã®äºŒå€¤åˆ†é¡)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’yahoo japan/JGLUEã®MARC-ja( https://github.com/yahoojapan/JGLUE )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚positive or negativeã®äºŒå€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for MARC-ja which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE MARC-ja dataset. You could use this model for binary classification (positive or negative) tasks.ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ accuracy of modelprecision : 0.967,accura
 * ğŸ“¥ 25 [jovyan/Swallow-MS-7b-v0.1-ChatVector](https://huggingface.co/jovyan/Swallow-MS-7b-v0.1-ChatVector) - Swallow-MS-7b-v0.1-ChatVectorJapanese "instruction tuned" model made by the technique of Chat VectorThe weights of this model are obtained not by any instruction tuning but by the following arithmetic:Swallow-MS-7b-v0.1 + Mistral-7B-Instruct-v0.2 - Mistral-7B-v0.1Chat Vectorã®æ‰‹æ³•ã‚’ä½¿ã£ã¦ã€å­¦ç¿’æ¸ˆã¿é‡ã¿ã®è¶³ã—å¼•ãã®ã¿ã§Swallow-MS-7b-v0.1ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å¯¾è©±èƒ½åŠ›ã‚’ä¸ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ã“ã¡ã‚‰ã®æ—¥æœ¬èªè¨˜äº‹ã§è§£èª¬ã—ã¦ã„ã¾ã™ã€‚Instruction formatThe promot format should be the same as Mistral-7B-Instruct-v0.2.E.g.text = "&lt;s&gt;[INST] What is your favourite condiment?
 * ğŸ“¥ 25 [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1. Please refer to the original model for license details and more information.
 * ğŸ“¥ 24 [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese) - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset. Kanji are converted into Hiragana using the pykakasi library during training and evaluation.
 * ğŸ“¥ 24 [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider) - Japanese CLIP ViT-H/14 (Wider)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space. Capable of multimodal tasks including zero-shot image classification,text-to-i
 * ğŸ“¥ 24 [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese) - Mixtral-8x7B-v0.1-japaneseMixtral-8x7B-v0.1-japaneseã¯Mixtral-8x7B-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ABEJAã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸMetagton-LMã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯ã“ã¡ã‚‰ã§ã™ã€‚ä½¿ã„æ–¹import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "abeja/Mixtral-8x7B-v0.1-japanese"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,use_cache=True,device_map="auto",)model.eval()input_text = """# systemèª å®Ÿã§ç´³å£«çš„ã§å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ç°¡æ½”ã§ã‚ã‹ã‚Šã‚„
 * ğŸ“¥ 23 [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition) - This is for (private) DEMO only.
 * ğŸ“¥ 23 [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base) - Finance Sentiment JA (base)Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news. It was trained on the translated version of Financial PhraseBank by Malo et al.
 * ğŸ“¥ 23 [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseConsidering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese. This model was trained using SentenceTransformers Cross-Encoder class.
 * ğŸ“¥ 22 [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation) - å›ç­”ã¨å›ç­”ãŒå‡ºã¦ãã‚‹ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ä¸ãˆã‚‹ã¨è³ªå•æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://github.com/sonoisa/deep-question-generationæœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚¹ãƒ†ãƒƒãƒ—æ¦‚è¦SQuAD 1.1ã‚’æ—¥æœ¬èªã«æ©Ÿæ¢°ç¿»è¨³ã—ã€ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯ç´„åŠåˆ†ï¼‰ã€‚ å›ç­”ãŒå«ã¾ã‚Œã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€è³ªå•æ–‡ã€è§£ç­”ã®3ã¤çµ„ãŒã§ãã‚‹ã€‚
 * ğŸ“¥ 22 [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 22 [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - luke-large-defamation-detection-japaneseæ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºå™¨This model is a fine-tuned version of studio-ousia/luke-japanese-large for the Japanese language finetuned for automatic defamation detection. The original foundation model was finetuned on a balanced dataset created by unifying two datasets:DefamationJapaneseYouTube : TBALabels:0 -&gt; "ä¸­å‚·æ€§ã®ãªã„ç™ºè¨€"1 -&gt; "è„…è¿«çš„ãªç™ºè¨€"2 -&gt; "ä¾®è”‘çš„ãªç™ºè¨€"3"-&gt; "åèª‰ã‚’ä½ä¸‹ã•ã›ã‚‹ç™ºè¨€"Example Pipeline# !
 * ğŸ“¥ 21 [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha [Kaggle] [X] [LinkedIn]This is a meta-llama/Meta-Llama-3-8B-Instruct model that finetuned on Japanese conversation dataset.
 * ğŸ“¥ 21 [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on Hiroshi Matsuda's BERT base Japanese, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * ğŸ“¥ 21 [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1. Please refer to the original model for license details and more information.
 * ğŸ“¥ 21 [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm) - ku-nlp/roberta-base-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/roberta-base-japanese-char-wwm')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/robe
 * ğŸ“¥ 20 [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small) - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain. æ—¥æœ¬èªç‰¹è¨±è«‹æ±‚é …è¦ç´„ï¼ˆåŒ»è–¬ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³é™å®šï¼‰"""ã€è«‹æ±‚é …ï¼‘ã€‘ãƒ’ãƒˆï¼£ï¼¤ï¼“ï¼˜ï¼ˆé…åˆ—ç•ªå·ï¼‘ï¼‰åŠã³ã‚«ãƒ‹ã‚¯ã‚¤ã‚¶ãƒ«ï¼£ï¼¤ï¼“ï¼˜ï¼ˆé…åˆ—ç•ªå·ï¼’ï¼‰ã«ç‰¹ç•°çš„ã«çµåˆã™ã‚‹å˜é›¢ã•ã‚ŒãŸæŠ—ä½“ã§ã‚ã£ã¦ã€ï½ï¼‰ä»¥ä¸‹ã‚’å«ã‚€é‡é–å¯å¤‰é ˜åŸŸï¼šï½‰ï¼‰é…åˆ—ç•ªå·ï¼“ã‚’å«ã‚€ç¬¬ï¼‘ã®ï¼£ï¼¤ï¼²ï¼›ï½‰ï½‰ï¼‰é…åˆ—ç•ªå·ï¼”ã‚’å«ã‚€ç¬¬ï¼’ã®ï¼£ï¼¤ï¼²ï¼›ï½‰ï½‰ï½‰ï¼‰é…åˆ—ç•ªå·ï¼•ã‚’å«ã‚€ç¬¬ï¼“ã®ï¼£ï¼¤ï¼²ï¼›åŠã³ï½‚ï¼‰ä»¥ä¸‹ã‚’å«ã‚€è»½é–å¯å¤‰é ˜åŸŸï¼šï½‰ï¼‰é…åˆ—ç•ªå·ï¼–ã‚’å«ã‚€ç¬¬ï¼‘ã®ï¼£ï¼¤ï¼²ï¼›ï½‰ï½‰ï¼‰é…åˆ—ç•ªå·ï¼—ã‚’å«ã‚€ç¬¬ï¼’ã®ï¼£ï¼¤ï¼²ï¼›ï½‰ï½‰ï½‰ï¼‰é…åˆ—ç•ªå·ï¼˜ã‚’å«ã‚€ç¬¬ï¼“ã®ï¼£ï¼¤ï¼²ï¼›ã‚’å«ã‚€ã€æŠ—ä½“ã€‚
 * ğŸ“¥ 20 [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese) - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset. The sentence outputs do not contain word boundaries.
 * ğŸ“¥ 20 [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora) - Japanese DialoGPT trained with Aozora(ja) é’ç©ºæ–‡åº«ã®ã‚»ãƒªãƒ•ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®DialoGPT Smallã§ã™(en) Japanese DialoGPT Small trained on Aozora Bunko. DemoDemo in this page is not working so well.
 * ğŸ“¥ 19 [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset ) ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on cl-tohoku/bert-large-japanese-v2This model is fine-tuned by using Wikipedia dataset.
 * ğŸ“¥ 19 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0) - Heron GIT Japanese StableLM Base 7BModel DetailsHeron GIT Japanese StableLM
 * ğŸ“¥ 19 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚This model is fine-tuned model for Named Entity Recognition (NER) which is based on deberta-v2-base-japaneseThis model is fine-tuned by using Wikipedia dataset. You could use this model for NER tasks.
 * ğŸ“¥ 19 [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news) - bart-base-japanese-news(base-sized model)This repository provides a Japanese BART model. The model was trained by Stockmark Inc.
 * ğŸ“¥ 18 [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos) - bert-large-japanese-luw-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-large-japanese-char-extended.
 * ğŸ“¥ 18 [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss) - æ—¥æœ¬èªã§trainingã—ãŸllama2model size:  130.78Mtrainingã¯ä»¥ä¸‹ã®scriptå‚ç…§https://github.com/Lightning-AI/lit-gpt/tree/mainusefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")import torchfrom transformers import GenerationConfigprompt="ã‚ã®ã‚¤ãƒ¼ãƒãƒˆãƒ¼ãƒ´ã‚©ã®ã™ãã¨ãŠã£ãŸé¢¨ã€"inputs = tokenizer(prompt, return_tensors="pt")input_ids = inputs["input_ids"]generation_config = Gener
 * ğŸ“¥ 18 [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator) - ELECTRA base Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 17 [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos) - deberta-large-japanese-unidic-luw-uposModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts for POS-tagging and dependency-parsing, derived from deberta-large-japanese-unidic.
 * ğŸ“¥ 17 [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2) - In-progess long-context Japanese-English translation model based on tinyllama. Input should be 500-1000 tokens long.
 * ğŸ“¥ 17 [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert) - nagisa_bertA BERT model for nagisa. The model is available in Transformers ğŸ¤—.A tokenizer for nagisa_bert is available here.
 * ğŸ“¥ 17 [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite) - Japanese transformer pipeline (bert-base). Components: transformer, parser, ner.FeatureDescriptionNameja_gsd_bert_wwm_unidic_liteVersion3.1.1spaCy&gt;=3.1.0,&lt;3.2.0Default Pipelinetransformer, parser, nerComponentstransformer, parser, nerVectors0 keys, 0 unique vectors (0 dimensions)SourcesUD_Japanese-GSDUD_Japanese-GSD r2.8+NESudachiDict_corecl-tohoku/bert-base-japanese-whole-word-maskingunidic_liteLicenseCC BY-SA 4.0AuthorMegagon Labs Tokyo.
 * ğŸ“¥ 16 [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja) - BERT base Japanese modelThis repository contains a BERT base model trained on Japanese Wikipedia dataset. Training dataJapanese Wikipedia dataset as of June 20, 2021 which is released under Creative Commons Attribution-ShareAlike 3.0 is used for training.
 * ğŸ“¥ 16 [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows:!pip install mecab-python3!pip install unidic-lite!python -m unidic downloadimport torchimport torchaudioimport librosafrom datasets import load_datasetimport MeCabfrom transformers
 * ğŸ“¥ 16 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0) - Heron BLIP Japanese StableLM Base 7BDEMOYou can play the demo of this model here.
 * ğŸ“¥ 16 [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator) - ELECTRA small Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * ğŸ“¥ 16 [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This pretrained model is almost the same as cl-tohoku/bert-base-japanese-char-v2 but do not need fugashi or unidic_lite. The only difference is in word_tokenzer_type property (specify basic instead of mecab) in tokenizer_config.json.
 * ğŸ“¥ 15 [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base) - MPT-7B-baseã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ Model DateJune 28, 2023Model LicenseApache-2.0è©•ä¾¡Jumtra/test_data_100QAã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®æ­£ç­”ç‡ã‚’è©•ä¾¡ã—ãŸmodel nameæ­£ç­”ç‡mosaicml/mpt-7b16/100mosaicml/mpt-7b-instruct28/100Jumtra/mpt-7b-base47/100Jumtra/mpt-7b-inst46/100ä½¿ç”¨æ–¹æ³•æ³¨æ„ï¼šã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã«trust_remote_code=Trueã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 15 [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst) - MPT-7B-instã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7b-instructã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ Model DateJune 28, 2023Model LicenseCC-BY-SA-3.0è©•ä¾¡Jumtra/test_data_100QAã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®æ­£ç­”ç‡ã‚’è©•ä¾¡ã—ãŸmodel nameæ­£ç­”ç‡mosaicml/mpt-7b16/100mosaicml/mpt-7b-instruct28/100Jumtra/mpt-7b-base47/100Jumtra/mpt-7b-inst46/100ä½¿ç”¨æ–¹æ³•æ³¨æ„ï¼šã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€from_pretrainedãƒ¡ã‚½ãƒƒãƒ‰ã«trust_remote_code=Trueã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 15 [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’yahoo japan/JGLUEã®JCommonsenseQA( https://github.com/yahoojapan/JGLUE ) ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 15 [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying) - yacis-electra-small-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection. The original foundation model was originally pretrained on 5.6 billion words YACIS blog corpus, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * ğŸ“¥ 14 [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-wase
 * ğŸ“¥ 14 [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * ğŸ“¥ 14 [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False) - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 1.7B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation. For Japaneseè©³ç´°ãªèª¬æ˜ã‚„å®Ÿé¨“ã«é–¢ã—ã¦ã¯ã€Œã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã€‘é‡å­åŒ–ã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ã®åŠ¹æœæ¸¬å®šã€ã‚’ã”è¦§ãã ã•ã„ã€‚How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinetokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-1.7b-instruction-sft", use_fast=False)model = AutoModelForCausalLM.from_pretrained("line-c
 * ğŸ“¥ 14 [mpasila/calm2-7b-safetensors](https://huggingface.co/mpasila/calm2-7b-safetensors) - This is a conversion of cyberagent/calm2-7b  to safetensors so you don't have to worry about getting hacked by downloading dirty pickled files. Original model card:CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets.
 * ğŸ“¥ 14 [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b) - Leia-Swallow-7BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language. This model is constructed by applying LEIA to Swallow, a Japanese-English bilingual LLM based on LLaMA 2.The model achieves enhanced performance on six Japanese question-answering benchmarks, as reported below.
 * ğŸ“¥ 14 [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja) - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset. This will translate Japanese sentences to English sentences.
 * ğŸ“¥ 14 [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos) - deberta-base-japanese-wikipedia-luw-uposModel DescriptionThis is a DeBERTa(V2) model pre-trained on Japanese Wikipedia and é’ç©ºæ–‡åº« texts for POS-tagging and dependency-parsing, derived from deberta-base-japanese-wikipedia.
 * ğŸ“¥ 14 [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp) - Genji-JP 6BPlease check our blog post for more details, samples, evaluations and more:BlogpostModel DescriptionGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model. This particular model is trained on Japanese web novels.
 * ğŸ“¥ 13 [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-smallFine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly as follows.from transformers import WhisperForConditionalGeneration, WhisperProcessorfrom datasets import load_datasetimport librosaimport torchLANG_ID
 * ğŸ“¥ 13 [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram) - Japanese BERT-base (Nothing + Unigram)How to load the tokenizerPlease download the dictionary file for Nothing + Unigram from our GitHub repository. Then you can load the tokenizer by specifying the path of the dictionary file to dict_path.from typing import Optionalfrom tokenizers import Tokenizer, NormalizedString, PreTokenizedStringfrom tokenizers.processors import BertProcessingfrom tokenizers.pre_tokenizers import PreTokenizerfrom transformers import PreTrainedTokenizerFast# load a tokenizerdict_path =
 * ğŸ“¥ 13 [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork) - LINE DistilBERT Japanese (forked by liwii)This is a forked version of DistilBERT model pre-trained on 131 GB of Japanese web text. The teacher model is BERT-base that built in-house at LINE.The model was trained by LINE Corporation.
 * ğŸ“¥ 13 [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base) - Japanese CLIP ViT-H/14 (Base)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space. Capable of multimodal tasks including zero-shot image classification,text-to-im
 * ğŸ“¥ 13 [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient) - outputç­‘æ³¢ 2.0035860538482666ã¤ãã° 1.6586617231369019ç ”ç©¶ 1.6227693557739258å¤§å­¦ 1.3798155784606934å®Ÿé¨“ 0.5522942543029785å­¦ç”Ÿ 0.42351895570755005åˆ†æ 0.37844282388687134å›½ç«‹ 0.3685397505760193ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ 0.36495038866996765èŒ¨åŸ 0.3056415021419525ç§‘å­¦ 0.2876652181148529é–¢æ± 0.24301066994667053åœ°åŸŸ 0.21340851485729218å®Ÿæ–½ 0.1976248174905777å…ˆç«¯ 0.192025288939476ã‚µã‚¤ãƒˆ 0.11629197001457214èª¿æŸ» 0.09159307181835175ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ 0.08552580326795578è­°è«– 0.07484486699104309æ¤œè¨ 0.007034890353679657
 * ğŸ“¥ 13 [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0) - (English part follows Japanese one. )SD-XL 1.0-jp-refiner Model Cardç·è¨ˆ5.8Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ŒSDXLã‚’æ—¥æœ¬èªå…¥åŠ›ã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ï¼
 * ğŸ“¥ 13 [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram) - Japanese BERT-base (Juman++ + Unigram)How to load the tokenizerPlease download the dictionary file for Juman++ + Unigram from our GitHub repository. Then you can load the tokenizer by specifying the path of the dictionary file to dict_path.from typing import Optionalfrom tokenizers import Tokenizer, NormalizedString, PreTokenizedStringfrom tokenizers.processors import BertProcessingfrom tokenizers.pre_tokenizers import PreTokenizerfrom transformers import PreTrainedTokenizerFastfrom pyknp import Jumanimport
 * ğŸ“¥ 13 [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese) - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 largeFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows.import torchimport librosafrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorLANG_ID =
 * ğŸ“¥ 13 [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT dataset{s}.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 13 [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese) - åè¨€æ¨è«–ãƒ¢ãƒ‡ãƒ«
 * ğŸ“¥ 12 [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo) - Tanuki-ZeroBase model: llm-jp/llm-jp-13b-v1.0Instruction data: Randomly sampled, 15k Jaster dataset (train)Code is here.
 * ğŸ“¥ 12 [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0) - Heron GIT Japanese ELYZA Llama 2 Fast 7BModel DetailsHeron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model that can converse about input images.
 * ğŸ“¥ 12 [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1. Please refer to the original model for license details and more information.
 * ğŸ“¥ 12 [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf) - Japanese-TextGen-Kage-v0.1-2x7BThis is a merge model using Mergekit-Evolve.We merged our model and Ninja-v1 with Mergekit-Evolve and then franken MoE.This model has been made more powerful by merging Ninja-v1! Prompt formatWe recommend Vicuna format.SYSTEM: &lt;ANY SYSTEM CONTEXT&gt;USER:ASSISTANT:ExampleUSER: ã€Œç”˜ã„ã€ã€ã€Œãƒ‘ã‚½ã‚³ãƒ³ã€ã€ã€Œå¥³æ€§ã€ã‚’ä½¿ã£ã¦æ–‡ç« ã‚’æ›¸ã„ã¦ãã ã•ã„ASSISTANT: ã‚ã‚‹å–«èŒ¶åº—ã§ã€éš£ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¼‚ã†ç¾å‘³ã—ãã†ãªç”˜ã„é¦™ã‚Šã«ã¤ã‚‰ã‚Œã€ãµã¨æŒ¯ã‚Šå‘ã‘ã°ã€å¯æ„›ã‚‰ã—ã„å¥³æ€§ãŒãƒ‘ã‚½ã‚³ãƒ³ã«å‘ã‹ã£ã¦çœŸå‰£ãªè¡¨æƒ…ã§ä»•äº‹ã‚’ã—ã¦ã„ã¾ã—ãŸã€‚
 * ğŸ“¥ 12 [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZA-japanese-Llama-2-13b-fast-instructã® GGUF å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * ğŸ“¥ 12 [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp) - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b. See intfloat/e5-mistral-7b-instruct page or evaluation notebook of oshizo/JapaneseEmbeddingEval for model usage.
 * ğŸ“¥ 12 [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction. This model is a AWQ quantized(miniaturized to 3.89GB) version of the original model(13.48GB).Model DetailsCurrently, this model is confirmed to work with Colab A100 or RTX 3000 Series on local PC.This is because autoAWQ uses NVIDIA's PTX assembly instructions, some of which are only supported on sm80 and higher architectu.
 * ğŸ“¥ 12 [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp) - Model Card for Japanese DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp', trust_remote_code=True)model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2
 * ğŸ“¥ 12 [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp) - whisper-large-v2-mix-jp model for CTranslate2This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format. This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.
 * ğŸ“¥ 12 [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora) - deberta-large-japanese-aozoraModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts. NVIDIA A100-SXM4-40GB took 127 hours 8 minutes for training.
 * ğŸ“¥ 12 [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese) - yacis-electra-smallThis is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus. The corpus was tokenized for pretraining with MeCab.
 * ğŸ“¥ 11 [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char) - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * ğŸ“¥ 11 [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA) - friendly_JA-Modelã€€(T5 fine-tuned model)MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexiconExamplesinputoutputæœ€é©åŒ–ã‚’å¿œç”¨ã—ãŸæ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾åº¦ã ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿œç”¨ã—ãŸãƒã‚·ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ã å½¼ã¯æ¶ç©ºã®ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹å½¼ã¯ã‚¤ãƒã‚¸ãƒŠãƒªãƒ¼ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«æ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«ã‹ã‹ã£ã¦ã—ã¾ã£ãŸæ·±å±¤å­¦ç¿’ã¯é›£ã—ã„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚€ãšã‹ã—ã„æ–°ãŸãªæ¦‚å¿µã‚’ç´¹ä»‹ã™ã‚‹æ–°ã—ã„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç´¹ä»‹ã™ã‚‹æ´¥æ³¢ã®è­¦å ±ãŒæµã‚ŒãŸãƒ„ãƒŠãƒŸã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒæµã‚ŒãŸå—æµ·ãƒˆãƒ©ãƒ•ã®ç½å®³ã¯éœ‡æºåœ°ã«ã‚ˆã‚‹å—æµ·ãƒˆãƒ©ãƒ•ã®ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒ¼ã¯ã‚¨ãƒ”ã‚»ãƒ³ã‚¿ãƒ¼ã«ã‚ˆã‚‹æ¯å­ã¯éš›ã©ã„å†…å®¹ã®æœ¬ã‚’
 * ğŸ“¥ 11 [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying) - electra-base-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on Izumi Lab ELECTRA small Japanese discriminator, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * ğŸ“¥ 11 [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - alabnii/jmedroberta-base-sentencepieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "æ‰æœ¬æµ·äºº and å£¹å²å¤ªä¸€ and çŸ¥ç”°æ‚ ç”Ÿ and é‡‘æ²¢è¼ä¸€ and ç›¸æ¾¤å½°å­",title =     "J{M}ed{R}o{BERT}a: æ—¥æœ¬èªã®åŒ»å­¦è«–æ–‡ã«ã‚‚ã¨ã¥ã„ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡",booktitle = "è¨€èªå‡¦ç†å­¦ä¼šç¬¬29å›å¹´æ¬¡å¤§ä¼š",year =      
 * ğŸ“¥ 11 [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’yahoo japan/JGLUEã®JCommonsenseQA( https://github.com/yahoojapan/JGLUE ) ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-base-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset. You could use this model for CommonsenseQA tasks.
 * ğŸ“¥ 11 [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * ğŸ“¥ 11 [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b) - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers. See this article for details(Japanese)https://note.com/oshizo/n/n9140df790315See intfloat/e5-mistral-7b-instruct page for model usage.
 * ğŸ“¥ 11 [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b) - Leia-Swallow-13BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language. This model is constructed by applying LEIA to Swallow, a Japanese-English bilingual LLM based on LLaMA 2.The model achieves enhanced performance on four out of six Japanese question answering benchmarks and equivalent performance on the remaining two, as reported below.
 * ğŸ“¥ 11 [AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer](https://huggingface.co/AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer) - Adapter bert-base-multilingual-cased-ja-wiki_pfeiffer for bert-base-multilingual-casedPfeiffer Adapter trained with Masked Language Modelling on Japanese Wikipedia Articles for 250k steps and a batch size of 64.This adapter was created for usage with the Adapters library. UsageFirst, install adapters:pip install -U
 * ğŸ“¥ 11 [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseThis model was trained using SentenceTransformers Cross-Encoder class. This model is based on tohoku-nlp/bert-base-japanese-v3.Training DataThe model was trained on following datasets.
 * ğŸ“¥ 11 [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels) - NLLB 1.3B fine-tuned on Japanese to English Light Novel translationThis model was fine-tuned on light and web novel for Japanese to English translation. It can translate sentences and paragraphs up to 512 tokens.
 * ğŸ“¥ 11 [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200) - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g. FrenchFine-tuned facebook/wav2vec2-large-xlsr-53 on {language} using the Common Voice, ... and ... dataset{s}. #TODO: replace {language} with your language, e.g. French and eventually add more datasets that were used and eventually remove common voice if model was not trained on common voiceWhen using this model, make sure that your speech input is sampled at 16kHz.

## Datasets

This list is sorted by downloads as of May 13, 2024.

 * ğŸ“¥ 27586 [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - Please feel free to open an issue or pull request. Dataset SummaryFrom JGLUE's README.md:JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese.
 * ğŸ“¥ 26232 [juletxara/mgsm](https://huggingface.co/datasets/juletxara/mgsm) - The same 250 problems from GSM8K are each translated via human annotators in 10 languages. The 10 languages are:SpanishFrenchGermanRussianChineseJapaneseThaiSwahiliBengaliTeluguGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems.
 * ğŸ“¥ 11934 [datasets/iwslt2017](https://huggingface.co/datasets/iwslt2017) - SummaryThe IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT systemacross all directions including English, German, Dutch, Italian and Romanian. As unofficial task, conventionalbilingual text translation is offered between English and Arabic, French, Japanese, Chinese, German and Korean.
 * ğŸ“¥ 11209 [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - JMTEB: Japanese Massive Text Embedding BenchmarkJMTEB is a benchmark for evaluating Japanese text embedding models.
 * ğŸ“¥ 2884 [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese. This dataset is licensed under CC-BY-SA-3.0Last Update : 2023-05-11databricks-dolly-15k-jahttps://github.com/kunishou/databricks-dolly-15k-jadatabricks-dolly-15khttps://github.com/databrickslabs/dolly/tree/master/data
 * ğŸ“¥ 1805 [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Avg. 
 * ğŸ“¥ 1684 [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - AutoWikiQAæ±å·¥å¤§ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MXã‚’ç”¨ã„ã¦ã€Wikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€Œè³ªå•(query)ã€ã¨ã€Œå›ç­”(answer)ã€ã‚’ç”Ÿæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã¨å›ç­”ã«ã¤ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ æ—¥æœ¬èªã®ãƒ•ãƒªãƒ¼ãªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã¯2024å¹´4æœˆç¾åœ¨ã§æœ€å¤§è¦æ¨¡ã¨ãªã£ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 1392 [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - range3/wiki40b-jaThis dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted. It is generated by the following python code.
 * ğŸ“¥ 1259 [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ELYZA-tasks-100: æ—¥æœ¬èªinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆData Descriptionæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ è©³ç´°ã¯ ãƒªãƒªãƒ¼ã‚¹ã®noteè¨˜äº‹ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
 * ğŸ“¥ 1186 [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples. Please also refer to the original dataset kunishou/hh-rlhf-49k-ja.
 * ğŸ“¥ 941 [datasets/wiki_atomic_edits](https://huggingface.co/datasets/wiki_atomic_edits) - Leaderboards[More Information Needed]LanguagesThe languages in the dataset are:deenesfritjp: Japanese (ja)ruzhDataset StructureData InstancesHere are some examples of questions and facts:Data Fields[More Information Needed]Data Splits[More Information Needed]Dataset CreationCuration Rationale[More Information Needed]Source Data[More Information Needed]Initial Data Collection and Normalization[More Information Needed]
 * ğŸ“¥ 861 [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼Œæ—¢å­˜ç ”ç©¶ [7] ã«å€£ã„ï¼ŒWikipedia2 ã®è¨˜äº‹åã‚’ç­”ãˆã¨ã—ãŸï¼Œæ—¥æœ¬èªã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³ QA ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹. Supported TasksJAQKET v1.0From the original paper:æœ¬ç ”ç©¶ã¦ã‚™æ‰±ã†æ—¥æœ¬èªã‚ªãƒ¼ãƒ•ã‚šãƒ³ãƒˆã‚™ãƒ¡ã‚¤ãƒ³ QA ã‚¿ã‚¹ã‚¯ã‚’å®šç¾©ã™ã‚‹.
 * ğŸ“¥ 813 [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
 * ğŸ“¥ 779 [llm-jp/oasst1-21k-ja](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) - oasst1-21k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of an English subset of oasst1 using DeepL.English subset is here.
 * ğŸ“¥ 729 [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - databricks-dolly-15k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of databricks-dolly-15k using DeepL.Send Questions tollm-jp(at)nii.ac.jpModel Card AuthorsThe names are listed in alphabetical order.
 * ğŸ“¥ 728 [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en) - The dataset was constructed in 3 steps:selecting business scenes,writing monolingual conversation scenarios according to the selected scenes, andtranslating the scenarios into the other language. Half of the monolingual scenarios were written in Japaneseand the other half were written in English.
 * ğŸ“¥ 539 [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - Japanese Anime Speech Datasetæ—¥æœ¬èªã¯ã“ã¡ã‚‰japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models. The dataset is comprised of thousands of audio clips and their corresponding transcriptions from different visual novels.
 * ğŸ“¥ 496 [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - JMMLUJapanese Massive Multitask Language Understanding BenchmarkJMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions). It is designed to assess the performance of large language models in Japanese.
 * ğŸ“¥ 453 [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA) - Dataset DetailsDataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset. This version is translated into Japanese using DeepL API and is aimed at serving similar purposes in the context of Japanese language.
 * ğŸ“¥ 450 [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - LLM ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆèƒ½åŠ›ã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ HumanEval ã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ã€‚æ©Ÿæ¢°ç¿»è¨³(DeepL, GPT-4)ã®ç¿»è¨³çµæœã‚’å…¨ã¦äººæ‰‹ã«ã‚ˆã£ã¦å†ä¿®æ­£ã—ã€ è¨³æ–‡ã‚’æ—¥æœ¬äººã®ãƒ—ãƒ­ã‚°ãƒ©ãƒãŒèª­ã‚“ã§ç†è§£ã—ã€ã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‘ã‚‹å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã€‚ãŸã ã—ã€è‹±èªç‰ˆ HumanEval ã®é–“é•ã„ã¯ã€ä¿®æ­£ã›ãšã«æ®‹ã—ã¦ã€ HumanEval åŒæ§˜ã«ä¸å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ç”Ÿæˆèƒ½åŠ›ã‚’è¦‹ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªLLM ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ãŠä½¿ã„ãã ã•ã„ã€‚LanguagesThe programmin
 * ğŸ“¥ 373 [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023) - Danbooru2023: A Large-Scale Crowdsourced and Tagged Anime Illustration DatasetDanbooru2023 is a large-scale anime image dataset with over 5 million images contributed and annotated in detail by an enthusiast community.
 * ğŸ“¥ 353 [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja) - hh-rlhf-12k-jaThis repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of a subset of hh-rlhf using DeepL.This dataset consists of 12,000 entries randomly sampled from hh-rlhf.
 * ğŸ“¥ 351 [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ æœ¬ã‚³ãƒ¼ãƒ‘ã‚¹ã¯ã€NHN Japanæ ªå¼ä¼šç¤¾ãŒé‹å–¶ã™ã‚‹ã€Œlivedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã®ã†ã¡ã€ä¸‹è¨˜ã®ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚ºãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒé©ç”¨ã•ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åé›†ã—ã€å¯èƒ½ãªé™ã‚ŠHTMLã‚¿ã‚°ã‚’å–ã‚Šé™¤ã„ã¦ä½œæˆã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 340 [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - Rakuda - Questions for Japanese modelsRepository: https://github.com/yuzu-ai/japanese-llm-rankingThis is a set of 40 questions in Japanese about Japanese-specific topics designed to evaluate the capabilities of AI Assistants in Japanese.
 * ğŸ“¥ 320 [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - OverviewThis dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.[For Japanese] æ—¥æœ¬èªã§ã®æ¦‚è¦èª¬æ˜ã‚’ Qiita ã«è¨˜è¼‰ã—ã¾ã—ãŸ: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodologyThe code to reproduce this dataset is made available on GitHub: globis-org/aozorabunko-exctractor.1. Data collectionWe firstly downloaded the CSV file that lists all works.
 * ğŸ“¥ 284 [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset) - Githubãƒªãƒã‚¸ãƒˆãƒªstockmarkteam/ner-wikipedia-datasetã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Citation@inproceedings{omi-2021-wikipedia,title = "Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰",author = "è¿‘æ±Ÿ å´‡å®",booktitle = "è¨€èªå‡¦ç†å­¦ä¼šç¬¬27å›å¹´æ¬¡å¤§ä¼š",year = "2021",url = "https://anlp.jp/proceedings/annual_meeting/2021/pdf_dir/P2-7.pdf",}LicenceWikipediaæ—¥æœ¬èªç‰ˆã¨åŒã˜CC-BY-SA 3.0ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
 * ğŸ“¥ 279 [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese. JaQuAD contains 39,696 question-answer pairs.
 * ğŸ“¥ 276 [neulab/odex](https://huggingface.co/datasets/neulab/odex) - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark. It contains 945 samples with a total of 1,707 human-written test cases, covering intents in four different natural languages --  439 in English, 90 in Spanish, 164 in Japanese, and 252 in Russian.
 * ğŸ“¥ 258 [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction) - ichikara-instruction (Non Commercial)LLMã®ãŸã‚ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›å¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚ ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã‚ã‚ŒãŸæ–¹ã¯ã€HPã¨å…±ã«ä¸‹è¨˜ã®é€šã‚Šã«ãŠé¡˜ã„ã—ã¾ã™ã€‚
 * ğŸ“¥ 239 [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - OpenMathInstruct-1 ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãª180ä¸‡ä»¶ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚ OpenMathInstruct-1 ã¯ã€GSM8K ãŠã‚ˆã³ MATH ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã® question ã¨ Mixtral-8x7B ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸ solution ã®ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã‚‹æ•°å­¦åˆ†é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 225 [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["æ–‡å­—é£ã„ç¨®åˆ¥"] == "æ–°å­—æ–°ä»®å"
 * ğŸ“¥ 222 [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus) - The corpus has 50,000 manually simplified and aligned sentences. This corpus contains the original sentences, simplified sentences and English translation of the original sentences.
 * ğŸ“¥ 192 [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli) - Dataset PreprocessingSupported Tasks and LeaderboardsLanguagesæ³¨é‡ˆã¯ã™ã¹ã¦æ—¥æœ¬èªã‚’ä¸»è¦è¨€èªã¨ã—ã¦ã„ã¾ã™ã€‚ Dataset Structureãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ TSV ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã€å„è¡ŒãŒãƒ©ãƒ™ãƒ«ã€å‰æã€ä»®èª¬ã®ä¸‰ã¤çµ„ã‚’è¡¨ã—ã¾ã™ã€‚
 * ğŸ“¥ 189 [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - llm-japanese-datasetLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³(ãƒãƒ£ãƒƒãƒˆ)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸»ã«ï¼Œè‹±èªã§æ§‹ç¯‰ã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼ â€»æ§˜ã€…ãªå…¬é–‹è¨€èªè³‡æºã‚’åˆ©ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸï¼
 * ğŸ“¥ 187 [transformersegmentation/CHILDES](https://huggingface.co/datasets/transformersegmentation/CHILDES) - Phonemized Child Directed Speech DatasetThis dataset contains utterance downloaded from CHILDES which have been pre-processed and converted to phonemic transcriptions by this processing script. Many of the columns from CHILDES have been preserved in case they may be useful for experiments (e.g. number of morphemes, part-of-speech tags, etc.).
 * ğŸ“¥ 185 [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - JQaRA : Japanese Question Answering with Retrieval Augmentation - æ¤œç´¢æ‹¡å¼µ(RAG)è©•ä¾¡ã®ãŸã‚ã®æ—¥æœ¬èª Q&amp;A ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜æ€§èƒ½ãª LLM ã®å°é ­ã«ä¼´ã„ã€LLM ã‚’ç”¨ã„ãŸè³ªç–‘å¿œç­”ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ ã—ã‹ã—ãªãŒã‚‰ã€LLM ã¯è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ãªå›ç­”ã™ã‚‹çŸ¥è­˜ã‚’æœ‰ã—ã¦ã„ãªã„ã¨ã€ç­”ãˆã‚‹ã“ã¨ãŒã§ããªã„ã ã‘ã§ãªãã€èª¤ã£ãŸå›ç­”ã‚’è¿”ç­”ã™ã‚‹ã¨ã„ã£ãŸèª²é¡ŒãŒå­˜åœ¨ã—ã¾ã™ã€‚
 * ğŸ“¥ 182 [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA) - è‡ªå‹•ç”ŸæˆQ&amp;Aç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡äºŒç¨®é¡ã®è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒå­˜åœ¨ã—ã¾ã™CommonCrawlã¾ãŸã¯ã€CC-BYç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚ å…ƒã®æ–‡ç« ã¨ã®é¡ä¼¼åº¦(ä¾æ‹ æ€§)ãŒä½ããªã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã€å…ƒæ–‡ç« ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«éƒ¨åˆ†æŠœç²‹ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨ã„ã¦ã„ã¾ã™
 * ğŸ“¥ 174 [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese. The "ng_translation" flag indicates that the translation was not successful, and "1" means that the translation failed.
 * ğŸ“¥ 166 [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - range3/cc100-jaThis dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded. ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€cc100ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ã€ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚
 * ğŸ“¥ 163 [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset) - Dataset overviewThis dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).The labels are categorized as "Relevant (1)" and "Not Relevant (0)". Problem Setting:Training Data: Repository descriptions from before 2022Test Data: Repository descriptions from 2023Objective: To detect repositories related to Japanese NLPData Collection:Positive Examples: Repositories listed in "awesome-japanese-nlp-resources" as of September 9, 2023Negative Examples: Coll
 * ğŸ“¥ 160 [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - Update:2023/12/25oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸoasst2-chat-68k-jaã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚This dataset was created by automatically translating "OpenAssistant/oasst2" into Japanese by DeepL."OpenAssistant/oasst2" ã‚’ DeepLç¿»è¨³ã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ Instruction ã¨ Output ï¼ˆprompterã®å‘½ä»¤ã¨assistantã®å›ç­”ï¼‰ã®å½¢å¼ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã“ã¡ã‚‰ã®ã‚³ãƒ¼ãƒ‰ã§å¤‰æ›ã—ã¦ä¸‹ã•ã„ï¼ˆå¤‰æ›ã«ã¯5åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ï¼‰ã€‚å¤‰æ›ã‚³ãƒ¼ãƒ‰å‚è€ƒhttps://github.com/h2oai/h2o-llmstudio/blob/5ebfd3879e226b4e1afd0a0b45eb632e60412129/app_utils/utils.py#L1888pip install datasetsfrom datasets import l
 * ğŸ“¥ 158 [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences) - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Licenceæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä½¿ç”¨ã—ã¦ã„ã‚‹ Wikipedia ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã€ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚ºè¡¨ç¤ºãƒ»ç¶™æ‰¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ 3.0 (CC BY-SA 3.0) ãŠã‚ˆã³ GNU è‡ªç”±æ–‡æ›¸ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (GFDL) ã®ä¸‹ã«é…å¸ƒã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 154 [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹2010ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’huggingfaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ï½¡2009 å¹´åº¦ã«ãŠã‘ã‚‹è‘—ä½œæ¨©æ³•ã®æ”¹æ­£ï¼ˆå¹³æˆ21å¹´é€šå¸¸å›½ä¼šã€€è‘—ä½œæ¨©æ³•æ”¹æ­£ç­‰ã«ã¤ã„ã¦ | æ–‡åŒ–åºï¼‰ã«åŸºã¥ãï¼Œæƒ…å ±è§£æç ”ç©¶ã¸ã®åˆ©ç”¨ã«é™ã£ã¦åˆ©ç”¨å¯èƒ½ã§ã™ï½¡å½¢æ…‹ç´ è§£æã‚’ç”¨ã„ã¦ï½¤è‡ªå‹•ã§å¥ç‚¹ã‚’ã¤ã‘ã¾ã—ãŸï½¡å¤‰æ›ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆå½¢æ…‹ç´ è§£æãªã©
 * ğŸ“¥ 131 [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - [github]. Please also refer to this repo.
 * ğŸ“¥ 111 [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - Japanese stopwords for nagisaThis is a stopword list of frequently used words in the Japanese language, created according to the tokenization rules of the Japanese text analysis library, nagisa. This list is constructed by extracting the top 100 most commonly used words from the CC-100 dataset and Wikipedia.
 * ğŸ“¥ 111 [hotchpotch/wikipedia-ja-20231030](https://huggingface.co/datasets/hotchpotch/wikipedia-ja-20231030) - Wikipedia Japanese data (20231030)Source Date: 2023/10/30Source: https://dumps.wikimedia.org/other/cirrussearch/LicenseCC BY-SA 4.0ExampleWIP
 * ğŸ“¥ 107 [Nexdata/multi_language](https://huggingface.co/datasets/Nexdata/multi_language) - SummaryThe dataset contains 25,000 hours of multi-language reading speech data. It's recorded by native speakers, covering English, French, German, Russian, Spanish, Portuguese, Italian, Japanese, Korean, Hindi, Vietnamese, Tagalog, Thai etc.
 * ğŸ“¥ 107 [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - Dataset detailsEach entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation. The intended use-case is for document translation tasks.
 * ğŸ“¥ 106 [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese) - The annotation is by majority decision by 5 - 10 crowd workers. Target tweets include "COVID" or "ã‚³ãƒ­ãƒŠ".
 * ğŸ“¥ 106 [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese) - fungi_indexed_mycological_papers_japaneseå¤§èŒè¼ªã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰====LanguagesJapaneseThis dataset is available in Japanese only. æ¦‚è¦Atsushi Nakajimaï¼ˆä¸­å³¶æ·³å¿—ï¼‰ãŒå€‹äººã§é‹å–¶ã—ã¦ã„ã‚‹Webã‚µã‚¤ãƒˆå¤§èŒè¼ª ã§ã¯ã€æ•°åƒä»¶ä»¥ä¸Šã®èŒé¡åˆ†é¡å­¦è«–æ–‡ã‚’ã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ã¨ã„ã†å½¢ã§è¦ç´„ãŠã‚ˆã³ç´¢å¼•ä»˜ã‘ï¼ˆã‚¤ãƒ³ãƒ‡ã‚­ã‚·ãƒ³ã‚°ï¼‰ã—ãŸæƒ…å ±ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 106 [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese) - fungi_diagnostic_chars_comparison_japaneseå¤§èŒè¼ªã€Œè­˜åˆ¥å½¢è³ªã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰====LanguagesJapaneseThis dataset is available in Japanese only. æ¦‚è¦Atsushi Nakajimaï¼ˆä¸­å³¶æ·³å¿—ï¼‰ãŒå€‹äººã§é‹å–¶ã—ã¦ã„ã‚‹Webã‚µã‚¤ãƒˆå¤§èŒè¼ª ã§ã¯ã€æ•°åƒä»¶ä»¥ä¸Šã®èŒé¡åˆ†é¡å­¦è«–æ–‡ã‚’ã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ã¨ã„ã†å½¢ã§è¦ç´„ãŠã‚ˆã³ç´¢å¼•ä»˜ã‘ï¼ˆã‚¤ãƒ³ãƒ‡ã‚­ã‚·ãƒ³ã‚°ï¼‰ã—ãŸæƒ…å ±ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 105 [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ãŸã‚‚ã®ã§ã™ã€‚ æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã¯å¤šããŒã€Œæ”¿åºœæ¨™æº–åˆ©ç”¨è¦ç´„ï¼ˆç¬¬2.0ç‰ˆï¼‰ã€ã«æº–æ‹ ã—ã¦ãŠã‚Šã€ã“ã®è¦ç´„ã¯CC-BY-4.0ï¼ˆå›½éš›ï¼‰ã¨äº’æ›æ€§ãŒã‚ã‚‹ã¨è¨˜è¿°ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 105 [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database) - fungi_trait_circus_databaseå¤§èŒè¼ªã€ŒTrait Circusã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆçµ±åˆ¶å½¢è³ªï¼‰æœ€çµ‚æ›´æ–°æ—¥ï¼š2023/12/29====LanguagesJapanese and EnglishPlease do not use this dataset for academic purposes for the time being.  (casual use only)å½“é¢ã®é–“ä»®å…¬é–‹ã¨ã—ã¾ã™ã€‚
 * ğŸ“¥ 103 [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - J-ResearchCorpusUpdate:2024/3/16è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š(NLP2024)ã‚’å«ã‚€ã€è«–æ–‡ 1,343 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 2024/2/25è¨€èªå‡¦ç†å­¦ä¼šèªŒã€Œè‡ªç„¶è¨€èªå‡¦ç†ã€ã®ã†ã¡ CC-BY-4.0 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ 360 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ æ¦‚è¦CC-BY-* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè«–æ–‡ã‚„å­¦ä¼šèªŒç­‰ã‹ã‚‰æŠœç²‹ã—ãŸé«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã‚„ RAG ç­‰ã§ã”æ´»ç”¨ä¸‹ã•ã„ã€‚
 * ğŸ“¥ 81 [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - llm-japanese-dataset-vanillaLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆizumi-lab/llm-japanese-dataset ã‹ã‚‰ï¼Œæ—¥è‹±ç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ã‚’æŠœã„ãŸã‚‚ã®ã§ã™ï¼ ä¸»ã«ï¼Œæ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼
 * ğŸ“¥ 81 [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus) - IntroductionThis is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus. The original JParaCrawl corpus was put together by automated means - aligning Japanese texts with their apparent English translations that were found in-the-wild, on the internet.
 * ğŸ“¥ 74 [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - JA-VG-VQA-500Dataset DescriptionJA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset. This dataset was used in the evaluation of EvoVLM-JP-v1-7B.Please refer to our report and blog for more details.
 * ğŸ“¥ 64 [gbenson/webui-dom-snapshots](https://huggingface.co/datasets/gbenson/webui-dom-snapshots) - It has been generated using this raw template. Dataset DetailsDataset DescriptionCurated by: Gary BensonLanguages: Mostly English (87%); Dutch, French, Chinese, Japanese (1-2% each);
 * ğŸ“¥ 57 [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - range3/wikipedia-ja-20230101This dataset consists of a parquet file from the wikipedia dataset with only Japanese data extracted. It is generated by the following python code.
 * ğŸ“¥ 56 [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions) - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja ã®ä¸­ã‹ã‚‰ JGLUEï¼ˆ JcommonsenseQA , MARC-ja , JSQuAD ï¼‰ã®è¦³ç‚¹ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã®è©³ç´°ã¯ã“ã¡ã‚‰ã‚’å‚è€ƒã«ã—ã¦ä¸‹ã•ã„ã€‚
 * ğŸ“¥ 55 [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli) - JSNLI Version 1.1 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆ (train_w_filtering)  ã¨æ¤œè¨¼ã‚»ãƒƒãƒˆ (dev) ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 53 [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick) - Dataset. JSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.
 * ğŸ“¥ 52 [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja) - Dataset used to train PokÃ©mon text to image model, add a Japanese Column of PokÃ©mon BLIP captionsBLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN). Original images were obtained from FastGAN-pytorch and captioned with the pre-trained BLIP model.
 * ğŸ“¥ 52 [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped) - chatbot-arena-ja-calm2-7b-chatã‹ã‚‰promptãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 51 [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki) - JaWikiWikipediaã®HTMLå½¢å¼ã®ãƒ€ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ Wikiextractorã«ã‚ˆã£ã¦æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ç•°ãªã‚Šã€æ®µè½ãªã©ã®æ–‡æ›¸æ§‹é€ ã‚’ç¶­æŒã—ãŸã¾ã¾ã€ä¸è¦ãªãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ã®ãªã„ãƒ†ã‚­ã‚¹ãƒˆãŒåˆ©ç”¨ã§ãã¾ã™ã€‚
 * ğŸ“¥ 51 [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja) - mmarcoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦ã€queryã‚’keyã¨ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ å…ƒãƒ‡ãƒ¼ã‚¿ä¸­ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‘¨ã‚Šã®ãƒŸã‚¹ã®ä¿®æ­£ã‚„NFKCæ­£è¦åŒ–ãªã©ã®å‰å‡¦ç†ã‚’è¡Œã£ã¦ã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 51 [Amani27/massive_translation_dataset](https://huggingface.co/datasets/Amani27/massive_translation_dataset) - Supported Tasks and LeaderboardsTranslationLanguagesEnglish (en_US)German (de_DE)Hindi (hi_IN)Spanish (es_ES)French (fr_FR)Italian (it_IT)Arabic (ar_SA)Dutch (nl_NL)Japanese (ja_JP)Portugese (pt_PT)
 * ğŸ“¥ 46 [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever) - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Licenceæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ä¸€éƒ¨ã®ã‚¯ã‚¤ã‚ºå•é¡Œã®è‘—ä½œæ¨©ã¯ abc/EQIDEN å®Ÿè¡Œå§”å“¡ä¼šã«å¸°å±ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã®ã‚¯ã‚¤ã‚ºå•é¡Œã¯æœ¬æ›¸ã«ãŠã‘ã‚‹ä½¿ç”¨è¨±è«¾ã‚’å¾—ã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 46 [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats) - OverviewThis dataset is of conversations extracted from Aozora Bunko (é’ç©ºæ–‡åº«), which collects public-domain books in Japan, using a simple heuristic approach.[For Japanese] æ—¥æœ¬èªã§ã®æ¦‚è¦èª¬æ˜ã‚’ Qiita ã«è¨˜è¼‰ã—ã¾ã—ãŸ: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodFirst, lines surrounded by quotation mark pairs (ã€Œã€) are extracted as utterances from the text field of globis-university/aozorabunko-clean. Then, consecutive utterances are collected and grouped together.
 * ğŸ“¥ 45 [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja) - oasst1-89k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚ ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã™ã‚‹éš›ã«ã”æ´»ç”¨ä¸‹ã•ã„ï¼ˆ1ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·ãŒå¤§ãã„ã®ã§ãã‚Œãªã‚Šã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦ã«ãªã‚Šã¾ã™ï¼‰ã€‚
 * ğŸ“¥ 42 [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset) - Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆGitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/LICENSE: CC-BY-SA 3.0Developed by Stockmark Inc.
 * ğŸ“¥ 42 [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation) - Japanese-Vietnamese Translated Sentence Pairs.
 * ğŸ“¥ 38 [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - CABank Japanese CallHome CorpusParticipants:   120Type of Study:  phone callLocation:   United StatesMedia type: audioDOI:    doi:10.21415/T5H59VWeb: https://ca.talkbank.org/access/CallHome/jpn.htmlCitation informationSome citation here. In accordance with TalkBank rules, any use of data from this corpus must be accompanied by at least one of the above references.
 * ğŸ“¥ 36 [neulab/mconala](https://huggingface.co/datasets/neulab/mconala) - Spanish, Japanese, and Russian. LanguagesSpanish, Japanese, Russian; PythonDataset StructureHow to Usefrom datasets import load_dataset# Spanish subsetload_dataset("neulab/mconala", "es")DatasetDict({test: Dataset({features: ['question_id', 'intent', 'rewritten_intent', 'snippet'],num_rows: 341})})
 * ğŸ“¥ 34 [simon3000/genshin-voice](https://huggingface.co/datasets/simon3000/genshin-voice) - Genshin VoiceGenshin Voice is a dataset of voice lines from the popular game Genshin Impact. Hugging Face ğŸ¤—  Genshin-VoiceLast update at 2024-04-30413429 wavs18016 without speaker (4%)22956 without transcription (6%)720 without inGameFilename (0%)Dataset DetailsDataset DescriptionThe dataset contains voice lines from the game's characters in multiple languages, including Chinese, English, Japanese, and Korean.
 * ğŸ“¥ 33 [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law) - Japanese LawsThis dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. Each entry furnishes comprehensive details about a particular law, encapsulating its number, title, unique ID, the date it came into effect, and its complete text. To ensure the dataset's uniqueness, deduplication was executed based on the most recent effective version as of August 1, 2023.A typical entry in this dataset is structured as follows:{"num": "Law Number (e.g., Reiwa 5th Year Pollut
 * ğŸ“¥ 32 [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct) - Amenokaku-Code-InstructUpdate:2023/12/27ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« JaxTon , ãƒ—ãƒ­ã«ãªã‚‹Java ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ 180 ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ æ¦‚è¦ã‚³ãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸ5.2Kã®Instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 32 [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR) - JaCWIR: Japanese Casual Web IR - æ—¥æœ¬èªæƒ…å ±æ¤œç´¢è©•ä¾¡ã®ãŸã‚ã®å°è¦æ¨¡ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªWebã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿‘å¹´ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å°é ­ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚’ç”¨ã„ãŸè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§è³ªå•ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã¦ã„ã¾ã™ã€‚ ã—ã‹ã—ãªãŒã‚‰ã€å¤šæ§˜ãªã‚¸ãƒ£ãƒ³ãƒ«ã® Web è¨˜äº‹ã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é©åˆ‡ã«ç­”ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ãªæƒ…å ±æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ååˆ†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
 * ğŸ“¥ 29 [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - CoTangentã¯äººæ‰‹ã§ä½œæˆã•ã‚ŒãŸé«˜å“è³ªã§ã‚¯ãƒªãƒ¼ãƒ³ãª100ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªCoTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ CoTangent_ja.json: CoTéƒ¨åˆ†ã¨outputéƒ¨åˆ†ãŒç¹‹ãŒã£ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 28 [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset) - å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã¯llm-book/ner-wikipedia-datasetã¨åŒæ§˜ã®ã‚‚ã®ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å…¨éƒ¨ã§8ç¨®é¡ (äººåã€æ³•äººåã€åœ°åã€è£½å“åã€æ”¿æ²»çš„çµ„ç¹”åã€æ–½è¨­åã€ãã®ä»–ã®çµ„ç¹”åã€ã‚¤ãƒ™ãƒ³ãƒˆå)ã‚ã‚Šã¾ã™ã€‚ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã£ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 28 [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - Japanese-Heron-BenchDataset DescriptionJapanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models). We collected 21 images related to Japan.
 * ğŸ“¥ 27 [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese. This dataset contains 69K ja-en-translation task data and is licensed under CC BY SA 3.0.Last Update : 2023-04-18databricks-dolly-15k-jahttps://github.com/kunishou/databricks-dolly-15k-jadatabricks-dolly-15khttps://github.com/databrickslabs/dolly/tree/master/data
 * ğŸ“¥ 26 [fujiki/wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - This dataset is a reformatted version of the Japanese portion of wiki40b dataset. When you use this dataset, please cite the original paper:@inproceedings{guo-etal-2020-wiki,title = "{W}iki-40{B}: Multilingual Language Model Dataset",author = "Guo, Mandy  andDai, Zihang  andVrande{\v{c}}i{\'c}, Denny  andAl-Rfou, Rami",booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",month = may,year = "2020",address = "Marseille, France",publisher = "European Language Resources Associati
 * ğŸ“¥ 26 [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives) - [Under Construction]This is a repository containing all the queries from the Japanese part of the MMarco dataset, the multilingual version of the MSMarco dataset. For each query, there are matching hard negatives:25 of them retrieved by the multilingual e5 base model.
 * ğŸ“¥ 25 [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp) - JBLiMPThis is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023). Only the validated pairs used for benchmarks are included, and only in JSONL format, since it's redundant with the TSV.For details see the original git repo or the paper.
 * ğŸ“¥ 23 [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs) - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ Licenceæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä½¿ç”¨ã—ã¦ã„ã‚‹ Wikipedia ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã€ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚ºè¡¨ç¤ºãƒ»ç¶™æ‰¿ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ 3.0 (CC BY-SA 3.0) ãŠã‚ˆã³ GNU è‡ªç”±æ–‡æ›¸ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (GFDL) ã®ä¸‹ã«é…å¸ƒã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 23 [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01) - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated. Row order has also been randomized to avoid clusters of similar translations.
 * ğŸ“¥ 21 [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO) - Chatbot Arena Conversationsã®è³ªå•æ–‡ã‹ã‚‰ã€aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ã‚’ä½¿ç”¨ã—ã¦å¿œç­”æ–‡ã‚’ä½œæˆã—ã¾ã—ãŸè³ªå•æ–‡ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®Promptéƒ¨åˆ†ã‚’ä½¿ç”¨ã—ã¾ã—ãŸChatbot Arena Conversations JA (calm2)ä»¥ä¸‹å¼•ç”¨ã§ã™ã€‚ æŒ‡ç¤ºæ–‡ï¼ˆpromptï¼‰ã¯lmsys/chatbot_arena_conversationsã®ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ï¼ˆCC-BY 4.0ï¼‰ã‚’å’Œè¨³ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 20 [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs) - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils. Pre-processing was performed using oshizo/wikipedia-utils, which is a fork of the original repository, singletongue/wikipedia-utils.
 * ğŸ“¥ 20 [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja) - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦æ‰‹å‹•ã§ä½œæˆã—ãŸDatabricksã«é–¢ã™ã‚‹è³ªå•ã¨å›ç­”ãƒšã‚¢ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ ä»¶æ•°ï¼šç´„1,300ä»¶æƒ…å ±æºï¼šDatabricks HPã®æ—¥æœ¬èªãƒ–ãƒ­ã‚°ã‚„FAQãªã©ã€ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒªãƒƒã‚¯ç¤¾å“¡ãŒãƒã‚¹ãƒˆã—ãŸQittaè¨˜äº‹https://github.com/yulan-yan/build-your-chat-bot-JPã€€ãƒ‡ãƒ¢ã«åˆ©ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
 * ğŸ“¥ 19 [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k) - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.Since it is a llama2 license, it can be used commercially for services. Some strange dialogue may be included as it has not been screened by humans.
 * ğŸ“¥ 19 [sudy-super/JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - DescriptionThis dataset was used to pre-train Co-Encoder's Context Encoder when we participated in LOCAL AI HACKATHON #000.The number of tokens (Using tokenizer of calm2-chat)LanguageThe number of tokensJapanese4.7bEnglish5bCode0.9bNOTEThis dataset has not passed sentence end boundary determination or Perplexity Filtering, so there is room for improvement in quality.
 * ğŸ“¥ 19 [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - Sakura_datasetå•†ç”¨åˆ©ç”¨å¯èƒ½ãªè¶…å°è¦æ¨¡é«˜å“è³ªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚categoryã¯ä»¥ä¸‹commonsense_qa: å¸¸è­˜å•é¡ŒCalc-ape210k: æ•°å­¦å•é¡Œjapanese-commonsense-openqa: æ—¥æœ¬ã®å¸¸è­˜å•é¡Œ(è‡ªä½œ)ä¸‹è¨˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚commonsense_qaMU-NLPC/Calc-ape210kLICENSEThis dataset is licensed under Database Contents License (DbCL) v1.0UpdateLast Update : 2023-06-07Example Code# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿import osfrom peft.utils.config import TaskTypeos.environ["CUDA_VISIBLE_DEVICES"]="0"import peftimport transformersimport datasets#
 * ğŸ“¥ 17 [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - mqaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ å…ƒãƒ‡ãƒ¼ã‚¿ä¸­ã®ãƒã‚¤ã‚¸ãƒ¼ãªãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚„NFKCæ­£è¦åŒ–ãªã©ã®å‰å‡¦ç†ã‚’è¡Œã£ã¦ã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 17 [RyokoAI/Syosetu711K](https://huggingface.co/datasets/RyokoAI/Syosetu711K) - Not all information here may be accurate or accessible. Dataset SummarySyosetu711K is a dataset composed of approximately 711,700 novels scraped from the Japanese novel self-publishingwebsite Syosetuka ni Narou (JA: å°èª¬å®¶ã«ãªã‚ã†, lit.
 * ğŸ“¥ 15 [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored) - For the English version, please click here. æ¦‚è¦databricks-dolly-15k-ja-scoredã¯kunishou/databricks-dolly-15k-jaã®æ´¾ç”Ÿã§ã‚ã‚Šã€BERTScoreã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ç¿»è¨³å“è³ªã‚¹ã‚³ã‚¢ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 14 [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja) - Japanese Wikipedia Human Retrieval datasetThis is a Japanese question answereing dataset with retrieval on Wikipedia articlesby trained human workers. ContributorsYusuke Odadefined the dataset specification, data structure, and the scheme of data collection.
 * ğŸ“¥ 14 [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese) - Sorry, it's no longer available on Hugging Face. Please reach out to those who have already downloaded it.
 * ğŸ“¥ 14 [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja) - ApolloCorpus-jaæ¦‚è¦å¤šè¨€èªåŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® ApolloCorpus ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸ 525k ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚ ApolloCorpus ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã‹ã¤å“è³ªã‚’æ‹…ä¿ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 14 [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja) - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild. Original dataset is liuhaotian/llava-bench-in-the-wild.
 * ğŸ“¥ 14 [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k) - cosmopedia-japanese-20kã®ãƒ‡ãƒ¼ã‚¿ã«ã€kunishouæ§˜ã‹ã‚‰20k-100kã‚’ã”æä¾›ã„ãŸã ã‘ã‚‹ã“ã¨ã«ãªã‚Š100kã¾ã§æ‹¡å¤§ã—ã¾ã—ãŸã€‚ https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-previewãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç¿»è¨³ã‚‚å«ã‚€ãƒ‡ãƒ¼ã‚¿ã¯ã€ä¸Šè¨˜ãƒ¬ãƒã‚¸ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
