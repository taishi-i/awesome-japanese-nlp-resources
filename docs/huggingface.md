# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1399 models and 554 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## üìñ Contents

Released [a tool üîé](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search) for searching through a large number of repository information.

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).
Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Multimodality](#Multimodality)
	 * [Text Generation](#Text-Generation)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Reasoning](#Reasoning)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## üéâ The latest additions

**Models**
11 models have been added.

- [Aratako/NemoAurora-RP-12B-GGUF](https://huggingface.co/Aratako/NemoAurora-RP-12B-GGUF)
- [mmnga/Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf)
- [cardiffnlp/tweet-topic-base-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-base-multilingual)
- [future-architect/Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B)
- [buildertakuya/Berghof-Takuya7B](https://huggingface.co/buildertakuya/Berghof-Takuya7B)
- [UEC-InabaLab/Llama-3.1-KokoroChat-Low](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Low)
- [UEC-InabaLab/Llama-3.1-KokoroChat-High](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-High)
- [UEC-InabaLab/Llama-3.1-KokoroChat-Full](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Full)
- [akkikiki/j-moshi-ext-mlx](https://huggingface.co/akkikiki/j-moshi-ext-mlx)
- [mmnga/alfredplpl-suzume-poc-gguf](https://huggingface.co/mmnga/alfredplpl-suzume-poc-gguf)
- [mayocream/manga-ocr-onnx](https://huggingface.co/mayocream/manga-ocr-onnx)


**Datasets**
11 datasets have been added.

- [SakanaAI/EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench)
- [Sakaji-Lab/LATGNJ](https://huggingface.co/datasets/Sakaji-Lab/LATGNJ)
- [ayousanz/OSCOR-2301-ja-cleaned-0](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned-0)
- [vsachi/sachi-dataset-ja](https://huggingface.co/datasets/vsachi/sachi-dataset-ja)
- [li-lab/JPMedReason](https://huggingface.co/datasets/li-lab/JPMedReason)
- [YUGOROU/Counseling-Reasoning-v1.8](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.8)
- [YUGOROU/Counseling-Reasoning-v1.9-parquet](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.9-parquet)
- [kujirahand/popular_anime_corpus](https://huggingface.co/datasets/kujirahand/popular_anime_corpus)
- [reep0610/AGI-japanese-text-dataset-for-Deep-Learning](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-for-Deep-Learning)
- [ayousanz/css10-ja-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ja-ljspeech)
- [OmniAICreator/Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M)


## üß† Models

This list is sorted by downloads as of June 17, 2025.
1399 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 2,567,620
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from xlm-roberta-base using a dataset of Japanese Wikipedia articles from Stockmark Inc., classifying tokens into PER and ORG entities.
  - Downloads: 658,315
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M parameter text embedding model by Retrieva, primarily for Japanese but also supporting English, built upon the modernbert-ja-310m architecture.
  - Downloads: 435,709
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri provides Japanese text embeddings‚Äîincluding v3 models up to 315M parameters with 8192 max length‚Äîfor use with Sentence Transformers via `pip install`.
  - Downloads: 314,990
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B is a high-performing Japanese text embedding model by Preferred Networks, achieving top scores on JMTEB for tasks like information retrieval and text classification.
  - Downloads: 279,087
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - Rinna's japanese-cloob-vit-b-16 is a Japanese contrastive language-image pre-training (CLIP) model, installable via pip and usable for image-text matching tasks.
  - Downloads: 265,590
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - This repository provides a Japanese BERT model pretrained with IPA dictionary-based word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 238,132
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This repository provides a refined Japanese Sentence-BERT model (v2) trained with MultipleNegativesRankingLoss, achieving improved accuracy over v1 and requiring `fugashi` and `ipadic` for inference.
  - Downloads: 165,672
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - This repository provides a Japanese BERT base model pre-trained on Jawiki-20200831 using character and word-level tokenization with whole word masking for improved language understanding.
  - Downloads: 134,241
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - This repository provides a Japanese BERT model pre-trained on CC-100 and JAWiki data, utilizing character and word-level tokenization with Unidic and whole word masking for improved language understanding.
  - Downloads: 109,346
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - This repository provides a Japanese BERT base model pre-trained on Japanese text using a combined word/character tokenization approach based on the IPA dictionary, mirroring the original BERT base architecture.
  - Downloads: 107,800
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - This repository provides a pre-trained Japanese DeBERTa V2 tiny model for masked language modeling, trained on Japanese Wikipedia, CC-100, and OSCAR datasets.
  - Downloads: 96,308
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - This repository provides a Japanese BERT model pre-trained on CC-100 and JAWIKI data using Unidic-lite word-level tokenization with whole word masking.
  - Downloads: 78,309
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - This repository provides a Japanese BERT base model pretrained on Japanese text, utilizing word-level tokenization with the IPA dictionary and WordPiece subword tokenization, mirroring the original BERT base architecture.
  - Downloads: 55,421
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This repository provides a Japanese Sentence-BERT model (version 1) for generating sentence embeddings, with a link to a more accurate version 2 and usage instructions.
  - Downloads: 54,570
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese sentence embedding model, built on LUKE and trained on diverse data, for tasks like semantic similarity and search.
  - Downloads: 53,418
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri provides Japanese general text embeddings using the `sentence-transformers` library with a pre-trained model ("cl-nagoya/ruri-small-v2") requiring specific prefixes ("„ÇØ„Ç®„É™:" or "ÊñáÁ´†:") for input texts.
  - Downloads: 46,496
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced large language model built on Meta Llama 3, optimized via pre-training and instruction tuning by ELYZA, Inc.
  - Downloads: 40,870
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 38,574
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained for enhanced Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 34,615
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 28,385
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, offering improved performance and efficiency with extended sequence length (8192 tokens) and a larger 100K vocabulary, utilizing FlashAttention.
  - Downloads: 27,280
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - This repository provides a BERT-based model fine-tuned for Japanese sentiment analysis of Amazon product reviews, classifying sentence polarity as positive or negative.
  - Downloads: 19,275
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1 to significantly improve Japanese language capabilities while maintaining English proficiency, utilizing a 200 billion token corpus.
  - Downloads: 18,145
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - This repository provides a Japanese DeBERTa V2 large language model pre-trained on diverse Japanese text corpora using character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 16,025
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - This repository provides a Japanese BERT model pretrained on Jawiki+20200831 using Unidic-lite word segmentation and whole word masking for improved language understanding.
  - Downloads: 14,464
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitions‚Äô repository hosts Japanese language models, including sarashina2.2-3b-instruct-v0.1, evaluated on Japanese and English tasks like MT Bench and Elyza-tasks-100.
  - Downloads: 13,368
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model built on Whisper v2.0, enhanced with speaker diarization, punctuation, and integrated into Hugging Face Transformers (v4.39+).
  - Downloads: 12,232
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese is a pre-trained, lightweight Japanese language model based on BERT, trained by LINE on a large corpus of web text and available via Hugging Face Transformers.
  - Downloads: 11,812
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring installation of `sentence-transformers`, `fugashi`, `sentencepiece`, and `unidic-lite` and specifying prefixes like "„ÇØ„Ç®„É™: " or "ÊñáÁ´†: " for input text.
  - Downloads: 11,754
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - Rinna‚Äôs japanese-gpt2-medium provides a pre-trained, medium-sized Japanese GPT-2 model for causal language modeling using the Transformers library.
  - Downloads: 11,635
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - This repository provides a Japanese BERT large model, pretrained using Unidic-lite word-level tokenization and whole word masking on CC-100 and JAWIKI datasets.
  - Downloads: 11,138
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - This repository provides a Japanese sentence-transformers model, based on studio-ousia/luke-japanese-base-lite and fine-tuned on jsnli, for generating 768-dimensional sentence embeddings useful for semantic search and clustering.
  - Downloads: 11,035
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository offers a base-sized Japanese RoBERTa model, trained with rinna's code and accessible via the `transformers` library for masked language modeling tasks.
  - Downloads: 10,204
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - This repository details an experiment applying differences extracted via a chat-vector approach between the suzume-llama-3-8B-japanese and meta-llama/Meta-Llama-3-8B-Instruct models to the larger meta-llama/Meta-Llama-3-70B-Instruct, yielding minimal changes and suggesting future exploration of scaling factors.
  - Downloads: 9,255
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1B-parameter English/Japanese pre-trained language model by Preferred Elements utilizing a hybrid Mamba-based architecture (like Samba) with added normalization for improved stability and performance.
  - Downloads: 9,032
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - This repository provides a spaCy v3-finetuned ELECTRA model (ja_ginza_electra) pretrained on Japanese mC4 data and built upon megagonlabs/transformers-ud-japanese-electra-base, distributed as a Python package with GiNZA v5.
  - Downloads: 8,630
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, trained on 312.5B Japanese tokens and achieving 8.68 perplexity, based on the EleutherAI/gpt-neox architecture.
  - Downloads: 8,610
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jsts) fine-tuned on the JSTS dataset for semantic similarity tasks, as detailed in Chapter 5 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 8,530
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - This repository details ku-nlp/gpt2-small-japanese-char, a 90M parameter Japanese character-level GPT-2 language model pre-trained on large Japanese text corpora and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 7,984
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Meta's Llama 3, enhanced with Japanese data and available in 8B & 70B Instruct and base versions released July 1, 2024.
  - Downloads: 6,379
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - LLM-jp-3-7.2b-instruct3 is a 7.2 billion parameter, instruction-tuned Japanese large language model developed by NII, provided in Hugging Face Transformers format with PyTorch dependencies.
  - Downloads: 6,316
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri provides Japanese general text embeddings with v3 models (30m-315m parameters, max length 8192) offering strong JMTEB performance, built on Sentence Transformers, Fugashi, SentencePiece, and UniDic-Lite.
  - Downloads: 5,960
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M is a computationally efficient Japanese BERT model trained on 4.09T tokens with RoPE and capable of handling long sequences using local and global attention.
  - Downloads: 5,679
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository hosts a small Japanese GPT-2 language model trained by rinna Co., Ltd., readily usable with the `transformers` library for causal language modeling.
  - Downloads: 5,506
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, offering models for improved Japanese text retrieval and ranking.
  - Downloads: 5,505
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - Rinna Co., Ltd.'s repository hosts a 1.3B-parameter Japanese GPT model for causal language modeling, readily usable with Hugging Face Transformers.
  - Downloads: 5,269
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper provides faster, distilled Whisper models for Japanese Automatic Speech Recognition (ASR) built upon OpenAI's large-v3 model, incorporating stable-ts punctuation for improved accuracy.
  - Downloads: 5,193
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Meta‚Äôs Llama 3.1, enhancing Japanese language capabilities while preserving English, using a 200 billion token corpus of Japanese web data, Wikipedia, and code.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2's Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 4,805
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - This repository provides a tiny, character-level Japanese DeBERTa V2 model pre-trained on diverse Japanese text corpora for masked language modeling tasks.
  - Downloads: 4,663
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - This repository provides a Japanese BERT model pretrained with character and word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 4,648
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, fine-tuned with Reinforcement Learning from Human Feedback (RLHF) to function as an instruction-following conversational agent.
  - Downloads: 4,571
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a multilingual BERT-based model providing sentence embeddings for 109 languages, trained with masked & translation language modeling for tasks like bi-text retrieval, and migrated to PyTorch from TensorFlow.
  - Downloads: 4,562
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - Rinna‚Äôs Japanese HuBERT Base model is a 12-layer transformer trained on 19,000 hours of Reazon Japanese speech data, replicating the original HuBERT Base architecture.
  - Downloads: 4,560
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a state-of-the-art Japanese text embedding model, based on Sarashina2.1-1B, that generates 1792-dimensional dense vectors for semantic similarity and search, achieving top performance on JMTEB.
  - Downloads: 4,423
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the CyberAgent DeepSeek-R1-Distill-Qwen-14B and 32B Japanese language models, trained with the imatrix dataset and compatible with llama.cpp.
  - Downloads: 4,360
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - This repository provides a GGUF-formatted version of the AXCXEPT-phi-4-deepseek-R1K-RL-EZO language model, trained with imatrix data and intended for use with llama.cpp.
  - Downloads: 4,108
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository hosts rinna's extra-small Japanese GPT-2 model, enabling causal language modeling via the `transformers` library.
  - Downloads: 4,067
- [reazon-research/japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh)
  - This repository provides a fine-tuned Wav2Vec2.0 base model for Japanese Automatic Speech Recognition (ASR) using the ReazonSpeech v2.0 corpus, compatible with the `transformers` library.
  - Downloads: 3,939
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly higher accuracy, particularly in qualitative evaluation.
  - Downloads: 3,923
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - llava-calm2-siglip is an experimental vision-language model enabling Japanese-language question answering about images using transformers.
  - Downloads: 3,879
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted conversion of the deepseek-r1-distill-qwen2.5-bakeneko-32b language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and intended for use with llama.cpp.
  - Downloads: 3,795
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including models varying in size (xsmall to large) with configurations detailed for improved Japanese text retrieval performance.
  - Downloads: 3,785
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - This repository provides a fine-tuned BERT model (llm-book/bert-base-japanese-v3-marc_ja) for Japanese sentiment analysis, trained on the JGLUE MARC-ja dataset and detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 3,753
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 3,673
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a Meta Llama 3-based model continually pre-trained with Japanese language data, offering both base and instruct/chat versions including 8B and 70B parameter sizes released in July 2024.
  - Downloads: 3,617
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX model finetuned for instruction-following conversations using translated Anthropic HH, FLAN, and Stanford datasets.
  - Downloads: 3,424
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - This repository provides a Japanese DeBERTa V3 base model pre-trained on the LLM-jp corpus, suitable for masked language modeling tasks using the `transformers` library.
  - Downloads: 3,384
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri provides large Japanese text embeddings using the Sentence Transformers library, requiring specific prefixes (‚Äú„ÇØ„Ç®„É™: ‚Äù or ‚ÄúÊñáÁ´†: ‚Äù) for query and passage texts during inference.
  - Downloads: 3,293
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 is a Japanese language model based on Qwen2.5-7B-Instruct, enhanced with distillation from a larger ABEJA model and ChatVector for improved instruction-following.
  - Downloads: 3,261
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - Rinna‚Äôs Japanese wav2vec2.0 Base model is a 12-layer transformer trained on ~19,000 hours of Japanese speech data, replicating the original wav2vec 2.0 Base architecture.
  - Downloads: 3,252
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - Youri-7b is a continually pre-trained 7-billion parameter language model based on Llama 2-7b, enhanced with 40B Japanese/English tokens to improve performance on Japanese language tasks.
  - Downloads: 3,189
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B large language model derived from Meta‚Äôs Llama 3.3, enhanced for Japanese language proficiency through pre-training on a 315 billion token corpus while maintaining English capabilities.
  - Downloads: 3,161
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides GGUF quantized versions of the 32B rinna/qwen2.5-bakeneko-32b-instruct model, compatible with llama.cpp applications, alongside other quantization formats like AWQ and GPTQ.
  - Downloads: 3,003
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., and readily usable via Hugging Face Transformers.
  - Downloads: 2,912
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 2,805
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - This repository hosts ABEJA's large Japanese GPT-2 model for text generation, requiring the `sentencepiece` library for usage.
  - Downloads: 2,751
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 2,688
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M is a computationally efficient Japanese BERT model trained on 4.39T tokens with RoPE and a 102,400 vocabulary, designed for handling long sequences.
  - Downloads: 2,670
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - ELYZA‚Äôs Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter language model based on Meta Llama 3, provided in quantized GGUF (Q4_K_M) format for use with llama.cpp.
  - Downloads: 2,555
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - LINE Corporation‚Äôs repository hosts a 1.7B parameter Japanese language model, fine-tuned for instruction-following and improved conversational ability.
  - Downloads: 2,524
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1, significantly enhancing Japanese language capabilities while maintaining English proficiency using a 200 billion token corpus.
  - Downloads: 2,505
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - Rinna‚Äôs Japanese HuBERT Large is a transformer-based speech model‚Äîtrained on 19,000 hours of Japanese speech data‚Äîreplicating the original HuBERT Large architecture with 24 layers and 16 attention heads.
  - Downloads: 2,488
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 base model pre-trained on extensive Japanese text corpora for masked language modeling tasks.
  - Downloads: 2,427
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model fine-tuned for instruction following, built upon japanese-stablelm-base-beta-70b and available in 7B and optimized versions.
  - Downloads: 2,410
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - This repository provides a high-performance Japanese SPLADE v2 model for converting text to sparse vectors, with a WebUI demo and utilizing YAST/YASEM for training and easy inference/token inspection.
  - Downloads: 2,407
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jnli) fine-tuned on the JGLUE MARC-ja dataset for Natural Language Inference (NLI) tasks, as detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 2,404
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, usable via Hugging Face Transformers.
  - Downloads: 2,402
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter language model fine-tuned on diverse Japanese data, built upon Llama-2-70B, and intended for high performance in Japanese language tasks.
  - Downloads: 2,398
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B is a continually pre-trained 8B parameter Llama 3 model, enhanced with 22B tokens of Japanese and English data to improve performance on Japanese language tasks.
  - Downloads: 2,395
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - CyberAgent‚Äôs DeepSeek-R1-Distill-Qwen-32B-Japanese is a finetuned 32B parameter language model for Japanese text generation, utilizing the DeepSeek-R1-Distill-Qwen architecture and compatible with the `transformers` library.
  - Downloads: 2,372
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained for enhanced instruction-following capabilities and designed for fast inference.
  - Downloads: 2,342
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository hosts LINE Corporation's 3.6B parameter Japanese language model for text generation, utilizing Hugging Face Transformers for easy implementation with provided code examples.
  - Downloads: 2,326
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - LINE Corporation‚Äôs repository hosts a 3.6B parameter Japanese language model, fine-tuned for improved conversational ability through instruction tuning.
  - Downloads: 2,323
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 2,303
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter large language model pretrained on a 220B Japanese token corpus by Stockmark Inc., with an instruction-tuned version (stockmark-13b-instruct) and AWS LLM development program support.
  - Downloads: 2,301
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built on Llama 2, further pre-trained to enhance its Japanese capabilities and optimized for fast inference.
  - Downloads: 2,299
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository provides a Japanese-fine-tuned version of the DeepSeek-R1-Distill-Qwen-14B language model, enabling it to generate thought processes and creative text directly in Japanese.
  - Downloads: 2,277
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B is a 7B parameter decoder-only language model pre-trained on 1.3T Japanese and English tokens, requiring transformers >= 4.34.1 for usage.
  - Downloads: 2,262
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to select the most relevant response from a set of candidates given a query.
  - Downloads: 2,230
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 2,212
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - This repository hosts LINE Corporation's 1.7B parameter Japanese language model for text generation, with code examples using Hugging Face Transformers.
  - Downloads: 2,210
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - This repository hosts the llm-jp-3-1.8b-instruct3, a 1.8 billion parameter Japanese language model developed by NII‚Äôs R&D Center for Large Language Models, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 2,185
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese and multilingual data, with a fine-tuned chat version (Karakuri LM Chat) utilizing continual learning and the SteerLM technique.
  - Downloads: 2,174
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX transformer model finetuned for instruction-following conversations, utilizing a new training data split for improved performance.
  - Downloads: 2,107
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including the hotchpotch/japanese-bge-reranker-v2-m3-v1 model, varying in layer count and hidden size for optimized sentence reranking performance.
  - Downloads: 2,107
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - Stockmark-100b is a 100 billion parameter language model pretrained by Stockmark Inc. on a 910 billion token Japanese/English corpus, with an instruction-tuned version (stockmark-100b-instruct-v0.1) available and supported by GENIAC.
  - Downloads: 2,103
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-7B is a 7 billion parameter language model fine-tuned for instruction following, utilizing an expanded Japanese vocabulary for improved text generation.
  - Downloads: 2,098
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - Karakuri LM is a Japanese-enhanced language model built upon Llama 2, with a fine-tuned chat version (Karakuri LM Chat) utilizing continual learning and the SteerLM technique for improved performance on Japanese and multilingual tasks.
  - Downloads: 2,096
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring extended sequence length (8192 tokens), a larger 100K vocabulary, and FlashAttention for improved performance and efficiency.
  - Downloads: 2,042
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-7B is a 7 billion parameter Llama-2-based language model fine-tuned on diverse Japanese data and featuring an expanded Japanese vocabulary for improved performance on Japanese language tasks.
  - Downloads: 2,016
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Rinna‚Äôs nekomata-7b is a continually pre-trained version of Qwen-7b on a mixed Japanese/English dataset, enhancing performance on Japanese tasks and leveraging Qwen‚Äôs large vocabulary for efficient Japanese text processing.
  - Downloads: 2,014
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1, significantly enhancing Japanese language capabilities while preserving English proficiency using a 200 billion token corpus.
  - Downloads: 2,011
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - LLM-jp provides collaboratively developed, Japanese and English instruction-tuned large language models‚Äîincluding 13B parameter versions‚Äîbuilt upon Dolly and other datasets, offering both v1.0 and v1.1 variants.
  - Downloads: 2,007
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is a 13 billion parameter Japanese language model, instruction-tuned by Stockmark Inc. using data from the Japanese Instruction data project for LLM development.
  - Downloads: 2,005
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B is a 7 billion parameter, decoder-only language model fine-tuned for instruction-following using datasets like Dolly-15k and HH, with larger 70B and faster versions also available.
  - Downloads: 1,998
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - Stockmark‚Äôs repository hosts a 1.4B parameter GPT-NeoX model pre-trained on a 20B Japanese token corpus, readily usable with the `transformers` library and optimized for various GPUs.
  - Downloads: 1,996
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned variants) developed by NII, utilizing Hugging Face Transformers with specified library versions.
  - Downloads: 1,995
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository offers a small Japanese GPT-NeoX model, trained with EleutherAI's code and compatible with Hugging Face's transformers library for causal language modeling.
  - Downloads: 1,986
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese sentence embedding model based on RoFormer, designed for efficient semantic similarity measurement and long sentence retrieval with a 1024 token limit.
  - Downloads: 1,979
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model, fine-tuned with Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational ability.
  - Downloads: 1,974
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B is a 7B-parameter decoder-only language model fine-tuned on Japanese data, built upon Llama-2-7b, and designed for strong performance in Japanese language tasks, with a corresponding instruction-following model also available.
  - Downloads: 1,971
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) fine-tuned on JA Wiki data using unsupervised SimCSE for sentence similarity and feature extraction.
  - Downloads: 1,956
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository provides a Japanese Sentence BERT base model, pretrained on colorfulscoop/bert-base-ja and fine-tuned using the Japanese SNLI dataset for sentence similarity and understanding tasks.
  - Downloads: 1,951
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - Rinna‚Äôs nekomata-14b is a continually pre-trained version of Qwen-14b on 66B Japanese/English tokens, enhancing Japanese language performance with an expanded vocabulary and long sequence support.
  - Downloads: 1,950
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a 13B parameter, Japanese large language model fine-tuned for improved instruction-following, fluency, and contextual understanding.
  - Downloads: 1,950
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,938
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - Cyberagent‚Äôs DeepSeek-R1-Distill-Qwen-14B-Japanese is a finetuned 14B parameter language model for Japanese, readily usable with the Hugging Face Transformers library.
  - Downloads: 1,930
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - This repository provides a Japanese BERT base model fine-tuned on the WRIME dataset for emotion analysis, specifically predicting intensity scores for eight emotions expressed by writers and readers of Japanese tweets related to vaccinations.
  - Downloads: 1,928
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - CyberAgent‚Äôs Mistral-Nemo-Japanese-Instruct-2408 is a continually pre-trained Japanese instruction-following language model built upon Mistral-Nemo-Instruct-2407, usable with the `transformers` library.
  - Downloads: 1,898
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging Wikipedia entity embeddings to generate contextualized representations of words and entities, differing from general NLP models.
  - Downloads: 1,876
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - This repository provides a modernBERT-base language model, llm-jp-modernbert-base, trained on a 3.4TB Japanese corpus with support for 8192-token sequences using the llm-jp-tokenizer.
  - Downloads: 1,860
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri provides pre-trained Japanese text embeddings (v3 models with up to 315M parameters & 8192 max length) using Sentence Transformers, offering strong JMTEB performance for Japanese natural language processing.
  - Downloads: 1,844
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - This repository provides GGUF format conversions of the ELYZA-japanese-Llama-2-7b and CodeLlama-7b models, including a faster, token-cost-reduced instruct version.
  - Downloads: 1,763
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - This repository hosts ABEJA's 2.7B-parameter Japanese GPT-NeoX model, compatible with transformers v4.23+, for text generation tasks.
  - Downloads: 1,705
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, alongside technical reports and usage instructions for improved Japanese text retrieval.
  - Downloads: 1,691
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, offering extended sequence length (8192 tokens) and a larger vocabulary (100K tokens) with integrated FlashAttention for improved performance and efficiency.
  - Downloads: 1,686
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - This repository provides a Japanese instruction-tuned language model, ‚ÄúLlama-3.1-70B-Japanese-Instruct-2407,‚Äù built upon Meta's Llama-3.1-70B-Instruct and utilizing the `transformers` library.
  - Downloads: 1,651
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese sentence reranking model based on Sentence Transformers, designed to select the most relevant response from a given input question and set of candidate answers.
  - Downloads: 1,554
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - KoichiYasuoka‚Äôs repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 1,533
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - This repository details a 717M parameter Japanese character-level GPT-2 language model pre-trained on diverse Japanese text data and usable via the `transformers` pipeline for text generation.
  - Downloads: 1,510
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - This repository provides a fine-tuned Japanese BERT model (‚Äúbert-base-japanese-jsnli‚Äù) for zero-shot text classification, achieving 92.88% accuracy on the JSNLI dataset.
  - Downloads: 1,457
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 1,448
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned variants) developed by NII's R&D center, utilizing Hugging Face Transformers and requiring PyTorch >= 2.3.0.
  - Downloads: 1,418
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese-enhanced version of CodeLlama, pre-trained for improved performance on Japanese language tasks and code generation, with usage examples provided.
  - Downloads: 1,416
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - This repository provides a Waseda RoBERTa model finetuned for evaluating truthfulness of generated answers on the JTruthfulQA dataset.
  - Downloads: 1,401
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1 to significantly enhance Japanese language capabilities while preserving English proficiency, utilizing a 200 billion token corpus.
  - Downloads: 1,399
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8 billion parameter language model, fully trained on 1.3T tokens and fine-tuned for dialogue using SFT and DPO, with available quantized versions including AWQ, GPTQ, and GGUF (though GGUF performance is potentially degraded).
  - Downloads: 1,393
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x1.8b-instruct3 Japanese language model, built using data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 1,364
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - This repository provides GGUF versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, optimized for LlamaEdge (v0.2.8+) with a 5120 context size and specific llama-2-chat prompt formatting.
  - Downloads: 1,358
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - This repository provides a 32K vocabulary T5 (Text-to-Text Transfer Transformer) model pre-trained on a large corpus of Japanese web text, including mC4 and wiki40b, with training code available.
  - Downloads: 1,350
- [stockmark/Stockmark-2-VL-100B-beta](https://huggingface.co/stockmark/Stockmark-2-VL-100B-beta)
  - Stockmark-2-VL-100B-beta is a 100B-parameter Japanese visual language model, built on Qwen2.5-VL-72B with Chain-of-Thought, designed for document comprehension and released as a beta for community feedback.
  - Downloads: 1,327
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-small-japanese) for improved speech recognition, trained on Japanese datasets and requiring 16kHz sampled audio input.
  - Downloads: 1,311
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 1,303
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B Japanese language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 1,302
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - This repository hosts a 6 billion parameter Japanese GPT-2 language model, ‚Äúwatashiha-gpt-6b‚Äù, fine-tuned on 6.93 million *okashi* (comedic response) data points and pre-trained on a 47.7 billion token Japanese corpus, utilizing AWS trn1 instances and the Megatron-LM framework under the Apache 2.0 license.
  - Downloads: 1,274
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Gamma 7B model, created with support from a16z and hardware from Massed Compute.
  - Downloads: 1,260
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - This repository provides a Japanese SimCSE model (pkshatech/simcse-ja-bert-base-clcmlp) for generating sentence embeddings from Japanese text, built on cl-tohoku/bert-base-japanese-v2 and trained with the JSNLI dataset, requiring fugashi and unidic-lite for tokenization.
  - Downloads: 1,219
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - This repository provides a GGUF-converted version of the AIBunCho japanese-novel-gpt-j-6b model, optimized for use with llama.cpp, though compatibility may change with future llama.cpp updates.
  - Downloads: 1,218
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of Mistral-7B-Instruct-v0.3, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm for Japanese language support and demonstrating usage with llama.cpp.
  - Downloads: 1,213
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
  - LLM-jp-3.1-13b-instruct4 is a 13 billion parameter Japanese large language model from NII, improved for instruction following through mid-training from the LLM-jp-3 series.
  - Downloads: 1,211
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M is a computationally efficient Japanese BERT model trained on 4.39T tokens, utilizing local and global attention with architectural improvements like RoPE for handling long sequences.
  - Downloads: 1,207
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,195
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B is a 7-billion parameter decoder-only language model pretrained on Japanese data, derived from Mistral-7B-v0.1, to enhance Japanese language performance.
  - Downloads: 1,189
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring extended sequence length (8192 tokens) and vocabulary (100K tokens) with FlashAttention for improved performance and efficiency.
  - Downloads: 1,087
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - This repository provides GGUF formatted versions of the llm-jp-3-8x13b-instruct3 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 1,078
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,043
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7B-parameter language model pre-trained on Japanese and English data to excel in Japanese language modeling and tasks, with an instruction-following version also available.
  - Downloads: 1,011
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - This repository provides a GGUF formatted version of the Moonlight-16B-A3B-Instruct language model, trained with Japanese data from TFMC/imatrix-dataset-for-japanese-llm, and intended for testing with llama.cpp.
  - Downloads: 999
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository hosts the llm-jp-3-13b-instruct3, a 13 billion parameter Japanese large language model developed by NII‚Äôs R&D center, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 999
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - This repository provides a GGUF-formatted version of Stability AI's japanese-stablelm-2-instruct-1_6b model, trained with the imatrix dataset, and requires agreement to the terms of use, including membership for commercial applications.
  - Downloads: 981
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository provides GGUF-formatted conversions of the open-calm-7b language model, compatible with llama.cpp, though subject to potential incompatibility with future updates to llama.cpp‚Äôs gptneox implementation.
  - Downloads: 978
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 models, including instruct and fast-tuned variants, as well as CodeLlama versions, for efficient inference.
  - Downloads: 947
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is a 13B LLaMA-based language model pre-trained on English and Japanese datasets and released by Preferred Networks under the Apache 2.0 license, utilizing libraries like transformers and sentencepiece.
  - Downloads: 922
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 70B language model, supported by a16z and utilizing hardware from Massed Compute.
  - Downloads: 877
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - This repository provides GGUF-formatted versions of the Vecteus-v1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and is designed for use with llama.cpp.
  - Downloads: 864
- [EQUES/JPharmatron-7B](https://huggingface.co/EQUES/JPharmatron-7B)
  - JPharmatron-7B is a 7B language model built on Qwen2.5, pre-trained on Japanese & English pharmaceutical data, and enhanced for conversational applications.
  - Downloads: 863
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 860
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B is a merged 7B Japanese language model evolved from Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004.
  - Downloads: 856
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, offering various quantizations (including IQ1_S at 2.0GB) for use with tools like those detailed in TheBloke's READMEs.
  - Downloads: 847
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - This repository provides a GGUF-formatted conversion of aixsatoshi‚Äôs Llama-3-8b-Cosmopedia-japanese model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm, alongside links to other related Japanese LLM models.
  - Downloads: 833
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T is a 3B-parameter Japanese language model pretrained from StableLM-3B-4E1T to excel in Japanese language modeling and downstream tasks.
  - Downloads: 813
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository details a Japanese typo detection model‚Äîbuilt on RoBERTa‚Äîthat identifies and scores the probability of various character-level errors like deletions, insertions, substitutions, and incorrect kanji conversions within text.
  - Downloads: 807
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - This repository provides a GGUF-formatted conversion of the r1-1776-distill-llama-70b language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 754
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-base model pretrained on Japanese Wikipedia and CC-100, designed for masked language modeling tasks.
  - Downloads: 734
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct, enhanced for instruction following using ChatVector and available via Hugging Face Transformers.
  - Downloads: 732
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, a Japanese language model based on Mistral AI‚Äôs Mistral-Nemo-Instruct, optimized for use with llama.cpp.
  - Downloads: 727
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of the TokyoTech-LLM Llama-3.1-Swallow-8B-Instruct-v0.3 model, trained with the imatrix dataset and usable with `llama.cpp`.
  - Downloads: 723
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF formatted versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, offering a faster, token-efficient Japanese language model based on Llama 2, alongside related models like CodeLlama versions.
  - Downloads: 708
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - This repository provides a Japanese-specific DeBERTa V3 model that performs inference without a morphological analyzer and maintains reasonable word boundary awareness.
  - Downloads: 700
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - This repository provides a large Japanese DeBERTa V2 model pre-trained on extensive Japanese text corpora for masked language modeling tasks.
  - Downloads: 696
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - This repository provides a 3B-parameter Japanese language model, fine-tuned for instruction following, based on the StableLM-3B-4E1T base model, with a larger 7B version also available.
  - Downloads: 690
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - This repository provides a Japanese BERT large model pretrained on Jawiki-20200831 using Unidic-lite word-level tokenization and whole word masking for improved language modeling.
  - Downloads: 688
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distilled-Qwen-14B language model, specifically for Japanese language processing, under the MIT License.
  - Downloads: 685
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on the Japanese Aozora corpus for dependency parsing and head-detection, utilizing masked tokens for ambiguous word clarification.
  - Downloads: 669
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
  - ABEJA-Qwen2.5-32b-Japanese-v1.0 is a 32B parameter Japanese language model fine-tuned from Qwen2.5 using SFT and DPO, built for enhanced performance (details at [https://tech-blog.abeja.asia/entry/geniac2-qwen25-32b-v1.0](https://tech-blog.abeja.asia/entry/geniac2-qwen25-32b-v1.0)).
  - Downloads: 657
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a 13B parameter, Apache 2.0 licensed, Japanese language model fine-tuned for instruction following, built on top of the 8192-context PLaMo-13B base model.
  - Downloads: 629
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama 7b models, including a fast variant with reduced token cost and increased speed.
  - Downloads: 628
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - This repository provides a GGUF-quantized version of the Aratako/calm3-22b-RP-v2 model, licensed under CC-BY-NC-SA 4.0 due to training data derived from OpenAI's GPT-4o-mini and Anthropic‚Äôs Claude 3.5 Sonnet.
  - Downloads: 620
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository provides a GGUF format conversion of the Fugaku-LLM-13B-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and requires acceptance of the usage terms.
  - Downloads: 619
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - This repository provides a pretrained Japanese RoBERTa base model for masked language modeling, trained on Japanese Wikipedia and CC-100 data.
  - Downloads: 615
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - This repository provides GGUF-formatted versions of the c4ai-command-r-plus language model, trained on the imatrix dataset, requiring file concatenation for larger quantization levels and usable with llama.cpp.
  - Downloads: 606
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA‚Äôs Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter language model based on Meta Llama 3, offered in AutoAWQ quantized format for improved performance.
  - Downloads: 590
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - This repository provides a GGUF-formatted version of Microsoft‚Äôs Phi-3-mini-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 589
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a non-commercial, Japanese instruct-tuned language model built on PLaMo-13B with an 8192 context length, released under a CC-BY-NC-4.0 license.
  - Downloads: 578
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - This repository provides a T5-base model fine-tuned on the livedoor-news corpus for Japanese text summarization, as detailed in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 575
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository hosts the llm-jp-3-980m-instruct3, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 569
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted conversion of the qwq-bakeneko-32b language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and is designed for use with llama.cpp.
  - Downloads: 538
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 is a 7B parameter Japanese language model fine-tuned for instruction following, building upon the Japanese-StableLM-Base-Alpha-7B foundation.
  - Downloads: 537
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - This repository provides a pre-trained Japanese ALBERT model (‚Äúken11/albert-base-japanese-v1‚Äù) intended for fine-tuning on various tasks, specifically addressing Sentencepiece tokenizer issues with the [MASK] token.
  - Downloads: 531
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - This repository hosts llm-jp-3-440m-instruct3, a 440M parameter Japanese large language model from NII‚Äôs LLM-jp-3 series, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 524
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - This repository hosts the llm-jp-3-150m-instruct3, a 150 million parameter Japanese language model developed by NII's R&D center, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 499
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - This repository hosts llm-jp-3-980m, a 980 million parameter Japanese large language model developed by NII, utilizing the Hugging Face Transformers format and requiring specific versions of torch, transformers, and tokenizers.
  - Downloads: 485
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-13b-instruct-v0.1 model, trained with the TFMC/imatrix dataset, alongside links to other related Swallow model variations.
  - Downloads: 479
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-13b-instruct3 Japanese language model, alongside other sizes, optimized for use with llama.cpp, noting limitations with custom chat templates.
  - Downloads: 473
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the Gemma-2-Llama-Swallow-9b-it-v0.1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 456
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the RakutenAI-2.0-mini-instruct language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 445
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 444
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs 70B Japanese StableLM Base Beta model, created with support from a16z and Massed Compute.
  - Downloads: 443
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides GGUF conversions of Japanese-Starling-ChatV-7B, a 7B parameter Japanese chat model built upon chatntq-ja-7b-v1.0 and enhanced with chat vectors derived from Starling-LM-7B-beta.
  - Downloads: 442
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Beta 7B model, created with support from a16z and Massed Compute.
  - Downloads: 426
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - This repository provides a Japanese BERT base model further pretrained on financial text, utilizing the architecture of BERT small and building upon Tohoku University's base Japanese model.
  - Downloads: 417
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - This repository details a 4.11GB quantized version of the ELYZA-japanese-Llama-2-7b-fast-instruct model‚Äîa Japanese-pretrained and speed-optimized Llama 2 variant‚Äîbalancing performance with reduced memory usage.
  - Downloads: 413
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - This repository provides the llm-jp-3-150m large language model, developed by NII's R&D Center for LLMs, with required libraries including torch, transformers, and tokenizers.
  - Downloads: 408
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight, transformer-based Japanese language foundation model optimized for resource-constrained environments, serving as a base for instruct models like RakutenAI-2.0-mini-instruct.
  - Downloads: 406
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA's Japanese-language CodeLlama-7b-instruct model, alongside faster and standard Japanese Llama 2 models.
  - Downloads: 397
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 391
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - This repository provides quantized GGUF versions of Google's Gemma-2-2b-jpn-it model, offering compatibility with tools like llama.cpp and LM Studio, built using a process similar to LLM-jp-3.
  - Downloads: 390
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - This repository provides a 6.3GB GPTQ-quantized version of the 10 billion parameter, Japanese-centric multilingual Weblab-10b-instruction-sft model, offering faster execution with a slight trade-off in inference performance.
  - Downloads: 380
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - This repository provides a Japanese BERT large model pretrained on CC-100 and JAWIKI datasets, utilizing character-level tokenization with Unidic word segmentation and whole word masking for improved language understanding.
  - Downloads: 378
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama 7b models, including standard, fast, and instruction-tuned variants.
  - Downloads: 370
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitions‚Äô repository hosts the sarashina2.2-1b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English benchmarks alongside other models like Qwen and RakutenAI.
  - Downloads: 368
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2's Japanese language abilities via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while maintaining English proficiency.
  - Downloads: 367
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M is a computationally efficient Japanese BERT model trained on 4.39T tokens, utilizing local and global attention with RoPE for long sequence handling and featuring a 102,400 vocabulary.
  - Downloads: 366
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This repository provides a Japanese XLNet language model utilizing Mecab and SentencePiece, normalized with NFKD (removing muddles/semi-muddles), for text generation and processing.
  - Downloads: 359
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - This repository provides a GGUF-formatted conversion of the karakuri-lm-32b-thinking-2501-exp large language model, trained with the imatrix dataset and designed for use with llama.cpp.
  - Downloads: 353
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository hosts the llm-jp-3-3.7b-instruct3, a 3.7 billion parameter, instruction-tuned Japanese large language model developed by NII‚Äôs R&D Center for LLMs, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 349
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides GGUF quantized versions of the rinna/qwen2.5-bakeneko-32b-instruct-v2 Japanese language model, compatible with llama.cpp applications, including merged reasoning models like DeepSeek R1 Distill.
  - Downloads: 344
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - This repository provides a pre-trained Japanese BART base model, `ku-nlp/bart-base-japanese`, for conditional generation tasks, requiring Japanese text to be pre-segmented with Juman++.
  - Downloads: 332
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 332
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters) developed by NII, provided as Hugging Face Transformers checkpoints with specific library requirements.
  - Downloads: 321
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the TokyoTech-LLM Swallow-MS-7b-instruct-v0.1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm dataset, and links to related model variations.
  - Downloads: 305
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository provides GGUF versions of Stability AI's Japanese-StableLM-3B-4E1T-Base language model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 301
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - This repository provides a GGUF-formatted conversion of Ryota39's Phi-3-mini-4k-instruct-dpo model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and demonstrating usage with llama.cpp for Japanese language tasks.
  - Downloads: 301
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository provides GGUF quantized versions of Stability AI's Japanese-StableLM-3B-4E1T-Instruct model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 299
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - This repository provides GGUF conversions of the TokyoTech-LLM Swallow-7b-instruct-v0.1 model, built with data from TFMC/imatrix-dataset-for-japanese-llm, and links to related model variations.
  - Downloads: 286
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - Luke-Japanese-Large-Lite is a lightweight, pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 273
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - This repository provides GGUF-formatted versions of the open-calm-3b language model, intended for use with llama.cpp, but may require updates upon GPTNeox implementation.
  - Downloads: 269
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - This repository provides a Japanese Llama-3-ELYZA-JP-8B model finetuned on a specific dataset and accelerated with Unsloth and TRL, licensed under Apache 2.0.
  - Downloads: 266
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - This repository provides a Japanese BERT small model pre-trained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimension architecture for financial and general language tasks.
  - Downloads: 262
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - This repository provides a GGUF conversion of the DeepSeek-R1-Distill-Qwen-7B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and is usable with llama.cpp.
  - Downloads: 259
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 255
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides a GGUF version of the Llama-3-8B-Japanese-Instruct model, optimized for use with LlamaEdge (v0.10.1+) and a specific llama-3-chat prompt template.
  - Downloads: 248
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation‚Äôs Japanese-large-lm-1.7b instruction-tuned language model, enabling its use with llama.cpp.
  - Downloads: 242
- [mmnga/llm-jp-3.1-8x13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3.1-8x13b-instruct4 Japanese language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 237
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3.1-13b-instruct4 Japanese language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 234
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - This repository provides GGUF formatted conversions of the Ninja-v1 Japanese language model, utilizing the TFMC/imatrix-dataset dataset, and includes instructions for use with llama.cpp.
  - Downloads: 230
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides a GGUF-formatted conversion of the qwen2.5-bakeneko-32b-instruct language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is designed for use with llama.cpp.
  - Downloads: 226
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100 for masked language modeling tasks.
  - Downloads: 224
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B is a 14B parameter vision language model developed by NII Japan, requiring Python 3.10.12 and specific library installations including flash-attention.
  - Downloads: 223
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-Japanese speech recognition model, trained on public datasets like Common Voice, requiring 16kHz sampled input audio.
  - Downloads: 221
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - LLM-jp-3-8x13b-instruct3 is a 8x13B parameter, instruction-tuned Japanese large language model developed by NII's R&D Center for LLMs, available in Hugging Face Transformers format with PyTorch >=2.3.0.
  - Downloads: 219
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - Luke-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities from text, utilizing Wikipedia data.
  - Downloads: 218
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - This repository hosts the llm-jp-3-8x1.8b-instruct3, a Japanese large language model developed by NII‚Äôs R&D Center for LLMs, utilizing the Hugging Face Transformers format and requiring torch>=2.3.0.
  - Downloads: 217
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-8x7B-instruct language model, trained on the imatrix dataset and compatible with llama.cpp for efficient inference.
  - Downloads: 216
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B is an instruction-tuned, Japanese language model built upon rinna/qwen2.5-bakeneko-32b, optimized for reasoning and available in various quantized formats (AWQ, GGUF, GPTQ).
  - Downloads: 214
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is a 47B parameter language model, fine-tuned for dialogue using SFT and DPO, and available in quantized formats (AWQ, GPTQ, GGUF) requiring flash attention for inference.
  - Downloads: 212
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - This repository provides GGUF-formatted versions of ELYZA's Japanese Llama 2 models, including 7b and 13b sizes, and a fast variant with reduced token cost and increased speed, alongside a Japanese CodeLlama version.
  - Downloads: 211
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - This repository provides GGUF conversions of the TokyoTech LLM Swallow-70b-instruct-v0.1 model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and lists related model variations.
  - Downloads: 209
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - This repository provides a RoBERTa-small model pre-trained on Japanese Aozora texts using the Japanese-LUW-Tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 205
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - This repository provides a Japanese-optimized, quantized GGUF version of the Gemma-2-2b-it model, leveraging iMatrix and compatible with accelerated performance via speculative decoding in llama.cpp.
  - Downloads: 204
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 202
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 201
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - This repository provides a GGUF-formatted version of DeepSeek-R1-Distill-Qwen-14B, a Japanese language model fine-tuned on the imatrix dataset, and includes instructions for use with llama.cpp.
  - Downloads: 197
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - AMBER is a 132M parameter text embedding model by Retrieva, primarily for Japanese but also supporting English, built upon the modernbert-ja-130m architecture.
  - Downloads: 195
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - This repository provides GGUF-formatted conversions of the Honyaku-13b language model and other related Japanese LLMs by aixsatoshi and mmnga, built using TFMC/imatrix-dataset-for-japanese-llm data.
  - Downloads: 192
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, with links and size information for various quantization levels.
  - Downloads: 187
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - This repository provides a Japanese medical named entity recognition (NER) model, fine-tuned RoBERTa on MedTxt-CR, to identify and tag entities like diseases, organs, medications, and clinical contexts using the IOB2 format.
  - Downloads: 182
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B large language model built upon Meta‚Äôs Llama 3.3, enhanced for Japanese language proficiency via pre-training on a 315 billion token corpus while maintaining English capabilities.
  - Downloads: 180
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - This repository provides a Japanese ELECTRA base model pretrained on Japanese Wikipedia data, featuring 12 layers, 768 hidden dimensions, and 12 attention heads.
  - Downloads: 178
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - This repository provides a Japanese BERT model‚Äîpretrained on Wikipedia data and fine-tuned for dependency parsing and question answering‚Äîbuilt upon extended character-level BERT and the UD_Japanese-GSDLUW dataset.
  - Downloads: 172
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese/English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, aiming for human alignment.
  - Downloads: 171
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - This repository provides a GGUF-formatted conversion of the stockmark-gpt-neox-japanese-1.4b language model, intended for use with llama.cpp, but potentially incompatible with future llama.cpp updates implementing native GPT-Neox support.
  - Downloads: 168
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîfeaturing GEGLU activation and optimized dropout‚Äîfor fine-tuning in various natural language processing tasks.
  - Downloads: 168
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - This repository provides GGUF-quantized versions of the LLM-jp-3-3.7b-instruct language model, optimized for use with llama.cpp, LM Studio, and LLMFarm.
  - Downloads: 160
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - This repository provides a GGUF-formatted version of the EZO-phi-4-v2_900 language model, trained with imatrix data, and intended for use with llama.cpp.
  - Downloads: 158
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - This repository provides a fine-tuned wav2vec 2.0 Large model for Japanese Automatic Speech Recognition (ASR) using the ReazonSpeech v2.0 dataset, readily usable with the `transformers` library.
  - Downloads: 154
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-large model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 153
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-32B language model, trained with the imatrix dataset and usable with llama.cpp for tasks like recipe generation.
  - Downloads: 153
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with a 128k context window and enhanced long-context memory, including NSFW variations.
  - Downloads: 152
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted files for the japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 model, enabling its use with tools supporting the GGUF format.
  - Downloads: 149
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides quantized GGUF versions of the Llama-3-8B-Japanese-Instruct model, optimized for use with GaiaNet, including a 3.18GB Q2_K option for limited resources.
  - Downloads: 148
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-1.5B language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - This repository provides GGUF quantized versions of rinna's Japanese-Italian Gemma 2B model, built using npaka's LLM-jp-3 conversion process, and intended for use with tools like llama.cpp and LM Studio.
  - Downloads: 147
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF-formatted, distilled versions of the 32B and 14B parameter DeepSeek-R1 models, specifically fine-tuned for Japanese language processing under the MIT License.
  - Downloads: 145
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - This repository provides a GGUF-formatted version of the cyberagent-open-calm-1b language model, compatible with llama.cpp, but potentially subject to future incompatibility with updates to llama.cpp‚Äôs gptneox implementation.
  - Downloads: 143
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - This repository provides a fine-tuned Japanese speech recognition model based on OpenAI‚Äôs whisper-base, trained on Japanese datasets and optimized for 16kHz audio input.
  - Downloads: 141
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot's ArrowMint-Gemma3-4B-YUKI-v0.1 is a Japanese language model fine-tuned from Gemma-3-4B-it, specializing in multi-turn, prompt-following conversational ability for AI VTubers with a focus on lightweight performance.
  - Downloads: 141
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 139
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - This repository provides a GGUF-formatted version of the AXCXEPT-phi-4-open-R1-Distill-EZOv1 language model, trained with Japanese data from TFMC/imatrix-dataset-for-japanese-llm, and designed for use with llama.cpp.
  - Downloads: 135
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - This repository provides a Japanese BERT small model pretrained on Wikipedia and financial corpora, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 134
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters) developed by NII's R&D center, including both base models and instruct-tuned variants, with support from GENIAC.
  - Downloads: 134
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - This repository provides a Japanese Question Answering model fine-tuned on JaQuAD, achieving F1 scores of 77.35/78.92 and Exact Match scores of 61.01/63.38 on development/test sets, built upon the BERT base Japanese model.
  - Downloads: 133
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository provides a fine-tuned luke-japanese-base model for binary sentiment classification (positive/negative) using the JGLUE MARC-ja dataset, achieving 0.9 accuracy.
  - Downloads: 132
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 131
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - Luke-Japanese is a pre-trained, lightweight Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia embeddings.
  - Downloads: 130
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri provides pretrained and fine-tuned Japanese text embedding models built on ModernBERT-Ja, offering various sizes (30m-70m parameters) optimized for general-purpose use and evaluated with JMTEB scores.
  - Downloads: 127
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted versions of the japanese-stablelm-instruct-gamma-7b model, based on Mistral 7B, for easier local use.
  - Downloads: 123
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - LLM-jp-3-7.2b-instruct offers Japanese large language models developed by NII, utilizing the Hugging Face Transformers format and requiring specific library versions for implementation.
  - Downloads: 121
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - This repository provides a Japanese RoBERTa-based model, fine-tuned on the JaQuAD dataset, for extractive question answering using data from Japanese Wikipedia.
  - Downloads: 120
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT-large model pre-trained on Wikipedia data for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 120
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech-to-text, outputting continuous character sequences from 16kHz audio inputs.
  - Downloads: 119
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - This repository provides a Japanese RoBERTa-base model, pre-trained on Japanese Wikipedia and CC-100 with character-level tokenization and whole word masking, for tasks like masked language modeling.
  - Downloads: 118
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - This repository provides an 8-layer distilled version of the oshizo/japanese-e5-mistral-7b_slerp model, trained on 800,000 Japanese sentences, with usage details available via intfloat/e5-mistral-7b-instruct.
  - Downloads: 116
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - This repository provides a RoBERTa-small model pre-trained on Japanese Aozora texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 116
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances the Japanese language abilities of the Gemma-2 model via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English proficiency.
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - This repository provides a 32B parameter, 8-bit quantized version of the Qwen2.5-Bakeneko-Instruct model using AutoGPTQ for reduced memory usage and faster inference.
  - Downloads: 113
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
  - This repository details a Japanese language model fine-tuned from OpenAI's Whisper-large-v3-turbo, achieving state-of-the-art performance on short-form content despite quality issues and hallucinations in longer sequences, with a smaller vocabulary for faster processing.
  - Downloads: 112
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository provides GGUF conversions of Line Corporation's 1.7B Japanese large language model, alongside instructions for use with llama.cpp.
  - Downloads: 111
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - KoichiYasuoka's repository provides a RoBERTa-large Japanese character-level model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing with FEATS.
  - Downloads: 111
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2's Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 110
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Asagi-2B is a large-scale Japanese Vision & Language Model trained on a diverse, primarily synthesized, dataset‚Äîavoiding LLMs with restrictive output usage policies.
  - Downloads: 109
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - This repository provides GGUF conversions of rinna's japanese-gpt-neox-3.6b model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation.
  - Downloads: 108
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository provides a medium-sized Japanese GPT-2 model with a BERT-like tokenizer (Unidic) built on PyTorch and Hugging Face Transformers for text generation.
  - Downloads: 108
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Asagi-14B is a 14 billion parameter Japanese Vision & Language Model trained on a diverse, largely synthetically generated, dataset utilizing CALM3-22B-Chat and Phi3.5-vision-instruct, prioritizing output usage freedom.
  - Downloads: 108
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - This repository provides a GGUF formatted conversion of the haqishen Llama-3-8B-Japanese-Instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 104
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Llama-8B language model, trained with the imatrix Japanese dataset and intended for use with llama.cpp.
  - Downloads: 104
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA's Japanese-specific CodeLlama-7b models, including both base and instruction-tuned variants, optimized for faster performance and reduced token cost.
  - Downloads: 100
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
  - LLM-jp-3.1-1.8b-instruct4 is a 1.8 billion parameter Japanese large language model from NII, improved for instruction following through mid-training techniques building on the LLM-jp-3 series.
  - Downloads: 100
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-1.8b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for its creation.
  - Downloads: 100
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - This repository provides a GGUF-formatted version of the WabiSabi-V1 Japanese language model, built using the TFMC/imatrix dataset and designed for use with llama.cpp.
  - Downloads: 99
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - This repository provides a JaQuAD-fine-tuned RoBERTa base model for Japanese question answering, with code and usage examples available.
  - Downloads: 97
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - This repository provides a GGUF-formatted conversion of the Japanese instruction-tuned Llama-3-8B model by alfredplpl, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 96
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including small (12-layer, 384 hidden size) and larger models, for improved text retrieval performance.
  - Downloads: 96
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from Kyoto University's BERT Japanese Pretrained model using the ner-wikipedia-dataset, requiring separate download of the tokenizer and installation of Juman++ and pyknp.
  - Downloads: 94
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This repository merges intfloat‚Äôs E5-Mistral-7B-Instruct and StabilityAI‚Äôs Japanese-StableLM-Base-Gamma-7B models to create a combined, instruction-following, Japanese-capable language model.
  - Downloads: 93
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - This repository provides a GPT-2 small language model trained on a 540M token subset of the Japanese Wikipedia dataset for Japanese text generation.
  - Downloads: 93
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository likely contains research and analysis data related to the Tsukuba region (including its university, campus, and surrounding areas in Ibaraki Prefecture, Kanto) potentially involving student projects and scientific investigations.
  - Downloads: 92
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-large Japanese question-answering model pre-trained on the Aozora corpus for dependency parsing and optimized for handling ambiguous words using the [MASK] token.
  - Downloads: 91
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 90
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Asagi-8B is a large-scale Japanese Vision & Language Model trained on a diverse, synthesized dataset‚Äîprimarily using CALM3-22B-Chat and Phi3.5-vision-instruct‚Äîwith openly usable outputs.
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository hosts the llm-jp-3-13b-instruct2, a 13 billion parameter Japanese large language model developed by NII‚Äôs R&D Center, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the cyberagent/Mistral-Nemo-Japanese-Instruct-2408 model, optimized for llama.cpp and runnable via the TensorBlock client.
  - Downloads: 87
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen1.5-110B-Chat large language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and licensed under Tongyi-Qianwen.
  - Downloads: 86
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - This repository provides a question encoder for a BPR document retrieval model, built by fine-tuning cl-tohoku/bert-base-japanese-v3 with llm-book/aio-retriever, as detailed in chapter 9 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - Izumi-lab‚Äôs repository provides a pre-trained DeBERTa V2 base model for Japanese masked language modeling, with associated pretraining code available elsewhere.
  - Downloads: 84
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - This repository provides a BERT-large model pretrained on Japanese Wikipedia data specifically for dependency parsing and question answering, building upon existing Japanese BERT models and utilizing the UD_Japanese-GSDLUW dataset.
  - Downloads: 83
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned Wav2Vec2-large-xlsr-53 model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 83
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - This repository provides a 4-bit quantized derivative of the llm-jp-3-172b-instruct3 large language model, reducing GPU/memory usage while maintaining performance via BitsAndBytes (fp4) quantization with float16/float32 computation.
  - Downloads: 83
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 80
- [buildertakuya/Berghof-Takuya7B](https://huggingface.co/buildertakuya/Berghof-Takuya7B)
  - This repository provides a finetuned Mistral 7B model (based on Elizezen/Berghof-NSFW-7B) trained with a self-made dataset for explicit content generation, utilizing Unsloth and TRL for accelerated training.
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora texts for Universal Part-of-Speech tagging and dependency parsing.
  - Downloads: 78
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese language model (based on sonoisa/sentence-luke-japanese-base-lite) for detecting aggression in social media comments, achieving a 71.3% F1-score, and presented at NLP2024.
  - Downloads: 77
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - This repository provides a Japanese BigBird base model pre-trained on large Japanese text corpora‚ÄîWikipedia, CC-100, and OSCAR‚Äîfor masked language modeling tasks.
  - Downloads: 74
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - This repository provides a Japanese 1B GPT model fine-tuned to mask Personally Identifiable Information (PII) ‚Äì including names, birthdays, phone numbers, addresses, and IDs ‚Äì within Japanese text.
  - Downloads: 71
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - This repository distributes a spaCy v3-finetuned ELECTRA model (ja_ginza_electra) pretrained on Japanese mC4 data and built upon megagonlabs/transformers-ud-japanese-electra-base, offering enhanced Japanese natural language processing via GiNZA v5.
  - Downloads: 71
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-Of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 71
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - This repository provides an ESPnet2 text-to-speech (TTS) model‚Äîspecifically, kan-bayashi/jsut_transformer‚Äîtrained on the JSUT dataset using the ESPnet framework and recipe.
  - Downloads: 70
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia data for universal POS-tagging and dependency parsing, building upon bert-base-japanese-v2 and utilizing long-unit words.
  - Downloads: 69
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - This repository provides a Japanese BERT large model pretrained on jawiki-20200831 with character-level tokenization and whole word masking for improved language understanding.
  - Downloads: 69
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - This repository provides a Japanese novel-focused language model, GPT-J-6B, pre-trained on Japanese data and fine-tuned on novels, runnable on Google Colab with provided installation instructions.
  - Downloads: 68
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese language model, with options tailored for varying VRAM capacities (8GB-16GB+).
  - Downloads: 68
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following, based on Japanese Stable LM Base Gamma and requiring Transformers 4.34.0+.
  - Downloads: 67
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - This repository details a 1.3B parameter Japanese GPT-2 model (‚Äúdolly-japanese-gpt-1b‚Äù) fine-tuned with RLHF on datasets like ‚Äúdatabricks-dolly-15k-ja‚Äù and ‚Äúoasst1-89k-ja‚Äù for conversational AI, requiring 7GB VRAM/RAM, though recent updates show decreased QA accuracy.
  - Downloads: 66
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a pre-trained Japanese Transformer Encoder built with Megatron-LM, offering improvements over traditional BERT models like PreNorm and featuring a recent bug fix for parameter initialization.
  - Downloads: 66
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository hosts a base-sized Japanese BART model, pre-trained by Stockmark Inc. on news data, utilizing a transformer encoder-decoder architecture for sequence-to-sequence tasks.
  - Downloads: 65
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - This repository provides a RoBERTa-large model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 65
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - This repository provides Llama-3.1-70B-EZO-1.1-it, a Japanese language model fine-tuned from Meta's Llama 3.1 achieving top performance‚Äîsurpassing gpt-4o-mini‚Äîon the ElyzaTasks-100 benchmark, under the Llama 3 Community License.
  - Downloads: 65
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a Japanese language model based on a Switch Transformer with a Prefix-LM structure, utilizing a unique "Spout" vector for controllable text generation and fine-tuning.
  - Downloads: 64
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - This repository provides a Japanese instruction-following language model, fine-tuned from Meta‚Äôs Llama 3 8B Instruct using QLoRA on a small dataset, achieving an average score of 3.12 on ELYZA-tasks-100.
  - Downloads: 64
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 64
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - This repository provides a Japanese natural language inference (NLI) model, built upon bert-base-japanese-v3 using SentenceTransformers‚Äô Cross-Encoder, and trained on JSNLI, JNLI, and JSICK datasets to classify sentence pair relationships as entailment, neutral, or contradiction.
  - Downloads: 63
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM provides open-source, locally deployable Japanese-to-Chinese translation models‚Äîfine-tuned on general and ACGN-style datasets‚Äîfor light novels and galgames, released under a non-commercial license.
  - Downloads: 63
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - This repository provides a pre-trained Japanese GPT2 model (‚Äúskytnt/gpt2-japanese-lyric-medium‚Äù) for generating Japanese song lyrics from given titles and prompts.
  - Downloads: 62
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 62
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This repository hosts a ChatML model created by SLERP merging yamatazen/Himeyuri-Magnum-12B with shisa-ai/shisa-v2, representing the HMS model family.
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - This repository provides a Japanese-tuned LoRA adaptation of OpenAI's Whisper Large V2 model for speech recognition, specifically tested for comedic content and requiring 9GB VRAM.
  - Downloads: 61
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - This repository details a v1.1 T5 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 60
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a Japanese-focused dataset, built by SambaNova Systems.
  - Downloads: 60
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - This repository provides a Japanese ELECTRA model, pretrained and finetuned on disaster tweets, for information triage tasks, licensed under CC BY-SA 4.0.
  - Downloads: 59
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Stability AI‚Äôs Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, instruction-following Japanese language model fine-tuned from the base Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 59
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, for masked language modeling tasks.
  - Downloads: 59
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora Bunko texts for universal part-of-speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 59
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - This repository provides a pre-trained Japanese BERT model (`bert-base-japanese-upos`) specifically for Universal Part-Of-Speech tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa-based model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependencies tasks like POS-tagging and dependency parsing, utilizing the goeswith subword tokenizer.
  - Downloads: 58
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - This repository details Japanese sentiment analysis experiments using BERT, optimized with Optuna‚Äîachieving best results with a cosine learning rate schedule (3.9e-05), batch size of 128, weight decay of 5.2e-05, 100 epochs, and early stopping.
  - Downloads: 58
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - DeBERTa-small-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 58
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - This repository provides a GGUF-formatted version of the Stockmark-2-100B-Instruct-beta language model, trained with the imatrix Japanese dataset and usable with llama.cpp.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - This repository provides a 3.89GB AWQ-quantized version of theELYZA/ELYZA-japanese-Llama-2-7b-instruct model, a Japanese instruction-tuned Llama 2 variant optimized for Colab A100 and RTX 3000 series GPUs.
  - Downloads: 57
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a continually pre-trained 32B parameter language model‚Äîbased on Qwen2.5‚Äîenhanced for Japanese language tasks using an 18B token dataset.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository provides a Japanese-adapted version of the Llama 3 8B model, commercially usable under the Llama 3 license, with instructions for quick demonstration, Colab usage, and local setup via `transformers` and `accelerate`.
  - Downloads: 56
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed collaboratively using the Fujaku supercomputer, permitting commercial and non-commercial use, modification, and redistribution while adhering to the specified conditions.
  - Downloads: 56
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - Rinna Co., Ltd.‚Äôs repository hosts a 1.3B-parameter Japanese GPT model, utilizing T5Tokenizer for text processing and supporting GPU acceleration via PyTorch.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Asagi-4B is a large-scale Japanese Vision & Language Model trained on a diverse, synthetically-generated Japanese dataset, leveraging models like CALM3 and Phi3.5-vision without usage restrictions on generated data.
  - Downloads: 56
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - This repository provides a fine-tuned Japanese GPT-2 model for generating *ES* (job application essays) based on a dataset of over 20,000 examples, leveraging code from rinnakk/japanese-pretrained-models and accessible through a web application.
  - Downloads: 55
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - This repository provides a Japanese BERT model (bert-base-japanese-v3) fine-tuned on the JGLUE JCommonsenseQA dataset for multiple-choice question answering, as featured in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 55
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - This repository provides a pre-trained Japanese character-level GPT-2 Medium model (310M parameters) fine-tuned on Japanese Wikipedia, CC-100, and OSCAR for text generation.
  - Downloads: 55
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - This repository provides a 1.5B-parameter Japanese GPT2 model, pretrained on Japanese Wikipedia and CC-100, for text generation and fine-tuning‚Äîrequiring pre-tokenization with Juman++.
  - Downloads: 54
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - This repository provides a fine-tuned Qwen3-8B-RP-v0.1 model optimized for role-playing, utilizing a system prompt to define character settings and dialogue scenarios, with an example using Ollama.
  - Downloads: 54
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz v2.5 is a finetuned GPT-2 Japanese language model, specifically for Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 53
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - This repository hosts a pretrained ESPnet2 Text-to-Speech (TTS) model, `jsut_vits_accent_with_pause`, trained on the JSUT dataset and imported from Zenodo.
  - Downloads: 53
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - This repository provides a large Japanese BART model pre-trained on Japanese Wikipedia, usable for conditional generation tasks with the `transformers` library after tokenizing input text with Juman++.
  - Downloads: 53
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - This repository provides a small GPT2 model, `skytnt/gpt2-japanese-lyric-small`, for generating Japanese lyrics, with example code and a demo available at lyric.fab.moe.
  - Downloads: 52
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-converted weights for ELYZA-japanese-Llama-2-13b-fast-instruct, a Japanese language model based on Llama 2, for use with llama.cpp.
  - Downloads: 52
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - This repository provides a merged language model created with mergekit, using the TIES method and combining shisa-ai/shisa-v2-mistral-nemo-12b with natong19/Mistral-Nemo-Instruct-2407-abliterated.
  - Downloads: 52
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the japanese-llama-3-8b-instruct-v2 model, offering various quantization levels for optimized performance and size.
  - Downloads: 52
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This repository provides a Japanese+English Sentence-BERT model (‚Äúsonoisa/sentence-bert-base-ja-en-mean-tokens‚Äù) pre-trained on cl-tohoku/bert-base-japanese-whole-word-masking, achieving improved English STS benchmark accuracy with fugashi and ipadic dependencies.
  - Downloads: 51
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - This repository provides a T5-base model fine-tuned on the XLSum dataset for Japanese summarization, achieving specific Rouge scores detailed in the description, with further training and usage details pending completion.
  - Downloads: 50
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - This repository provides a Rust implementation of the cl-tohoku/bert-large-japanese Japanese BERT model, offering instructions for cloning, project setup, and usage with the `rust-bert` crate.
  - Downloads: 50
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a large corpus of Japanese text (approximately 890GB) requiring fine-tuning for specific tasks, and acknowledges potential biases in its outputs.
  - Downloads: 50
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - This repository provides an early-access Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, continuously pre-trained and evaluated by ABEJA, with instructions for use via Hugging Face Transformers.
  - Downloads: 49
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - This repository provides a Japanese BERT-base model ("Vaporetto + WordPiece") and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 48
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the `transformers` library for implementation.
  - Downloads: 48
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation, with usage examples provided.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for loading.
  - Downloads: 47
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-large (V2) model for Japanese, trained on the Aozora Bunko corpus with BertJapaneseTokenizer, and suitable for fine-tuning tasks like POS-tagging and dependency parsing.
  - Downloads: 47
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This repository provides a Japanese sentence-T5 model, pre-trained using sonoisa/t5-base-japanese, requiring `sentencepiece` for inference and achieving accuracy comparable to sonoisa/sentence-bert-base-ja-mean-tokens.
  - Downloads: 47
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - This repository hosts Llama-3.3-SuperSwallow-70B-Instruct-v0.1, a mergekit-created language model demonstrated with a Japanese-RP example conversation showcasing its instruction-following capabilities.
  - Downloads: 47
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA-Small model leveraging subword tokenization from Japanese Wikipedia with MeCab, designed for use within the Hugging Face Transformers library.
  - Downloads: 46
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pre-trained on Japanese Wikipedia, intended for text generation or fine-tuning after word segmentation with Juman++.
  - Downloads: 46
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 is a Japanese LLaMA-based language model, trained entirely in Japanese with 76,000 steps on a Wikipedia-centric dataset, designed to run on 24GB VRAM and utilizing Flash Attention.
  - Downloads: 46
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - This repository provides a Japanese GPT2 base model (version 2) trained on Japanese Wikipedia data with a 60,000 token vocabulary, usable for text generation via the `transformers` pipeline.
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository provides AWQ quantized files for Stability AI‚Äôs Japanese StableLM Instruct Gamma 7B, optimized with hardware from Massed Compute and supported by a16z grant.
  - Downloads: 45
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - This repository provides a 32B Japanese language model‚Äîan AWQ-quantized version of DeepSeek-R1-Distill-Qwen-32B, fine-tuned with TFMC/imatrix data.
  - Downloads: 45
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This repository provides a BART-large Japanese model converted from Kyoto University‚Äôs original, compatible with the `BartJapaneseTokenizer` and usable via Hugging Face `transformers` for sequence-to-sequence tasks.
  - Downloads: 44
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training for enhanced Japanese capabilities, designed for code generation and instruction following.
  - Downloads: 44
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B is a GGUF conversion of a 7B Japanese language model built by combining ChatNTQ-ja-7b-v1.0 with a modified WizardLM-2-7b, aiming to enhance Japanese language performance.
  - Downloads: 44
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - This repository provides a base model card template and details a small, pretrained Japanese/English T5 text-to-text transformer model, requiring further information regarding its developer, type, language specifics, and license.
  - Downloads: 44
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - This repository provides a pretrained Japanese GPT-2 model, trained on Japanese Wikipedia and CC-100, for text generation and fine-tuning, requiring word segmentation with Juman++.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - DeBERTa-v2 pre-trained on Japanese Wikipedia and Aozora Bunko texts provides a foundation for fine-tuning various Japanese NLP downstream tasks.
  - Downloads: 43
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 70B language model, offering various parameter options for optimized performance.
  - Downloads: 43
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-instruct-gamma-7b using Slerp merging across the first 32 layers, based on Mistral-7B as the base model.
  - Downloads: 43
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa(V2) model, `deberta-large-japanese-wikipedia-ud-goeswith`, pretrained on Japanese text for universal dependency parsing and part-of-speech tagging using the goeswith subword tokenizer.
  - Downloads: 42
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation for efficient inference.
  - Downloads: 42
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - This repository provides GGUF format conversions of Meta-Llama-3-8B-Instruct, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 42
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - This repository provides a Japanese-enhanced version of Llama3.1-8B-instruct, fine-tuned with Mergekit to improve performance as a helpful and honest Japanese assistant.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - This repository provides a fine-tuned Japanese language model, based on studio-ousia/luke-japanese-large, for automatic detection of defamatory speech categorized into threats, insults, and reputational damage.
  - Downloads: 41
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B language model, offering various parameter options for optimized performance.
  - Downloads: 41
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - This repository details Japanese sentiment analysis experiments using bert-base, the WRIME dataset, Adafactor optimization, and hyperparameter tuning with Optuna, exploring learning rates, batch sizes, and weight decay for optimal performance.
  - Downloads: 40
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Base Beta 70B large language model, optimized for efficiency using hardware from Massed Compute.
  - Downloads: 39
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - This repository provides ja_ginza_electra, a spaCy v3 package distributing an ELECTRA model pretrained on Japanese mC4 data and finetuned for Universal Dependencies, requiring SudachiTra tokenization via GiNZA v5.
  - Downloads: 39
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - This repository provides a Japanese error detection and correction model, a fine-tuned mt5-base trained on 20,000 text pairs, utilizing a "correction: " prefix for text-to-text transformation.
  - Downloads: 39
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built upon a base model trained on 42 billion Japanese tokens from the Cultura-X dataset.
  - Downloads: 38
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small, fast, entirely Japanese-trained language model based on LLaMA, capable of running on 24GB VRAM, and known for generating amusing, albeit not always helpful, Japanese text.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - KoichiYasuoka/bert-base-japanese-char-extended is a pre-trained Japanese BERT model, extending bert-base-japanese-char-v2 with enhanced character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 37
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base is a 3 billion parameter language model pre-trained on Japanese and English data, utilizing the RetNet architecture and a retention mechanism for research purposes, released under the MIT license.
  - Downloads: 37
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - This repository provides a 7B-parameter, instruction-following Japanese language model (Japanese Stable LM Instruct Gamma 7B) fine-tuned from the base Japanese Stable LM Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 37
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - KoichiYasuoka‚Äôs roberta-large-japanese-luw-upos is a RoBERTa-large model pre-trained on Aozora texts for Japanese Universal Part-Of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 37
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - ArrowNeo-AME-4x3B-v0.1 is a 4x3B MoE model built upon sarashina-2.2-instruct-v0.1 with Unsoth and Mergekit-MoE, combining a base model with three specialized experts‚Äîcoding, instruction-following, and multi-turn conversation‚Äîto create a lightweight AItuber AI.
  - Downloads: 37
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 36
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-based Japanese question-answering model, pretrained on Aozora Bunko for dependency parsing and optimized for handling ambiguous words using the [MASK] token.
  - Downloads: 36
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using a character-level tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 36
- [yasu-oh/Llama-3-Swallow-Infused-R1776-70B](https://huggingface.co/yasu-oh/Llama-3-Swallow-Infused-R1776-70B)
  - Llama-3-Swallow-Infused-R1776-70B is a 70B parameter language model merging reasoning from r1-1776-distill-llama-70b with instruction-following from the Swallow model for improved English and Japanese performance.
  - Downloads: 36
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - This repository offers a GGUF quantized version of the 32B-parameter DeepSeek-R1-Distill-Qwen-Japanese language model, licensed under MIT.
  - Downloads: 36
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 35
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - This repository provides a 1.3B parameter NLLB-200 model fine-tuned for Japanese-to-English translation of the ‚ÄúAscendance of a Bookworm‚Äù web novel.
  - Downloads: 35
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation's Japanese large language model, `japanese-large-lm-3.6b-instruction-sft`, for use with tools like `llama.cpp`, noting potential future incompatibility with mainline implementations.
  - Downloads: 35
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - This repository provides a Japanese Natural Language Inference (NLI) model, built on bert-base-japanese-v3 and trained with SentenceTransformers' Cross-Encoder on the JSNLI dataset, outputting entailment scores for sentence pairs.
  - Downloads: 34
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained, bilingual Japanese-English language model adapting Llama-2-7b with 42B tokens from Cultura-X, achieving state-of-the-art results in perplexity and translation.
  - Downloads: 34
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Unigram for enhanced Japanese language processing.
  - Downloads: 34
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - JMedRoBERTa-base-manbyo-wordpiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 34
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese NLP model pretrained on the Aozora corpus for universal dependencies tasks like POS-tagging and dependency parsing, utilizing the goeswith subword tokenizer.
  - Downloads: 34
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 34
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia, extending a prior character-based BERT model.
  - Downloads: 34
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - KoichiYasuoka/bert-large-japanese-char-extended is a pre-trained, enhanced Japanese BERT model (based on bert-large-japanese-char) with extended character embeddings, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 34
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm is a Japanese RoBERTa-large model pre-trained on Japanese Wikipedia and CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 33
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - This repository provides a XLM-RoBERTa-base model fine-tuned on Japanese mMARCO data using an ANCE warmup strategy, achieving a peak MRR@100 of 0.242 at 50k steps, with dataset preparation scripts linked.
  - Downloads: 33
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - This repository provides a Japanese BERT-base model with a Nothing + BPE tokenizer, requiring a downloaded dictionary file for loading and use with `transformers`.
  - Downloads: 33
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - This repository provides a Japanese-StableLM-Base-Alpha-7B model fine-tuned to emulate the speech patterns of Reimu Hakurei from *Touhou Project*, enabling conversational interactions using a specific prompt format, and released under the Apache 2.0 license with 4-bit quantization.
  - Downloads: 33
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - Rinna/nekomata-7b-instruction-gguf provides a GGUF-formatted, 7B instruction-following language model compatible with llama.cpp, with recommended 4-bit quantization using GGUF q4_K_M to address potential stability issues.
  - Downloads: 33
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Byte Pair Encoding (BPE), requiring a downloaded dictionary file for processing.
  - Downloads: 33
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100B-parameter Japanese-focused large language model pre-trained on 1.5T tokens and instruction-tuned with Qwen2.5-generated synthetic data.
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - This repository provides a QLoRA-finetuned version of cyberagent/calm3-22b-chat optimized for role-playing, utilizing the ChatML prompt format and offering a demo and GGUF conversion.
  - Downloads: 32
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing the Nothing + WordPiece approach, requiring a downloaded dictionary file for loading.
  - Downloads: 32
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - This repository provides a 417.12M parameter Llama 2 model trained on Japanese text, utilizing the `if001/sentencepiece_ja` tokenizer and demonstrating generation with a sample prompt.
  - Downloads: 32
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - Fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding model‚Äîbased on bert-base-japanese-v3 and trained on limited data‚Äîfor tasks like similarity, entailment, and retrieval using datasets including JSTS, JSNLI, and MMARCO.
  - Downloads: 32
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on JaQuAD data, offering a non-transformer alternative for sequence modeling.
  - Downloads: 32
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - This repository provides an automatically generated model card detailing a ü§ó Transformers model hosted on the Hub, including information on its development, funding, license, and source.
  - Downloads: 32
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - This repository provides a fine-tuned T5-base-Japanese model for Japanese title generation from input text, pretrained on a 100GB Japanese corpus including Wikipedia and OSCAR data.
  - Downloads: 32
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA model (ShinoBU) tokenized with SudachiTra/WordPiece, trained on 200M sentences and readily usable with Hugging Face Transformers.
  - Downloads: 31
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa v2 large Japanese language model‚Äîpretrained for universal dependency parsing and POS-tagging with goeswith subword tokenization‚Äîderived from `deberta-v2-large-japanese` and requiring fugashi.
  - Downloads: 31
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts a 1.7B-parameter, 8-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following and causal language modeling.
  - Downloads: 31
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - This repository provides a commercially usable, Japanese-speaking AI model (based on Gemma 2B and PEFT) fine-tuned for conversational assistance, exclusively using Japanese language.
  - Downloads: 31
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - This repository provides a pretrained DeBERTa V2 small model for Japanese masked language modeling, with associated pretraining code available elsewhere.
  - Downloads: 31
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants (Jaster, Dolly, OASST), along with pre-trained checkpoints.
  - Downloads: 31
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - This repository provides a BERT model, fine-tuned on Japanese novel data, for classifying text (titles & summaries) into genres, built upon the cl-tohoku/bert-base-japanese-char-v3 base model.
  - Downloads: 31
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - This repository provides a 1.3B parameter Japanese GPT model‚Äîtrained on alpaca_ja and GuanacoDataset‚Äîfor conversational AI, requiring 7GB VRAM or RAM for operation.
  - Downloads: 31
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-base model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword tokenizer.
  - Downloads: 31
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - TohokuNLP's BERT-alpha 500M is a Japanese BERT model based on the Llama architecture, enabling long sequence input (up to 8,192 tokens) as an encoder-type language model.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a 370 million parameter Japanese chat language model built on the Mamba state-space architecture for efficient, linear-time sequence modeling.
  - Downloads: 31
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - This repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of character-level long-unit words.
  - Downloads: 30
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Zenz-v1 is a 90M parameter Japanese GPT-2 language model finetuned for kana-kanji conversion, intended for use with the Zenzai neural conversion system, and licensed under CC-BY-SA 4.0.
  - Downloads: 30
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - This repository provides a Japanese instruction-tuned Llama 2 model (based on `if001/llama2_ja_small`) fine-tuned on instruction data using the Lit-GPT script.
  - Downloads: 30
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This repository provides a QLoRA-finetuned Llama-2-13b-chat-hf model, trained on a large Japanese/Chinese dataset, with improved performance and testing scripts provided.
  - Downloads: 30
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - This repository provides a Japanese BERT-base model with a Juman++ and WordPiece tokenizer, requiring a downloaded dictionary file for usage with Transformers.
  - Downloads: 30
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pre-trained on 8K vocabulary web text data (mC4 & wiki40b) with training code available.
  - Downloads: 30
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This repository provides phi-4-open-R1-Distill-EZOv1, a Japanese-focused reasoning model inspired by Deepseek-R1's distillation technique, capable of flexible English integration.
  - Downloads: 30
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This repository provides a Wav2Vec2-XLS-R-1B model fine-tuned on the Mozilla Common Voice 8.0 Japanese dataset, achieving a Word Error Rate of 1.0132 with specified training hyperparameters.
  - Downloads: 29
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, featuring 12 layers, 64 hidden dimensions, and 1 attention head.
  - Downloads: 29
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - This repository provides a fine-tuned Japanese GPT-2 model for generating *ES* (r√©sum√©s/applications) using a dataset of 140,000 examples, built upon rinna‚Äôs japanese-pretrained-models and accessible via http://www.eswrite.com.
  - Downloads: 29
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts a 1.7B parameter, 4-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation, with details available in a linked intern report.
  - Downloads: 29
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - This repository hosts a merged, work-in-progress Japanese language model based on Mixtral-8x7B-Instruct-v0.1, enhanced with vocabulary expansion and released with preliminary evaluation results from ABEJA.
  - Downloads: 29
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This repository outlines the terms of use for Fugaku-LLM, a large language model developed through a collaborative project leveraging the Fugaku supercomputer, permitting both commercial and non-commercial use, modification, and redistribution.
  - Downloads: 29
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with an extended 128k context window and improved long-context memory.
  - Downloads: 29
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This repository provides a JAX/Flax-based Japanese transformer language model (0.1B parameters) trained on a Japanese dataset, adapted from the Flax lm1b example, and includes benchmark scores & causal language modeling support.
  - Downloads: 29
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese BERT base model specifically for super short unit words (SSUW), requiring full-width conversion and SSUW segmentation as pre-processing steps.
  - Downloads: 29
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository hosts a 1.3B-parameter Japanese GPT2 model finetuned by jweb (based on rinna's work), offering both PyTorch and Rust versions and requiring T5Tokenizer for use.
  - Downloads: 29
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - This repository provides a Japanese ELECTRA model‚Äîpretrained on 200M sentences using SudachiTra WordPiece tokenization‚Äîfor use with the `transformers` library.
  - Downloads: 29
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - This repository provides a pre-trained Japanese RoBERTa-large model, fine-tunable for tasks like POS tagging and dependency parsing, based on the ÈùíÁ©∫ÊñáÂ∫´ (Aozora Bunko) text corpus and Japanese-LUW-Tokenizer.
  - Downloads: 29
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - This repository provides a GGUF-formatted conversion of the matsuo-lab Weblab-10B model, compatible with llama.cpp examples, utilizing a development branch for faster updates.
  - Downloads: 29
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This repository details a Japanese GPT-1B-based model fine-tuned for extractive question answering and answer refinement within the gpt-index (v0.2.5) framework, utilizing specific prompt templates for context-based responses.
  - Downloads: 28
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 28
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert is a Japanese encoder-only masked language model, part of a 75-model multilingual set trained by the HPLT project using a BERT-base configuration (768 hidden size, 12 attention heads).
  - Downloads: 28
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - Nekomata-7b-gguf provides a GGUF quantized version of the rinna/nekomata-7b model optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 28
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Vaporetto + Unigram, requiring a downloaded dictionary file for proper loading and processing.
  - Downloads: 28
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and WordPiece for effective Japanese natural language processing.
  - Downloads: 28
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - This repository provides a Japanese-language model, fine-tuned from Google‚Äôs mt5-base, specifically for summarizing patent claims within the pharmaceutical domain.
  - Downloads: 28
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - Sarashina 2.2-3B-RP-v0.1 is a fine-tuned 3B language model for role-playing, optimized with a GGUF version and designed to respond to prompts with specified character settings and scenarios.
  - Downloads: 28
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository offers GPTQ quantized models for Stability AI's Japanese StableLM Base Beta 70B, providing various parameter options for optimized performance.
  - Downloads: 27
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and WordPiece for natural language processing.
  - Downloads: 27
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - This repository provides a BERT base model trained on the Japanese Wikipedia dataset (June 20, 2021) for Japanese natural language processing tasks.
  - Downloads: 27
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - This repository provides a transformer-aligned Japanese-to-Hebrew translation model (jpn-heb) trained on OPUS data, utilizing SentencePiece preprocessing and including benchmark scores for BLEU and chr-F on the Tatoeba test set.
  - Downloads: 27
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - This repository provides a 4-bit quantized MLX version of the DeepSeek-R1-Distill-Qwen-32B-Japanese language model, enabling efficient inference with the `mlx-lm` library.
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - This repository provides the llm-jp-3-980m-instruct2 large language model‚Äîpart of the LLM-jp-3 series developed by NII‚Äîin Hugging Face Transformers format, requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - This repository provides a Japanese language ELECTRA small model pretrained on Japanese Wikipedia data for financial text generation, utilizing the original ELECTRA architecture.
  - Downloads: 26
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 26
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 26
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository provides a Japanese language model, ‚Äúluke-japanese-wordpiece-base‚Äù, built upon studio-ousia/luke-japanese-base, pre-trained on Japanese Wikipedia data, utilizing WordPiece tokenization and improved handling of unknown entities.
  - Downloads: 26
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - This repository provides a modified DistilBERT model pre-trained on a large Japanese web corpus by LINE, updated for compatibility with transformers v4.34 and featuring an improved tokenizer.
  - Downloads: 26
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - This repository provides a Japanese language-extended version of Mixtral-8x7B-Instruct-v0.1, pretrained by ABEJA and built upon Metagton-LM.
  - Downloads: 26
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - This repository hosts Tokara-0.5B-v0.1, a 0.5B parameter language model fine-tuned from Qwen1.5-0.5B with 5B Japanese/English tokens and enhanced with multi-turn chat capabilities, benchmarked on Japanese MT-bench categories.
  - Downloads: 26
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA is a Japanese-language vision-language model enabling conversational image understanding, built using a Chat Vector method combining weights from llava-v1.5-7b, Llama-2-7b-hf, and ELYZA-japanese-Llama-2-7b.
  - Downloads: 26
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - This repository provides a Japanese pretrained version of the 275.86M parameter Mixtral model, demonstrated with example code for text generation using a SentencePiece tokenizer.
  - Downloads: 26
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - This repository provides an inference model cloned from rinna's "japanese-gpt-1b" and fine-tuned using the "databricks-dolly-15k-ja" dataset, offering instructions for local fine-tuning and inference within a Windows 10/Python 3.9.6/RTX4070 environment.
  - Downloads: 26
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - This repository provides a pretrained FastText word embedding model for Japanese, along with a Google Colaboratory example demonstrating its usage with transformers and MeCab for text feature extraction.
  - Downloads: 26
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - This repository provides a fine-tuned Japanese GPT-2 model specifically for generating application essays (ES) for IT industry job seekers, built upon rinna's pretrained models.
  - Downloads: 26
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - This repository provides a Japanese pre-trained MobileBERT model, offering a faster alternative to standard BERT for natural language processing tasks, and is compatible with the `transformers` library using the Tohoku University tokenizer.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository provides a distilled Japanese GPT-2 model, trained using rinna/japanese-gpt2-medium as the teacher, achieving a perplexity of around 40 on Wikipedia data, and utilizing Hugging Face Transformers with modifications from rinna‚Äôs training code.
  - Downloads: 25
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - This repository details hyperparameter optimization‚Äîusing Optuna‚Äîfor a Japanese sentiment analysis model (cl-tohoku/bert-base-japanese-whole-word-masking on the multilingual-sentiments dataset) with a learning rate of 2.82e-05, gradient accumulation of 1, and weight decay of 0.00017, employing a cosine learning rate schedule.
  - Downloads: 25
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero is a 13B Japanese language model fine-tuned on 15k samples from the Jaster dataset for instruction following.
  - Downloads: 25
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - This repository provides a fully fine-tuned Phi-3-mini-4k-instruct model‚Äîtrained on the llm-jp/hh-rlhf-12k-ja dataset‚Äîfor Japanese instruction following with English reasoning.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - This repository provides a Japanese BERT-base model (‚ÄúVaporetto + BPE‚Äù) and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 25
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - This repository provides a Japanese chat model‚Äîa variant of ebisuke/liz-nojaloli-ja‚Äîfine-tuned from abeja/gpt-neox-japanese-2.7b for personal study, utilizing a specific input format for conversational turns.
  - Downloads: 25
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-large Japanese model, pretrained on the Aozora corpus and fine-tuned for Universal Dependencies (POS-tagging & dependency parsing) using the goeswith subword approach.
  - Downloads: 25
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - This repository provides a Japanese BERT base model, finetuned for cyberbullying detection using a combined dataset of BBS and Twitter comments, licensed under CC BY-SA 4.0.
  - Downloads: 25
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - This repository provides a Japanese T5 language model fine-tuned on a 100GB corpus of Japanese text‚Äîincluding Wikipedia, OSCAR, and CC-100‚Äîfor adapted language modeling (next token prediction).
  - Downloads: 25
- [tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF](https://huggingface.co/tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF)
  - This repository provides GGUF quantized files for the elyza/ELYZA-japanese-Llama-2-13b-instruct model, compatible with llama.cpp (b4242 commit), and supported by TensorBlock‚Äôs infrastructure and related MCP server projects.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - This repository provides an uncensored (abliterated) version of vecteus v1, a high-performance Japanese large language model specializing in novel-style text generation and offering improved freedom in natural language tasks like generation, question answering, and summarization.
  - Downloads: 25
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository provides the 32B Japanese language model DeepSeek-R1-Distill-Qwen, converted to the MLX format for efficient inference using the `mlx-lm` library.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - This project provides code and models to convert standard Japanese text into ‚ÄúOjisan‚Äù (middle-aged man) style‚Äîcharacterized by specific phrasing, katakana particles, and emojis‚Äîusing Unsloth, LoRA, and GRPO for efficient and high-performance transformation.
  - Downloads: 24
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 24
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - This repository provides a Japanese pre-trained ALBERT model ("ken11/albert-base-japanese-v1-with-japanese-tokenizer") optimized for easier tokenization and intended for fine-tuning on various downstream tasks.
  - Downloads: 24
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model, trained with the Heron library, enabling conversational image understanding using GitGPTNeoX.
  - Downloads: 24
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - This repository provides a 4-bit GPTQ quantized version of the C3TR-Adapter model for English-Japanese and Japanese-English translation, runnable on Colab (with improved quality on paid tiers), requiring source installation via AutoGPTQ.
  - Downloads: 24
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - This repository fine-tunes Microsoft‚Äôs Phi-3-mini-4k-instruct model using the llm-jp/hh-rlhf-12k-ja dataset for improved Japanese instruction-following and response generation.
  - Downloads: 24
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
  - This repository provides a 4-bit quantized ExLlamaV2 model of augmxnt/shisa-base-7b-v1, a 7B Mistral model extended with 8B Japanese tokens for improved performance on Japanese language tasks.
  - Downloads: 24
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - This repository provides a fine-tuned RoBERTa-base Japanese model for zero-shot text classification on the JSNLI dataset, achieving 93.28% accuracy and requiring Juman++ word segmentation for input.
  - Downloads: 24
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B is a pre-trained Japanese text generation model, similar to GPT2/GPT3, trained on a large Japanese corpus and usable via the `transformers` library.
  - Downloads: 24
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised Chinese-Japanese masked language model pretraining framework leveraging the Unihan database and a coarse-to-fine approach to exploit shared morphological knowledge between the languages.
  - Downloads: 24
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 24
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts using the Japanese-LUW-Tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, mirroring the original ELECTRA small architecture.
  - Downloads: 23
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - This repository provides a RoBERTa-large Japanese model pretrained for universal dependency parsing and POS tagging on Japanese text, utilizing the goeswith subword tokenization and requiring the fugashi library.
  - Downloads: 23
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - This repository hosts a Japanese-tuned version of Meta's Llama 3 8B Instruct model, enhanced with ChatVector and QLoRA, achieving an average score of 3.32 on ELYZA-tasks-100 (Q5_K_M quant).
  - Downloads: 23
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - REV-Mix is an anime and realistic style diffusion model for Stable Diffusion, inspired by DateMix and RDtMix, optimized with specific settings like DDIM/DPM++ Karras samplers, and enhanced with embeddings.
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja is a Japanese language LayoutLM model pretrained for token classification tasks and finetuned from cl-tohoku/bert-base-japanese-v2, released under CC BY-SA 3.0.
  - Downloads: 23
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - This repository provides a 4-bit AutoAWQ quantized version of the phi-4 language model, calibrated with both Japanese and English data for improved performance in both languages.
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - This repository provides a Japanese ELECTRA base model pretrained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 22
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-large Japanese model pre-trained on the Aozora corpus for dependency parsing and question answering, building upon existing character-level and UD Japanese models.
  - Downloads: 22
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - Nagisa-BERT is a BERT model and tokenizer for Japanese text, specifically designed for use with the Transformers library and installable via pip for Python 3.7+ on Linux/macOS.
  - Downloads: 22
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 is a Japanese-language GPT-2 model finetuned on the ATOMIC dataset using causal language modeling for text generation, offering reproducible results with a defined seed.
  - Downloads: 22
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository provides a medium-sized, right-to-left Japanese GPT-2 model built with a BERT-like tokenizer, requiring PyTorch, fugashi, and Hugging Face Transformers for usage.
  - Downloads: 22
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 7B model, offering various parameter permutations for optimized performance.
  - Downloads: 22
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - This repository provides a LoRA-finetuned language model (Ninja-v1-NSFW) for role-playing, built upon Aratako/Ninja-v1-RP and utilizing the Vicuna chat template with Japanese datasets like Rosebleu and LimaRP.
  - Downloads: 22
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 22
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - This repository provides a Japanese ELECTRA-small model pre-trained on Japanese Wikipedia subword units using Byte-Pair Encoding and mecab-ipadic-NEologd tokenization.
  - Downloads: 22
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
  - LLM-jp-3.1-8x13b-instruct4 is a Japanese large language model from NII, enhanced with instruction pre-training for improved instruction-following capabilities, building upon the LLM-jp-3 series.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - jp-ModernBert-base-preview is a 150M parameter Japanese language model, trained on approximately 300B tokens of Japanese data (fineweb2), featuring an 8192 context length and utilizing BertJapaneseTokenizer for efficient inference, potentially with FlashAttention.
  - Downloads: 22
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 1.7B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following and causal language modeling.
  - Downloads: 21
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI's Japanese StableLM Instruct Beta 70B, optimized for efficiency using hardware from Massed Compute and supported by a16z and community contributions.
  - Downloads: 21
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - This repository provides a 7B language model‚Äîfine-tuned for Q&A and RAG with context, 4-bit quantized using AutoGPTQ/AutoAWQ, and aiming for performance exceeding GPT-3.5.
  - Downloads: 21
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard is a high-performing, 7 billion parameter Japanese language model, fine-tuned from Meta's Llama-2-7b, that outperforms ChatGPT-3.5 on the JGLUE benchmark without using JGLUE or ChatGPT data in its training.
  - Downloads: 21
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese & English generation with improved long-context memory, including NSFW variants and 128k context versions.
  - Downloads: 21
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - This repository provides a Japanese-finetuned version of Meta-Llama-3-8B-Instruct, trained on a 49k conversation dataset using h2o-llmstudio with an 8k context length, and usable with the transformers library.
  - Downloads: 21
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - This repository provides a merged 7B Japanese language model‚Äîcreated with `mergekit` using a linear method‚Äîcombining Aratako/Antler-7B-RP-v2, NTQAI/chatntq-ja-7b-v1.0, and TFMC/Japanese-Starling-ChatV-7B, configured with bfloat16 precision and int8 quantization.
  - Downloads: 21
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - This repository provides Japanese-finetuned versions of Meta-Llama-3-8B-Instruct, compatible with both Transformers and the original Llama3 codebase, trained on a 49k conversation dataset with an 8192 context length.
  - Downloads: 21
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed through a collaboration including Fujitsu, RIKEN, and several universities, permitting both commercial and non-commercial use, modification, and redistribution.
  - Downloads: 21
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model, finetuned from Llama-2-7b via direct preference optimization on a large Japanese dataset, developed by SambaNova Systems.
  - Downloads: 21
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - This repository provides a Japanese SPLADE model (aken12/splade-japanese) fine-tuned on the mMARCO dataset, built upon tohoku-nlp/bert-base-japanese-v2, and includes example code for query encoding.
  - Downloads: 21
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for Universal Dependencies-style part-of-speech tagging and dependency parsing, utilizing the goeswith subword tokenizer.
  - Downloads: 21
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - RoBERTa-long-japanese is a Japanese language model pretrained on 200M sentences, extending the base RoBERTa model with a 1282 maximum position embedding to process longer inputs after Juman++ and SentencePiece tokenization.
  - Downloads: 21
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - DeBERTa-large-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko text corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 20
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 20
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - DeBERTa-v3-base-japanese-ud-goeswith is a Japanese language model pretrained on LLM-jp v1.0 for universal dependency parsing and part-of-speech tagging using the goeswith subword tokenization.
  - Downloads: 20
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)
  - This repository provides a Japanese-adapted version of BLOOM-7B, fine-tuned using LAPT and random initialization, offering a readily usable model and tokenizer for causal language modeling with 8-bit loading support.
  - Downloads: 20
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository provides a fine-tuned luke-japanese-base model for calculating sentence similarity (JSTS) with a Pearson correlation of 0.8971, utilizing the yahoo japan/JGLUE dataset and the `transformers` library.
  - Downloads: 20
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This repository provides a Japanese BERT-base model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 20
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This repository provides a 6.8 billion parameter Japanese language model, built upon EleutherAI‚Äôs Mesh Transformer JAX (like GPT-J-6B), utilizing T5Tokenizer and SentencePiece for tokenization and requiring SentencePiece normalization.
  - Downloads: 20
- [Aratako/MistralPrism-24B](https://huggingface.co/Aratako/MistralPrism-24B)
  - This repository provides a merged and enhanced 24B parameter roleplay model based on Mistral-Small-3.1-24B-Instruct, optimized for character-driven conversations using a specific chat template.
  - Downloads: 20
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - jp-modernbert-large-preview is a 396M parameter Japanese language model (based on BERT) with 8192 context length, trained on ~100B tokens including fineweb2 data, and optimized for efficient inference with FlashAttention.
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - This repository provides GGUF versions of the Llama-3.3-Swallow-70B-Instruct-v0.4 base model, fine-tuned with the imatrix Japanese language dataset.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - This repository provides a ctranslate2-compatible dataset converted from the AIBunCho japanese-novel-gpt-j-6b model, featuring 8-bit quantization which may impact accuracy.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 19
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-large Japanese language model pretrained on Wikipedia & ÈùíÁ©∫ÊñáÂ∫´ for dependency parsing and question answering, utilizing [MASK] tokens for disambiguation.
  - Downloads: 19
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - This repository provides a RoBERTa-based Japanese language model pretrained for universal dependency parsing and part-of-speech tagging, built upon `roberta-base-japanese` and utilizing the `goeswith` subword tokenizer, requiring `fugashi` for usage.
  - Downloads: 19
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B is a 7 billion parameter Japanese language model built with LAPT and random initialization, available via PEFT and Transformers for tasks like causal language modeling.
  - Downloads: 19
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-base-gamma-7b via Slerp merging, focusing on layers 0-32 to create a Japanese language model.
  - Downloads: 19
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Unigram, requiring a downloaded dictionary file for proper loading and processing.
  - Downloads: 19
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This repository provides a fine-tuned open-calm-7b language model, trained with H2O LLM Studio on a Japanese quiz dataset, and ready for text generation using the transformers library with specified dependencies.
  - Downloads: 19
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese RoBERTa-base model optimized for super short unit word (SSUW) text, requiring full-width conversion and SSUW segmentation as preprocessing steps.
  - Downloads: 19
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - This repository provides a fine-tuned Japanese BERT model for multi-class classification of grammar points, trained on dictionary data and LLM-augmented examples, for use in language learning applications.
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - Sarashina2.2-3Bx8-moe is a Japanese Mixture of Experts language model‚Äîbuilt upon sbintuitions/sarashina2.2-3b-instruct-v0.1 with mergekit-moe‚Äîthat combines eight specialized models for diverse and high-quality instruction-following text generation.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - This repository provides a 4-bit quantized, Japanese-language fine-tune of the Llama-2-Chat 70B model, trained on the Alpaca-JA dataset, requiring adherence to Meta's LLaMA license.
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia text, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 18
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - This repository provides a Japanese BERT-base tokenizer trained with Nothing + Unigram, requiring a downloaded dictionary file for loading and use with Transformers.
  - Downloads: 18
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 18
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Instruct Beta 7B, optimized for efficiency using hardware from Massed Compute and supported by a16z and Patreon contributions.
  - Downloads: 18
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
  - This repository provides a Japanese-adapted version of Mistral-7B, fine-tuned with LAPT and heuristics (untied), and easily loadable via PEFT and Transformers libraries.
  - Downloads: 18
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository provides a GPTQ quantized version of ELYZA-japanese-CodeLlama-7b-instruct, calibrated using a 1k sample from the Japanese Wikipedia and the ELYZA-tasks-100 dataset.
  - Downloads: 18
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - This repository provides a fine-tuned Japanese language model based on japanese-novel-gpt-j-6b, enabling conversations with the Touhou Project character, Marisa Kirisame, using a specific prompt format, and trained with 4-bit quantization.
  - Downloads: 18
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - This repository provides a Japanese BERT base model pretrained with character-level tokenization and whole word masking, offering a simplified dependency-free alternative to cl-tohoku/bert-base-japanese-char-v2 by using basic tokenization instead of MeCab.
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specializing in Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a fine-tuned Mistral-7B language model boasting a 128k context window, high-quality Japanese & English generation, and enhanced long-context memory, including NSFW capabilities.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is a 90M parameter Japanese GPT-2 language model finetuned for kanji-to-kana conversion, built upon ku-nlp/gpt2-small-japanese-char and licensed under CC-BY-SA 4.0, offering improved performance and contextual conversion capabilities.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - This repository provides a fine-tuned Japanese LLM for Whisper, specifically trained on Dominion card terminology to improve speech-to-text accuracy, having learned all cards as of December 19, 2023, and addressing challenges with similar-sounding or weakly-pronounced terms.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - KoichiYasuoka/deberta-large-japanese-wikipedia is a DeBERTa(V2) language model pre-trained on Japanese Wikipedia and Aozora Bunko texts, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling.
  - Downloads: 17
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture improved with GEGLU activation and intended for fine-tuning with dropout re-enabled.
  - Downloads: 17
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE is a suite of Japanese decoder-only language models by CyberAgent, Inc., fine-tuned with LoRA using PEFT, PyTorch, and Transformers.
  - Downloads: 17
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - This repository provides the llm-jp-1.3b-v1.0-aya model, a 1.3 billion parameter Japanese language model fine-tuned on Cohere‚Äôs aya dataset, with example code for usage via Hugging Face Transformers.
  - Downloads: 17
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 large language model, currently offering the Q4_K_M quantization.
  - Downloads: 17
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - This repository provides a Japanese-adapted version of Mistral-7B, fine-tuned with LAPT and CLP techniques, and accessible via PEFT and Transformers libraries.
  - Downloads: 17
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts a 3.6B parameter, 8-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation, with usage examples provided.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-base Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, built upon existing char-level and UD Japanese resources.
  - Downloads: 17
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-large Japanese model, pretrained on Aozora Bunko for dependency parsing and question answering, utilizing UD_Japanese-GSDLUW and optimized for long-unit words with [MASK] tokens for disambiguation.
  - Downloads: 17
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository provides HuBERT-base model weights trained on JTubeSpeech for speech recognition tasks, specifically functioning as an encoder to embed audio into latent variables, and is *not* a speech generation model.
  - Downloads: 17
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - Rinna/nekomata-14b-instruction-gguf provides a GGUF quantized version of the 14B rinna/nekomata-14b-instruction model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization.
  - Downloads: 17
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - ModernBERT-large-japanese-aozora is a pre-trained Japanese language model based on the ÈùíÁ©∫ÊñáÂ∫´ text corpus, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - This repository hosts Japanese large language models‚Äîincluding 13B and 1.3B parameter variants‚Äîdeveloped by LLM-jp, offering both pre-trained and instruction-tuned (LoRA & full) checkpoints in Hugging Face format.
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B is a 7B-parameter Japanese language model fine-tuned for ACG content generation, built upon Japanese Stable LM Base Gamma 7B as an experimental, quality-focused project.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for live Japanese speech-to-text translation, trained on a combination of datasets including Common Voice, JSUT, and CSS10.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - DeBERTa-base-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-based model pre-trained on Japanese text (Wikipedia & Aozora Bunko) for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 16
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-13B language model, available on Hugging Face.
  - Downloads: 16
- [mmnga/alfredplpl-suzume-poc-gguf](https://huggingface.co/mmnga/alfredplpl-suzume-poc-gguf)
  - This repository provides a GGUF-formatted conversion of the `suzume-poc` model, based on Gemma-2b, requiring agreement to the Gemma terms of use for usage and utilizing `llama.cpp` for inference.
  - Downloads: 16
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 16
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for processing.
  - Downloads: 16
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B is a 1 billion parameter language model pre-trained on 100 billion tokens, utilizing a Differential Transformer architecture with Differential Attention to improve context focus and efficiency via patch-level training and the Muon optimizer.
  - Downloads: 16
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B is a 12B parameter language model merged from Orihime-12B with ForgottenMaid-12B, Francois-PE-V2-Huali-12B, and Ohashi-NeMo-12B using the Model Stock merge method.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-large Japanese language model pre-trained on Wikipedia and Aozora Bunko texts for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on Japanese Wikipedia and Aozora texts, specifically for dependency parsing and handling ambiguous words using the [MASK] token.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - DeBERTa-base-Japanese-Juman-UD-goeswith is a pretrained Japanese DeBERTa(V2) model for POS-tagging and dependency parsing, requiring fugashi and utilizing the goeswith subword tokenizer.
  - Downloads: 15
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly higher accuracy, particularly in qualitative evaluations, and requiring SentencePiece for inference.
  - Downloads: 15
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - This repository reproduces a 7B-parameter Japanese language model, fine-tuned for instruction following using translated Dolly-15k and other datasets, based on Japanese Stable LM Base Gamma 7B and trained with the Notus codebase.
  - Downloads: 15
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - This repository details swallow-hermes-st-v1, a Japanese LLM created by merging an English language model's "story vector" with a base Japanese model using an evolutionary strategy, aiming to generate relaxed, bedtime stories‚Äîinspired by Chat Vector and EvoLLM techniques‚Äîas an alternative to rigid outputs from models like GPT-4.
  - Downloads: 15
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 15
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-7B language model, available on Hugging Face.
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Unigram, requiring a downloaded dictionary file for operation.
  - Downloads: 15
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU is an experimental T5-based machine translation model for translating between Japanese and the Ainu language.
  - Downloads: 15
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - This repository provides a Japanese natural language processing pipeline featuring a BERT-based transformer, parser, and NER model (ja_gsd_bert_wwm_unidic_lite v3.1.1) trained on UD_Japanese-GSD and utilizing the CC BY-SA 4.0 license.
  - Downloads: 15
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
  - Shirayukihime-12B is a 12B parameter language model merged via the TIES method, building upon natong19/Mistral-Nemo-Instruct-2407-abliterated and incorporating shisa-ai/shisa-v2-mistral-nemo-12b and Elizezen/Himeyuri-v0.1-12B, using bfloat16 precision.
  - Downloads: 15
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - This model merges differences between Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1 based on Swallow-MX-8x7b-NVE-v0.1, improving Japanese naturalness and offering top-tier performance with a 32k context size for local LLM use.
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - This repository provides a Japanese language model, llm-jp-3-3.7b-instruct, fine-tuned for long-text generation using the Japanese-LongWriter-3k dataset, trained with a learning rate of 1e-05 and specific batch sizes.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Rinna's Llama 3 Youko 70B is a continually pre-trained version of Meta-Llama-3-70B‚Äîusing 5B Japanese/English tokens‚Äîwith improved Japanese performance, and is available in 8B sizes including instruction-tuned and quantized (GPTQ) versions.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - Rinna/nekomata-14b-gguf provides a GGUF quantized version of the 14 billion parameter rinna/nekomata-14b model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 14
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - This repository provides a Japanese chat model, ‚Äúliz-nojaloli-ja‚Äù, fine-tuned from rinna/japanese-gpt-neox-3.6b, designed for personal study and featuring a specific conversational style, with instructions for usage involving input formatting for consistent character responses.
  - Downloads: 14
- [ayousanz/modernbert-vtuber-finetuned](https://huggingface.co/ayousanz/modernbert-vtuber-finetuned)
  - This repository provides a finetuned ModernBERT-Ja-130M model for binary classification determining if a YouTube channel is associated with a VTuber, trained on channel metadata like titles and descriptions.
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository provides a fine-tuned ModernBERT-ja-310m reward model for evaluating the quality of Japanese novels, primarily intended for reinforcement learning of text generation models, by predicting user ratings as a regression task.
  - Downloads: 14
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - This repository provides a Japanese-enhanced, EPR-focused fine-tune of Mistral-Nemo (v0.2 in GGUF format) using a diverse dataset, optimized for roleplaying with a recommended temperature of 0.3 and Japanese output prompting to minimize English mixing.
  - Downloads: 14
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - This repository showcases a merged Stable Diffusion model (‚ÄúKokuwa‚Äù), built upon KiwiMix and other models, specializing in uniquely stylized, slightly quirky character generation with some seed-dependent variability.
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository provides a Japanese-finetuned version of the DeepSeek-R1-Distill-Qwen-14B model, optimized for Japanese output and utilizing a system prompt to function as a helpful Japanese assistant.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - This repository provides a fine-tuned DistilHuBERT speech recognition model for Japanese, trained on a combination of publicly available and custom-recorded speech datasets, requiring adherence to the JVS corpus terms of use.
  - Downloads: 13
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - This repository hosts a merged language model‚Äîbuilt with mergekit and based on Aratako/Ninja-v1-RP-WIP‚Äîoptimized for roleplaying through task vector addition and model stock merging, utilizing the Vicuna chat template and requiring an `eos_token` for multi-turn conversations.
  - Downloads: 13
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - This model generates Japanese novels using QLoRA fine-tuning on a dataset of 216 highly-rated web novels, Aozora Bunko texts, and Wikipedia articles, guided by genre, keywords, and prompts, though it's a prototype with unverified behavior and may contain special tokens.
  - Downloads: 13
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 13
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - This repository provides a 7B-parameter, instruction-following Japanese language model (Japanese Stable LM Instruct Gamma 7B) fine-tuned from the base Gamma model, requiring Transformers 4.34.0+.
  - Downloads: 13
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - This repository hosts a 130.78M parameter Llama 2 model trained on Japanese text, utilizing the `transformers` library and a sentencepiece tokenizer for causal language modeling.
  - Downloads: 13
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - This repository provides GGUF conversions of Line Corporation's japanese-large-lm-3.6b language model, intended for use with llama.cpp and potentially subject to future incompatibility with updates to llama.cpp‚Äôs GPT-NeoX/GPT-2 implementations.
  - Downloads: 13
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository provides a Japanese BERT-large model fine-tuned on the JCommonsenseQA dataset for performing commonsense question answering tasks.
  - Downloads: 13
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - Bert-base-sudachitra-v11 is a Japanese language model based on SudachiTra, differing from chiTra v1.1 through surface form normalization and vocabulary adjustments for improved tokenization.
  - Downloads: 13
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated)
  - This repository provides a bfloat16 merged model combining shisa-ai/shisa-v2-mistral-nemo-12b with the nbeerbower/Mistral-Nemo-12B-abliterated-LORA, along with code for LoRA merging using Qwen3.
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja is a fine-tuned, Japanese-language speech-to-text model based on OpenAI's Whisper Small, trained on the Common Voice 17.0 dataset with specific hyperparameters like a 1e-05 learning rate.
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - Isekai-BERT-v1 is a fine-tuned Japanese BERT model (cl-tohoku/bert-base-japanese-v3) with reported loss of 1.9164, but lacking detailed information on data, intended use, and training procedures beyond hyperparameters like learning rate and batch size.
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 12
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - This repository provides a fine-tuned Wav2Vec2 model (facebook/wav2vec2-large-xlsr-53) for Japanese accent recognition, achieving a 15.82% Word Error Rate on 16kHz audio inputs.
  - Downloads: 12
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 12
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - This repository provides a Japanese BART model‚Äîconverted from Kyoto University‚Äôs original‚Äîfor sequence-to-sequence tasks using the BartJapaneseTokenizer and the Hugging Face Transformers library.
  - Downloads: 12
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - This repository provides a pretrained Japanese ELECTRA model (ShinoBU), tokenized with SudachiTra WordPiece, trained on 200M sentences and usable with the Hugging Face Transformers library.
  - Downloads: 12
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and context retrieval (RAG) with both GPTQ and AutoAWQ 4-bit quantization, aiming for performance exceeding GPT-3.5.
  - Downloads: 12
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-v2 model for Japanese, specifically trained on Aozora Bunko texts using the BertJapaneseTokenizer and compatible with tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - This repository showcases a fine-tuned Japanese large language model (japanese-large-lm-3.6b-instruction-sft) for middle and high school content, enhanced with instruction tuning using the sakura dataset and requiring specific Python library installations.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 model, currently offering only the Q4_K_M quantization.
  - Downloads: 12
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - Zenz v2.5 is a Japanese GPT-2-based conditional language model specialized for Kana-to-Kanji conversion, offered in small, medium, and large sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is a full, instruction-following language model based on Meta-Llama-3-8B-Instruct, developed and tested on ConoHa VPS with NVIDIA H100 GPUs.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - This repository features a Japanese language model exhibiting inconsistent persona (shifting gender & unstable personality) but consistently cheerful, intended for experimentation‚Äînot merging‚Äîwith parameters optimized for a 150-token, sampled output focused on self-introduction and conversation.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - ModernBERT-base-japanese-char is a pre-trained Japanese language model based on BERT, trained on Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository provides the DeepSeek-R1-Distill-Qwen-32B-Japanese large language model converted to MLX format for use with the `mlx-lm` library.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository provides a fine-tuned modernbert-ja-130m reward model for evaluating the quality of Japanese novels, intended for use with reinforcement learning of text generation models, and predicts user ratings as a regression score, acknowledging potential biases beyond text quality.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset for improved conversational ability.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - This repository provides a pretrained ESPnet2 text-to-speech (TTS) model‚Äîkan-bayashi/jsut_full_band_vits_prosody‚Äîtrained on the JSUT dataset and available via Zenodo.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Syntactic Text Processing
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 4,936
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, offering a small, readily usable model for causal language modeling via Hugging Face Transformers.
  - Downloads: 4,378
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B is a suite of 7-billion parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., readily usable with transformers for text generation.
  - Downloads: 3,395
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7B, 13B, and 70B instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 3,121
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B is a 3-billion parameter suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., and readily usable with Hugging Face Transformers.
  - Downloads: 2,842
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - This repository provides a modified version of CreativeML Open RAIL-M with added authorship by sazyou_roukaku, retaining the original license but emphasizing adherence to its usage restrictions, particularly prohibiting illegal or specialized (e.g., medical) applications.
  - Downloads: 2,806
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow is a continually pre-trained language model, based on Llama 2 and enhanced with Japanese data, with released instruction-tuned versions (7b, 13b, 70b) available as previews.
  - Downloads: 2,687
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow is a continually pre-trained language model, based on Llama 2 and enhanced with Japanese data, offering instruction-tuned versions (7b, 13b, 70b) released as preview models.
  - Downloads: 2,509
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, enabling text generation via the Hugging Face Transformers library.
  - Downloads: 2,461
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-NSFW-128k Japanese language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 2,212
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 2,121
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 is a Japanese and English language model continuously pre-trained from Mixtral-8x7B-Instruct-v0.1, utilizing the same tokenizer and focusing on improved Japanese performance.
  - Downloads: 2,112
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,050
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium is a suite of decoder-only language models pre-trained by CyberAgent on Japanese datasets, readily usable via the `transformers` library for text generation.
  - Downloads: 2,042
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7b, 13b, and 70b instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 2,040
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,019
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,017
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 2,007
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,968
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,936
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-8B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,522
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-30B-A3B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,366
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,329
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - This model, licensed under CreativeML Open RAIL++-M, provides image generation with recommended settings (DPM++ 2M SDE karras, 30-40 steps, 1152x896 resolution) but strictly prohibits generating violent, sexually explicit, or exploitative content, especially involving minors or real individuals without consent.
  - Downloads: 1,060
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - This repository provides GGUF-formatted versions of rinna's llama-3-youko-8b and other models, built with data from TFMC/imatrix-dataset-for-japanese-llm, and demonstrates usage with llama.cpp.
  - Downloads: 978
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - This repository provides GGUF formatted versions of the NSFW Ninja-v1 language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 758
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-7.2b-instruct3 Japanese language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp (excluding custom chat templates/`-cvn`).
  - Downloads: 548
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-128k language model, trained on the imatrix dataset and intended for use with llama.cpp for Japanese novel writing.
  - Downloads: 472
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - This repository presents evaluation results for several Japanese SPLADE models (including versions optimized for efficiency and inference speed) on information retrieval benchmarks like MIRACL and JQaRA, reporting metrics such as nDCG, Recall, and MRR.
  - Downloads: 467
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - This repository provides a Stanza NLP model for Japanese, offering tools for linguistic analysis including syntax and entity recognition, automatically generated via Hugging Face.
  - Downloads: 427
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - This repository provides statically quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model in GGUF format, offering various quantization levels (like Q2_K) for efficient use.
  - Downloads: 396
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - Shisa-base-7b-v1 is a 7B language model built upon Mistral 7B, enhanced with 8B Japanese tokens from MADLAD-400 and a 120k extended tokenizer for improved Japanese language efficiency (~2.3 characters/token).
  - Downloads: 349
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - Wanabi-24B is a Japanese large language model fine-tuned for novel writing assistance, built upon Mistral-Small-24B, and currently available in GGUF format with ongoing development and increasing training data with each version (currently v0.3).
  - Downloads: 348
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores archived and experimental Stable Diffusion 1.5 models‚Äîincluding discarded, merged, and playful variations‚Äîprimarily focused on a ‚Äúlametta‚Äù-style deformation effect achieved through merging with other models, offering a heavily stylized and simplified output.
  - Downloads: 288
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - This repository provides a faster, CTranslate2-converted Japanese speech recognition model based on OpenAI's Whisper-large-v2, optimized for use with the `faster-whisper` library.
  - Downloads: 247
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - This repository provides GGUF-formatted versions of the shisa-7b-v1 language model, demonstrated with example prompts for Pok√©mon-related questions and Japanese-to-English translation using llama.cpp.
  - Downloads: 230
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - This repository provides a GGUF-formatted version of the matsuo-lab weblab-10b-instruction-sft model, compatible with llama.cpp, utilizing a modified branch for faster development.
  - Downloads: 214
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - wanabi-24B is a preview of a 24B Japanese language model fine-tuned for fiction writing, based on Mistral-Small-24B, and specializing in tasks like idea generation and text continuation, currently available in GGUF format as a proof-of-concept with limited training.
  - Downloads: 197
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - This repository provides GGUF-formatted versions of RakutenAI-7B, a base language model, compatible with llama.cpp for inference and experimentation.
  - Downloads: 193
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-A-v1-7B base model, utilizing models like Shisa, Gamma, and Mistral 7B, and is intended for use with llama.cpp.
  - Downloads: 150
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Ninja-v1-RP language model, requiring users to check the original model for licensing details.
  - Downloads: 122
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-v1-7B base model, a merged model utilizing Shisa Gamma, WizardMath, and Abel, for use with llama.cpp.
  - Downloads: 116
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/Ninja-v1-RP-expressive language model, referencing the original model for licensing and details.
  - Downloads: 103
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha is a vision-language model that generates Japanese descriptions from images and optional text inputs, utilizing transformers for instruction-following tasks.
  - Downloads: 101
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - This repository hosts static, quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, offering various quantization levels (Q2_K, Q3_K_S) for different size/quality trade-offs.
  - Downloads: 91
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on chatntq-ja-7b-v1.0, originally derived from Mistral-7B-v0.1, with details and GGUF versions available.
  - Downloads: 75
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 69
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 69
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository releases a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS, featuring a custom tokenizer and currently in beta.
  - Downloads: 69
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - This repository provides a GGUF-formatted, 7B parameter version of Deepreneur's Blue Lizard language model, designed for use with llama.cpp and adhering to the Llama 2 license.
  - Downloads: 65
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - This repository provides a model for generating article bodies from titles, as detailed in the linked Qiita article.
  - Downloads: 64
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps speech recognition model to the CTranslate2 format for faster inference using CTranslate2 or faster-whisper.
  - Downloads: 57
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - This repository provides a research-focused, 5000-step fine-tuned version of OpenAI's whisper-large-v2 model specifically for Japanese speech recognition using the CommonVoice v11 dataset, achieving a 0.7449 WER.
  - Downloads: 56
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - Suzume-POC is a commercially-usable, Japanese language base model derived from Google‚Äôs Gemma-2B, optimized for small devices despite potential instruction-tuning challenges.
  - Downloads: 52
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - This repository details research on generating more characterful conversational responses using reinforcement learning.
  - Downloads: 50
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - This repository provides statically quantized versions of the Japanese-Llama-3-8B-Instruct-v2 model in GGUF format, offering various quantization levels (like Q2_K) for efficient use, with links to the original model and usage guidance.
  - Downloads: 41
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - This repository provides GGUF-formatted conversions of the Tora-7B-v0.2 language model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt text data.
  - Downloads: 39
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - YACIS-ELECTRA-Small is a Japanese language model pretrained on a 5.6 billion-word blog corpus using the ELECTRA Small architecture with WordPiece tokenization and a 32,000 token vocabulary.
  - Downloads: 33
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - This repository provides a Japanese language Bloom model with a 10,000 vocabulary size, 12 layers, and 8 attention heads.
  - Downloads: 32
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository provides a Japanese novel quality assessment Reward Model, fine-tuned from sbintuitions/sarashina2.1-1b for applications like reinforcement learning of text generation, predicting user ratings to indirectly evaluate text quality while acknowledging potential biases.
  - Downloads: 30
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - This repository provides Mixtral-8x7B-v0.1-japanese, a Japanese language model built upon Mixtral-8x7B-v0.1 with extended vocabulary and continued pre-training, detailed in the linked ABEJA tech blog and Metagton-LM repository.
  - Downloads: 28
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - This repository provides a GGUF conversion of ChatNTQ-JA-7b-v1.0, a Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b (based on Mistral 7B).
  - Downloads: 27
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - This repository provides GGUF-formatted, K-quantized versions of the Tora-7B-v0.1 language model, enhanced with iMatrix using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 25
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - This repository implements a quote-based reasoning model for inference and potentially natural language understanding tasks.
  - Downloads: 24
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - This repository provides a Japanese language model example utilizing AutoTokenizer, AutoModelForCausalLM, and Unifine formatting for in-context and instruction learning.
  - Downloads: 24
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - This repository provides Tokara-0.5B-v0.1, a Japanese-enhanced language model based on Qwen1.5-0.5B, continuously pre-trained with 5B Japanese-English tokens for more stable Japanese output, and evaluated on jsquad, jcommonsenseqa, and jnli benchmarks.
  - Downloads: 22
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - This repository hosts a merged language model, built upon OpenBioLLM-8B and Llama-3-youko-8b, aimed at improving Japanese-language biomedical knowledge, though it may exhibit some hallucination and weaker content filtering.
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - This repository provides a GGUF-formatted version of the Tanuki-ZeRo base model, enabling its use with llama.cpp for natural language processing tasks.
  - Downloads: 20
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - This repository showcases Japanese-language text generation examples using the Phos 7B model, demonstrating creative writing and dialogue focused on a plea for mercy and assistance.
  - Downloads: 20
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - This repository hosts a merged language model‚Äîbased on Aratako/Ninja-v1-RP and Elizezen/Antler-7B‚Äîenhanced for expressive roleplaying using the Vicuna chat template and requiring `eos_token` for multi-turn conversations.
  - Downloads: 20
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - This repository provides a dummy Japanese tokenizer‚Äîtrained on the snow_simplified_japanese_corpus using Hugging Face datasets‚Äîfor tokenizing Japanese sentences, with adaptable training code included.
  - Downloads: 19
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on Mistral-7B-v0.1 and chatntq-ja-7b-v1.0, with available 6-bit quantized and GGUF versions.
  - Downloads: 19
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B is a Japanese causal language model base, fine-tunable via PEFT, utilizing LAPT and heuristics for improved performance, and available in 8-bit precision.
  - Downloads: 18
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - This repository implements Named Entity Recognition (NER) using the modernBERT-ja-130m model and a custom tokenizer for Japanese, targeting entities like person, organization, location, and product names.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - This model adapts Vecteus to be compatible with LLava, leveraging a combination of Vecteus-v1 and the EvoVLM-JP-v1-7B (shisa-gamma-7b-v1) recipe for enhanced expressiveness.
  - Downloads: 16
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - This repository provides a modified version of tokyotech-llm/Swallow-MS-7b-instruct-v0.1 with an altered chat template and a default system message for a helpful Japanese assistant.
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares experimental, lightly-tuned merges of various anime-style diffusion models (based on lametta_v1921 and others like vorpal_v1 & snowpearAnime_v10) created during spekulatius merging, offering unique, though potentially imperfect, results.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - This repository provides a Japanese SentencePiece tokenizer‚Äîwith a 52000 vocabulary‚Äîspecifically trained for creative writing tasks with AI Novelist's SuperTrin and Damsel 20B models.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - Sarashina2.2-3Bx4-moe is a ~12B parameter Mixture of Experts (MoE) model created by merging one base model with three "sbintuitions/sarashina2.2-3b-instruct-v0.1" expert models to improve performance and generate high-quality responses.
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - This model is a QLoRA fine-tuned version of sbintuitions/sarashina2.2-3b-instruct-v0.1, enhanced with Japanese-Pythonic-FunctionCall and Kendamarron/jimba-instruction-all to enable calling Python functions via a specific system tool format.
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This model fine-tuned from SakanaAI/TinySwallow-1.5B-Instruct generates numerous, concise, bullet-pointed slides (max 15 characters per message) in the style of the Takahashi Method for impactful presentations.
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - DataPilot's Arrival-32B-Instruct-v0.4 is a Japanese-enhanced, instruction-tuned language model finetuned from Qwen2.5-32B and incorporating DeepSeek-R1-Distill-Qwen-32B-Japanese via ChatVector, built without access to the original base model.
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository links to the AfterRealXL_beta2 model hosted on Civitai, released under the CreativeML Open RAIL++-M license with standard usage restrictions regarding prohibited purposes like illegal activity and specific professional applications.
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is an English-to-Japanese translation model built with Marian-NMT, utilizing transformers and sentencepiece, and easily usable via Hugging Face pipelines.
  - Downloads: 75,771
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT is a Japanese-to-English translation model built with Marian-NMT, transformers, and sentencepiece, offering direct use via a Hugging Face pipeline.
  - Downloads: 56,654
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - Shisa-gamma-7b-v1 is a Japanese language model fine-tuned from Stable LM Base Gamma 7B, sharing results from JA MT-Bench for those interested in its performance.
  - Downloads: 20,300
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source, multilingual (Chinese, English, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, offering demos, benchmarks, and technical documentation.
  - Downloads: 6,633
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - Ultralytics YOLOv11 is a state-of-the-art, fast and flexible model for object detection, tracking, segmentation, classification, and pose estimation, building on previous YOLO versions.
  - Downloads: 5,103
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX transformer model, trained on a large corpus including Japanese CC-100, C4, and The Pile, accessible via Hugging Face as Bilingual 4B MiniGPT4.
  - Downloads: 4,061
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FinguAI-Chat-v1 is a multilingual (English, Korean, Japanese) AI model designed to improve language proficiency and provide education on finance, investment, and legal frameworks within a global context.
  - Downloads: 2,831
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - Gemma-2-2b-jpn-it-translate-gguf is a 2B-parameter Small Language Model delivering near-7B model translation quality for Japanese-English and English-Japanese tasks with a compact ~2GB file size.
  - Downloads: 2,738
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - This repository provides a GGUF-formatted version of the Aya-23-8B language model, trained with the imatrix dataset and usable with llama.cpp for inference.
  - Downloads: 2,719
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL is a 260M parameter translator model, finetuned from sbintuitions/modernbert-ja-130m and Dart v3, that converts Japanese and English into Danbooru tags, licensed under Apache-2.0.
  - Downloads: 1,934
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - This repository provides a Japanese-to-Korean translation model leveraging BERT for Japanese encoding and KOGPT2 for Korean decoding, with a Hugging Face Space demo available.
  - Downloads: 1,730
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 1,639
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-inclusive, 4-bit quantized version of Google‚Äôs Gemma-3-12b-it-qat model using imatrix, runnable with the latest llama.cpp and supporting image input via llama-mtmd-cli and mmproj.gguf.
  - Downloads: 1,587
- [Aratako/NemoAurora-RP-12B-GGUF](https://huggingface.co/Aratako/NemoAurora-RP-12B-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/NemoAurora-RP-12B language model, released under a CC-BY-NC-4.0 license.
  - Downloads: 1,407
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - This repository provides a Korean sentence transformer, finetuned from Alibaba-NLP/gte-multilingual-base, that encodes text into 768-dimensional vectors for semantic tasks like similarity search and classification.
  - Downloads: 1,089
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - This repository provides GGUF-formatted conversions of the multilingual Suzume-Llama-3-8B model, built with data from TFMC/imatrix-dataset-for-japanese-llm, alongside links to other related language models.
  - Downloads: 903
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 is a family of open-weight, bilingual (Japanese/English) chat models developed by Shisa.AI, emphasizing improved Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 886
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT-BT-ja-en is an openly licensed Japanese-to-English translation model built upon the ElanMT-base family and trained solely on publicly available corpora and Wikipedia back-translation data.
  - Downloads: 747
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-inclusive, 4-bit quantized version of the Gemma 3B model, optimized for use with the latest llama.cpp and supporting image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 596
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - This repository provides GGUF versions of the Mistral-nemo-ja-rp-v0.2 Japanese language model, referencing the original model for further details.
  - Downloads: 486
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for improved Japanese language performance.
  - Downloads: 403
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin-inst-merge language model, utilizing TFMC/imatrix data and licensed under Tongyi-Qianwen, compatible with llama.cpp.
  - Downloads: 368
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - This repository provides GGUF-formatted conversions of the lightblue-suzume-llama-3-8B Japanese language model, built using imatrix data, alongside other related lightblue/mmnga models.
  - Downloads: 294
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [shisa-ai/shisa-v2-llama3.1-405b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 242
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi-mini-12B-GGUF is a Japanese large language model fine-tuned for novel writing support, offering comparable functionality to wanabi-24B with enhanced accessibility and improved performance due to a high-quality, focused dataset, and is available in quantized GGUF format for GPUs with 8GB+ VRAM.
  - Downloads: 230
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - This model is based on CreativeML Open RAIL-M, adding ‚Äúsazyou_roukaku‚Äù as an additional author, while adhering to the original license‚Äîspecifically prohibiting uses outlined in restriction A (criminal intent, medical imaging, etc.)‚Äîand disclaiming all responsibility for generated content.
  - Downloads: 209
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - QuinceMix ‚ÄúDefacta‚Äù is a merged Stable Diffusion model excelling in backgrounds and effects, optimized for use with DDIM/DPM++ SDE Karras samplers, and recommended with settings like 20-30 steps, CFG 5-8, and denoising strength of 0.4-0.7.
  - Downloads: 194
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - This repository provides a 1.3B parameter NLLB model fine-tuned for Japanese to English translation of light novels, capable of processing up to 512 tokens.
  - Downloads: 165
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - This repository offers a VAE-integrated model, SakuraMixSeries, prioritizing both background and character quality, licensed under a modified CreativeML OpenRAIL-M license permitting commercial use and modification, but prohibiting resale without attribution.
  - Downloads: 146
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository provides GGUF conversions of rinna‚Äôs japanese-gpt-neox-3.6b-instruction-ppo model, intended for use with llama.cpp, but potentially incompatible with future updates.
  - Downloads: 138
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 137
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the Karasu-Mixtral-8x22B-v0.1 language model and related lightblue/mmnga models, utilizing data from TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 128
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT-BT-en-ja is an English-to-Japanese translation model built on openly licensed data and Wikipedia back-translation, derived from the ElanMT-base family and avoiding web-scraped or machine-translated corpora.
  - Downloads: 123
- [cardiffnlp/tweet-topic-base-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-base-multilingual)
  - This repository provides a multilingual topic classification model‚Äîfinetuned from cardiffnlp/twitter-xlm-roberta-base on 198M tweets‚Äîfor categorizing content into topics like business, science, and fashion in English, Spanish, Japanese, and Greek.
  - Downloads: 114
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - This repository details llm-jp-clip-vit-large, a 467M parameter Japanese CLIP model trained on a translated subset of ReLAION-5B, enabling zero-shot image classification with `open_clip_torch`.
  - Downloads: 111
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin language model, trained with TFMC/imatrix data and licensed under Tongyi-Qianwen, compatible with llama.cpp.
  - Downloads: 107
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 105
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 96
- [future-architect/Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B)
  - Llama 3.1 Future Code Ja is an 8B parameter large language model continually pre-trained on a mixture of code (The Stack V2) and Japanese natural language (LLM-jp Corpus v3) data, then merged with an instruct variant of Meta's Llama 3.1.
  - Downloads: 93
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus with support for Chinese, English, Japanese, and Korean, and offering demos, benchmarks, and downloads.
  - Downloads: 92
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 85
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, and Japanese, with readily available demos, benchmarks, and downloads.
  - Downloads: 82
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B is an open-source, multilingual large language model series by OrionStarAI, trained on 2.5T tokens of diverse languages including Chinese, English, and Japanese, with associated demos, benchmarks, and technical documentation.
  - Downloads: 80
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - This repository provides a T5-based machine translation model‚Äîfine-tuned on the friendly_JA corpus‚Äîthat simplifies Japanese by prioritizing Latin/English-derived *katakana* over traditional Sino-Japanese vocabulary.
  - Downloads: 78
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 is a family of open-weight, bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased training data and efficiency.
  - Downloads: 74
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medLLM is a trilingual (English, Japanese, Chinese) biomedical large language model built on Llama-3-8B, trained via continued pre-training and supervised fine-tuning for enhanced domain expertise and multilingual capabilities.
  - Downloads: 70
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - This repository provides a 3.8B parameter English-Japanese bilingual GPT-NeoX model with an extended 8192 context length achieved through RoPE positional interpolation fine-tuning.
  - Downloads: 67
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 64
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and technical documentation.
  - Downloads: 54
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - This repository provides a fine-tuned version of the MosaicML MPT-7B model, evaluated on a 100-question dataset and requiring `trust_remote_code=True` due to its custom architecture, featuring training optimizations like FlashAttention.
  - Downloads: 54
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides a transformer-aligned machine translation model for Japanese to Malay, utilizing normalization and SentencePiece preprocessing with language ID tokens, based on the OPUS dataset (opus-2020-06-17).
  - Downloads: 52
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This repository provides a fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en, trained on the bsd_ja_en dataset.
  - Downloads: 50
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source, multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean with available demos, benchmarks, and documentation.
  - Downloads: 48
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - This repository details the karakuri-MS-01 model.**(Explanation of choices:** "karakuri-MS-01 model" is the core subject, and "details" captures the repository's purpose. It's direct and retains the necessary information.)**
  - Downloads: 43
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - This repository details llm-jp-clip-vit-base-patch16, a 248M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset using OpenCLIP for zero-shot image classification.
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - This repository hosts a Japanese large language model, built upon Llama2-13b and fine-tuned for witty responses, utilizing a 45,046-token vocabulary and trained with AWS Trainium instances on a 65 billion token corpus.
  - Downloads: 34
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and documentation.
  - Downloads: 34
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - This repository provides a Japanese-adapted refiner model for Stable Diffusion XL 1.0, fine-tuning only the OpenCLIP-ViT/G or CLIP-ViT/L text encoder using English-Japanese parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer to enable Japanese text prompts.
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - This project provides a Japanese-to-English machine translation model, fine-tuned for Weiss Schwarz (WS) trading card text, deployable locally or via a Hugging Face Spaces Gradio app.
  - Downloads: 33
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a DPO-trained, Japanese-language version of Qwen/Qwen2.5-7B-Instruct, excelling in conversational benchmarks and recommended for general AI conversation tasks.
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a translation model built with Marian-NMT and transformers, translating from German, English, Spanish, French, Italian, Russian, and Ukrainian to Japanese, and utilizing data from ParaCrawl alongside the repository's data.
  - Downloads: 30
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling is a 7B multilingual language model continually pretrained on Korean, English, Chinese, Japanese, and a 500-language corpus, based on Google's Gemma-7B.
  - Downloads: 28
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - This repository provides Japanese language models based on LLaMA 2, including base, LoRA, and instruction-tuned (Alpaca) versions in both full and LoRA formats.
  - Downloads: 27
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, offering demos, benchmarks, and technical documentation.
  - Downloads: 27
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - This repository provides a LoRA-fine-tuned Llama 3 Youko 8B model for English-to-Japanese translation, achieving a COMET score of 0.9126 and BLEU of 35.2 on Flores200.
  - Downloads: 25
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source multilingual large language model series developed by OrionStarAI, trained on 2.5T of diverse text data including Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 25
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - This repository releases a Japanese Mixture-of-Experts (MoE) model built by merging and instruction-tuning elyza/ELYZA-japanese-Llama-2-13b and elyza/ELYZA-japanese-Llama-2-13b-instruct, inheriting the Llama 2 license.
  - Downloads: 24
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B is a Japanese-English bilingual LLM built on LLaMA 2, trained with the LEIA technique to improve cross-lingual transfer and achieve enhanced Japanese question answering performance.
  - Downloads: 23
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - This repository provides a Japanese mT5-based doc2query model for enhancing document retrieval through query expansion and re-weighting, improving lexical search with synonyms and boosted important terms.
  - Downloads: 23
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - This repository provides a 2x7B Mixture-of-Experts (MoE) Japanese language model built by merging and instruction-tuning the elyza/ELYZA-japanese-Llama-2-7b-fast and elyza/ELYZA-japanese-Llama-2-7b-fast-instruct models, licensed under Llama 2 Community License.
  - Downloads: 22
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository provides a quantized EXL2 version of a merged Qwen-14B model fine-tuned for translating Japanese game scripts into fluent Chinese, utilizing character prompts and historical context.
  - Downloads: 22
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - This repository provides a Japanese-input-compatible version of the SDXL 1.0 base model, achieved by fine-tuning the OpenCLIP-ViT/G or CLIP-ViT/L text encoders with Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 22
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - ByT5-small-ain-jpn-mt is a pretrained and fine-tuned machine translation model for Ainu-to-Japanese translation, built using Google's ByT5-small architecture and web-crawled data.
  - Downloads: 21
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - This repository provides a work-in-progress Japanese-to-English translation model built on tinyllama, designed for long-context inputs (500-1000 tokens) and requiring deterministic inference settings.
  - Downloads: 20
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - This repository hosts Swallow-MoE-2x13B-v0.1, a merged Mixture-of-Experts model built upon and inheriting licenses from tokyotech-llm/Swallow-13b-instruct-hf and nitky/Superswallow-13b-v0.2, with benchmark results included.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba is a multilingual NLI/text classification model based on XLM-RoBERTa, served via TensorFlow, and trained on GLUE, CLUE, JGLUE, KLUE, and private datasets.
  - Downloads: 18
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 is a 70B language model currently exhibiting performance issues‚Äîpotentially due to bugs related to repetition penalty and temperature‚Äîresulting in lower benchmark scores than its predecessor, Swallow.
  - Downloads: 18
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is a Japanese-English bilingual LLM built on LLaMA 2, enhanced with the LEIA training technique to improve cross-lingual transfer and performance on Japanese question-answering tasks.
  - Downloads: 18
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T multilingual corpus with support for Chinese, English, Japanese, and Korean, and includes demos, benchmarks, and technical documentation.
  - Downloads: 18
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - This repository provides the Japanese-Alpaca-2-13B language model in GGUF format, sourced from Hugging Face.
  - Downloads: 18
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B provides both a foundational 7B language model and a LoRA adaptation for Japanese language processing.
  - Downloads: 17
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - This repository provides a compiled version of the Watashiha-Llama-2-13B-Ogiri-sft model optimized for deployment on AWS inf2 instances using Neuron, requiring approximately 50GB storage and a specific Deep Learning AMI.
  - Downloads: 17
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens to prioritize strong Japanese language performance.
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, optimized for Japanese language performance with a more efficient tokenizer and extensive Japanese pre-training.
  - Downloads: 15
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for strong performance in both languages.
  - Downloads: 14
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - This repository details the karakuri-midrose-mg model.
  - Downloads: 14
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository provides a 2x7B Mixture-of-Experts (MoE) model for Japanese language processing, created by merging and instruction-tuning the ELYZA-japanese-Llama-2-7b and ELYZA-japanese-Llama-2-7b-instruct models under the Llama 2 Community License.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a Japanese-optimized tokenizer and 8B additional Japanese tokens for strong performance in both languages.
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - This repository provides full and LoRA models for Japanese-Alpaca-2-13B, an instruction-following model built upon the Japanese-LLaMA-2-13B base model.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR is a Japanese text recognition system built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga content ‚Äì including vertical text, furigana, and varied fonts ‚Äì while also functioning as a general-purpose printed Japanese OCR tool.
  - Downloads: 413,582
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - This repository provides a GGUF-formatted, Japanese-optimized version of DeepSeek-V3, meticulously sliced for efficient loading and focusing on natural language processing rather than code generation, built with data from TFMC/imatrix-dataset-for-japanese-llm and requiring llama.cpp for usage.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - Rinna‚Äôs Japanese CLIP model, `japanese-clip-vit-b-16`, enables contrastive pre-training for image-text understanding in Japanese via a pip-installable package and PyTorch.
  - Downloads: 27,634
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - This repository provides a Japanese CLIP model, trained on a billion web images and texts, for zero-shot image classification and multimodal retrieval tasks.
  - Downloads: 11,424
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet-tdt_ctc-0.6b-ja is a 0.6B parameter Japanese ASR model developed by NVIDIA NeMo, utilizing a Hybrid FastConformer TDT-CTC architecture to transcribe speech with punctuation.
  - Downloads: 8,273
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - This repository provides a 619M parameter, subword-based RNN-T speech recognition model‚Äîbuilt with a Longformer-enhanced Conformer architecture‚Äîfor long-form Japanese audio inference using the ReazonSpeech v2.0 corpus.
  - Downloads: 7,920
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is a Japanese Automatic Speech Recognition (ASR) model building on kotoba-whisper-v2.0, enhanced with integrated punctuation and postprocessing pipelines developed in collaboration with Asahi Ushio and Kotoba Technologies.
  - Downloads: 3,817
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is a Japanese ASR model building on kotoba-tech/kotoba-whisper-v1.0, enhanced with integrated postprocessing for automatic punctuation.
  - Downloads: 2,546
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the Umievo-itr012-Gleipnir-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp.
  - Downloads: 2,518
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B is a Japanese and English vision language model built on the NVILA-Lite architecture using Qwen2.5-1.5B-Instruct and a paligemma-siglip vision encoder, requiring specific dependencies like transformers==4.45.0.
  - Downloads: 2,184
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B is a Japanese and English vision-language model built on the NVILA-Lite architecture, utilizing Qwen2.5-14B-Instruct and a paligemma-siglip vision encoder, with specific compatibility noted for Transformers versions 4.45.0, 4.46.0, and 4.49.0.
  - Downloads: 1,593
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a multilingual speech foundation model offering ASR, LID, SER, and AED capabilities, with pre-trained models available on ModelScope and Hugging Face and demos for easy access.
  - Downloads: 1,365
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - This repository provides a fine-tuned OpenAI Whisper-large-v3 model on the Common Voice 16.1 dataset, achieving a 0.4057 loss and trained for 4000 steps‚Äîthough exhibiting overfitting as indicated by increased WER.
  - Downloads: 1,239
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - Karamaru is a conversational AI model by Sakana AI fine-tuned on a large Edo-period Japanese dataset‚Äîincluding both human-transcribed and AI-OCR'd historical texts‚Äîto generate responses in that style.
  - Downloads: 1,058
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - This repository provides a Japanese CLIP model for encoding text and images, enabling tasks like similarity calculation, embedding, and multimodal search, with accompanying documentation and sample code.
  - Downloads: 667
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - This repository provides a GGUF format conversion of the DataPilot-ArrowPro-7B-KUJIRA large language model, trained with TFMC/imatrix-dataset-for-japanese-llm data and usable with llama.cpp.
  - Downloads: 583
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - This repository provides a Japanese text-to-speech model, SpeechT5 fine-tuned on the JVS dataset with 100 speakers, utilizing a 16-dimensional speaker embedding for voice independence.
  - Downloads: 555
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the ELYZA-Shortcut-1.0-Qwen-7B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is designed for use with llama.cpp.
  - Downloads: 490
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing Qwen2.5-0.5B-Instruct and requiring specific transformer, accelerate, and opencv-python versions for setup.
  - Downloads: 460
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - This repository provides a GGUF-formatted version of the stockmark-100b language model, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm and runnable with llama.cpp.
  - Downloads: 442
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-performing Japanese Large Vision Language Model, built on Sarashina2-7B and Qwen2-VL-7B, achieving state-of-the-art results on multiple benchmarks.
  - Downloads: 439
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - This repository provides a Japanese Contrastive Language-Image Pretrained (CLIP) model‚Äî`japanese-clip-vit-b-32-roberta-base`‚Äîfor tasks like zero-shot image classification and text-image retrieval, licensed under CC-BY-4.0.
  - Downloads: 348
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - This repository provides a GGUF-formatted version of the DataPilot-ArrowPro-7B-RobinHood language model, trained on the TFMC/imatrix-dataset-for-japanese-llm dataset and usable with llama.cpp.
  - Downloads: 331
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a high-performing Japanese large vision language model, built on Sarashina2-13B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 316
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - This repository provides a GGUF-formatted version of cyberagent's Mistral-Nemo-Japanese-Instruct-2408 model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 312
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B is a text-to-speech model based on sarashina2.2-0.5b-instruct-v0.1, offering fine-grained voice control via prompt-based adjustments for pitch, gender, and noise, and utilizing the XCodec2 audio decoder.
  - Downloads: 300
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut is a base-sized model fine-tuned on a synthetic dataset of visual novel images, enabling text recognition and dialogue option generation as demonstrated in the provided Colab notebook.
  - Downloads: 299
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input audio.
  - Downloads: 193
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing a siglip2 vision encoder and Qwen2.5-32B-Instruct LLM.
  - Downloads: 181
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - NABLA-VL is a Japanese vision-language model by NABLAS that processes images, multiple images, and videos to understand and generate text for multimodal tasks.
  - Downloads: 174
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - This repository provides a GGUF-formatted version of the QwQ-32B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 156
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a multilingual text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio, with a demo available at Fish Audio.
  - Downloads: 151
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - This repository provides a GGUF version of the Ocuteus model, optimized for use with Koboldcpp, and recommends reduced image resolution and a context size of 16384.
  - Downloads: 130
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - This repository provides a fine-tuned Japanese Hubert-base ASR model, trained on common_voice_11_0, specifically for predicting Hiragana with reported WER results.
  - Downloads: 91
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B parameter, end-to-end Transformer model for fluent Japanese text-to-speech generation and one-shot voice cloning, building upon the metavoice framework.
  - Downloads: 87
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing Llama tokenizer.
  - Downloads: 81
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - This repository provides GGUF-formatted, K-quantized language models derived from Local-Novel-LLM, enhanced with iMatrix using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 78
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the Heron library, capable of image-based conversation and utilizing LlamaTokenizer.
  - Downloads: 74
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a Japanese vision-language model fine-tuned from llm-jp/llm-jp-1.3b-v1.0, capable of image-based conversation and trained on datasets like LLaVA-CC3M-Pretrain and Japanese Visual Genome.
  - Downloads: 74
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR is a Japanese text recognition tool utilizing a Vision Encoder Decoder framework, specifically designed for high-quality OCR of diverse and challenging manga text, including vertical writing, furigana, and varied fonts/image quality.
  - Downloads: 72
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - This repository provides optical character recognition (OCR) specifically designed for Japanese text, particularly within Japanese manga.
  - Downloads: 65
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - This repository provides a Japanese language Stable Diffusion model for generating Pokemon images from text prompts, trained with diffusers and licensed under CreativeML OpenRAIL-M.
  - Downloads: 52
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - This repository provides a Japanese CLIP model (ViT-H/14) pretrained for multimodal tasks like zero-shot image classification, mapping Japanese text and images to a shared embedding space under a CC BY-NC-SA 4.0 license.
  - Downloads: 52
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - This repository provides GGUF-formatted and K-quantized versions of the Japanese-Chat-Umievo-itr004-7b model, utilizing iMatrix with the c4_en_ja_imatrix.txt text for improved performance.
  - Downloads: 51
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - This repository provides a fine-tuned XLSR-53 model for Japanese two-speaker speech diarization, specifically trained on the CallHome phone-call dataset.
  - Downloads: 50
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input audio.
  - Downloads: 49
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - This repository provides GGUF conversions of the ArrowPro-7B-KUJIRA language model, including K-quantized versions enhanced with iMatrix for improved Japanese text generation using the c4_en_ja_imatrix dataset.
  - Downloads: 49
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool built on a Vision Encoder Decoder framework, specializing in accurately extracting text from manga, including vertical text, furigana, and varied fonts, even in low-quality images.
  - Downloads: 49
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository provides a CTranslate2-compatible version of the whisper-large-v2-mix-jp speech recognition model, enabling faster transcription with CTranslate2 and projects like faster-whisper.
  - Downloads: 46
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository provides a CTranslate2-formatted version of the whisper-large-v2-jp speech recognition model, enabling faster and more efficient transcription with CTranslate2 and projects like faster-whisper.
  - Downloads: 45
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - This repository provides fine-tuned Wav2Vec2-Large-XLSR-53 speech models for specific languages using datasets like Common Voice, requiring 16kHz sampled input.
  - Downloads: 41
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max is a 1.3B parameter Japanese vision-language model built on LLaVA architecture, utilizing a ConvNeXt Large vision encoder and trained on a custom Japanese dataset with 1280x1280 resolution and 1024 token context length, licensed under Apache 2.0.
  - Downloads: 40
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - This repository provides a fine-tuned wav2vec2-base model for Japanese ASR, specifically predicting Hiragana and achieving a WER of 1.0 on the common_voice_11_0 dataset.
  - Downloads: 39
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - This repository provides Canary-TTS-150M, a Japanese Text-to-Speech model based on llm-jp/llm-jp-3-150m-instruct3 and XCodec2, enabling fine-grained voice control via prompting, and serving as an experimental model for Canary-TTS 0.5B.
  - Downloads: 36
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-1b model for Japanese speech recognition, trained on ~60 hours of combined public datasets (Common Voice, JUST, JSSS, CSS10) and achieving benchmark WER results on Common Voice.
  - Downloads: 35
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned wav2vec2-xls-r-300m model for Japanese speech recognition, converting Kanji to Hiragana and evaluated using Character Error Rate (CER) with reported scores on Common Voice and speech-recognition-community-v2 datasets.
  - Downloads: 33
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for Japanese audio transcription directly into Hiragana, achieving a 0.2227 Character Error Rate on the Common Voice dataset.
  - Downloads: 32
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - Rinna‚Äôs Japanese data2vec Audio Base model is a 12-layer transformer trained on ~19,000 hours of Japanese audio, replicating the original data2vec architecture and training process.
  - Downloads: 31
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - This repository provides a Japanese CLIP model (ViT-H/14) pretrained for multimodal tasks like zero-shot image classification, mapping Japanese text and images into a shared embedding space under a CC BY-NC-SA 4.0 license.
  - Downloads: 28
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, enabling joint embedding of Japanese text and images under a CC BY-NC-SA 4.0 license.
  - Downloads: 28
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - This repository provides a Japanese VL-T5 model, pretrained on a Japanese corpus for unifying vision-and-language tasks via text generation, based on the VL-T5 architecture.
  - Downloads: 28
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR is a fine-tuned Whisper-large-v3 model for accurate Japanese speech recognition, including non-speech sound detection and improved punctuation, requiring specific post-processing for optimal performance.
  - Downloads: 28
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the heron library, enabling conversational image understanding and generation demonstrated via a provided demo and code.
  - Downloads: 26
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - This repository provides a fine-tuned Whisper Large V3 model for Japanese speech transcription into Katakana with pitch accent annotation, trained on Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 23
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - ReazonSpeech-ESPnet-Next provides cutting-edge Japanese Automatic Speech Recognition (ASR) models and datasets, actively incorporating community feedback for rapid research dissemination.
  - Downloads: 22
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech recognition, achieving 9.34% CER on Common Voice Japanese data with 16kHz audio input and continuous sentence output.
  - Downloads: 20
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - This repository provides a Japanese Text-to-Speech (TTS) model, Amitaro, finetuned from Plachtaa's VITS using free voice data, offering 600-epoch training and sample usage.
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO is a Japanese-specific Stable Diffusion model fine-tuned for generating photo-realistic images from text prompts, built upon ü§ó Diffusers.
  - Downloads: 19
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This repository provides a Style Bert VITS2 voice clone capable of text-to-speech generation in English, Japanese, and Chinese, featuring a young, neutral voice suitable for diverse applications like virtual content creation.
  - Downloads: 16
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - This repository provides a pre-trained ESPnet model for Japanese automatic speech recognition, trained on 15,000 hours of the ReazonSpeech corpus and requiring 16kHz audio input.
  - Downloads: 16
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - This repository provides a Japanese VITS-TTS voice model finetuned on Sakura Miko's voice data, intended for non-commercial use and adhering to Cover Corporation's secondary creation guidelines.
  - Downloads: 15
- [mayocream/manga-ocr-onnx](https://huggingface.co/mayocream/manga-ocr-onnx)
  - This repository provides an ONNX model for manga Optical Character Recognition (OCR) based on the kha-white/manga-ocr-base model, exported for efficient inference.
  - Downloads: 15
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned Whisper tiny model for real-time Japanese speech recognition, achieving a 30.16% Word Error Rate on the Common Voice dataset.
  - Downloads: 14
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA is a fine-tuned speech-to-text model for Japanese, trained on the Common Voice 11.0 dataset and achieving a 17.73% Character Error Rate.
  - Downloads: 13
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - This repository provides a free, commercially-usable voice generation model‚Äîa "cool" version of RikkaBotan‚Äîspecializing in gentle, childish voices ideal for narration, with alternative versions for emotionality, English, ASMR, and Chinese speech.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool, built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga content ‚Äì including vertical text, furigana, and varied fonts ‚Äì but usable for general printed Japanese text.
  - Downloads: 12
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned, real-time Japanese speech recognition model based on OpenAI‚Äôs Whisper-tiny, trained on the Common Voice dataset with a learning rate of 1e-05, achieving a WER of 225.233037 and a loss of 0.549100.
  - Downloads: 12
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository provides a free, commercially-usable voice model‚Äîan ASMR version of RikkaBotan‚Äîspecializing in gentle, childish voices, with variations for emotional, English, Chinese, and logical speaking styles.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
### Text Generation
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a 100GB corpus including Wikipedia, OSCAR, and CC-100, requiring fine-tuning for specific tasks and acknowledging potential biases in its output.
  - Downloads: 16,130
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and post-trained models, including one for pairwise evaluation.
  - Downloads: 8,081
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository provides a Japanese text-to-speech model based on Parler-TTS Mini, offering lightweight, high-quality voice generation with a custom, incompatible tokenizer.
  - Downloads: 3,131
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - This repository hosts the v2 update of chilled_remix and reversemix models, licensed under CreativeML Open RAIL-M with additional authorship by sazyou_roukaku, while clarifying the original author bears no responsibility for outputs beyond the license's restrictions.
  - Downloads: 2,503
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitions‚Äô sarashina2.2-0.5B-instruct-v0.1 is a Japanese autoregressive language model evaluated on Japanese and English tasks, demonstrating competitive performance against other models like Qwen and RakutenAI.
  - Downloads: 2,280
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - This repository provides a Japanese diffusion model licensed under CreativeML Open RAIL-M, prohibiting the generation of violent, sexually explicit (especially involving minors), or non-consensual content, and requiring consent for depictions of real individuals.
  - Downloads: 1,844
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 1,123
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) v1.1 model pretrained on a 100GB corpus of Japanese text‚Äîincluding Wikipedia, OSCAR, and CC-100‚Äîrequiring fine-tuning for specific tasks and acknowledging potential biases in generated output.
  - Downloads: 917
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides a GGUF-formatted version of the Qwen2.5-bakeneko-32b-instruct-v2 language model, trained with Japanese LLM data, and designed for use with llama.cpp.
  - Downloads: 841
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/gemma-3-4b-it-RP-v0.1 model, inheriting the Gemma Terms of Use and Prohibited Use Policy.
  - Downloads: 782
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - This repository details a Japanese question generation model fine-tuned on a cleaned, machine-translated SQuAD 1.1 dataset, using a Japanese T5 model to generate questions from provided answers and contexts.
  - Downloads: 697
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen3-30B-A3B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 678
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - This repository provides a fine-tuned mt5-small model for Japanese summarization, specifically trained on BBC news articles using headlines as summaries and article bodies as source text.
  - Downloads: 657
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - This repository provides a quantized, GGUF version of Qwen/Qwen2.5-3B-Instruct, optimized with a Japanese-focused importance matrix (iMatrix) to enable accurate summarization of extremely long texts exceeding 32K tokens.
  - Downloads: 655
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - This repository provides a GGUF formatted version of Microsoft's Phi-3-medium-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 645
- [mmnga/Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen3-EZO-8B-beta language model, using TFMC/imatrix-dataset-for-japanese-llm data, and is intended for use with llama.cpp.
  - Downloads: 636
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the Cogito-v1-preview-Qwen-32B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and is usable with llama.cpp.
  - Downloads: 571
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual provides faster, distilled Whisper models for Japanese & English speech recognition and translation, built upon OpenAI's large-v3 and developed by Asahi Ushio & Kotoba Technologies.
  - Downloads: 527
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a Japanese/English instruction-tuning dataset (Sarashina2.2) built from fineweb-edu and fineweb-2, aiming for higher accuracy than existing imatrix datasets, and is designed for use with tools like Ollama.
  - Downloads: 404
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) model, fine-tuned from rinna/japanese-hubert-large on reazonspeech and common_voice datasets, specifically for Hiragana prediction, inspired by vumichien‚Äôs training approach.
  - Downloads: 308
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - This repository provides a GGUF-formatted conversion of the QwQ-32B-Preview language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 244
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository provides GGUF quantized versions of a VNTL LLaMA 3 8B QLoRA merge, featuring a new chat mode optimized for Japanese grammar and including translation prompt examples.
  - Downloads: 241
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF format conversions of the RakutenAI-2.0-mini-instruct model for use with tools like llama.cpp and text-generation-webui.
  - Downloads: 202
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This repository releases a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS Mini, featuring a custom tokenizer and currently in beta.
  - Downloads: 182
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the sarashina2.2-3b-instruct-v0.1 Japanese language model, trained with imatrix data and compatible with llama.cpp.
  - Downloads: 133
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the ABEJA-Qwen2.5-32b-Japanese-v0.1 large language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 132
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - This repository provides a model for automatically generating article titles from text content, as detailed in the linked Qiita article.
  - Downloads: 117
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) model, fine-tuned on the uniTKU dataset and predicting only Hiragana, achieving low Word Error Rates (WER) as detailed in the training results.
  - Downloads: 114
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and evaluation models for improved translation performance.
  - Downloads: 75
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/c4ai-command-r-v01-japanese-instruct model for Japanese instruction-following.
  - Downloads: 72
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - Gendec is a machine learning framework for detecting gender from Japanese names (given in romaji) as presented in the ISDA'23 paper, providing male/female predictions.
  - Downloads: 66
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - This repository provides a Japanese ByT5 model‚Äîa tokenizer-free Text-to-Text Transfer Transformer‚Äîpretrained on a 100GB corpus of Wikipedia, OSCAR, and CC-100 data, requiring fine-tuning for specific tasks and acknowledging potential biases in its output.
  - Downloads: 64
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository provides GGUF quantized versions of the VNTL Gemma 2 27B model, enhanced with a Japanese grammar-focused "chat mode" and example translation prompts.
  - Downloads: 59
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides statically quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model in GGUF format, offering various quantization levels for different size/quality trade-offs.
  - Downloads: 56
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a 6 billion parameter Japanese language model finetuned from EleutherAI‚Äôs GPT-J 6B specifically for generating Japanese web novels, utilizing RoPE embeddings and a 50,400 GPT-2/3 vocabulary.
  - Downloads: 54
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - This repository provides a T5 model pretrained on a balanced 500GB English-Japanese corpus, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 53
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
  - PLaMo is a translation-focused large language model developed by Preferred Networks, offering base and post-trained models alongside a pairwise evaluation model.
  - Downloads: 50
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Kage-v0.1-2x7B is a Japanese text generation model, merged from Ninja-v1 using Mergekit-Evolve and franken MoE, optimized for Vicuna prompt formatting.
  - Downloads: 48
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - This repository provides a GGUF-formatted version of the Sarashina 2.1-1B-SFT Japanese language model, trained with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 47
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - This repository hosts an alpha version of a Japanese-language AI assistant, fine-tuned from calm2-7b-chat, designed to continue writing provided text, trained on ~150M novel tokens and usable with TextGen-WebUI.
  - Downloads: 43
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 is a Japanese T5 model finetuned on the ATOMIC dataset for text-to-text generation, offering a pipeline for predicting subsequent events.
  - Downloads: 41
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository provides a fine-tuned version of MosaicML‚Äôs MPT-7B-instruct, evaluated on a 100QA dataset achieving 46% accuracy, and requires `trust_remote_code=True` due to its custom MPT architecture with features like FlashAttention.
  - Downloads: 40
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - This repository details EZO2.5-gemma-3-12b-it, a Japanese language model trained with a novel technique (‚ÄúEZO‚Äù) mixing GRPO/PPO concepts to enhance performance on benchmarks like Japanese MT Bench and Elyza Tasks100, offering a potentially low-cost alternative to complex reinforcement learning methods.
  - Downloads: 34
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - This repository provides a model with a modified CreativeML OpenRAIL-M license permitting commercial use, including selling generated images, merges, and services, while explicitly allowing usage without creator credit.
  - Downloads: 28
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - This repository details a modified CreativeML OpenRAIL-M license allowing commercial use, image selling, and model merging/redistribution‚Äîeven with different permissions‚Äîwithout requiring creator credit.
  - Downloads: 27
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - Tokara-0.5B-v0.1 is a fine-tuned 0.5 billion parameter language model, based on Qwen1.5-0.5B and pretrained on Japanese-English data, designed for multi-turn conversations using datasets like databricks-dolly-15k-ja and jimba-instuction-1k-beta, though repetition may require penalty adjustments.
  - Downloads: 24
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - Saikyou Shield 30M is a lightweight (30M parameter) Japanese text classification model designed to classify *all* prompts as dangerous, including jailbreaks and prompt injections, for ultimate safety‚Äîreleased as an April Fool's joke.
  - Downloads: 20
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model, trained on the Guanaco dataset with 49,000 chat samples, exhibiting improved performance in Chinese and Japanese, and testable via the provided `test.py` script.
  - Downloads: 19
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on machine-translated preference datasets (Ultrafeedback & hh-rlhf-12k-ja) based on the STF and Japanese Stable LM Instruct Gamma 7B models.
  - Downloads: 19
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri is a Japanese large language model fine-tuned with LLaVA on visual genome and STAIR captions data to perform image-based witty responses, utilizing a CLIP-ViT-B-32 vision encoder under the LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 19
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on machine-translated preference data, based on the STF Japanese Stable LM Instruct Gamma 7B.
  - Downloads: 19
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 v2 is a finetuned, large-version GPT-2 model based on the ATOMIC dataset, enabling causal language modeling for reproducible text generation.
  - Downloads: 18
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - This repository provides a fully instruction-tuned 1.7B parameter Japanese language model based on line-corporation/japanese-large-lm-1.7b.
  - Downloads: 17
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository provides a 32B Japanese language model‚Äîmerged from pre-trained models using mergekit‚Äîoptimized for code generation with parameters based on FuseO1-Preview, demonstrated by a FizzBuzz example.
  - Downloads: 17
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - This repository details a fine-tuned Distil-Whisper/distil-large-v2 ASR model specifically for transcribing Japanese audio, particularly from visual novels.
  - Downloads: 16
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - This repository provides a finetuned GPT-2 XL model (COMET-GPT2) on the ATOMIC dataset for causal language modeling and text generation with reproducibility features.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 is a Japanese language model based on DeepSeek-V3, optimized for stability and performance by selectively reconstructing each layer with the 64 most frequently used Mixture of Experts (MoE) from Japanese example outputs.
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - This repository details a modified CreativeML OpenRAIL-M license permitting commercial use, image selling, and model merging/redistribution‚Äîeven with altered permissions‚Äîwithout requiring creator credit.
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - This repository details the "karakuri-midroze-CV" model, likely related to computer vision.
  - Downloads: 13
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [OmeletRice/japanese-wav2vec2-base-bf16-asmr](https://huggingface.co/OmeletRice/japanese-wav2vec2-base-bf16-asmr)
  - This repository details a ü§ó transformers model pre-trained on Japanese ASMR data using reazon-research/japanese-wav2vec2-base, with further model details pending completion.
  - Downloads: 11
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - This repository provides a Japanese-language AI model‚Äîtuned for a female/‚Äúdaughter-like‚Äù persona‚Äîwith specific settings for generation (layer adjustments, sampling parameters) and includes a sample self-introduction demonstrating its conversational style.
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Natural Language Interfaces
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [dahara1/gemma-3-1b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-1b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-focused, 12B parameter Gemma model quantized with imatrix using llama.cpp, offering an example of speculative decoding for faster inference‚Äînote the 1B model lacks vision capabilities.
  - Downloads: 1,155
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - This repository provides a question-answering model fine-tuned from luke-japanese-base-lite using the JSQuAD dataset, achieving an F1 score of 0.876 and exact match of 0.758.
  - Downloads: 551
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex dialogue system built upon the 7B Moshi model, offering pre-trained models, training code, and demos for real-time, natural turn-taking conversations, despite being a prototype with limitations in instruction-following.
  - Downloads: 521
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - This repository offers static quantized versions of the DeepSeek-R1-Distill-Qwen-7B-Japanese model in GGUF format, with potential for imatrix quants available upon request and usage guidance linked to TheBloke‚Äôs resources.
  - Downloads: 375
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository provides statically quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model in GGUF format, with potential for imatrix quants upon request, and references TheBloke‚Äôs documentation for usage guidance.
  - Downloads: 309
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - This repository provides a question-answering model fine-tuned from luke-japanese-large-lite using the DDQA dataset, achieving 86.3% exact match accuracy.
  - Downloads: 154
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the DDQA dataset, achieving an exact match accuracy of 0.845933.
  - Downloads: 104
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - This repository demonstrates solving simple arithmetic problems using GRPO, featuring a specific prompt format with `<think>` and `<answer>` blocks for thought process and final solution, and utilizes dynamically generated training data.
  - Downloads: 102
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - This repository provides GGUF quantized versions of the ArrowPro-7B-RobinHood language model, including K-quantization with iMatrix applied using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 99
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository provides a DeBERTa-v2-tiny-japanese model fine-tuned on the DDQA dataset for Japanese question-answering tasks, compatible with the SQuAD format.
  - Downloads: 62
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - This repository hosts Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, a Japanese language model extending Mixtral-8x7B to a 32K context window and enhanced instruction-following capabilities through merging with English model differences.
  - Downloads: 59
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This repository provides a DeBERTa-v2-base-Japanese model fine-tuned for Question-Answering tasks using the DDQA dataset, and compatible with SQuAD.
  - Downloads: 59
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - This repository provides a small Japanese DialoGPT model, trained on dialogue extracted from Aozora Bunko public domain books, due to GPU memory limitations.
  - Downloads: 46
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - Karasu-LoRA-JP-QA-Chat is a LoRA-fine-tuned Karasu model for Japanese question answering and RAG systems, built upon a merged Karasu base and trained with a dedicated Q&A dataset.
  - Downloads: 38
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 is a question-answering model built on japanese-stablelm-instruct-gamma-7b, designed to help users learn Japanese in English via a specific prompt format and requiring Transformers 4.34.0+.
  - Downloads: 37
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with limitations in instruction-following.
  - Downloads: 31
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - Lightblue's QLoRA finetune specializes in Japanese closed-question answering, trained on SNOW TyDiQA & XLSUM datasets, using OpenOrca's 13B model.
  - Downloads: 30
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 is a merged language model excelling in helpful, harmless role-playing‚Äîparticularly as a Japanese speaker‚Äîand multi-turn conversations with optimized output settings (temperature 0.1, top_p 1.0).
  - Downloads: 30
- [UEC-InabaLab/Llama-3.1-KokoroChat-Low](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Low)
  - Llama-3.1-KokoroChat-Low is a Japanese language model fine-tuned on a 6,000+ psychological counseling dialogue dataset, enabling empathetic and context-aware responses for mental health conversations.
  - Downloads: 27
- [UEC-InabaLab/Llama-3.1-KokoroChat-High](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-High)
  - Llama-3.1-KokoroChat-High is a Japanese language model fine-tuned on a 6,000+ dialogue dataset of psychological counseling sessions, enabling empathetic and context-aware mental health conversations.
  - Downloads: 27
- [UEC-InabaLab/Llama-3.1-KokoroChat-Full](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Full)
  - Llama-3.1-KokoroChat-Full is a Japanese language model fine-tuned on a 6,400+ dialogue dataset of psychological counseling conversations to generate empathetic, context-aware responses for mental health applications.
  - Downloads: 25
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - This repository hosts a Japanese instruction-tuned language model (c4ai-command-r-v01) further refined with *ichikara-instruction* using LoRA on 4x A6000 GPUs, and evaluated on datasets like jsquad and jcommonsenseqa.
  - Downloads: 22
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [akkikiki/j-moshi-ext-mlx](https://huggingface.co/akkikiki/j-moshi-ext-mlx)
  - This repository provides an MLX-converted version of the J-Moshi language model for macOS, adhering to the license and restrictions of the original nu-dialogue/j-moshi-ext and kyutai/moshi models.
  - Downloads: 16
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This repository provides a Japanese GPT-2 medium model finetuned on the Yuyuyui scenario corpus for generating character-specific dialogue based on provided conversational context, utilizing special tokens for character and end-of-sentence markers.
  - Downloads: 16
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - This repository details a permissively licensed (MIT) Japanese causal language model, finetuned from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, designed for casual conversation but limited by its small training dataset.
  - Downloads: 16
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - This repository hosts a Japanese instruction-tuned model, tiny_mixtral_ja, trained on instruction data and available on Hugging Face.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - This repository provides a fine-tuned Qwen2.5-7B-Instruct language model designed to generate chain-of-thought reasoning from given questions and answers, formatted with specific &lt;Query&gt;, &lt;Answer&gt;, and &lt;Thought&gt; tags.
  - Downloads: 13
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model built by fine-tuning `cl-tohoku/bert-base-japanese-v3` on the `llm-book/ner-wikipedia-dataset`, as featured in the book ‚ÄúIntroduction to Large Language Models‚Äù.
  - Downloads: 83,248
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - This repository provides a Japanese Named Entity Recognition (NER) model based on BERT, extracting eight entity types‚Äîincluding person, organization, location, and product names‚Äîfrom text.
  - Downloads: 3,663
- [Tetsuo3003/ner-medical-japanese](https://huggingface.co/Tetsuo3003/ner-medical-japanese)
  - This repository provides a fine-tuned Japanese NER model, based on XLM-Roberta, specialized for extracting entities (names, organizations, locations, etc.) from medical conversations and documents for tasks like information extraction and anonymization.
  - Downloads: 1,178
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - This repository provides a fine-tuned Japanese BERT model (tohoku-nlp/bert-base-japanese-v3) for Named Entity Recognition (NER) using a Wikipedia-derived dataset from Stockmark Inc.
  - Downloads: 923
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository requires acceptance of conditions for public access to its files and content.
  - Downloads: 584
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from luke-japanese-base using a Wikipedia dataset, achieving 77% precision, recall, and F1-score for organizational names.
  - Downloads: 461
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This repository provides a Japanese medical named entity recognition (NER) model and script (`predict.py`) for identifying disease, medication, and key attributes within medical text.
  - Downloads: 244
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - Kurumi_flux_lora_v1.0 is a non-commercial LoRA model based on flux1-dev, optimized for realistic, beautiful girl depictions, with usage subject to Black Forest Labs' terms and prohibiting violent/explicit content or retraining.
  - Downloads: 218
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This repository provides a pre-trained Japanese medical named entity recognition model, a prediction script outputting XML-formatted tags (compatible with MedTxt-CR-JA), and entity normalization methods.
  - Downloads: 210
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune‚Äîtrained on an expanded VNTL dataset‚Äîto enhance English translation of Japanese visual novels, achieving improved accuracy and stability over previous versions without chat mode.
  - Downloads: 163
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - This repository hosts weighted and static quantized (GGUF) versions of the TFMC/Japanese-Starling-ChatV-7B language model, offering various quantization levels (including IQ1_S at 1.7GB) for efficient use.
  - Downloads: 154
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - This repository details a binary classification model (ID 59362) trained with AutoNLP for Japanese sentiment analysis, achieving 95.27% accuracy, and accessible via a Hugging Face API endpoint.
  - Downloads: 153
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from cl-tohoku/bert-large-japanese-v2 using a Wikipedia dataset, achieving an overall accuracy of 0.862.
  - Downloads: 141
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune on an expanded VNTL dataset, enhancing English translation accuracy and stability for Japanese visual novels without chat mode.
  - Downloads: 121
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository provides fastText classifiers‚Äîtrained on Wikipedia and LLM-generated data‚Äîto assess the educational value of Japanese web pages, both licensed under CC BY-SA 4.0.
  - Downloads: 108
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model, fine-tuned on the `ner-wikipedia-dataset` using `cl-tohoku/bert-base-japanese-v3` with a CRF layer, as presented in the book ‚ÄúIntroduction to Large Language Models‚Äù.
  - Downloads: 82
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - This repository provides a fine-tuned luke-japanese-large model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.845.
  - Downloads: 47
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - This repository provides a Japanese text classifier, finetuned with BERT, to predict JLPT levels (N1-N5) with reported precision, recall, and F1-scores reaching up to 0.95 on training data.
  - Downloads: 36
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - This repository provides a Japanese language model‚Äîfine-tuned from `studio-ousia/luke-japanese-large-lite`‚Äîthat scores short text for sexual content on a 0-1 scale, aiding in content moderation.
  - Downloads: 34
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - `ja_core_news_lg` is a CPU-optimized spaCy 3.7+ pipeline for Japanese natural language processing, including tokenization, morphology, parsing, sentence segmentation, named entity recognition, and attribute rules, trained on UD Japanese GSD v2.8 data with 300-dimensional vectors.
  - Downloads: 28
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - `ja_core_news_md` is a CPU-optimized spaCy 3.7+ Japanese NLP pipeline featuring tok2vec, morphological analysis, parsing, NER, and trained on UD Japanese GSD v2.8 with 300-dimensional vectors.
  - Downloads: 27
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This repository provides a named entity recognition model, with accompanying scripts and data, specifically for Japanese medical documents, identifying entities like diseases, treatments, and timelines.
  - Downloads: 27
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - This repository provides a Fully Convolutional Neural Network (FCN) model trained for classifying images of 49 Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 26
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - This repository provides a model for automatically generating article titles from text content, as detailed in the linked Qiita article.
  - Downloads: 26
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model, trained on the full 49000 chat and 280000 non-chat Guanaco dataset, with improved Chinese and Japanese performance and a provided test script.
  - Downloads: 24
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for Japanese Named Entity Recognition (NER) using the Wikipedia-based NER dataset from StockMark.
  - Downloads: 21
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - WRIME is a dataset of 20k weakly-related image-text pairs designed for evaluating multimodal retrieval and text-to-image generation models under challenging scenarios.
  - Downloads: 21
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from deberta-v2-large-japanese using a Wikipedia-based dataset.
  - Downloads: 17
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This repository provides a baseline transformer model trained and evaluated on the awesome-japanese-nlp-classification-dataset, achieving 97% accuracy with precision, recall, and F1-scores detailed in the description.
  - Downloads: 17
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - This repository details a binary classification model (ID 59363) trained with AutoNLP for Japanese sentiment analysis, achieving 97% accuracy, precision, recall, and F1 score, and accessible via a Hugging Face API endpoint.
  - Downloads: 14
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This repository provides a language model trained on 2022 Japanese parliamentary proceedings, built as a multi-GPU/node training example for the #ABCILLM hackathon, and includes question-answer examples from Diet records.
  - Downloads: 13
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This So-vits-svc 4.0 model creates natural-sounding, approachable female vocals‚Äîsynthesized from the author's own voice and expanded with ElevenLabs‚Äîand includes necessary checkpoints & notebooks for inference/training, acknowledging potential pronunciation quirks and similarity to real voices.
  - Downloads: 13
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - `ja_core_news_trf` is a spaCy 3.7.x Japanese NLP pipeline featuring a `cl-tohoku/bert-base-japanese-char-v2` transformer model for tokenization, morphology, parsing, and named entity recognition.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Aratako/Japanese-Novel-Reward-TinySwallow-1.5B is a finetuned reward model based on SakanaAI/TinySwallow-1.5B, designed to predict user evaluation scores for Japanese novel text, primarily for reinforcement learning of text generation models.
  - Downloads: 11
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - This repository provides a 4-bit quantized Llama-2-70b-chat model fine-tuned on the izumi-lab/llm-japanese-dataset for improved Japanese language performance.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - This repository requires acceptance of a License Agreement and acknowledges a Privacy Policy from Stability AI before access.
  - Downloads: 318
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository provides a private demonstration project.
  - Downloads: 264
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 248
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - This repository provides a Japanese language ELECTRA Small model finetuned for cyberbullying detection using data from online forums and Twitter.
  - Downloads: 114
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model with a CreativeML OpenRAIL-M license granting usage rights while prohibiting illegal/harmful outputs and establishing user accountability for generated content.
  - Downloads: 87
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 71
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - This repository details a fine-tuned Twitter/twhin-bert-large model for offensive language detection in social media comments, achieving a macro-averaged F1-score of 64.8% on a manually labeled dataset.
  - Downloads: 45
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI‚Äôs Privacy Policy before access.
  - Downloads: 43
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - This commercially usable Stable Diffusion model, licensed under CreativeML Open RAIL++-M, generates images with hires support but prohibits depictions of violence, explicit or exploitative content, and unauthorized likenesses of real people‚Äîplease tag creations with #tsubaki_mix.
  - Downloads: 39
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA Base model finetuned for cyberbullying detection using a combined dataset of BBS and Twitter comments, licensed under CC BY-SA 4.0.
  - Downloads: 37
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 28
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged model‚Äîsimilar to HimawariMix‚Äîfocused on strong backgrounds and detail, tuned with ideas from "riga," and includes a standard VAE, but prohibits commercial use, resale, illegal output, or altered permissions when sharing.
  - Downloads: 22
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - This repository provides a Japanese ELECTRA-Small model finetuned for cyberbullying detection using a combined dataset of online comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 14
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - This repository provides instruction-tuned language models for Japanese (llm-jp).
  - Downloads: 14
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - This repository provides a fine-tuned language model (based on studio-ousia/luke-japanese-large-lite) for Japanese social media comment offensiveness detection, achieving a macro-averaged F1-score of 64.0% and 65.0% accuracy on a manually labeled dataset.
  - Downloads: 13
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese BERT model (twhin-bert-base) for classifying online comment offensiveness, achieving a macro-averaged F1-score of 64.7% and accuracy of 65.6% on a manually labeled dataset.
  - Downloads: 13
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Reasoning
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-QwQ32b-Reasoning-Japanese-v1.0 large language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 1,112
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - This repository provides a Japanese-tuned version of the DeepSeek-R1-D model, addressing inconsistencies in Japanese output commonly found in the original bilingual (English/Chinese) DeepSeek models.
  - Downloads: 1,067
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 is a Japanese reasoning model built upon ABEJA-Qwen2.5-32b-Japanese-v0.1 by merging Qwen/QwQ-32B chat vectors and further training, utilizing `<think>` tags to guide reasoning.
  - Downloads: 779
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B is a Japanese reasoning model built upon Qwen/Qwen2.5-32B-Instruct and enhanced with imitation learning using Monte Carlo Tree Search-generated, long Chain of Thought data.
  - Downloads: 564
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - This repository provides a GGUF-formatted version of Microsoft‚Äôs Phi-4-reasoning-plus model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and intended for use with llama.cpp.
  - Downloads: 529
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - This repository details Polyglot-math-4x7b-24b, a multilingual Mixture of Experts model merging Chinese, Japanese, and English capabilities, fine-tuned on GSM8k with a 20GB VRAM footprint, and provides inference code.
  - Downloads: 285
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is an anime-style diffusion model built by merging and fine-tuning Stable Diffusion and Wifu Diffusion variants, incorporating LoRA aesthetics from Niji Journey and openly documenting its creation process for transparency, specializing in solo female characters with distinct outlines.
  - Downloads: 207
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - This repository provides a GGUF-formatted version of YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1, a Japanese language model for mathematical tasks, using data from TFMC/imatrix-dataset-for-japanese-llm and runnable with llama.cpp.
  - Downloads: 195
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference (NLI) model, trained on JGLUE-JNLI/JSICK using SentenceTransformers, to classify sentence pair relationships as contradiction, entailment, or neutral.
  - Downloads: 122
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository provides a fine-tuned version of luke-japanese-large for the JCommonsenseQA task, achieving high accuracy (83.82%) on commonsense question answering using the JGLUE dataset.
  - Downloads: 99
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - This repository provides a luke-japanese-base model fine-tuned on the JGLUE JNLI dataset for natural language inference, achieving 89.77% accuracy in determining textual relationships (entailment, neutral, contradiction).
  - Downloads: 20
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-Japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 20
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-base-japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 17
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - This model merges reasoning abilities from DeepSeek-R1-Distill-Llama-8B into the Japanese Llama-3.1-Swallow-8B-v0.2, enhancing its reasoning capabilities.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman for morphological analysis.
  - Downloads: 16
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a Japanese commonsense knowledge model finetuned from COMET on the Japanese TimeATOMIC dataset using causal language modeling, detailed in a LREC-COLING2024 paper and utilizing Juman++ and SentencePiece for text processing.
  - Downloads: 13
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - This repository provides a highly accurate (80.07%) Japanese language model, fine-tuned from luke-japanese-base using the JGLUE JCommonsenseQA dataset for multiple-choice question answering.
  - Downloads: 11
### Sentiment Analysis
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model trained on the chABSA dataset, achieving 1.0 accuracy and F1 score.
  - Downloads: 10,993
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - This repository provides a Japanese BERT Base model finetuned for both sentiment analysis and automatic irony/sarcasm detection, licensed under CC BY-SA 4.0.
  - Downloads: 2,064
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model finetuned from scratch using the Japanese Sentiment Polarity Dictionary dataset and based on jarvisx17/japanese-sentiment-analysis.
  - Downloads: 1,728
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This repository provides a Japanese emotion analysis model, fine-tuned from Luke-japanese-large-lite on the wrime dataset, to detect eight emotions‚Äîjoy, sadness, anticipation, surprise, anger, fear, disgust, and trust‚Äîwithin text.
  - Downloads: 1,498
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This repository provides a Japanese BERT-based model fine-tuned for emotion detection and classification across 10 emotional categories using a 1,000-sentence blog post dataset.
  - Downloads: 1,419
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - This repository provides a Japanese BERT model finetuned for Twitter sentiment analysis using the JTS1k dataset, classifying tweets as negative, neutral, or positive.
  - Downloads: 1,390
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - This repository provides a GGUF-formatted version of the Umievo-itr001-7b Japanese chat model, built using the TFMC/imatrix-dataset-for-japanese-llm, and runnable with llama.cpp.
  - Downloads: 269
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - This repository provides a Japanese BERT Base model finetuned for cyberbullying detection using a combined and balanced dataset, licensed under CC BY-SA 4.0.
  - Downloads: 158
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - This repository provides a Japanese ELECTRA-based model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 53
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - This repository provides a Japanese BERT-based model, trained on translated Financial PhraseBank data, for classifying financial news sentiment as positive, negative, or neutral.
  - Downloads: 45
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - This repository provides a fine-tuned language model (calm-2-7b-chat) using the Tsukuyomi corpus for conversational AI, released under specific usage licenses detailed in the description.
  - Downloads: 42
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - This repository provides a sentiment analysis model trained on Japanese stock comments to classify opinions as bullish or bearish, aiding investors and analysts.
  - Downloads: 40
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese is a finetuned language model for detecting irony and sarcasm in Japanese tweets, based on ELECTRA and licensed under CC BY-SA 4.0.
  - Downloads: 29
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository provides an Electra-based Japanese language model finetuned for irony detection using ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 17
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - This repository provides a Japanese BERT model pretrained on a Twitter corpus, optimized for Japanese social media tasks like sentiment and defamation detection, and used as a base for further finetuned models.
  - Downloads: 13
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - This repository provides a free, commercially usable voice generation model specializing in a soft, childish tone, with variations for different speaking styles (cool, English, ASMR, Chinese) and emotional text reading.
  - Downloads: 13
### Responsible NLP
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a Japanese speech recognition model fine-tuned on a large anime voice acting dataset, excelling in that domain but also offering unique performance on other audio, and requiring use *without* initial prompts to avoid hallucinations.
  - Downloads: 2,912
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - This repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime models (with specified ratios) for use in Colab WebUI, noting potential over-saturation when merged with realistic models and compatibility issues with SD 2.1 768‚Äîit includes download commands and rewrite instructions.
  - Downloads: 475
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - This Japanese Stable Diffusion model prioritizes low-ratio, young female characters with emphasized eye highlights, requiring careful age adjustment and likely incompatible with most LoRAs, and benefits from short prompts and DPM++ 2M Karras sampling.
  - Downloads: 463
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - Suzume-mix v1.0 is a non-commercial, merged FP8 model based on flux1-dev, designed to soften facial features and intended for personal use with attribution when sharing generated images‚Äîprohibiting retraining.
  - Downloads: 149
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - YaguruMagiku 0.6, a merged Stable Diffusion model based on AbyssOrangeMix2, aims for realistic black-haired ponytail looks, potentially containing NAI leak elements, and benefits from multi-image generation and a custom VAE for improved color.
  - Downloads: 54
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 merges DreamShaper 3.3 with Waifu/Stable Diffusion VAEs to improve color vibrancy and generate highly realistic, beautiful images‚Äîpotentially including idealized features‚Äîwhile addressing instability issues present in the original models, and is runnable via Colab.
  - Downloads: 27
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - This repository hosts Llama-3-Umievo-Shizuko-sqlcoder-2x8B, a Mixture of Experts (MoE) language model created with MergeKit, combining Japanese language skills with SQL generation capabilities through fine-tuning on SQL datasets and utilizing quantized gguf versions.
  - Downloads: 24
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - This repository provides GGUF quantized weights for Sarashina2.2-3B-Instruct, a Japanese language model, and utilizes the imatrix dataset for training.
  - Downloads: 23
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - This repository details a merged Stable Diffusion model (MoeDiffusion & HassanBlend & VMix03) optimized for generating black-haired ponytail hairstyles, potentially exhibiting some instruction-following issues and preferring SFW content, and is designed for use with web UIs like Colab.
  - Downloads: 16
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performing, CPU-runnable Japanese text embedding model optimized for semantic similarity and retrieval tasks like passage searching, achieving state-of-the-art results on benchmarks such as MIRACL.
  - Downloads: 15,538
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - This repository distributes LoRA models created by Hotaru Jujo, released under dual MIT/CreativeML Open RAIL-M licenses with no usage restrictions, but social media sharing is appreciated.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT v1 is a Japanese document retrieval model based on ColBERT, achieving near state-of-the-art performance despite evaluation on out-of-domain datasets.
  - Downloads: 612
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - This repository provides a series of small and fast Japanese reranker models (v2) varying in layer count, hidden size, and performance scores, with speed measurements on GPUs.
  - Downloads: 429
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - This repository provides a series of small and fast Japanese reranker models (v2) with varying layer counts and hidden sizes, benchmarked for accuracy and GPU speed.
  - Downloads: 105
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - This repository provides a passage encoder for the BPR document retrieval model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 87
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - This repository provides a Japanese NLP model‚Äîfinetuned from `tohoku-nlp/bert-base-japanese-v2`‚Äîfor extracting keywords (AREA, TYPE, SZN, INGR) from cooking-related questions to facilitate recipe search.
  - Downloads: 51
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 is a 7 billion parameter Japanese language model fine-tuned for instruction following, achieving a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 22
## üß† Datasets

This list is sorted by downloads as of June 17, 2025.
554 datasets are listed.

### Information Extraction & Text Mining
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset provides news articles from livedoor News, used in the book *Introduction to Large Language Models*, under a Creative Commons Attribution-NoDerivs 2.1 Japan license for Named Entity Recognition (NER) tasks.
  - Downloads: 7,939
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - FineWeb2 Edu Japanese is a high-quality, 89.3B token Japanese dataset of 120 million educational texts, with provided subsets for sampling and shorter-length content.
  - Downloads: 2,440
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - This repository provides a cleaned Japanese news corpus of 612M tokens extracted from Common Crawl (July-October 2024) using Uzushio, filtered with pipeline_03a.conf, and tokenized for llm-jp/llm-jp-13b-v1.0.
  - Downloads: 2,061
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a multilingual sentence dataset enabling translation tasks via language pair specification, with customizable versions accessible through the `load_dataset` function.
  - Downloads: 1,885
- [SakanaAI/EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench)
  - EDINET-Bench is a Japanese financial benchmark dataset, built from FSA's EDINET documents, for evaluating LLM performance on tasks like fraud detection and earnings forecasting.
  - Downloads: 1,393
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - WRIME is a dataset for emotional intensity estimation, featuring both self-reported and reader-annotated intensity levels of social media posts from 50 crowd-sourced participants.
  - Downloads: 1,319
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a processed, machine learning-ready dataset of public-domain Japanese literature from Aozora Bunko, built using the globis-org/aozorabunko-extractor code.
  - Downloads: 628
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA is a Hugging Face mirror of the AWS Open Data Registry‚Äôs Japanese image captioning dataset, detailed in the linked tech blog.
  - Downloads: 317
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - JSICK is a Japanese natural language inference and semantic textual similarity dataset created by translating the English SICK dataset, designed for researching multilingual compositional inference.
  - Downloads: 299
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition dataset, version 2.0, created by Stockmark Inc. and used in the book *Introduction to Large Language Models*, licensed under CC-BY-SA 3.0.
  - Downloads: 271
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - This dataset provides labeled GitHub repository descriptions to train a binary classifier distinguishing those relevant (1) or not relevant (0) to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing.
  - Downloads: 253
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository provides a Japanese-language subset of the CC100 dataset, formatted as sharded Parquet files.
  - Downloads: 252
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - This dataset contains Japanese news articles from September and October 2024, cleaned from CC-news-2024-July-October-cleaned, and optimized to ~1000 tokens for efficient training with the llm-jp/llm-jp-3-13b tokenizer, assuming an output token limit of 1024.
  - Downloads: 236
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This repository provides a filtered Japanese summarization dataset (XL-Sum) processed with PaLM 2 filters to reduce 15-gram overlap, containing 4215 training, 758 validation, and 766 test examples.
  - Downloads: 223
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - This dataset provides Japanese named entity recognition (NER) training data extracted from Wikipedia, licensed under CC-BY-SA 3.0 and developed by Stockmark Inc.
  - Downloads: 205
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - This dataset contains Japanese three-line summaries and indexing information for thousands of mycological taxonomy papers sourced from Atsushi Nakajima's Daikinrin website.
  - Downloads: 191
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - This dataset provides Japanese Wikipedia data from January 1, 2023, formatted as a Parquet file generated using the `datasets` library.
  - Downloads: 177
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - This Japanese dataset compiles manually extracted diagnostic characters‚Äîshared or differing traits‚Äî<font of thousands<binary data summary summary contains manually extracted diagnostic characters‚Äî<binary data and summary description provides summaries and: summary: This Japanese dataset compiles manually extracted diagnostic character summary: This manually extracted: This dataset compiles manually extracted summary dat dataset: This dataset summary repository contains manually extracted summary: This repository contains summaries.summary. This dataset contains manually extracted compilation: This compilation: This: This repository contains compilation and assembled from: This compilation: This repository as of to: This the to: This repository contains This repository contains the: This to: This repository contains: This the: This is a dataset of summary This compiles the: Assembler of: This is This: Assembler This Assembler: AssemblerThis repository This Assembled This Assembled This: This repository Assembled This repository compiles this As. As of to This: This Assembler This. As of to assembly this: This of This Assembler assembler This as of This Assembl be: This This assembler This assembler This This: This This areThis assembly This is As. assembler ThisAs of This assembler of As assembler: This of assembler Assembler compilation: Assembler: of The of The<: The: TheThe: areThe the: TheOf the: The: the: :The: The: are :This: The of. compilation: compilation: the: The: The:This theThis Assembler :This: Of the: Of:< of:Of: Of: ofOf: of the Of of: Of the of: Of: Of :Of Of :Of :Of Compilation: Compilation: Of :The: Compilation: of the :Of the the: Of : The the:The compilation:The the Compilation:< Compilation:Compilation Compilation: The:The Compilation: The: Of The:The Compilations: the Compilation:This. Of :This Compilation:The:Of :The theThe:Of. Of : This The:The :The :The:The :The :< :The:The:The:The:The:The :The:The:The:The:The:The:The:The:The:The:The :The:The:TheThe:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:The:The:The:The:The:The:The:The:Com compilation:TheThe:TheThe:TheThe:The:The:TheThe: The:TheThe:The:The:The:The:The:The:The: The:TheThe:The: The:The:The:The:The:The:The:The>:The:The:The:The:The:The:The The:The:TheThe:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:There:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:The:The:The:The:The:The:The:,the the the the the theThe:The: theThe:The:TheThe:The:TheThe:The:This is:The theThe:The:The:The: TheThe:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:The:The:The:The:The:TheThe:The:The:The:The:The:TheTheThe:TheThe:TheTheTheTheTheTheTheThe:TheTheThe:TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe
  - Downloads: 160
- [OmniAICreator/Japanese-Wikipedia-202506](https://huggingface.co/datasets/OmniAICreator/Japanese-Wikipedia-202506)
  - This dataset provides a snapshot of Japanese Wikipedia content as of June 1, 2025.
  - Downloads: 157
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - This Japanese dataset provides multi-label annotations of NLP research fields for GitHub repositories, utilizing repository content like descriptions, READMEs, and images for training data.
  - Downloads: 149
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - This repository provides a dataset card for ‚Äújapanese_alpaca_data,‚Äù built upon the japanese-alpaca-lora project, with a note that further information is pending.
  - Downloads: 144
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - This dataset contains cleaned Japanese news articles from September and October 2024, preprocessed with dates prepended for continued pre-training with a 1024 token output limit and adjusted to approximately 1000 tokens using the llm-jp/llm-jp-3-13b tokenizer.
  - Downloads: 137
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - This repository provides a comprehensive JSON anime dataset with metadata and links to popular anime platforms like MAL, AniList, and Kitsu, based on the Manami Project's offline database.
  - Downloads: 132
- [clnine/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-nikkei225)
  - This dataset provides a working sample of records extracted from the Japanese Wikipedia (January 1, 2023) specifically focusing on articles categorized under "Nikkei 225" (Êó•ÁµåÂπ≥ÂùáÊ†™‰æ°).
  - Downloads: 128
- [clnine/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-financial-terms)
  - This dataset provides a working sample of Parquet files extracted from the January 2023 Japanese Wikipedia dataset, specifically articles categorized under "Investment".
  - Downloads: 128
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This dataset provides Japanese Wikinews articles with named entity recognition (NER) labels‚Äîincluding person, organization, location, and product‚Äîfor eight entity types, serving as a test set for large language model experimentation, licensed under CC BY 2.5.
  - Downloads: 128
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository provides a Japanese-only subset of the wiki40b dataset, consisting of three parquet files generated using a provided Python script.
  - Downloads: 121
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This dataset provides 8.75K records of comprehensive Japanese law details‚Äîincluding number, title, ID, effective date, and full text‚Äîdeduplicated to the latest versions as of August 1, 2023.
  - Downloads: 119
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - This dataset provides images for evaluating Japanese image classification models across four tasks, including 101 types of Japanese food, released by Recruit Co., Ltd. under a CC-BY-4.0 license.
  - Downloads: 111
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench is a 102-question benchmark dataset of 21 Japanese images‚Äîcategorized by conversation, detail, and complexity, and tagged with seven subcategories‚Äîdesigned for evaluating Vision-Language Models (VLMs).
  - Downloads: 102
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - This dataset, zenz-v2.5, comprises 190M pairs of left context, input (kana), and output (kanji) for training conditional language models for kana-to-kanji conversion, with accompanying trained models (small, medium, large) and an evaluation benchmark (AJIMEE-Bench).
  - Downloads: 95
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a list of 100 common Japanese stopwords extracted from CC-100 and Wikipedia, designed for use with the nagisa text analysis library and accessible via a provided Python script requiring the Huggingface `datasets` library.
  - Downloads: 89
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
  - This repository provides counts of nouns (204,661) and verbs (16,454) extracted from a 400,000-article Japanese Wikipedia dataset (izumi-lab/wikipedia-ja-20230720), totaling 221,115 entries.
  - Downloads: 89
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR is a 209K-instance dataset of narrative text extracted from manga, combining annotations from Manga109 and a dedicated onomatopoeia dataset to support manga text recognition research.
  - Downloads: 88
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
  - This dataset provides high-quality Japanese-English text, sourced from Blue Sky Literature, Japanese/English Wikipedia, and Project Gutenberg, for knowledge distillation with the llama-imatrix model.
  - Downloads: 86
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - This dataset provides a filtered 10 billion Japanese token corpus from CommonCrawl, processed to remove potentially sensitive personal information (PPI) using rule-based and machine learning techniques, licensed under CC terms.
  - Downloads: 82
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - This repository provides a dataset of 53,640 annotated Japanese tweets (Jan-Jun 2020) for COVID-19-related text classification, requiring users to retrieve original tweets via the Twitter API.
  - Downloads: 80
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - This repository provides scripts for downloading, parsing, and preprocessing the publicly available en-ja-alignÊó•Ëã±ÂØæË®≥Êñá dataset (Uchiyama et al., 2003) without redistributing the data itself, utilizing libraries like `datasets`, `bs4`, and `lxml`.
  - Downloads: 79
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - This dataset is provided under a license agreement that users must acknowledge and carefully review before use.
  - Downloads: 73
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - This dataset comprises 5,000 annotated Japanese tweets (Feb-Jun 2022) for detecting online defamation, labeling both the target of abuse and the type of abusive content.
  - Downloads: 64
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - This repository provides a Parquet-formatted dataset of anime song lyrics for research and enthusiast use, with accompanying code available on a related GitHub account.
  - Downloads: 61
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - This repository provides a collection of anime quotes with character attribution, sourced from Anime Motivation, formatted as a list of dictionaries for analysis and use.
  - Downloads: 60
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - This repository provides a cleaned, UTF-8 encoded Japanese subtitle dataset from OpenSubtitles‚Äîcontaining over 7000 titles with text, timing, and metadata‚Äîformatted as Parquet files, including a version adhering to Open Assistant guidelines.
  - Downloads: 59
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-following dataset containing 6,259 manually annotated input-output pairs for use in tasks like instruction tuning and language modeling.
  - Downloads: 54
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - J-NER is a 1,570-example Japanese Named Entity Recognition (NER) dataset comprising 157 entity types‚Äîsourced from Wikipedia and designed for LLM training‚Äîwith five positive and negative examples per entity.
  - Downloads: 50
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - This dataset provides 55.4 billion tokens of Japanese web novels ‚Äì totaling over 80 billion characters across 23 million records ‚Äì for machine learning research purposes, requiring a detailed use-case explanation for access.
  - Downloads: 49
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains extracted sections from 2024 Japanese Securities Reports (Êúâ‰æ°Ë®ºÂà∏Â†±ÂëäÊõ∏) published on EDINET, providing company information like names, codes, financial periods, and JCN (corporate number) with URLs linking back to the source documents.
  - Downloads: 47
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository provides a voice label dataset for Nene Kusakabe (CV: Machico) from *Project Sekai Colorful Stage! feat. Hatsune Miku*, with plans to expand and standardize the dataset, and a linked QQ group for a full character dataset.
  - Downloads: 45
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides a JSONL dataset of validated linguistic minimal pairs for Japanese, used for benchmarking natural language processing models, as detailed in Someya and Oseki (2023).
  - Downloads: 43
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - This repository provides a HuggingFace-compatible version of the Kyoto University Japanese Wikipedia Input Error Dataset (v2) licensed under CC-BY-SA 3.0, originally published by the Language Media Research Lab.
  - Downloads: 41
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - This dataset provides RAW text from fineweb-2-edu-japanese, Unicode-normalized and cleaned with noise inference (threshold 0.7, length ‚â•4) using fineweb-2-japanese-text-cleaner, with noise spans identified in the `noise_spans` column, and licensed under ODC-By.
  - Downloads: 39
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This AutoTrain dataset, created for the tam_jp project, provides Japanese (ja) language data instances with "context" fields for machine learning tasks.
  - Downloads: 39
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - This repository provides a Japanese RLHF dataset reformatted as a classification task (chosen/rejected labels) for reward model training, utilizing text generated by Phi-3-medium with moderate quality.
  - Downloads: 38
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - JParaCrawl is a large, publicly available English-Japanese parallel corpus created by NTT through web crawling and automatic sentence alignment, accessible via the `datasets` library.
  - Downloads: 37
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - This repository provides a working sample of Parquet files extracted from the Japanese Wikipedia dataset (range3/wikipedia-ja-20230101), specifically focusing on articles categorized under "Category:ÊäïË≥á" (Investment).
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is a JSONL dataset of English and Japanese articles from en.touhouwiki.net, designed for unsupervised text generation and potentially other text-based tasks.
  - Downloads: 36
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - Giellm is a Japanese general information extraction dataset built from the Livedoor News corpus and used for training a large language model leveraging mutual reinforcement learning, as detailed in the linked arXiv paper.
  - Downloads: 36
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Jibiki.fr provides a collaborative, large-coverage French-Japanese dictionary and aligned bilingual corpus built from multiple sources, currently containing over 154,000 Japanese-French entries.
  - Downloads: 36
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - This repository provides a work-in-progress Japanese-English dataset sourced from Ruby/Rails documentation, including synthetically generated data, for training and evaluating machine translation or code translation models.
  - Downloads: 35
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - This repository provides the English/Japanese dataset used to train the shisa-7b-v1 language model, with further details available in that model‚Äôs documentation.
  - Downloads: 34
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - This repository provides a clustered dataset, derived from customs advance ruling data, for training and evaluating embedding models to predict HS code sections (‚ÄúÈÉ®‚Äù) based on combined ‚Äúgeneral item name‚Äù and ‚Äúcargo summary‚Äù text, split into train/test sets with balanced label proportions.
  - Downloads: 34
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - This project automatically generates question-and-answer pairs from Japanese Wikipedia articles using Mixtral 8x22b (GGUF 5bit) and the TSUBAME4.0 supercomputer, acknowledging potential inaccuracies requiring filtering.
  - Downloads: 33
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - This repository provides the dataset and code for the SLG framework, a sentence-to-label generation approach for multi-task Japanese sentence classification and named entity recognition, detailed in the linked research paper.
  - Downloads: 32
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - This dataset provides English-Japanese paragraph pairs with labels indicating whether they describe the same or different entities, extending the original PubChem & Wikipedia classification task to a multilingual setting.
  - Downloads: 31
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This synthetic question-answering dataset, built from the sakura_japanese_dataset, was generated using Nurture-intelligence/Gemma-2-108B-DPO-v0.1 with Pro-type inference and is subject to both the original dataset's and Gemma Terms of Use licenses.
  - Downloads: 30
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - This dataset provides furigana annotations derived from bibliographic data of the National Diet Library, available as a downloadable ZIP file.
  - Downloads: 28
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences is a Japanese Wikipedia sentence dataset providing article and section titles alongside cleaned text, generated from Wikipedia dumps under CC BY-SA 4.0 and GFDL licenses.
  - Downloads: 26
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - This dataset provides a work sample of records extracted from the January 2023 Japanese Wikipedia dataset, specifically articles categorized under ‚ÄúNikkei 225‚Äù (Êó•ÁµåÂπ≥ÂùáÊ†™‰æ°).
  - Downloads: 26
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M is a corpus of 3.5 million unique Japanese light novel character names from syosetu.com, designed for culturally aware NLP applications like NER and name generation.
  - Downloads: 26
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - This dataset contains 221 Japanese haiku poems ‚Äì many with author & judge comments, translations, and image URLs ‚Äì sourced from the Itoen Shinhaiku Grand Prize competition.
  - Downloads: 26
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a Japanese language benchmark dataset designed to evaluate long-context LLM performance on extractive QA and abstractive summarization tasks using data from websites and generated by GPT-4/Claude-3.5-Sonnet.
  - Downloads: 24
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - This repository provides a Japanese text generation dataset, Cosmopedia-Japanese-20k expanded to 100k with contributions from kunishou, including translated prompts, available on Hugging Face Datasets.
  - Downloads: 24
- [OmniAICreator/Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M)
  - This repository provides a 55.4 billion token Japanese web novel dataset of 23 million records‚Äî80 billion characters total‚Äîintended solely for machine learning research with restricted access requiring a detailed use case explanation.
  - Downloads: 23
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This dataset contains extracted chapters from Japanese securities reports (2014-2022) filed via EDINET, including company information, document details, and reporting periods.
  - Downloads: 22
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - This repository provides a manually translated, passage- and sentence-level English-Japanese dataset of Wikipedia introductions, created with a permissive license for machine learning use, primarily avoiding machine translation tools in favor of LLM-assisted (CALM3-22B-Chat & Qwen 2.5 32B) translation.
  - Downloads: 22
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository provides a Japanese subset of the NTX dataset converted to the Aya instruction format, licensed under CC-BY-SA 4.0, and linked to the full instruction dataset and associated research paper.
  - Downloads: 21
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - This dataset comprises approximately 7,000 Japanese medical journal entries (2003-2023) linking diseases, symptoms/findings, supporting text, and article metadata in JSON-Lines format.
  - Downloads: 20
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - This repository provides a JSONL conversion of the Dolly-15k-ja dataset, formatted for use with the SFTTrainer, licensed under CC BY SA 3.0.
  - Downloads: 18
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - This dataset contains crawled data from the ‚ÄúKawaryu Toshu Marusen‚Äù Japanese haiku site, comprising 5346 submissions for 376 prompts, formatted for text-to-text tasks and including prompt IDs and content.
  - Downloads: 16
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - This repository provides a Japanese cooking recipe search dataset with question-answer pairs annotated for location, type, season, and ingredients, alongside code for fine-tuning language models and building a related application.
  - Downloads: 15
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - This repository provides a clustered dataset‚Äîsourced from Japan's PMDA website and split into train/test sets‚Äîfor learning and evaluating embedding models, featuring text data (‚Äúgeneric name‚Äù + ‚Äúgeneric name definition‚Äù) labeled with ‚Äúclassification codes.‚Äù
  - Downloads: 13
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully is a commercially-usable dataset of potentially harmful prompts in Japanese and other languages, intended solely for improving LLM safety and prohibiting use for bypassing safety measures, with specific redistribution and derivative work guidelines.
  - Downloads: 12
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - This repository provides a Japanese text dataset ("Washi") created via DSIR sampling from CulturaX, focusing on documents similar to XLSum and Aozora Bunko for improved language model performance.
  - Downloads: 11
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset provides 600 Japanese sentences from Wikipedia articles about racehorses, annotated with nine named entity types ‚Äì including a dedicated ‚Äúracehorse name‚Äù tag ‚Äì for named entity recognition research, acknowledging potential data imperfections due to Wikipedia/DBpedia link rot and lacking negative samples.
  - Downloads: 11
### Multimodality
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a 5+ million image anime illustration dataset with detailed, community-sourced tags (averaging 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 5,576
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 is a 292,637-clip audio-text dataset from visual novels intended to improve automatic speech recognition accuracy, and is distinct from its V1 predecessor.
  - Downloads: 1,541
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - This repository mirrors the Reazon-Speech2 dataset, denoised and with background music removed using UVR, cleaned with 8 A800 GPUs over 10 days.
  - Downloads: 1,493
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - MOMIJI is a large 56M-document Japanese dataset of image-text web data‚Äîcontaining 110B characters and 249M images‚Äîcollected from Common Crawl for training vision-language models.
  - Downloads: 1,315
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese-anime-speech is an audio-text dataset of transcribed dialogue from visual novels created to improve automatic speech recognition accuracy for Japanese anime and similar media.
  - Downloads: 740
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark designed to rigorously evaluate Large Multimodal Model (LMM) performance, addressing cultural dependencies found in existing benchmarks through expert-crafted, culture-agnostic questions.
  - Downloads: 683
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - ReazonSpeech is a large, free 35,000+ hour Japanese speech dataset in FLAC format intended for Automatic Speech Recognition (ASR) research, subject to Japanese copyright law Article 30-4.
  - Downloads: 673
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - This repository provides a CC-0 licensed dataset of AI-generated anime images with Japanese captions (including English translations) for ethical AI training, comprising images created with Emi 2 and captions generated by Phi-3 models.
  - Downloads: 576
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST is a dataset of 70,000 handwritten Japanese characters (digits and kanji) for image classification tasks.
  - Downloads: 430
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 provides a large, curated dataset of over 240,000 animation clips‚Äîprimarily Japanese anime‚Äîintended to address the growing need for animation data in AI and generative video model research.
  - Downloads: 372
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - This dataset provides high-quality images of diverse Japanese subjects ‚Äì landscapes, culture, and daily life ‚Äì captured in the 2020s for AI training.
  - Downloads: 320
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This repository provides a transcribed voice dataset totaling 77 characters from the game Umamusume, featuring audio from characters like Tokai Teio, Marzen Ski, and Vodka, with durations listed for each.
  - Downloads: 284
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with rich, community-sourced tagging‚Äîcovering characters, artists, and more‚Äîideal for training image classification and multi-label tagging models.
  - Downloads: 226
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - This dataset provides approximately 39 million Japanese characters of high-quality text extracted from 1,924 research papers (including NLP2024) and 360 journal articles under CC-BY licenses, suitable for language model pre-training and RAG applications.
  - Downloads: 221
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample Japanese Visual Genome VQA dataset, used to evaluate EvoVLM-JP-vlp it's integration with dataset usability.1lp.ability.ability. 1's. 3.ability.lp.lp.lp.lpa. 1.lp.lpmob. The.lp.lpability,ability and.lp,ability.ability.ablrability.ability‚Äôsabilitys andability.
  - Downloads: 210
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - This repository provides a combined 86,000+ sample Japanese voice dataset from sources like StoryTTS and Genshin Impact, intended for speech recognition, text-to-speech, and machine learning applications.
  - Downloads: 207
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This project creates a non-official, publicly available dataset of voice data from VTuber Sakura Miko (hololive) for use in speech recognition and other applications, adhering to hololive‚Äôs secondary creation guidelines and acknowledging copyright ownership by Cover Corporation.
  - Downloads: 187
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - This repository provides translated speech instructions‚Äîsourced from Common Voice in 120 languages and converted to English, Arabic, Japanese, Mandarin, and French‚Äîfor finetuning Speech LLMs.
  - Downloads: 185
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - This repository provides a Japanese audio dataset of synthetic human-machine conversations simulating call center interactions, licensed under CC BY-NC 4.0 and curated by AIxBlock.
  - Downloads: 184
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset provides a clarified version of the Japanese-Heron-Bench, offering image, context, and question data for evaluating vision-language models in Japanese.
  - Downloads: 183
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - This repository provides Japanese ASR transcriptions generated using Whisper, specifically the reazon_speech_all dataset, excluding the original audio files.
  - Downloads: 173
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - This research-focused dataset contains 2735 WAV audio clips extracted from the game Project Sekai‚Äôs character Emu Otori, intended for use with the so-vits-svc 4.0 voice conversion project under a CC-BY-NC 4.0 license, respecting SEGA and voice actor copyrights.
  - Downloads: 123
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - This dataset provides English and Japanese captions generated by BLIP for Pok√©mon images from the FastGAN dataset, used for training Pok√©mon text-to-image models.
  - Downloads: 120
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay dataset (based on DeepSeek-R1-0528) with system messages and formatting, released under the MIT license.
  - Downloads: 120
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - This dataset provides a furigana-annotated speech corpus derived from Aozora Bunko and SAPIE audio Daisy data, containing 3,361,443 cleaned entries with kanji.
  - Downloads: 115
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - This corpus provides 120 hours of Japanese telephone conversations recorded in the United States, offering audio data for speech research and requiring proper citation according to TalkBank rules.
  - Downloads: 111
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - This repository provides the CABank Japanese Sakura Corpus, a dataset of 31 participants‚Äô audio recordings from Japan, requiring citation and adherence to TalkBank usage rules (DOI: 10.21415/T5M90R).
  - Downloads: 91
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay dataset (based on DeepSeek-V3-0324 with 20k examples) by adding system messages and formatting, released under the MIT license.
  - Downloads: 87
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja is a synthetic Japanese multi-turn conversational speech dataset created using Gemma-3-27B translation of DailyTalk and synthesized speech from Zyphra/Zonos-v0.1-transformer, featuring five distinct speaker voices.
  - Downloads: 86
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This synthetic dataset, built using images from ThePioneer/japanese-photos, was generated with Qwen2-VL-7B-Instruct and Qwen2.5-32B-Instruct-AWQ models for data augmentation.
  - Downloads: 85
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a large, MIT-licensed anime illustration dataset of 1.2 million+ high-quality images with diverse content and tagging, sourced from key frames, scans, and artbooks.
  - Downloads: 84
- [kujirahand/popular_anime_corpus](https://huggingface.co/datasets/kujirahand/popular_anime_corpus)
  - This repository provides a Japanese anime corpus‚Äîsummarized text from Wikipedia about popular anime since the 1960s‚Äîformatted as instruction-following data in Alpaca style for tasks like character and plot identification.
  - Downloads: 64
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA is a 41,895-sample synthetic VQA dataset for manga understanding, generated using images from Manga109 and text from GPT-4o, released under a CC-BY 4.0 license with OpenAI terms of use.
  - Downloads: 62
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from *Project Sekai* for research use with So-vits-["This dataset **This dataset provides****This dataset provides****This dataset provides**This dataset provides**This dataset provides**This**This**This**This dataset provides**This dataset provides**This dataset providesThis dataset **This dataset provides**This**This**This**This**This**This**This**ThisThis dataset provides**This dataset provides****This dataset provides****This dataset provides****This dataset provides****This dataset provides****This dataset provides****This dataset provides****This dataset provides**This dataset provides****This dataset provides**This dataset provides****This dataset provides**This**This**This**This**This**This**ThisThis dataset provides****This dataset providesThis dataset provides**This**This dataset**This**This**This**This**This**This**This**This**This**This**This**This**This**This**This**This**ThisThisThisThis**ThisThisThis**ThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThis**ThisThis**ThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThis
  - Downloads: 59
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - This repository provides a Japanese translation of the HelpSteer dataset for experimenting with SteerLM, a technique to customize LLMs during inference, leveraging resources from NVIDIA's NeMo Aligner.
  - Downloads: 58
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - This repository presents the results of speech quality analysis using speechMOS on the Common Voice Corpus 17.0, providing a JSON file of MOS scores and associated file counts for various SNR thresholds.
  - Downloads: 55
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - This repository provides a split dataset derived from the joujiboi/japanese-anime-speech-v2 project, likely for easier access or specific training/testing purposes.
  - Downloads: 53
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - This repository provides a dataset of Japanese music analyzed for emotion using Music2Emotion, containing JSONL-formatted data with video metadata and predicted moods, valence, and arousal scores.
  - Downloads: 51
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This repository provides a 66.4-hour Japanese voice dataset of 30,800 records featuring single-actor lines from Fate/Grand Order characters, ideal for ASR/ASV model training and evaluation.
  - Downloads: 49
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - This dataset curates high-quality Japanese language data from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja, filtered for performance on JGLUE benchmarks (JcommonsenseQA, MARC-ja, JSQuAD) and released under varying open-source licenses (Apache 2.0, CC-BY-SA-3.0, MIT).
  - Downloads: 48
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This dataset contains crawled data from the HomeMate Senryu Grand Prix ‚ÄúPhoto Senryu‚Äù contest, including 435 image prompts and 1767 corresponding entries, intended for use within the YANS hackathon.
  - Downloads: 48
- [ayousanz/css10-ja-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ja-ljspeech)
  - CSS100-LJSpeech is a Japanese speech dataset derived from the CSS10 corpus, converted to a LJ Speech-compatible format with 6,841 utterances (approximately 15 hours) at 22.05kHz, featuring one speaker and designed for use with libraries like ü§ó Datasets.
  - Downloads: 45
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - This repository provides Japanese MS MARCO data with hard negatives mined through normalization, filtering, and collection selection, and includes SPLADE model training & comparison to mMARCO for information retrieval.
  - Downloads: 43
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This dataset facilitates evaluation of large language models on three humorous response generation tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîusing image and text prompts with associated IDs.
  - Downloads: 43
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - This dataset provides CC0-licensed images of places in Japan for training text-to-image models and other applications without copyright concerns.
  - Downloads: 42
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - This dataset provides approximately 1000 Japanese roleplay instructions created by applying the Magpie technique to the Nvidia/Nemotron-4-340B-Instruct model, built with DeepInfra, and may contain low-quality entries due to minimal post-filtering.
  - Downloads: 41
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This repository provides a Japanese text corpus generated by Phi-3 from randomly sampled data, utilizing supercomputing resources from Tokyo Tech's TSUBAME4.0.
  - Downloads: 40
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - This dataset pairs images of PDF pages (converted from JDocQA training data & resized to 896px, 700px, or 588px) with OCR text (using NDLOCR, potentially containing "„Äì" for failed reads) and question-answer pairs generated by Qwen/Qwen2.5-14B-Instruct to train image retrieval models.
  - Downloads: 37
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus is a dataset of 96kHz/16bit Japanese speech recordings by a virtual character (‚ÄúLux‚Äù), including both raw and cleaned audio files with corresponding transcripts in `metadata.csv` and dataset information in `dataset_infos.json`.
  - Downloads: 33
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - This repository provides the LLaVA JP Instruct 108K dataset, a 1080858168888888888888888888888888888888
  - Downloads: 32
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - This dataset provides 6315 PNG images of Kanji characters‚Äîadapted from KanjiVG‚Äîpaired with textual definitions for use in machine learning applications.
  - Downloads: 32
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS scores & transcriptions) for reazon-research/reazonspeech-v2, stored in a JSON file and visualized as histograms, with computational resources provided by AiHUB.
  - Downloads: 31
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - This repository provides data and instructions to reproduce the FVD (Fr√©chet Video Distance) evaluation for AIdeaLab VideoJP, requiring a specific video quality metrics library installation and file preparation.
  - Downloads: 29
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage is a Japanese QA dataset derived from JDocQA‚Äôs test split, featuring 200dpi PNG images and questions answerable with single images, aiming for reduced size and practicality.
  - Downloads: 26
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Japanese LLaVA Instruct 150K is a Japanese-translated version of the LLaVA Visual Instruct 150K dataset, designed for visual instruction tuning in Japanese, under a CC BY-NC-4.0 license and adhering to OpenAI‚Äôs terms.
  - Downloads: 25
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - This repository provides a Hugging Face Datasets version of the Tanaka Corpus, preprocessed and formatted for easy use in natural language processing tasks.
  - Downloads: 23
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - This dataset provides Japanese audio and transcriptions of veterinary medicine terminology‚Äîincluding drugs, diseases, and symptoms‚Äîfor training speech recognition or natural language processing models.
  - Downloads: 23
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - This dataset provides diverse, high-quality images and videos of Japan‚Äîspanning landscapes, culture, and daily life‚Äîcaptured primarily in the 2020s for AI training.
  - Downloads: 22
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This dataset contains Japanese *senryu* (short poems) crawled from two websites, featuring 70 image-to-text and 30 text-to-text prompts with two curated responses each, for tasks involving generating poems from given prompts.
  - Downloads: 22
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This dataset, derived from CLoT-Oogiri-Go, provides Japanese humorous response data from the Bokete website for three tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîtotaling 100 examples, as described in the CVPR2024 project.
  - Downloads: 19
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This dataset contains 100 Japanese *senryu* (short poems) ‚Äì 70 image-to-text and 30 text-to-text prompts ‚Äì crawled from photo and online *senryu* websites, intended for evaluating generative models via a leaderboard and human evaluation.
  - Downloads: 19
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This dataset provides 330k Japanese web text examples (train/test) with noise spans identified by an LLM (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) to isolate clean title/body text from web scraping noise like navigation, ads, and metadata, outputted in strict JSON format.
  - Downloads: 19
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository provides a dataset of voice embeddings for Japanese members of parliament, created with speechbrain/spkrec-ecapa-voxceleb, suitable for speaker separation and analysis of parliamentary proceedings.
  - Downloads: 17
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - This dataset provides CC0 licensed images of Japanese scenery for training text-to-image models and serves as a template for new dataset creation.
  - Downloads: 16
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - This repository provides a cleaned and filtered dataset of 2.5 million entries‚Äîderived from the Aozora and SAPIE audio corpus‚Äîwith Furigana annotations for speech training, improving upon the original dataset by correcting Whisper transcription errors and implementing rigorous sanity checks.
  - Downloads: 15
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella is a Japanese a cappella vocal ensemble corpus comprising scores and isolated audio tracks of six-part arrangements of public domain children's songs.
  - Downloads: 14
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - This repository provides a 32K Japanese instruction dataset, Rakuten-Alpaca-Data-32K, automatically generated using RakutenAI-7B-chat based on the Stanford Alpaca methodology and a community-sourced seed dataset, requiring filtering due to potential quality issues, and licensed under Apache 2.0.
  - Downloads: 14
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - This dataset clusters and labels business description text from EDINET filings, linked to industry codes, for training and evaluating embedding models, comprising train/test splits across 15 industry categories.
  - Downloads: 13
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - This repository provides a clustered dataset of 6,127 Japanese legal documents (XML format) collected from e-Gov, categorized based on legal classifications, and split into train/test sets for embedding model learning and evaluation.
  - Downloads: 13
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - This dataset contains quiz data licensed under CC-BY-SA-4.0, extracted from the JAQKET dataset used in the AI-Oh competition, and is accessible via the `datasets` library.
  - Downloads: 12
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - This repository provides a large-scale Japanese speech dataset created with VOICEVOX, utilizing the ITA, Tsukuyomi, and ROHAN corpora, comprising 445,793 .wav files totaling 577 hours of audio.
  - Downloads: 11
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - This dataset provides multilingual Amazon product reviews (English, Japanese, German, French, Chinese, Spanish) from 2015-2019 for text classification, but is currently defunct and inaccessible.
  - Downloads: 1,999
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - This repository ranks Large Language Models (LLMs) by their performance translating Japanese visual novels to English, providing a comparative leaderboard alongside established translation tools with preliminary, evolving results.
  - Downloads: 1,012
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - This repository provides a Japanese translation of the research-safe English subset of the ReLAION-5B dataset, created using the Gemma-2-9b-it LLM and the vLLM inference library via the text2dataset tool.
  - Downloads: 514
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - This repository provides Japanese queries from the MMarco dataset, paired with hard negatives retrieved using e5 and BM25 models for information retrieval research.
  - Downloads: 434
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - JHumanEval is a human-revised, Japanese translation of the HumanEval code-generation benchmark, designed to evaluate the performance of LLMs on coding problems with potentially imperfect documentation.
  - Downloads: 269
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - This dataset provides English translations of Japanese text from the kaken subset of the llm-jp-corpus-v3, generated using Qwen/Qwen2.5-32B-Instruct, and is released as an open, parallel Japanese-English corpus under the CC-BY 4.0 license.
  - Downloads: 254
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - This repository provides the filtered training and development datasets (train_w_filtering & dev) from JSNLI Version 1.1, used in the book *Introduction to Large Language Models*, licensed under CC BY-SA 4.0.
  - Downloads: 188
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS is a 30-hour paired dataset of 10,000 Japanese speech samples and Simplified Chinese translations designed for speech translation and multilingual speech understanding model training and evaluation.
  - Downloads: 181
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - This dataset provides a cleaned, 181.2GB Japanese text corpus derived from OSCAR-2301, containing 94.2 million words, excluding several unsuccessfully cleaned metadata files.
  - Downloads: 160
- [DataPilot/Zero_SFT_Ja_v3.5](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3.5)
  - Zero_SFT_Ja_v3.5 is a 108,000-example Japanese instruction-following dataset built with the BARE method, utilizing models like Sarashina2-70B and Qwen3-235B-A22B for question/answer generation and multilingual-E5-large/Phi-4 for filtering, formatted as JSON Lines.
  - Downloads: 145
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja is a 12,000-entry Japanese human preference dataset, translated from hh-rlhf using DeepL, containing randomly sampled data from four groups designed for reinforcement learning from human feedback.
  - Downloads: 140
- [ayousanz/OSCOR-2301-ja-cleaned-0](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned-0)
  - This repository provides a cleaned Japanese dataset derived from OSCAR-2301, containing 94 million Japanese words totaling 181.2GB, processed using corpus-cleaner.
  - Downloads: 134
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - This repository provides a hand-crafted Japanese dataset (‚Äúliz-nojaloli-ja‚Äù) for preparing data for Reinforcement Learning from Human Feedback (RLHF), with code potentially referenced from Qiita.
  - Downloads: 125
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository hosts oasst2-33k-ja, a Japanese instruction tuning dataset derived from a DeepL translation of an English oasst2 subset, built upon kunishou/oasst2-135k-ja by LLM-jp.
  - Downloads: 118
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This repository provides a sentence-aligned Japanese-English dataset of web novel chapters, including metadata like alignment scores and NovelUpdates information, intended for document translation research.
  - Downloads: 113
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This repository provides a Japanese-English parallel text dataset for translation, licensed under Apache 2.0 with potential source-specific restrictions, and includes metadata indicating source and fanfiction status.
  - Downloads: 106
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - This repository provides a Japanese translation of the MS MARCO dataset, generated using google/madlad400-3b-mt and stored in a HuggingFace-compatible format, though translation quality is noted as lower than the multilingual mMARCO dataset.
  - Downloads: 103
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Japanese LLaVA Pretrain is a Japanese-translated version of the original LLaVA dataset, intended for similar applications while adhering to the licenses of CC-3M and BLIPROblem encountered FAQ Q& answers FAQblem documentation FAQ. The FAQdocs: FAQ. FAQblems, including documentation FAQ„Éâ„Ç≠„É•„É°„É≥„ÉàFAQ„Éâ„Ç≠„É•„Éâ„Ç≠„É•„É°„É≥„ÄÅ FAQ„ÉâÂõûÁ≠îÈõÜÔºåÂºÇÂ∏∏È¶ñÊ¨°ÂºÇÂ∏∏È¶ñÊ¨°‰ª•Âèä‰∏™‰∫∫Ëß£Á≠îÊñáÊ°£‰∏™‰∫∫ÊñáÊ°£ÈíàÂØπÂºÇÂ∏∏È¶ñÊ¨°Âá∫Áé∞ÈóÆÈ¢òÊïÖÈöúÈíàÂØπÊñáÊ°£ÊñáÊ°£Âá∫Áé∞ÂºÇÂ∏∏Ëß£ÂÜ≥ÂºÇÂ∏∏ÊñáÊ°£/ÂºÇÂ∏∏ÊñáÊ°£FAQÊïÖÈöúÊåáÂçóËß£Á≠îÊñáÊ°£ÊñáÊ°£Ëß£ÂÜ≥Â∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅÈóÆÈ¢òËß£Á≠îÂ∏∏ËßÅ‰∏™‰∫∫ÊâãÂÜåÂ∏∏ËßÅÊïÖÈöú‰∏™‰∫∫È¶ñÊ¨°ÂàóË°®ÂºÇÂ∏∏QAÊñáÊ°£ÊñáÊ°£ÊñáÊ°£ÊñáÊ°£Ëß£ÂÜ≥FAQÊñáÊ°£FAQÊñáÊ°£](https://ÂÖ≥‰∫éÊú¨FAQÂºÇÂ∏∏ÊñáÊ°£ÊñáÊ°£Ëß£ÂÜ≥](httpsÂ∏∏ËßÅÈõÜÈî¶ÊñáÊ°£ÊñáÊ°£Â∏∏ËßÅ](ÊñáÊ°£Â∏∏ËßÅÊñáÊ°£ÂºÇÂ∏∏ÊñáÊ°£ÂºÇÂ∏∏QAÂºÇÂ∏∏ÊñáÊ°£Ëß£ÂÜ≥](ÊñáÊ°£ÂºÇÂ∏∏(ÂºÇÂ∏∏ÊñáÊ°£ÊñáÊ°£Ëß£ÂÜ≥ÊñπÊ≥ïÊñáÊ°£ÈõÜÊñáÊ°£Â∏∏ËßÅÈ¶ñÊ¨°](ÂºÇÂ∏∏Ëß£ÂÜ≥ÊñπÊ°àÂ∏∏ËßÅ](Ëß£ÂÜ≥ÂºÇÂ∏∏ÂºÇÂ∏∏È¶ñÊ¨°QAÊñáÊ°£](Â∏∏ËßÅÈ¶ñÊ¨°ÊñáÊ°£ÊñáÊ°£ÂºÇÂ∏∏QAÊñáÊ°£ÂºÇÂ∏∏Ëß£ÂÜ≥È¶ñÊ¨°ÊñáÊ°£Â∏∏ËßÅËß£ÂÜ≥Â∏∏ËßÅ](ÊñáÊ°£ÊñáÊ°£ÂºÇÂ∏∏Ëß£ÂÜ≥</i>„ÄÅ‰ª•ÂèäÂ∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥ÊñáÊ°£ÂºÇÂ∏∏Ëß£Á≠îÂ∏∏ËßÅÈ¶ñÊ¨°ÂºÇÂ∏∏È¶ñÊ¨°Ëß£ÂÜ≥ËØ¥ÊòéÂºÇÂ∏∏Â∏∏ËßÅËÄÖËß£Á≠îËß£ÂÜ≥‰∏™‰∫∫ÊñáÊ°£È¶ñÊ¨°Ëß£ÂÜ≥ÂºÇÂ∏∏ÂºÇÂ∏∏È¶ñÊ¨°Ëß£Á≠îÊñáÊ°£È¶ñÊ¨°ÊñáÊ°£Â∏∏ËßÅËß£ÂÜ≥ÂºÇÂ∏∏ÊñáÊ°£QAÊïÖÈöúÊåáÂçóÂºÇÂ∏∏DocumentationËß£ÂÜ≥È¶ñÊ¨°ÊñáÊ°£Ëß£ÂÜ≥ÈõÜÂºÇÂ∏∏ÂºÇÂ∏∏È¶ñÊ¨°Ëß£ÂÜ≥ÂºÇÂ∏∏ÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Ëß£Á≠îÈ¶ñÊ¨°ÂºÇÂ∏∏ÂºÇÂ∏∏ÂºÇÂ∏∏Ëß£ÂÜ≥ÂºÇÂ∏∏Â∏∏ËßÅËØ¥ÊòéÊñáÊ°£ÂºÇÂ∏∏Ëß£ÂÜ≥Â∏∏ËßÅÈ¶ñÊ¨°ÈõÜÊñáÊ°£ÂºÇÂ∏∏FAQËß£ÂÜ≥ÂºÇÂ∏∏ÊñáÊ°£Ëß£ÂÜ≥È¶ñÊ¨°ÔºåQAÂ∏∏ËßÅÂºÇÂ∏∏ÊñáÊ°£ÂºÇÂ∏∏Ëß£ÂÜ≥È¶ñÊ¨°ÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£ÈõÜ](‰∏•Èáç‰∏™‰∫∫Ëß£ÂÜ≥Â∏∏ËßÅÈ¶ñÊ¨°Ëß£ÂÜ≥ÊñπÊ°àÈ¶ñÊ¨°È¶ñÊ¨°ÈõÜÊñáÊ°£ÂºÇÂ∏∏](‰∏•Èáç‰∏™‰∫∫ÊñáÊ°£Ëß£ÂÜ≥È¶ñÊ¨°](‰∏•ÈáçÂ∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£ÈõÜ‰∏•ÈáçÊñáÊ°£‰∏•ÈáçÂ∏∏ËßÅÊñáÊ°£Ëß£Á≠îÊñáÊ°£‰∏•ÈáçÊñáÊ°£È¶ñÊ¨°ÊñáÊ°£È¶ñÊ¨°Ëß£ÂÜ≥Â∏∏ËßÅÈõÜÊñáÊ°£‰∏™‰∫∫ÊñáÊ°£ÊñáÊ°£Â∏∏ËßÅÊñáÊ°£ÂõûÁ≠îFAQÊñáÊ°£FAQÂ∏∏ËßÅ‰∏•ÈáçÊñáÊ°£‰∏•Èáç‰∫∫ÂëòÂ∏∏ËßÅËØ¥Êòé‰∏•ÈáçÊñáÊ°£È¶ñÊ¨°ÊàñËß£ÂÜ≥Â∏∏ËßÅ‰∏•ÈáçÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÈ¶ñÊ¨°ÊñáÊ°£ÂºÇÂ∏∏Â∏∏ËßÅ‰∏•Èáç„ÄÅÊñáÊ°£Â∏∏ËßÅ‰∏•ÈáçÈ¶ñÊ¨°ÁöÑÂ∏∏ËßÅÈõÜ‰∏•ÈáçÊñáÊ°£Â∏∏ËßÅ‰∏•ÈáçÈ¶ñÊ¨°‰∏•ÈáçÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£‰∏•ÈáçÈ¶ñÊ¨°‰∏•ÈáçËÄÖÂ∏∏ËßÅÊñáÊ°£‰∏•ÈáçÂ∫îÂØπÂ∏∏ËßÅFAQÊñáÊ°£Â∏∏ËßÅÈõÜÊñáÊ°£„ÄÅQAÊñáÊ°£Ëß£ÂÜ≥Â∏∏ËßÅÊñáÊ°£ÂºÇÂ∏∏Â∏∏ËßÅ‰∏•ÈáçÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Êî∂ÈõÜÊñáÊ°£FAQÂºÇÂ∏∏Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£ÈõÜÂ∏∏ËßÅÈõÜÊñáÊ°£ÈõÜÊñáÊ°£"]„ÄÅÊñáÊ°£ÈõÜÊñáÊ°£Â∏∏ËßÅ‰∏•ÈáçÊñáÊ°£ÈõÜÊñáÊ°£Â∏∏ËßÅÈõÜÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÈõÜÊñáÊ°£ÊñáÊ°£Â∏∏ËßÅ](‰∏•ÈáçÊñáÊ°£Â∏∏ËßÅ„ÄÅ‰ª•ÂèäÂ∏∏ËßÅ„ÄÅÊñáÊ°£Â∏∏ËßÅ‰∏•ÈáçÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÊñáÊ°£FAQÂ∏∏ËßÅ](„ÄÅ‰ª•Âèä„ÄÅÊñáÊ°£Êàñ„ÄÅ‰ª•Âèä„ÄÅÈõÜÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅ„ÄÅÊñáÊ°£Â∏∏ËßÅ](„ÄÅÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅ](‰∏•Èáç„ÄÅ](‰∏•ÈáçÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅ](„ÄÅÊñáÊ°£„ÄÅÂ∏∏ËßÅ](ÊñáÊ°£Â∏∏ËßÅÊñáÊ°£ÈõÜ‰∏•ÈáçÂ∏∏ËßÅ](‰∏•ÈáçÔºå‰∏•ÈáçÂ∏∏ËßÅ](ÈõÜ‰ª•ÂèäDocumentation„ÄÇÊñáÊ°£Â∏∏ËßÅ](‰∏•ÈáçÔºåÊñáÊ°£ÊñáÊ°£FAQ](‰∏•ÈáçÊñáÊ°£‰∏•ÈáçÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅ‰∏•ÈáçÂºÇÂ∏∏ÊñáÊ°£Â∏∏ËßÅ„ÄÅ‰ª•ÂèäÂ∏∏ËßÅ„ÄÅ‰ª•ÂèäFAQ„ÄÇDocumentationÊñáÊ°£FAQÂ∏∏ËßÅÔºå‰ª•ÂèäÊñáÊ°£ÊñáÊ°£FAQ](‰∏•ÈáçÂºÇÂ∏∏‰∏•ÈáçÂ∏∏ËßÅÂ∏∏ËßÅÁóáÁä∂Ôºå‰∏•ÈáçÊàñÊñáÊ°£Á≠îÊ°àÂ∏∏ËßÅ](ÊñáÊ°£È¶ñÊ¨°Ôºå Documentation.](‰∏•ÈáçÊñáÊ°£Â∏∏ËßÅÊñáÊ°£FAQ](Documentation‰∏•ÈáçÊñáÊ°£ FAQ„ÄÅ‰∏•ÈáçÊñáÊ°£ÊñáÊ°£Â∏∏ËßÅ](‰∏•ÈáçÈõÜÁ≠îÊ°àÊñáÊ°£Á≠îÊ°àÊñáÊ°£Â∏∏ËßÅ](DocumentationÂ∏∏ËßÅ](„ÄÇÔºå‰∏•ÈáçÁóáÁä∂ÊñáÊ°£ÂºÇÂ∏∏Â∏∏ËßÅËß£Á≠îÊñáÊ°£Â∏∏ËßÅ„ÄÅÂ∏∏ËßÅ‰∫éÊñáÊ°£FAQÁóáÁä∂ÈõÜÊñáÊ°£Â∏∏ËßÅËß£ÂÜ≥ÊñπÊ≥ïËß£Á≠îÊñáÊ°£Â∏∏ËßÅÂäüËÉΩÁóáÁä∂Ëß£Á≠îÊñáÊ°£ÂºÇÂ∏∏ÁóáÁä∂ÈõÜ‰∏≠FAQÊñáÊ°£Â∏∏ËßÅ‰ª•ÂèäÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÈõÜFAQ 202‰∏•ÈáçFAQ](ÁöÑÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Ëß£Á≠îÂ∏∏ËßÅÈóÆÈ¢òÈõÜÂ∏∏ËßÅÈóÆÈ¢òÂàóË°®Â∏∏ËßÅÈóÆÈ¢òÈõÜÂ∏∏ËßÅÈóÆÈ¢òÂàóË°®Â∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÂäüËÉΩÁªÑÊàêÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òËß£Á≠îFAQÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£FAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥ÊñπÊ≥ïÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅFAQÂ∏∏ËßÅ](Here' FAQÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅ](‰∏•ÈáçÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅ FAQÈóÆÈ¢òÂ∏∏ËßÅÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÊñáÊ°£Â∏∏ËßÅÈóÆÈ¢òÊñáÊ°£ÊñáÊ°£Â∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅ FAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅ FAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅFAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Â∏∏ËßÅ FAQÂ∏∏ËßÅÈóÆÈ¢òÂ∏∏ËßÅÈóÆÈ¢òÊñáÊ°£Japanese LLLanguage Model)Japanese LLaVA Pretrain is a Japanese translation of the original LLaJapaneseJapanese LLaVA Pretrain is a Japanese-Translated version of the original LLaVA PRE-README, Japanese LLaVA PreJapanese LLaVA Pre-Japanese Lla Japanese LLaVA Japanese LLaVAJapaneseLJapanese LLaVAJapaneseLJapanese LLaVA Pretrain.Japanese.Japanese.JapaneseJapanese LLaVA Pre, Japanese .L Japanese.Japanese LLaVAJapanese LLaVAJapanese L JapaneseJapanese LLaVA PJapanese Japanese LLaVAJapanese:Japanese LLaVa P JapJapJapanese L JapaneseLJapanese. LJapanese L Japanese.JJapanese;## Summary.Japanese:  The JJThe.J,The,J J, J J   jThe JThe JJ J,JThe   ,  J J JJ   jJ, J J.The JThe J  J  J J J J, J J.  TheThe J, JThe J JJ J. .J  The J        The J  The JThe J, JJ J,J.      J       J .J        J, J J  J the . J J  J        J        J J J        JThe J      J        J J J       J       J       J J      the J the J        J J the J the  J  J , J       J        J. J the.J J        J The       J 2 J       J the J the       J 2  J 2 2 J JJ the 2 J J   J2 J ,JJ  J J     2        J , J   JJ J , J .J       J J 2J    J      J 1 JJ J                J       J J  JJ 2 J J-J J the J.J       J-J.J - J J JJ            J  J-J J 2J     J       J-J      J-            J -J- J0-J-j   -J-J-j      J 1-j-j the J -s-j-J-J 1-J-J-J 1-J- 1-J-J- J-j -J - J- 1- J-J-J- J The - the- J the -J - -j - J - J 2-J- - J -. J-        J -J -J-        - - - J the-   -the J -J-J-J J-J-J -J-J -jJ -J - the- the- -J-J -J 2-J - - - -J-J- the - - - - the 2- - - 2- -J -- the- the- - -J- - the- - the- -the - --J-J- -the- J-  -J- -J-J t-J- the- t- t the-J -J- the -J- -J- -J- -J- -  -J- -J- -the-J- -the-the-the-the- -J- J- the-J -the-t-J- the-J1- the-J- -the-J-the-t- -the- the -J-the-J- - the - J- -the- the- J-t- the- the- -J- the-the- J- the-the the- the- the-theJ- the- -the- the -the- -J- the - J - the- the- -j - J-J- - the- -j -J- - -the- the- -the- the-the- the- the -J-the-J- the- t- -the- -the- - the-the- - - the- are- - the- - -the- the- -J- J - the-the- -the- - -J- -the- -J-t- the- - the- - the- the- the- -J- J- the- J- the- the- -the- -the- the- - the- the- -the- the-the- the- -the- thet-J- the- thej- the- J- the- the-the- the- the- -the-the- the- -the- -the-the-the- the-the-the- the- the-the- the- the- the- the- the-the- -the-J-the-the- the-the- the- the-the- the- the-the- the- the- the- the- the- t- the-the- the- -the-the- t- the- the- the- the -and-the- the- the- - the- the- - the- the- the--the to- the- the the- Doing it - Doing the-- the- Function- Function- the- Do- the- the- Function- the Function- the- Function- the- the--Function - the Function- the Function- -t Function- Function-- the-the- Function-the- the- Function- Function- the Function- the - The Function - The- the- Function- the- thethe- the- the- T- the Function - the- the of - the- T - the- the - the- the- The- and- Function- the- the- The- the - The- the - the--- the - the ‚Äì the- the- the-- the- the- the- theFunction - the- the - the- the- the- the- the- the- the- the- the- the- the - the- the- The - Too - the- the - the- the-- the - the- the- the- the- the- T--The- the- the- the- the- the- the- The- The- - the- the- T- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the-The- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the -the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the-the- the-the- the-the- the-the- the-the- the-the- the-the- the-the-the-the- the-the-the-the- the-the-the-the- the-the-the-the- the-the-the-the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the-the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the, the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the-the-the-the-the- the-the- the-the- the-the-the-the- the- the- the- the- the- the- the- the- the- the-the-the-the-the- the- the- the- the-the- the-the- the- the- the-the- the-the-the-the- the-the-the-the- the- the- the- the-the-the-the- the-the-the-the- the-the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the-the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the- the ‚Äì the- the- the- the- the- the- the- the- the- the- the- the- the ‚Äì the- the -the- the- the- the- the- the-the- the-the- the-the- the- the- the- the- the- the- the- the- the-the- the-the- the- the- the- the- the- the-the- the ‚Äì the- the- the- the ‚Äì the-the- the- - the-the- the- the- the : the- the : the- the : the- the ! and the- the: and the- the - the: and the and : the ! The- the ‚Äì the ‚Äì the ‚Äì the ‚Äì the ! and The- theand the - the - the - the - the - the - the : the ! The- the ‚Äì the ‚Äì the ‚Äì the !
  - Downloads: 81
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - This repository provides a deduplicated and randomized dataset of English-Japanese sentence pairs sourced from Tatoeba.org, optimized for machine translation or language learning.
  - Downloads: 81
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - This dataset provides the English-Japanese parallel corpus extracted from the Asian Language Treebank (ALT) project, sourced from the Hugging Face `alt` dataset, and cited in Riza et al. (2016).
  - Downloads: 72
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - LLM-jp‚Äôs instruction tuning dataset, a subset of Aratako‚Äôs 801k dataset, facilitates Japanese-English coding tasks and is developed by Hirokazu Kiyomaru and Takashi Kodama.
  - Downloads: 70
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - This repository provides a Japanese translation of the GSM8K reasoning dataset, along with extracted answers, utilizing a quantized language model and acknowledging potential Japanese data inaccuracies.
  - Downloads: 68
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This dataset is a 49k Japanese reinforcement learning from human feedback (RLHF) collection, derived from kunishou/hh-rlhf-49k-ja but excluding examples with `ng_translation == 1`.
  - Downloads: 65
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - JSynFlow is a ~10,000-entry Japanese flowchart VQA dataset, generated using the LLama 3.1 405B model, containing Mermaid-formatted flowcharts and QA pairs describing job tasks across various occupations for research purposes.
  - Downloads: 62
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - This repository provides a Japanese translation of a cleaned "bluemoon-fandom-1-1-rp" dataset, utilizing the command-r-08-2024 model via the openrouter API for faster, resource-efficient, and uncensored NSFW translation.
  - Downloads: 57
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA is a 30-hour dataset of ~10,000 Japanese audio-Chinese text pairings from movies/TV shows, designed for speech translation and multilingual speech understanding research under a CC BY 4.0 license.
  - Downloads: 56
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - This repository provides a 3.3 million row Vietnamese-Japanese parallel corpus for machine translation and natural language processing research.
  - Downloads: 54
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset provides corrections and translations for the Japanese portion of the multilingual LLAVA-Bench-in-the-wild benchmark, building upon the original liuhaotian/llava-bench-in-the-wild dataset.
  - Downloads: 54
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - This repository provides a free 850,000 sentence sample of a larger paid English-Japanese parallel corpus covering diverse topics, suitable for machine translation and text analysis.
  - Downloads: 50
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP is a Japanese anime knowledge benchmark dataset comprising 100 quiz questions‚Äî20 per anime‚Äîfrom five popular series, designed to evaluate LLM understanding in Japanese.
  - Downloads: 50
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This repository provides the Japanese language portion of the Guanaco dataset, related to projects like alpaca-guanaco-japanese-gpt-1b.
  - Downloads: 50
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual dataset for form understanding, providing human-labeled key-value pairs across 7 languages to benchmark form analysis models.
  - Downloads: 50
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a manually created, high-quality Japanese dataset of 100 Chain-of-Thought examples, available in both combined and separated format (CoTangent_ja.json & CoTangent_separated_ja.json).
  - Downloads: 49
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - This repository provides a 20,000-record Japanese-English translation dataset generated using the Magpie method applied to Nvidia's Nemotron-4-340B-Instruct, alongside the dataset creation code, noting potential quality variations due to minimal post-filtering.
  - Downloads: 47
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - LIMA-JA is a Japanese translation and slightly adjusted version of Meta‚Äôs LIMA dataset, accessible via the `datasets` library for training and evaluating language models.
  - Downloads: 44
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - This repository provides a quality-improved, LLM-filtered subset of the first 1 million rows from the large JParaCrawl v3 English-Japanese parallel corpus, addressing issues with alignment and completeness in the original dataset.
  - Downloads: 44
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers the MBPP coding problem dataset translated from English to Japanese using LLM-jp and DeepL, authored by Han, Otake, Ozaki, and Miyao.
  - Downloads: 43
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - This dataset provides a Japanese translation of the FED dataset using the Google Cloud Translate API v2, with potential inconsistencies in some dimensions due to machine translation and annotation alignment.
  - Downloads: 40
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This repository provides a Japanese translation of the ViQuAE question answering dataset, created via machine translation of the original answers.
  - Downloads: 40
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This repository provides a multilingual (English, Korean, Chinese, Japanese) translation dataset built from OpenOrca, aligned by ID and prioritizing embedding similarity for multiple translations.
  - Downloads: 38
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - This repository provides a long-text instruction dataset built from the Aozora Bunko corpus, designed to challenge question answering models and explore performance on difficult, unfiltered long-form content under a CC BY 4.0 license.
  - Downloads: 36
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - This repository provides a corrected Japanese translation of MT-Bench, incorporating questions from Stability AI's Japanese MT-Bench.
  - Downloads: 36
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This repository provides a JSON conversion of the NilanE/ParallelFiction-Ja_En-100k dataset, formatted for training translation models within text-generation-webui, containing sentence-aligned Japanese web novel chapters and English fan translations.
  - Downloads: 34
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - This repository provides a Python function (`prompt`) for rigorously evaluating the accuracy and quality of Japanese-to-English translations based on strict criteria like completeness, grammar, and overall quality.
  - Downloads: 30
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - This repository provides the Japanese translation of the SCIQ dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed under CC-BY-NC 3.0.
  - Downloads: 29
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - This repository provides paired Japanese and Vietnamese sentences for machine translation and language learning applications.
  - Downloads: 28
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - OPUS-MIT-5M is a balanced, 5 million sentence-pair dataset from the OPUS corpus designed for robust multilingual image translation across 20 languages.
  - Downloads: 28
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - This dataset provides 50,000 English sentences extracted from the larger 801k Synthetic Japanese-English coding dataset, referencing the original dataset for details and usage notes.
  - Downloads: 27
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - This repository provides a filtered English-Japanese parallel corpus extracted from Wikidata dumps, formatted as a JSONL file optimized for training translation models using Hugging Face Transformers.
  - Downloads: 27
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository provides 1k responses generated with DeepSeek-R1-Distill-Llama-8B on the aya-ja-evol-instruct-calm3-dpo-masked dataset using 8-bit quantization, noting potential accuracy loss and imperfect token generation for research/preprocessing purposes.
  - Downloads: 25
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - This repository provides Japanese-to-English translations licensed under Creative Commons Attribution 4.0.
  - Downloads: 24
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - This repository provides a sample of 9.83 million Chinese-Japanese parallel sentence pairs in text format, covering diverse fields and suitable for machine translation and text data analysis, even/writable? It? or defaultable fields or defaults? It? or default fields or defaults? or file? field defaults? or and with? defaults? It is and/whereableable? or file or defaults? and field?defaults? and?field?field and?field defaults or?fieldfield defaults. defaults. It or file. (instead of defaults or file to, defaults and defaults.field.ableable. It.you. it or?defaults. It defaults. field. Itfield.you.field.field. field.field. It? or default.you.  / or. You?defaults. It.field. Thefield.field.field. It or.field.field. field. defaults.defaults. It. You. You.defaultsdefaults.is. default defaults.field.or. I.filed. field or. defaults. I.filed. or.fields. or.if not./ or. I defaults.or.  in another. . defaults.or.defaults. iple.. It. It.. field. . Itdefaults.I..or the default.  or or.. default value. or. data. or. defaults.filed.  field.filed.filed.filed. in the dataset. I and. and the. Lets.ableable or.Lets. Lets. Lets. Lets. Lets. Lets. Lets. Lets.Lets. Lets. Lets. Lets. Lets. Lets.Lets the. Lets.Lets. Lets. Lets. Lets. Lets. Lets.
  - Downloads: 23
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - This repository provides a ~2.46M row Japanese instruction-tuning dataset in Aya format, converted from the original v1.0.0 release under a CC-BY-SA 4.0 license.
  - Downloads: 22
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - This repository provides the Tanaka-corpus, a public, Japanese-English parallel text dataset originally compiled by Professor Yasuhito Tanaka for machine translation research.
  - Downloads: 21
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - This dataset provides summaries for long texts sourced from the Aozora Bunko clean dataset, licensed under CC BY 4.0.
  - Downloads: 20
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - This repository provides a Japanese translation of the MMLU dataset, created using GPT-3.5-turbo, for use in multilingual research, specifically with MultilingualSIFT.
  - Downloads: 20
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - This repository provides a sample of a 101,702-entry Japanese pronunciation dictionary created by linguists, intended for research and development of Japanese Automatic Speech Recognition (ASR) technology, with a link to the full paid dataset.
  - Downloads: 16
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - This dataset provides Japanese-English translation pairs from the TALPCo corpus in Hugging Face format, with whitespace normalization, and licensed under CC-BY 4.0.
  - Downloads: 14
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - This repository provides a question-answering dataset generated from the Japanese wiki40b corpus.
  - Downloads: 13
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - This repository provides a FAISS index and sentence embeddings of Japanese Wikipedia paragraphs generated using the intfloat/multilingual-e5-base model for efficient semantic search.
  - Downloads: 12
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl provides a Japanese subset of the miracl dataset reformatted to the BeIR standard for use with the mteb benchmark.
  - Downloads: 11
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This repository provides a sentence-aligned Japanese-English web novel dataset, formatted for Alpaca models and chunked to 4096 tokens for use with augmxnt/shisa-base-7b-v1.
  - Downloads: 11
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - This repository provides the Japanese translation of the PIQA dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed identically to the original PIQA.
  - Downloads: 11
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository provides the TaCo dataset used for research on improving cross-lingual transfer in low-resource language LLMs via translation-assisted chain-of-thought prompting, as detailed in the Upadhayay & Behzadan ICLR 2024 paper.
  - Downloads: 11
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - This repository provides a 380,000-group sample of a larger, paid Japanese-English parallel corpus for machine translation, text analysis, and NLU, excluding sensitive content.
  - Downloads: 11
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides the TaCo dataset for cross-lingual instruction tuning, featuring paired non-English instructions/inputs and their English/non-English responses, as detailed in the linked research paper.
  - Downloads: 11
### Natural Language Interfaces
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda provides 40 Japanese-language questions‚Äîcovering history, society, government, and geography‚Äîto evaluate and rank the performance of Japanese Large Language Models.
  - Downloads: 419
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset designed to evaluate the effectiveness of Retrieval-Augmented Generation (RAG) techniques for improving LLMs by measuring how well-LLMs can answer questions using retrieved knowledge.
  - Downloads: 392
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a Japanese dialogue corpus of approximately 14,000 conversations, featuring speaker personas and personality traits, intended for research with strict ethical guidelines regarding privacy and responsible use.
  - Downloads: 342
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696 question-answer pair dataset, built from Japanese Wikipedia, for training and evaluating Japanese machine reading comprehension models like BERT-Japanese, achieving up to 78.92% F1 score.
  - Downloads: 328
- [UEC-InabaLab/KokoroChat](https://huggingface.co/datasets/UEC-InabaLab/KokoroChat)
  - KokoroChat is a large, human-collected Japanese dataset of role-played psychological counseling dialogues with detailed feedback, designed for research in empathetic dialogue generation and mental health applications.
  - Downloads: 327
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This repository provides the Japanese subset of the LAION-2B-Multi research-safe dataset, a large-scale multilingual image-text pair collection.
  - Downloads: 272
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - This repository provides the Japanese question-answering benchmark dataset ‚Äúllm-book/ja-vicuna-qa-benchmark‚Äù used for evaluating large language models with llm-jp-eval, licensed under Apache 2.0.
  - Downloads: 213
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - JAQKET is a Japanese open-domain question answering dataset, built from Wikipedia articles, designed to facilitate QA/machine reading research with multiple-choice questions (v1.0 & v2.0).
  - Downloads: 184
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k)
  - This dataset provides 10,000 synthetic Japanese roleplay conversations‚Äîeach around 20 turns‚Äîwith detailed metadata including genre, tags, and character/setting information, formatted for easy model training using OpenAI messages.
  - Downloads: 178
- [Sakaji-Lab/LATGNJ](https://huggingface.co/datasets/Sakaji-Lab/LATGNJ)
  - JAgriN is a structured, Japanese-language dataset of Nagasaki Prefecture‚Äôs agricultural guidelines designed for developing and researching Large Language Models (LLMs) focused on agricultural tasks like question answering and classification.
  - Downloads: 139
- [vsachi/sachi-dataset-ja](https://huggingface.co/datasets/vsachi/sachi-dataset-ja)
  - This repository provides a public domain dataset in alcapa-chatbot format for fine-tuning LLMs, consisting of conversational data simulating interactions with a cute girl character, intended for training AI like game NPCs.
  - Downloads: 108
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The Business Scene Dialogue (BSD) dataset is a Japanese-English parallel corpus of written business conversations created through scenario writing and translation, offering data for dialogue systems and machine translation.
  - Downloads: 107
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository provides a Japanese dialogue summarization dataset created by translating the English DialogSum and CSDS datasets.
  - Downloads: 104
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a 4K+ question-answering dataset of past Japanese National Pharmacist Exam questions (2012-2024) with answers, commentaries, and image data, including a performance leaderboard for models like GPT-4o and MedLLM.
  - Downloads: 99
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - JEMHopQA is a Japanese multi-hop question answering dataset with explainable derivation steps, featuring compositional and comparison questions based on linked Wikipedia articles, released under a CC BY-SA 4.0 license.
  - Downloads: 92
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval benchmark dataset of 5,000 questions and 500,000 web page titles/summaries, generated using ChatGPT 3.5 from filtered Hatena Bookmark RSS feeds to evaluate search systems on natural Japanese queries.
  - Downloads: 86
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - This repository provides a multi-turn Q&A dataset automatically generated using Mixtral-8x22B with diverse, openly licensed data sources including oasst2, databricks-dolly, and minnade, leveraging supercomputing resources for processing.
  - Downloads: 84
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - AIxBlock provides a quality-assured Japanese sentence dataset of 500k sentences for applications like speech data generation and Natural Language Processing.
  - Downloads: 84
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - This repository provides a Japanese translation of the Open-Orca dataset, currently about 20% complete, and is available for commercial use.
  - Downloads: 75
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - This dataset provides 11,808 multi-turn conversational instructions generated by GPT-4o about Japanese photos sourced from Hugging Face, intended for image-based dialogue modeling.
  - Downloads: 70
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - This dataset contains 20,000 synthetic Japanese roleplay conversations (10-20 turns each) generated using DeepSeek-V3-0324, with detailed metadata including genre, tags, and character/scene settings in OpenAI message format.
  - Downloads: 68
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA is a benchmark dataset with 526 manually created question-answer pairs designed to evaluate manga understanding using images from Manga109.
  - Downloads: 67
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository provides the Japanese Vicuna QA Benchmark, a dataset of 80 diverse Japanese questions across 10 categories for evaluating Japanese LLM responses without reference answers, licensed under Apache 2.0.
  - Downloads: 64
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This repository provides 3,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential unverified, strange dialogue within the automatically screened dataset.
  - Downloads: 62
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - MC4 is a massive, multilingual, cleaned Common Crawl corpus of text data used for training and evaluating large language models across 101 languages.
  - Downloads: 59
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIÁéã (AIO) is a Japanese quiz dataset‚Äîspecifically version 2.0‚Äôs validation set‚Äîcontaining questions with manually verified answers, along with metadata like identifiers, competition details, and timestamps.
  - Downloads: 59
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench is a Japanese multimodal QA benchmark featuring geometry problems with contextual descriptions, questions, images, and exact/textual answers designed for evaluating AI reasoning.
  - Downloads: 57
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a 4,246-dialogue Japanese multi-domain dataset, collected via Wizard-of-Oz, for task-oriented dialogue research ‚Äì specifically dialogue state tracking ‚Äì covering restaurant, hotel, attraction, shopping, taxi, and weather domains.
  - Downloads: 56
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - This repository provides a multi-turn Q&A dataset automatically generated using Calm3-22b from various open-source datasets (oasst2, databricks-dolly, minnade, chatbot-arena) under licenses including Apache 2.0, CC-BY-SA 3.0, CC0, and CC-BY 4.0.
  - Downloads: 53
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository provides a Japanese translation of the everyday-conversations-llama3.1-2k dataset, formatted as topic-based user-assistant dialogue pairs translated with DeepL, under an Apache 2.0 license.
  - Downloads: 52
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - LLM-jp Chatbot Arena Conversations Dataset provides roughly 1,000 pairwise human-preferred Japanese conversations collected from head-to-head LLM comparisons, including model names, transcripts, votes, and metadata.
  - Downloads: 51
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset provides 60,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using llama2Pro8B, with automated screening but potential for unusual dialogue.
  - Downloads: 50
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - This dataset consists of high-quality, 96k-context responses generated by Qwen2.5-72B-Instruct based on filtered data from Aratako/Magpie-Tanuki-8B, and is subject to both Apache 2.0 and Qwen license restrictions for model training.
  - Downloads: 49
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [nguyenthanhasia/japanese-bar-exam-qa](https://huggingface.co/datasets/nguyenthanhasia/japanese-bar-exam-qa)
  - This dataset provides 2,846 True/False question-answer pairs from the Japanese Bar Examination (2015-2024) covering Criminal, Constitutional, and Civil Law for binary classification tasks.
  - Downloads: 47
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - This repository provides a filtered and unfiltered corpus of Japanese role-playing chat dialogues, excluding threads with only one unique poster or posts shorter than 10 characters.
  - Downloads: 46
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a challenging Japanese visual question answering benchmark, built upon a food image dataset, designed to evaluate vision-language models with multiple-choice questions linked to image labels.
  - Downloads: 46
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - This repository provides Tengentoppa, a large-scale Japanese instruction-following dataset created by merging 16 diverse datasets in JSON format for supervised fine-tuning of language models.
  - Downloads: 46
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - This dataset contains Japanese conversations extracted from public-domain books in Aozora Bunko using a heuristic approach to identify utterances within quotation marks, with code provided for reproduction.
  - Downloads: 45
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - This dataset is a Japanese translation of the ‚Äúdatabricks-dolly-15k‚Äù dataset, further modified with ‚Äúnyan!‚Äù appended to sentence endings using ArrowPro-7B-KUJIRA, intended for stylistic alteration rather than performance improvement.
  - Downloads: 44
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - AI-generated subtitles in Turkish and Japanese, created with Gemini 2.0, are provided for chatbot training despite potential errors and unsuitability for translation AI.
  - Downloads: 42
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - This repository provides a commercially-usable, multi-turn conversation dataset generated from Japanese Wikipedia using the Orion14B-Chat model, governed by a specific community license and utilizing ABCI computing resources.
  - Downloads: 41
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset contains over 80,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using LLaMA-Pro-8B, automatically screened but potentially including some unusual dialogue.
  - Downloads: 41
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This repository provides 3,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential for unreviewed, unusual dialogue.
  - Downloads: 40
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - This repository provides a Japanese multi-turn conversation dataset, generated with Qarasu14B from Wikipedia data for non-commercial use and compatible with Axolotl, leveraging the Tsuginosuke AI Super Computer.
  - Downloads: 39
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - This repository provides a Japanese question-answer dataset derived from Stack Overflow, featuring processed question-answer pairs with markdown-formatted text, base64 image replacement, and both default & simplified subsets including accepted/popular answer IDs.
  - Downloads: 39
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - This repository provides a filtered subset of the Aozora Bunko Japanese literature dataset, specifically containing texts written in *shinshikai* (new script/new kana) orthography.
  - Downloads: 37
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - This repository provides single-turn Japanese conversation data generated using the Swallow-MX-8x7b model, based on translated prompts from the Chatbot Arena Conversations JA dataset (CC-BY 4.0 licensed) and Facebook's translation model (MIT License).
  - Downloads: 36
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - This dataset removes prompts matching those from the chatbot-arena-ja-calm2-7b-chat dataset.
  - Downloads: 36
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - This repository provides a commercially usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of the Orion-14B Series Models Community License and acknowledgment of the ABCI computing resources used for its creation.
  - Downloads: 36
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - This dataset is a Japanese translation of the WikiHowNQA dataset, providing question-answering pairs based on WikiHow articles.
  - Downloads: 34
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - This dataset contains approximately 39,600 synthetic Japanese roleplay conversations, generated with gpt-4o-mini, each with 5-10 turns and detailed metadata including genre, tags, and character/scene settings, formatted for easy model training.
  - Downloads: 34
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - This dataset is a manually cleaned, Japanese translation of the WikiHow-NFQA question answering dataset.
  - Downloads: 33
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese instruction tuning dataset generated by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, alongside the code used for its creation, noting the potential for low-quality records due to a lack of post-filtering.
  - Downloads: 33
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository provides the 1.56B token text dataset‚Äîsourced from publicly available Japanese datasets like accommodation dialogues and movie recommendations‚Äîused for pre-training the AKU-d_ms-0.5B-chat-v0.1 model, with processing scripts and individual dataset licenses detailed within.
  - Downloads: 33
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - This dataset provides question-answer pairs derived from Japanese Stack Exchange data, processed with markdown formatting, base64 image replacement, and includes IDs for questions, answers, accepted, and popular responses in both default and simplified formats.
  - Downloads: 33
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - This dataset contains approximately 1,000 synthetic Japanese roleplay dialogues, each with 10 turns, generated using Nvidia's Nemotron-4-340B-Instruct and Magpie, potentially including lower-quality records and a tendency to prematurely end longer conversations.
  - Downloads: 32
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This repository provides a growing, manually-created dataset designed for training Japanese chatbots.
  - Downloads: 30
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - This repository provides a Japanese-only subset of the OpenAssistant Conversations Dataset, formatted as paired human-assistant messages‚Äîpotentially lacking full conversational context‚Äîfor dialogue research.
  - Downloads: 29
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 is a 200-question Japanese QA dataset derived from shunk031/JDocQA, featuring PDF questions converted to single 200dpi images and limited to 50 questions per question type for faster processing.
  - Downloads: 23
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - This dataset consists of human-checked and corrected instructions for open-source LLMs, with outputs generated using Swallow-MX, though accuracy isn't guaranteed, and was created during the LOCAL AI HACKATHON #000.
  - Downloads: 22
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This repository provides a sample of the Nexdata Japanese Conversational Speech dataset, featuring roughly 1000 speakers engaging in natural, manually-transcribed face-to-face conversations on diverse topics.
  - Downloads: 21
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - This repository provides a 100k Japanese question answering dataset with answer candidates generated and evaluated using Qwen models (0.6B & 14B) derived from the hotchpotch/japanese-qa-reasoning-100k dataset.
  - Downloads: 14
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW is a synthetic evaluation dataset for benchmarking Japanese language model roleplaying ability, featuring diverse settings, character configurations, and dialogue tones, licensed for broad use but derived from Claude 3.5 Sonnet outputs.
  - Downloads: 13
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - This dataset provides simple Japanese example sentences created with the `if001/elementray_m calm3-22b` model, covering various grammatical patterns like politeness, negation, desire, and speculation for language model training or evaluation.
  - Downloads: 11
### Semantic Text Processing
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a Japanese biomedical LLM benchmark dataset with a provided evaluation framework (med-eval) intended for assessing and improving performance in this domain.
  - Downloads: 3,764
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a Japanese text embedding benchmark comprising 24 datasets across 6 tasks for evaluating model performance on Japanese language understanding.
  - Downloads: 1,812
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - JGLUE is a novel, natively-Japanese NLU benchmark dataset created by Yahoo Japan and Waseda University to evaluate and advance general language understanding capabilities in Japanese.
  - Downloads: 1,726
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - This repository provides Japanese/English synthetic conversation datasets‚Äîderived from LMSYS-Chat-1M‚Äîused for post-training Llama-3.1-Swallow and Gemma-2 models.
  - Downloads: 1,301
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - This repository provides Japanese Wikipedia embeddings and a FAISS index for RAG applications, including demos, conversion scripts, and evaluations of various Japanese embeddings (including OpenAI's) for search and Q&A tasks, under a mixed license of CC-BY-SA-4.0 and OpenAI‚Äôs for specific files.
  - Downloads: 807
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - This repository provides a Japanese instruction-following (chat) dataset for fine-tuning LLMs‚Äîlike LoRA training for English-based models‚Äîwith updates addressing licensing changes and data quality improvements from sources like Alpaca, Wikipedia, and ALT.
  - Downloads: 751
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - This repository provides a Japanese subset‚Äîapproximately 200GB‚Äîextracted from the large multilingual FineWeb2-HQ dataset, comprising multiple JSONL files with text data and word/byte counts.
  - Downloads: 267
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - This repository merges and extracts lines of 256 characters or less from the neody/c4-ja-cleaned, neody/cc100-ja-cleaned, and neody/oscar-ja-cleaned Japanese datasets.
  - Downloads: 225
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - This repository provides a Japanese chat dataset‚Äîderived from izumi-lab/llm-japanese-dataset‚Äîfor fine-tuning large language models, particularly for instruction-response tasks using methods like LoRA.
  - Downloads: 224
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - This dataset provides simple conversational data for the "Zunda Mon" character, formatted for LLM-jp and ChatGPT, intended for LLM development and testing with a focus on respecting the licensing terms provided by („Åö„Éªœâ„Éª„Åç„Çá).
  - Downloads: 197
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - This repository provides a Windows executable for running the Japanese GPT-2 model (ggml-japanese-gpt2) using ggml and SentencePiece formats, requiring downloaded model binaries for execution, though the xsmall model currently has issues.
  - Downloads: 185
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - This repository provides a clean, 5.1M-sentence Japanese dataset with context, suitable for training unsupervised semantic similarity models.
  - Downloads: 153
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - This repository provides a dataset of Japanese Wikipedia sentences used in the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 147
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - This repository provides a Markdown-formatted, language-labeled version of the RyokoAI/ShareGPT52K dataset, utilizing tools for CJK whitespace, HTML-to-Markdown conversion, Chinese conversion, and language detection.
  - Downloads: 126
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository hosts databricks-dolly-15k-ja, a Japanese translation of the Dolly 15k instruction tuning dataset created collaboratively by LLM-jp.
  - Downloads: 101
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - SimpleStories is a diverse, annotated dataset of short stories generated by gpt-4o-mini (and soon other models), available in English and Japanese, built upon TinyStories and designed for easy NLP data filtering.
  - Downloads: 93
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - llm-jp-instructions is a manually created Japanese instruction dataset with train, development, and test splits accessible via the `datasets` library.
  - Downloads: 72
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - This repository provides a continuously updated, GPT-4 synthesized Japanese question-answering dataset for fine-tuning open-source, non-English language models.
  - Downloads: 67
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - This repository provides a Japanese fake news dataset, converted for use with Hugging Face datasets, featuring labeled text with details on content origin (real, partial GPT-2, or fully GPT-2 generated) and character counts for real and fake portions.
  - Downloads: 62
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - This repository provides the Japanese Wikipedia paragraphs dataset used in the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 60
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - This repository provides document-level merged versions of the cc100/cc100-ja Japanese text dataset, originally line-separated, while maintaining the original license.
  - Downloads: 51
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - Shisa-base-7b-v1 is pre-trained on a dataset of 90% Japanese and 10% English tokens sampled from MADLAD-400 using DSIR.
  - Downloads: 51
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - This dataset consists of bullet-point text generated from Japanese Wikipedia using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and DeepSeek-R1-Distill-Qwen-32B-Japanese, licensed under CC-BY-SA 4.0.
  - Downloads: 50
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This dataset provides Japanese passage data from the "AI King" competition, enhanced with binary passage embeddings generated using llm-book/bert-base-japanese-v3-bpr-passage-encoder, and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 46
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - This dataset, created by the University of Tokyo's Matsuo & Iwasawa Lab's 2024 LLM course, contains human-authored inputs and outputs from two language models‚Äîwatashiha-gpt-6b and Watashiha-Llama-2-13B-Ogiri‚Äîfor supervised fine-tuning (SFT) practice, intended for educational and research use only.
  - Downloads: 44
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - This repository provides a 69k Japanese-English coding dialogue dataset generated using models like Nemotron, Phi-3, Mixtral, and Calm3 via the Magpie method, along with the code used for its creation, noting potential quality variations due to minimal post-filtering.
  - Downloads: 44
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - This dataset provides 39.6k formatted Japanese roleplay conversations generated with gpt-4o-mini, including system messages, licensed under CC-BY-NC-SA 4.0, and prohibits use for competing with OpenAI services.
  - Downloads: 43
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 42
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG is a 20,000-record benchmark for evaluating the Japanese Retrieval-Augmented Generation (RAG) performance of Large Language Models, using (Context, Question, Groundtruth Answer) data.
  - Downloads: 42
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - This dataset provides Japanese multi-turn conversation data, formatted as ShareGPT, for fine-tuning large language models‚Äîrequiring significant compute resources due to long token lengths‚Äîbased on the OpenAssistant/oasst2 dataset.
  - Downloads: 41
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - This repository provides manually created variations of input method editor (IME)-style task and bracket matching tasks, developed to address weaknesses in a model created by @pokutuna for a 2024 University of Tokyo competition on large language model applications.
  - Downloads: 38
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - This dataset comprises conversational data generated by GPT-3.5-Turbo based on the Japanese Wikipedia (izumi-lab/wikipedia-ja-20230720), and is not licensed for commercial use.
  - Downloads: 38
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - This repository provides a Japanese translation of the Databricks Dolly 15k dataset, enhanced with BERTScore-calculated translation quality scores to identify and address issues like inconsistent phrasing and translation errors.
  - Downloads: 37
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - This dataset records Pok√©mon VGC Regulation F team selection data collected from YouTube battle streams, including data from the author (trainer_id 13), and was used for a presentation at the Remote Pok√©mon Society in May 2024.
  - Downloads: 36
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - This repository provides a filtered and modified Japanese/Chinese parallel dataset from WikiMatrix v1, enhanced with semantic similarity filtering (LaBSE, threshold 0.6) and Traditional to Simplified Chinese conversion using `zhconv`.
  - Downloads: 34
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - YokaiEval is a Japanese dataset of 810 multiple-choice questions designed to evaluate Large Language Models' knowledge of Japanese folklore, specifically concerning *yokai* (supernatural creatures), covering aspects like behavior, appearance, origins, and regional legends.
  - Downloads: 34
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - This repository hosts the datasets used for kanalizer, a library that predicts pronunciations from English words, with code at VOICEVOX/kanalizer and trained models at VOICEVOX/kanalizer-model.
  - Downloads: 33
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 33
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - This dataset provides Japanese example sentences generated with calm3-22b, covering various grammatical patterns including politeness, negation, desire, progressive tense, and more, with cleaned, failed generations removed.
  - Downloads: 33
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - This dataset comprises human-written (OSCAR) and LLM-generated (GPT-3.5 Turbo) Japanese text for evaluating LLM-generated text detection performance.
  - Downloads: 32
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-example Japanese coding dialogue dataset generated using the Magpie technique applied to Nvidia's Nemotron-4-340B-Instruct, along with the code used for its creation‚Äînote that the dataset lacks post-generation filtering and may contain low-quality examples.
  - Downloads: 30
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - This dataset provides 191k synthetic Japanese preference examples, generated using five open-source models (including Tanuki and Qwen) and judged by Qwen/Qwen2.5-72B, focusing on high-quality instructions and addressing potential positional bias.
  - Downloads: 30
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - DataPilot converted the kinokokoro/ichikara-instruction-003 Japanese instruction dataset into the widely-used ShareGPT format, providing JSON Lines data optimized for fine-tuning conversational large language models.
  - Downloads: 30
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, and built as an improvement upon TinyStories for NLP tasks.
  - Downloads: 30
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One designed to enhance realism, increase image complexity, and simplify anime-style illustration generation within the Stable Diffusion web UI.
  - Downloads: 28
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - This repository provides a model fine-tuned with both a public dataset and unique, personality-driven tweets, generating tweets and assigning scores ranging from 10 (excellent) to 8 (so-so) based on quality.
  - Downloads: 28
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - This repository provides an experimental dataset of 50 queries and evaluations‚Äîgenerated with ChatGPT-4o focusing on five aspects including patent attorney references (excluding direct referrals)‚Äîalong with manually created answers sourced from open patent databases.
  - Downloads: 28
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small is a Japanese synthetic dataset of high-quality prompts and AI outputs automatically generated using the Mistral Small 3.1 24B Instruct model, formatted as JSONL for supervised fine-tuning.
  - Downloads: 26
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - This dataset converts the Open-Platypus-Japanese-masked dataset into OpenAI message format for use in language model training and evaluation.
  - Downloads: 24
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This repository provides a Japanese translation of Databricks‚Äô Dolly project, licensed under CC BY-SA 3.0, utilizing and building upon data from sources like Wikipedia, also under the same license.
  - Downloads: 22
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - This repository provides a Japanese translation of the HelpSteer2 dataset, used for training and aligning large language models like Nemotron-4-430B-Reward within the NVIDIA SteerLM framework.
  - Downloads: 19
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - This repository provides a 26.5k Japanese instruction dataset created by applying Magpie methodology and Evol-Instruct techniques‚Äîusing models like Tanuki and Qwen‚Äîto cluster and refine an initial 100k instruction set, licensed under Apache 2.0 with Qwen license considerations.
  - Downloads: 19
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository provides a 60k Japanese synthetic instruction dataset, created using Self-Instruct with Qwen2.5-72B, building upon and expanding the Aratako/Magpie-Tanuki dataset, and subject to both Apache 2.0 and Qwen license terms.
  - Downloads: 17
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - This repository provides human-generated responses to the ELYZA-tasks-100 benchmark for Japanese LLM evaluation, achieving an average GPT-4o score of 3.69 and outperforming Claude 3.5 Sonnet with automated scoring.
  - Downloads: 17
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - This dataset is a Japanese role-playing learning dataset‚Äîa translation of Bluemoon_Top50MB_Sorted_Fixed_ja using karakuri-lm-8x7b-chat-v0.1-awq, processed with 3-shot prompting and DeepInfra, with some records shortened or removed due to length/LLM repetition issues and inaccurate token counts.
  - Downloads: 16
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - This dataset provides educational scores (0-4) for Japanese text from the fineweb-2 dataset, generated using the Deepseek API and a method inspired by the FineWeb-Edu classifier, comprising approximately 280k training and 30k testing examples.
  - Downloads: 16
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - This dataset provides Japanese example sentences created with calm3-22b, covering various grammatical patterns including politeness, negation, desire, progressive tense, and more to facilitate language model training and evaluation.
  - Downloads: 14
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset provides detailed, natural language overviews of VTubers‚Äîincluding their character, activities, collaborations, and style‚Äîcollected using the GPT-4o Search Preview API with a cost of $27.04 for 36,276 tokens.
  - Downloads: 13
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese temporal inference dataset designed to evaluate the generalization capacity of language models, featuring templates and data split by tense, format, and span.
  - Downloads: 12
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - This repository mirrors the non-Wikipedia portion of the LLM-jp Corpus v3, excluding content licensed under CC-BY-SA.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - This repository explores prompt extraction using the Magpie method and the rinna/llama-3-youko-8b language model, based on prompts from a research paper, despite the model not being instruction-tuned.
  - Downloads: 11
### Text Generation
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - This repository provides a clustered Japanese text corpus of approximately 10,000 items, created from cleaned web corpora like mc4-ja, for information analysis purposes, with files partially in Parquet format and available via Git LFS.
  - Downloads: 913
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard provides a Japanese RAG performance evaluation across five industries‚Äîfinance, IT, manufacturing, public sector, and retail‚Äîassessing Parser, Retrieval, and Generation components with publicly available datasets for informed RAG solution consideration.
  - Downloads: 432
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese language benchmark evaluating large language model performance via multiple-choice questions covering both translated MMLU subjects and uniquely Japanese cultural topics.
  - Downloads: 394
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual (English, Spanish, Japanese, Russian) benchmark with 1,707 test cases designed for evaluating NL-to-Code generation models in an open-domain execution setting.
  - Downloads: 200
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This dataset from YANS-official contains Japanese humorous responses (bokete) for three tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîsourced from the Bokete website and related to the CLoT-Oogiri-Go dataset, totaling 600 prompts and 2355 responses as of August 30th.
  - Downloads: 166
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - This repository provides a templated dataset of ~40 Japanese downstream tasks‚Äîup to 20,000 samples each with 0-shot and few-shot examples‚Äîdesigned for high-quality, non-machine-translated instruction tuning of LLMs.
  - Downloads: 116
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - This repository extracts text snippets of 256 characters or less from the cleaned OSCAR Japanese dataset (neody/oscar-ja-cleaned).
  - Downloads: 114
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15 & T23 is a 50,000-sentence Japanese corpus with aligned original, simplified, and English translations, designed for text simplification and Japanese-English translation using a 2,000-word vocabulary.
  - Downloads: 102
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - This Japanese preference dataset, built upon llm-jp/magpie-sft-v1.0, uses Aratako/Llama-Gemma-2-27b-SFT-trial1 to generate responses judged against Qwen/Qwen2.5-32B-Instruct outputs by google/gemma-2-27b-it, and is subject to META LLAMA 3.1, Gemma Terms of Use, and Qwen licensing (requiring attribution for models trained on it).
  - Downloads: 84
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - This 5.2K instruction dataset focuses on code‚Äîincluding generation, behavior checks, and bug fixes‚Äîderived from commercially-usable, and permissively licensed programming learning content (with English content translated to Japanese).
  - Downloads: 69
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - This repository provides a dataset of approximately 3000 Japanese children's stories, synthetically generated with GPT-4o-mini using only simple vocabulary, based on the methodology detailed in the linked research paper.
  - Downloads: 61
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - This dataset contains question-answer pairs automatically generated from Japanese Wikipedia using DeepSeek-R1-Distill-Qwen-32B-Japanese, and is released under a CC-BY-SA 4.0 license.
  - Downloads: 59
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - This repository provides a synthetic, 801k-item code dataset for supervised fine-tuning (SFT) of Japanese-English translation models, expanded from the Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k dataset using techniques like Evol-Instruct and including model information (Nemotron, Phi-3, Mixtral, Calm3) and evolution history.
  - Downloads: 50
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - This repository provides a prototype dataset, generated by Deepseek-V3-0324, designed to improve adherence to constrained system prompts, with code and data licensed under MIT.
  - Downloads: 46
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset (BCP-47 ja-JP) intended for advancing research in multimodal ad text generation models.
  - Downloads: 45
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This dataset provides question data generated using Qwen/Qwen2.5-32B-Instruct on Ollama, intended for building reasoning models, with a warning that only the questions are licensed under Apache 2.0 due to answer generation using another model (Mistral Large).
  - Downloads: 45
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - This Japanese dataset provides 10K-100K question-answer pairs labeled with "evil" and "justice", generated using anthracite-org/magnum-v4-12b, for both classification and generation tasks under an Apache-2.0 license.
  - Downloads: 45
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This dataset from Bokete, a Japanese humor site, provides text, image, and text-image pairs for three tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîtotaling 56 prompts and 112 responses, originally part of the CLoT-Oogiri-Go dataset.
  - Downloads: 45
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This repository provides a Japanese translation of the English quotes dataset from Hugging Face, created using the llm-jp/llm-jp-3-3.7b-instruct model and licensed under CC BY 4.0.
  - Downloads: 42
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - This repository provides automatically generated Japanese Q&A data, created using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF and diverse sources including team-created data and Common Crawl (subject to its terms of use), with intentionally low textual similarity to the original sources, and requires cleaning due to potential unnatural phrasing.
  - Downloads: 38
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository provides a Japanese preference dataset for SimPO training, created by scoring and selecting responses generated with Llama-Gemma-2-27b based on Qwen/Qwen2.5-72B, and is subject to META LLAMA 3.1 and Gemma licensing.
  - Downloads: 38
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This dataset is a Japanese translation of the Open_o1_sft_Pro dataset, created using Qwen2.5-14B-Instruct and formatted with user/assistant conversation templates, adhering to the original dataset's license.
  - Downloads: 38
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - This dataset converts level 2 filtered data from the llm-jp-corpus-v3 warp_html into Hugging Face format, adding article titles sourced from the original URLs, all under a CC-BY 4.0 license.
  - Downloads: 38
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This dataset facilitates evaluation of large language models on humorous response generation, featuring both text-to-text (senryu/haiku creation from prompts) and image-to-text (captioning/one-liner generation) tasks.
  - Downloads: 37
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends the CommonCatalog CC-BY dataset with English dense captions generated by Phi-3 Vision and their Japanese translations by Phi-3 Medium, accessible via a CSV or Hugging Face dataset for easy streaming and commercial use under the CC BY license.
  - Downloads: 36
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset provides Japanese instruction data, translated using KUJIRA, focused on investment topics including Berkshire Hathaway and Warren Buffett, originally from the glaive-ai's in-foxhound dataset.
  - Downloads: 35
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - This repository provides a Japanese Reinforcement Learning from Human Feedback (RLHF) dataset, reformatted as a classification task with chosen/rejected sentence labels (1/0), based on the open_preference_v0.1 dataset, acknowledging potential quality issues due to synthetic and translated text.
  - Downloads: 35
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - This repository provides automatically generated Q&A data sourced from Common Crawl, utilizing MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, with random text excerpts to minimize reliance on original content, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 34
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - This dataset converts a subset of the LLM-JP-Corpus-v3 into Hugging Face format, augmenting it with article titles retrieved from associated URLs, all licensed under CC-BY 4.0.
  - Downloads: 34
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - This dataset provides 3,907 Japanese news articles from Livedoor News with 3-line summaries, formatted with prompts for Llama v2 and recommending the addition of `[R_START]` and `[R_END]` as special tokens for training.
  - Downloads: 33
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - This repository provides a Japanese dataset generated by ELYZA-japanese-Llama-2 for evaluating AI text detection and self-instruct methods, sourced from GPT-4-Self-Instruct-Japanese with licensing based on the underlying model.
  - Downloads: 30
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - This repository provides Japanese translations of the aimo-validation-aime dataset, generated using ChatGPT-4o, with a unique 0-30 index differing from the standard AIME24 format.
  - Downloads: 29
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This repository provides a Japanese translation of the NLVR (Natural Language Visual Reasoning) dataset, originally developed for multimodal reasoning tasks.
  - Downloads: 28
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - This repository provides a Japanese dataset generated by Qwen/Qwen1.5-14B, designed for evaluating AI text detection and compatible with self-instruct techniques, utilizing instructions from the GPT-4-Self-Instruct-Japanese dataset.
  - Downloads: 27
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset, created using DeepSeek-v2.5 and null-instruct-ja with ollama on 7 A5000 GPUs, is licensed under the DeepSeek license and acknowledges contributions from DeepSeek-ai, null-instruct-ja's Googlefan, and MDL.
  - Downloads: 26
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - This repository presents a JSON dataset (reazonspeech-all-wada-snr.json) containing SNR values and transcriptions for Reazon Speech V2 audio data analyzed with WADA SNR, including a count of 1,208,360 entries with SNR values exceeding 100, and acknowledges AiHUB for providing computational resources.
  - Downloads: 25
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 22
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository provides a Japanese preference dataset for iterative Direct Preference Optimization (DPO) created by scoring responses generated with Llama-Gemma and Qwen models against the Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k instruction dataset, adhering to Meta Llama 3.1, Gemma, and Qwen licenses.
  - Downloads: 17
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - This repository provides a Japanese-English glossary of Generative AI terminology, intended to improve translation quality with models like GPT-4, though accuracy isn't guaranteed.
  - Downloads: 16
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - This repository provides 1k Japanese responses generated using DeepSeek-R1-Distill-Qwen-32B, based on the aya-ja-evol-instruct-calm3-dpo-masked dataset, with noted accuracy limitations and issues generating `<think>` tokens for potential use as a reference or for preprocessing.
  - Downloads: 13
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - This repository provides a synthetic instruction dataset, Jimba-Wiki-Instruction-Calm3, created using the Calm3-22B-Chat model and Japanese Wikipedia data, exhibiting relatively low hallucination but requiring caution due to unfiltered, raw outputs.
  - Downloads: 11
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository provides a dataset of all *Keitai Oogiri* (mobile comedy challenge) questions and responses from NHK broadcasts, crawled from a Hatenablog archive and structured into columns including question ID, episode, type, question text, and a list of responses with their IDs.
  - Downloads: 11
### Syntactic Text Processing
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This repository integrates data from erai-raws and MyAnimeList for 2056 anime, providing IDs and resources like RSS feeds, and publication dates across platforms including AniDB, Kitsu, LiveChart, and MAL.
  - Downloads: 308
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - This dataset provides human-annotated preferences‚Äîincluding strength from -3 to 3‚Äîfor paired question-response sets in Japanese, covering diverse topics like general knowledge, history, medicine, coding, and creative writing, while acknowledging limited variation within and across genres.
  - Downloads: 275
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - This dataset contains 2139 human-evaluated question-answer pairs and preference judgments collected from LLMChat, a system comparing responses from 13 LLMs (including Tanuki and Llama-3) during a week-long evaluation in August 2024, similar to Chatbot Arena.
  - Downloads: 272
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - This repository provides Japanese datasets transformed for easy SentenceTransformers training, particularly contrastive learning, sourced and filtered from several Hugging Face datasets using rerank scores to create (anchor, positive/negative) pairs.
  - Downloads: 258
- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
  - This dataset facilitates author identification modeling by providing cleaned paragraphs from 17 Japanese authors, focusing on stylistic features rather than specific works, with controlled data balancing and paragraph length (100-400 characters) to avoid overlap between training and validation sets.
  - Downloads: 201
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully is a commercially-usable, multi-lingual dataset designed to improve LLM safety, prohibiting use for bypassing safety measures and requiring attribution for derivative works, while acknowledging it contains potentially harmful content.
  - Downloads: 176
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - This repository provides Japanese text data extracted from PDF files sourced from Common Crawl.
  - Downloads: 128
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)
  - This dataset provides high-quality, human-verified synthetic data for fine-tuning reasoning models, featuring question-answer pairs with detailed `<think>`-tagged thought processes illustrating the reasoning steps.
  - Downloads: 115
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki is a text dataset extracted from a Japanese Wikipedia HTML dump (as of January 1, 2024) offering structured, clean text with preserved paragraphs and supporting data for various NLP tasks, alongside preprocessing scripts.
  - Downloads: 106
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - This repository provides a dataset of approximately 150,000 Danbooru tag pairs with Japanese translations, filtered using fasttext and the calm3-22b-chat LLM to improve accuracy and ensure at least one Japanese translation per tag.
  - Downloads: 90
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - This repository provides a multilingual dataset released under the MIT license.
  - Downloads: 78
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - This repository provides a modified parsing and chunking method for Wikipedia data, pre-processed with a fork of singletongue/wikipedia-utils and crawled between December 5-8, 2023.
  - Downloads: 75
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - Alpaca-jp-python is a Japanese synthetic dataset, curated by HachiML under the Apache 2.0 license, generated using the Stanford Alpaca methodology and the mistralai/Mixtral-8x22B-Instruct-v0.1 model, with cleaned versions available via the `datasets` library.
  - Downloads: 61
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - This dataset contains approximately 2800 high-quality (beauty score 87+) synthetic female images, specifically versions 2.1 & 2.6, designed to mitigate portrait rights issues in realistic AI model training, with over 1000 images scoring 90+ for beauty.
  - Downloads: 59
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - This dataset annotates the Aratako/Magpie-Tanuki-8B-97k dataset‚Äîcreated by applying the Magpie method to Tanuki-8B‚Äîwith difficulty, quality, and category labels using cyberagent/calm3-22b-chat to assess instruction complexity.
  - Downloads: 59
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - This repository provides a validated dataset of Furigana (reading aids) derived from the National Diet Library‚Äôs bibliographic data, with 5064 inconsistencies corrected.
  - Downloads: 56
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This repository provides a dataset of fungal traits extracted from descriptions using natural language processing, currently for casual use only, with potential inaccuracies due to automated extraction‚Äîdata last updated December 29, 2023.
  - Downloads: 55
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - This repository provides a CBZ-format manga dataset from Nhentai‚Äîcontaining adult content‚Äîfor research in image analysis (segmentation, detection) and Japanese text recognition.
  - Downloads: 48
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k is a 97,269-record Japanese dialogue dataset created by applying the Magpie method to Tanuki-8B-dpo-v1.0, potentially containing low-quality entries due to lack of post-filtering.
  - Downloads: 48
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - This repository provides a furigana dataset derived from Aozora Bunko and Sappie braille data, with 307 validation-identified mismatches removed from the original corpus.
  - Downloads: 44
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - This repository provides fun stickers featuring „Çã„Çä (Ruri).
  - Downloads: 40
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - This repository likely provides resources or a project related to the popular Japanese character "Chiikawa" („Å°„ÅÑ„Åã„Çè), inspired by the "hachiwari/„ÅØ„Å°„Çè„Çå" creator.
  - Downloads: 40
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSEC„Éõ„Éº„É†„Éö„Éº„Ç∏ likely provides information and resources related to the Japan System Engineering Consortium (JSEC).
  - Downloads: 39
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - This repository provides a processed dataset of 50,000 Delite posts by t_w, optimized for embedding learning, with usage permitted for training but redistribution prohibited under Japanese law due to the lack of a license.
  - Downloads: 37
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - This repository provides a Japanese question-answer dataset of approximately 1,300 pairs manually created for Databricks, sourced from official blogs, FAQs, and Qitta articles by Databricks employees.
  - Downloads: 34
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja is a 525k instruction-tuning dataset created by automatically translating the open-source, high-quality ApolloCorpus (originally in English) into Japanese for use with medical LLMs, with a caveat regarding potential translation errors.
  - Downloads: 34
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - This repository scrapes metadata (excluding full text) from the Dengeki Bunko novecomi novel website.
  - Downloads: 34
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Japanese LLaVA v1.5 Instruct 620K is a 620K-image Japanese translation of the LLaVA v1.5 Visual Instruct dataset, licensed for non-commercial use and adhering to OpenAI policies.
  - Downloads: 33
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - This repository provides a 280-image dataset of AI-generated female illustrations (created with Niji Journey v5) for LoRA model training, including some copyrighted characters, with accompanying tags, intended for transparent model merging but subject to usage restrictions.
  - Downloads: 32
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - This dataset contains freely reusable quiz questions sourced from Quiz Forest as of August 5, 2024, suitable for RAG, document search, and other applications, with a license permitting broad secondary use while respecting Quiz Forest and related parties.
  - Downloads: 31
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - This repository provides voice data for Chiaki Nanami from the *Danganronpa* series.
  - Downloads: 30
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - This repository provides a pre-processed dataset of 50,000 Delite posts by t_w, improved for embedding learning with structural changes and intended for research use only ‚Äì redistribution is prohibited under Japanese law.
  - Downloads: 30
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - This repository provides automatically generated question-answer pairs, created using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF from modified, permissively licensed data sources (including Wikipedia, research corpora, and datasets like Dolly and OASST) with randomized text excerpts to reduce similarity, though cleaning is recommended.
  - Downloads: 29
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - This repository provides the first 300,000 entries of the CommonCatalog-CC-BY-JA dataset, pairing captions generated by alfredplpl/commoncatalog-cc-by-ja with images resized to within 512px.
  - Downloads: 29
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - This repository provides a 3-turn multi-turn instruction dataset generated with Qwen/Qwen2.5-32B-Instruct-AWQ, containing mixed English and Chinese data sourced from Aratako/Magpie-Tanuki-8B-annotated-96k and requiring potential filtering.
  - Downloads: 29
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data is provided.
  - Downloads: 25
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and acknowledging support from IPA (Information-technology Promotion Agency).
  - Downloads: 23
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - This repository provides a Japanese mathematics question-answering dataset of approximately 950 examples, translated from MetaMathQA using RekaAI/reka-flash-3, with formatting issues removed‚Äîrequiring further output verification and cleaning.
  - Downloads: 17
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, formatted as JSON with optional Chain-of-Thought reasoning, and licensed under Apache-2.0.
  - Downloads: 13
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthetic dataset, created using the Evol-Instruction method and Mistral AI's Mixtral-8x22B-Instruct-v0.1, based on Stanford Alpaca's seed tasks and licensed under Apache 2.0.
  - Downloads: 13
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - This repository provides a 26,728-dataset subset of annotated Japanese instruction-following data, generated with Qwen-2.5-turbo, for fine-tuning small Japanese chat LLMs‚Äîexcluding coding‚Äîfocused on information seeking, reasoning, planning, and editing tasks with high-quality, easy-to-medium difficulty inputs.
  - Downloads: 11
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - This dataset contains freely reusable quiz questions sourced from Quiz Works as of August 4-5, 2024, suitable for RAG and document retrieval system development.
  - Downloads: 11
### Responsible NLP
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, offered for research purposes with acknowledgment to IPA (Information-technology Promotion Agency) and requiring copyright permission for non-research use.
  - Downloads: 1,400
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 886
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to the IPA (Information-technology Promotion Agency) for resources.
  - Downloads: 775
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset comprises manually extracted FAQs from Japanese government websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models and also serving as a linked resource.
  - Downloads: 515
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository provides the Japanese Web Corpus 2010, a dataset for research purposes with automatically added punctuation via morphological analysis, subject to copyright limitations.
  - Downloads: 269
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - This dataset contains query-answer pairs generated by an LLM, based on paraphrased text from Japanese Wikipedia, and is released under the CC-BY-SA 4.0 license.
  - Downloads: 250
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - This repository provides a cleaned and deduplicated query-passage dataset (mqa) with pre-processed text, where passage IDs correspond to indices within the `collection` data for easy access, adhering to the original dataset's license.
  - Downloads: 140
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - This repository provides texts regenerated with Phi-3 from large (tens of GB) datasets‚ÄîWikibooks, Wikipedia, Cosmopedia, and legal case data‚Äîpotentially requiring Git LFS for full download, with some computations performed on the TSUBAME4.0 supercomputer.
  - Downloads: 109
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This dataset provides filtered Japanese Wikipedia typo data‚Äîspecifically kanji conversion errors‚Äîsplit into pre- and post-error text segments for use with Hugging Face models.
  - Downloads: 98
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - This repository provides a large, automatically translated Japanese-English corpus generated by re-generating Japanese text (from Wikibooks, Wikipedia, and code) with Phi-3, potentially requiring Git LFS for full dataset access due to its multi-gigabyte size.
  - Downloads: 87
- [reep0610/AGI-japanese-text-dataset-for-Deep-Learning](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-for-Deep-Learning)
  - This repository implements Self-Descriptive Autonomous Deep Learning, a framework enabling AI to learn without external rewards by forming internal representations and achieving stable working memory, long-term context retention, and self-organization‚Äîultimately aiming for self-awareness and semantic understanding.
  - Downloads: 52
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This repository provides a Japanese-only subset of the Open Assistant dataset, sourced from the larger `timdettmers/openassistant-guanaco` dataset on Hugging Face.
  - Downloads: 38
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - This dataset converts the oasst1-89k-ja Japanese dataset into a ShareGPT-formatted, multi-turn conversational format suitable for fine-tuning large language models, requiring significant computational resources.
  - Downloads: 37
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - This repository provides data based on Japan Post's May 9, 2024, English/Chinese translations of item descriptions and HS codes for international mail, with details available at the linked website.
  - Downloads: 36
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - This repository provides Japanese prompts from the GuanacoDataset, identified and extracted using language detection.
  - Downloads: 36
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - This repository provides automatically generated Japanese question-answer pairs for RAG systems, sourced from Wikibooks, Wikipedia, and case law data, intended for pre-training rather than instruction tuning, with some computations leveraging the TSUBAME4.0 supercomputer.
  - Downloads: 34
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and requiring adherence to all applicable laws and responsible disclosure to third parties.
  - Downloads: 34
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst2 dataset, converted into a chat/instruction-following format suitable for fine-tuning language models, utilizing DeepL translation and a provided conversion script.
  - Downloads: 34
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - This dataset contains Japanese translations of English Wikipedia text generated using DeepSeek-R1-Distill-Qwen-32B, including input/raw output and licensed under CC-BY-SA 4.0.
  - Downloads: 32
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset comprises 1243 carefully selected tweets (May 2022 - May 2024) known for expressing nuanced ideas or unique perspectives, intended for fine-tuning language models‚Äîspecifically for tasks like personality expression and tweet generation with fixed system prompts and inputs.
  - Downloads: 28
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting enjoyment-based use, disclaiming quality guarantees, and requiring legal compliance from all users and distributors.
  - Downloads: 28
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - This repository provides a low-quality dataset of 1002 examples for equipping chat LLMs with Python function calling, generated using Qwen2.5 and Phi-4, and containing potential issues with missing/Chinese tools and repetitive/low-quality responses.
  - Downloads: 27
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - This repository provides automatically generated multi-turn dialogue data created with Calm3-22B-chat, based on randomly extracted text from the Aozora Bunko library, specifically using a cleaned version of "I Am a Cat."
  - Downloads: 27
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - This dataset contains Japanese sentences generated by Phi-3 based on ConceptNet 5.7 triples, using a specific prompt to express relationships between subjects, relations, and objects.
  - Downloads: 21
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 17
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - This dataset provides a deduplicated, preprocessed version of the mmarco query-passage pairs, with IDs referencing indices within the `collection` subset for direct data access, and adheres to the original dataset's license.
  - Downloads: 16
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset contains cleaned lines from the anime ‚ÄúMy Next Life as a Villainess,‚Äù featuring dialogue primarily from the character Lay and responses from Claire, with no usage responsibility assumed by the creator.
  - Downloads: 15
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publishers.
  - Downloads: 14
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - This repository provides publicly available models and datasets under a license prohibiting enjoyment of the expressed ideas/feelings and disclaiming any warranty or liability for their use, requiring legal compliance from both users and those they share it with.
  - Downloads: 14
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the LLM-JP 3.13B Instruct model, featuring instructions, optional Chain-of-Thought responses, and self-refined final responses in JSON format.
  - Downloads: 13
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This repository provides a 16,000-sample Japanese instruction-following dataset, automatically generated using Qwen2.5 72B Instruct, containing instructions, reasoning steps, initial & refined responses for LLM training and evaluation.
  - Downloads: 12
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and requiring adherence to legal compliance and responsible distribution.
  - Downloads: 12
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 11
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - This repository provides publicly available models and datasets with a usage agreement prohibiting commercial enjoyment of the content and requiring adherence to legal compliance by both users and any third parties it's shared with.
  - Downloads: 11
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset provides question-answer pairs about characters from the Touhou Project's Tokama Club, suitable for training chatbots and question answering/machine learning systems.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA (Information-technology Promotion Agency).
  - Downloads: 11
### Reasoning
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a synthetically generated, assured-correct Japanese mathematical dataset, created by translating English PRM800K & GSM8K problems and providing chain-of-thought reasoning.
  - Downloads: 142
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - This dataset, abc-multiple-choice, is a Japanese multiple-choice question answering set based on questions from the ‚Äúabc‚Äù quiz competition, intended for research purposes only, as detailed in the NLP2024 workshop paper.
  - Downloads: 130
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - JSNLI is a Japanese natural language inference (NLI) dataset, a translation of the SNLI benchmark, provided in TSV format with morphologically analyzed text using JUMAN++.
  - Downloads: 129
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - This repository provides a commercially-usable, 1.8 million instruction-tuning dataset ‚Äì a Japanese translation of OpenMathInstruct-1 ‚Äì focused on mathematics, generated with Mixtral-8x7B and validated against GSM8K & MATH benchmarks under a permissive NVIDIA license.
  - Downloads: 117
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a human-crafted Japanese dataset of multi-turn conversations and passages for training and evaluating logical reasoning capabilities, demonstrated with Qwen2.5-7B models on Japanese MT-Bench.
  - Downloads: 106
- [li-lab/JPMedReason](https://huggingface.co/datasets/li-lab/JPMedReason)
  - JPMedReason is a translated Japanese version of the MedReason benchmark dataset, designed to evaluate complex clinical reasoning in LLMs via multiple-choice questions with English and Japanese chain-of-thought reasoning.
  - Downloads: 106
- [YUGOROU/Counseling-Reasoning-v1.8](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.8)
  - This dataset provides 917 counseling interactions with chain-of-thought (CoT) reasoning, generated by Manus AI, for problem-solving and answer generation research.
  - Downloads: 99
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using the Qwen3-235B-A22B model, formatted as JSONL for instruction tuning.
  - Downloads: 93
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset is a small, commercially usable, high-quality Japanese dataset for commonsense reasoning and math problem solving, built upon existing datasets and licensed under DbCL v1.0.
  - Downloads: 89
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - This repository provides a 17k synthetic instruction dataset, generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, and filtered for consistent answers from two system prompts focused on logical/mathematical reasoning and Python-based problem-solving.
  - Downloads: 78
- [YUGOROU/Counseling-Reasoning-v1.9-parquet](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.9-parquet)
  - This dataset provides 917 counseling conversations with Chain-of-Thought (CoT) solutions generated by the Manus AI model, including problem statements, expected answers, and metadata for research on problem-solving and response generation.
  - Downloads: 71
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - JSQuAD is a Japanese reading comprehension dataset, based on SQuAD 1.1 and utilizing a 2021 Wikipedia dump, designed for evaluating and reproducing scores with SB Intuitions.
  - Downloads: 70
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - This repository offers a 50k-subset of the NuminaMath CoT dataset, enhanced to promote reflective, multistep reasoning in Japanese language models by encouraging repeated self-evaluation.
  - Downloads: 65
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - JCommonsenseQA is a Japanese multiple-choice question answering dataset, based on CommonsenseQA and ConceptNet, designed to evaluate commonsense reasoning ability and sourced from the JGLUE benchmark.
  - Downloads: 56
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - This dataset provides Japanese question-answer pairs, reasoning traces, and corresponding text excerpts generated using DeepSeek-R1, based on the fineweb2-edu-japanese dataset and licensed under ODC-By.
  - Downloads: 55
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - This dataset converts the magpie-qwen2.5-32b-reasoning-100k dataset into OpenAI message format for easier use with large language models.
  - Downloads: 49
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - This repository provides a high-quality, 1800-example Japanese instruction-following, reasoning, and answer dataset generated using the Qwen/Qwen2.5-32B-Instruct model, based on SkunkworksAI/reasoning-0.01.
  - Downloads: 46
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - This repository provides a 100k question-answering dataset in Japanese, generated using Gemini Pro 2.5, and licensed under the Open Data Commons Attribution family.
  - Downloads: 45
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - This dataset is a filtered version of DeL-TaiseiOzaki's magpie-reasoning-llama-nemotron-70b-100k, containing only examples without "ÊîπËâØ" in the refined_answer column and formatted for OpenAI messages.
  - Downloads: 44
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE (JNLI) is a Japanese Natural Language Inference dataset for evaluating relationships ‚Äì entailment, contradiction, or neutral ‚Äì between premise and hypothesis sentences.
  - Downloads: 41
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - This dataset consists of synthetic instruction-following data generated with Magpie using the rinna/qwen2.5-bakeneko-32b-instruct model, filtered for consistency between responses from two distinct system prompts focused on logical/mathematical assistance and Python-based problem-solving.
  - Downloads: 34
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - This repository provides a 125,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, featuring diverse, multi-persona instructions and Chain-of-Thought reasoning, in JSONL format under the Apache-2.0 license.
  - Downloads: 33
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - This repository provides a 100k Japanese question answering and reasoning dataset, evaluated using Qwen/Qwen3-14B, and acknowledges support from the TPU Research Cloud program.
  - Downloads: 32
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository provides a synthetic, Japanese-language multi-turn conversation dataset‚Äîderived from cosmopedia‚Äîfeaturing high information density encompassing reasoning, knowledge, and conversational exchanges.
  - Downloads: 31
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suite‚Äîbased on and extending the FraCaS dataset‚Äîfor evaluating textual entailment by assessing native speaker judgments of implication, neutrality, or contradiction between premise-hypothesis pairs.
  - Downloads: 30
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - JCQ is a Japanese NLP dataset of 700 creative thinking questions‚Äîbased on the Torrance Test and recent research‚Äîdesigned to evaluate creativity across tasks like unusual uses, consequences, and hypothetical scenarios.
  - Downloads: 27
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench is a Japanese reasoning benchmark utilizing challenging mathematics entrance exam questions from Kyoto University to evaluate the advanced problem-solving skills of Large Language Models.
  - Downloads: 26
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - This repository provides a 100k-sample Japanese translation of the NuminaMath CoT dataset, featuring 860k math problems with Chain of Thought solutions, translated using the Gemma-2-27b-it language model.
  - Downloads: 25
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - OpenO1-SFT (Japanese Translation) is a 77,312-sample dataset of Japanese Chain of Thought reasoning examples, translated from the original OpenO1-SFT using google/gemma-2-27b-it, intended for language model fine-tuning.
  - Downloads: 23
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa is a 210-question, multi-turn Japanese language benchmark evaluating reasoning across seven domains‚Äîmath, writing, coding, understanding, grammar, culture, and general logic‚Äîwith 15 tasks per category.
  - Downloads: 22
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - This dataset consists of 200 simplified instruction-following examples derived from the Kendamarron/jimba-instuction-1k-beta dataset, created to replicate the "Wizard LM" In-depth evolving process as a result of the LOCAL AI HACKATHON #000.
  - Downloads: 14
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Responsible & Trustworthy NLP
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - This repository releases a 42k Japanese-English parallel dataset‚Äîa subset of Swallow-Magpie-Ultra-v0.1‚Äîfor instruction tuning Llama-3.1-Swallow models.
  - Downloads: 155
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec is a machine learning framework and dataset for gender detection from Japanese names, detailed in a research paper accepted at ISDA'23, intended for research purposes only.
  - Downloads: 120
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - JaNLI is a Japanese adversarial Natural Language Inference (NLI) dataset‚Äîinspired by HANS‚Äîdesigned to test model understanding of Japanese linguistics and highlight potential vulnerabilities.
  - Downloads: 119
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA is a 1402-question Japanese dataset for red teaming, evaluating LLM vulnerability to generating harmful responses through adversarial prompts, and contains potentially offensive content.
  - Downloads: 70
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset is a Japanese language dataset for evaluating and mitigating toxicity in large language models, available via the linked URL.
  - Downloads: 40
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - This repository provides a dataset of Japanese medical national examination questions (NMLE, 110th-117th exams) for model evaluation, RAG, and overviewing exam content, licensed under CC-BY-NC-ND 4.0 for non-commercial use.
  - Downloads: 36
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository provides a dataset detailing game states with unit-level information‚Äîincluding ID, class, team, location, and visibility‚Äîacross various timestamps for strategic game analysis.
  - Downloads: 17
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - This dataset provides a 10,341-hour sample of unsupervised Japanese speech covering 28 domains, designed for improving AI model performance while prioritizing data privacy and legal compliance.
  - Downloads: 16
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - This repository contains pairwise comparison data evaluating responses from two LLMChat models using various other models, created to verify the consistency between human and automated (open LLM) evaluations, and adheres to the team-hatakeyama-phase2/LLMChat dataset license, potentially restricting model training with some outputs.
  - Downloads: 13
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - This repository provides access to code and instructions for a dataset intentionally kept private to prevent its inclusion in large language model training.
  - Downloads: 11
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - Cauldron-JA is a Japanese vision-language dataset derived from the large-scale Idefics2 training set (excluding OCR, coding, and graph data) translated using the DeepL API.
  - Downloads: 29,664
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This repository provides a Japanese translation of the Databricks Dolly 15k dataset, licensed under CC-BY-SA-3.0.
  - Downloads: 688
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst1 dataset, flagged for translation success/failure ("ng_translation": 0/1) and includes manually corrected code-related translation errors.
  - Downloads: 116
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository provides a collection of ~40 natively Japanese, open-source datasets for downstream tasks and LLM instruction finetuning, avoiding machine translation.
  - Downloads: 103
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN is a Japanese-translated, deduplicated subset of the CT-RATE dataset, offering chest CT volumes and radiology reports for developing Japanese medical AI.
  - Downloads: 73
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - This repository provides a Japanese-Korean paired text dataset from Tatoeba-Challenge for training translation models, excluding commercial use.
  - Downloads: 55
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This repository provides a 69K-example Japanese translation of the databricks-dolly-15k dataset, licensed under CC BY SA 3.0.
  - Downloads: 43
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - This repository provides a 260k-sentence parallel corpus of Japanese laws sourced from the Japanese Law Translation database, enabling research in Japanese-English legal translation.
  - Downloads: 26
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka is a dataset of Japanese ‚Äúkaidan‚Äù (ghost stories) linked to the Hyakumonogatari ritual, offering resources for exploring Japanese folklore and supernatural tales.
  - Downloads: 21
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated Japanese dataset‚Äîcomprising 6,537 training and 995 test samples‚Äîderived from the ultrachat_200k dataset using DeepSeek-R1-Distill-Qwen-32B-Japanese, with potential missing IDs.
  - Downloads: 11
### Information Retrieval
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - This repository provides reranking scores for existing Japanese search and QA datasets, using five multilingual/Japanese rerankers to score positive and negative example relevance, including average scores per example.
  - Downloads: 458
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - This repository provides AutoWikiQA, the largest freely available Japanese question-answer dataset generated using Swallow-MX from Wikipedia text, characterized by diverse question-answer formats and intended for QA model training or RAG model development.
  - Downloads: 200
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - This dataset provides QA pairs for training document retrieval models, sourced from cl-tohoku/quiz-datasets and the ‚ÄúAI King‚Äù competition, with varied licensing including CC BY-SA 4.0, CC BY-SA 3.0, and GFDL for quiz questions and Wikipedia passages.
  - Downloads: 98
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset provides automatically translated Japanese versions of records 20k-100k from the Cosmopedia-100k dataset, excluding entries with translation errors, and will be merged with and eventually removed from the 0-20k translation dataset.
  - Downloads: 53
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
  - JCrRAG is a 20,000-record Japanese RAG benchmark evaluating a system's ability to accurately process *given* context for question answering across diverse, multi-level complexity categories.
  - Downloads: 50
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - This dataset provides Japanese question-answering pairs with human-retrieved Wikipedia articles, detailing the data collection process and structure for research purposes.
  - Downloads: 49
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This repository provides the passage dataset used in the ‚ÄúAI King‚Äù competition featured in the book *Introduction to Large Language Models*, sourced from cl-tohoku/quiz-datasets and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 36
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - This repository provides 1 million synthetic, multi-turn chat entries‚Äîderived from rewritten web text using long-context LLMs‚Äîfor continued pre-training and research into data/internet culture, building upon the Refined-Anime-Text dataset.
  - Downloads: 33
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - This dataset provides short, consecutive-sentence passages (under 400 characters) from the April 2022 Japanese Wikipedia snapshot, used for question answering baselines like those in the AIÁéã competition.
  - Downloads: 26
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - This dataset provides JSONL-formatted metadata for YouTube channels, labeled as either VTuber (1) or non-VTuber (0), to facilitate text classification model training and evaluation, primarily in Japanese with potential multilingual content, and licensed under MIT.
  - Downloads: 18
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - This dataset archives 11 years of historical comments from the now-defunct "NikoNiko Realtime Commentary" service, preserving a valuable record of online discussions before its transition to NikoNiko Live Broadcasting.
  - Downloads: 910,399
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100 is a 100-example Japanese instruction-tuning evaluation dataset featuring complex tasks‚Äîincluding summarization, reasoning, and creative generation‚Äîwith annotated evaluation criteria for assessing helpful AI assistant performance.
  - Downloads: 2,493
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - This dataset provides a binary sentiment classification version of the WRIME Japanese sentiment analysis dataset, labeled as positive/negative based on Reader Sentiment scores, and intended for use with the "Large Language Model Introduction" book's sample code.
  - Downloads: 582
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
  - This repository generates and filters Japanese sentences using the phi4 model, covering 53 grammatical patterns with a 2364-vocabulary base, focusing on common sentence structures like politeness, negation, desire, and obligation.
  - Downloads: 303
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - This repository provides the Sayoko TTS corpus, an 81-year-old female voice dataset with noisy (wav_noise) and denoised (wav) audio files, alongside phoneme labels, downloadable from Google Drive or Hugging Face Hub for use in speech synthesis tasks.
  - Downloads: 155
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - This dataset adapts the databricks-dolly-15k dataset to emulate the emotionless speaking style of Yuki Nagato from "The Melancholy of Haruhi Suzumiya" by modifying Japanese politeness levels.
  - Downloads: 39
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - This repository provides a Japanese instruction dataset created by manually checking and correcting the output of the calm2-7b-chat model, detailed in the linked Zenn article.
  - Downloads: 25
### Linguistics & Cognitive NLP
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - NewsQ is a freely available Japanese question answering benchmark for current affairs, accessible via Hugging Face after agreeing to terms of use and providing application details for access verification and communication regarding usage and updates.
  - Downloads: 49
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - This repository provides FuguMT-translated and refined English text from the `chosen` dataset within the `helpful-base` portion of the anthropics/hh-rlhf project, filtering out poorly translated examples.
  - Downloads: 36
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - ‚ÄúNewsQ‚Äù is a free, Japanese-language question-answering benchmark for current events, distributed via Hugging Face with access granted upon agreeing to terms and providing application details for verification and communication regarding usage and updates.
  - Downloads: 21
