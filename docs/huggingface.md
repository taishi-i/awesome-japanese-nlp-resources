# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 1171 models and 435 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents

 * [Models](#models)
 * [Datasets](#datasets)


## The latest additions üéâ

**Models**
21 models have been added.

- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)


**Datasets**
5 datasets have been added.

- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)


## Models

This list is sorted by downloads as of February 04, 2025.
1171 models are listed.

- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 2,781,135
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - BERT base Japanese (IPA dictionary)
  - Downloads: 2,303,783
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - xlm-roberta-ner-japanese (Japanese caption : Êó•Êú¨Ë™û„ÅÆÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„ÅÆ„É¢„Éá„É´)
  - Downloads: 1,159,727
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This is a Japanese sentence-BERT model.
  - Downloads: 561,578
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - What‚Äôs this?
  - Downloads: 339,838
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - rinna/japanese-cloob-vit-b-16
  - Downloads: 268,705
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - BERT base Japanese (IPA dictionary, whole word masking enabled)
  - Downloads: 253,615
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 240,668
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - shisa-gamma-7b-v1 For more information see our main Shisa 7B model We applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
  - Downloads: 151,067
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 144,405
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 125,097
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 111,461
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - BERT base Japanese (character tokenization)
  - Downloads: 110,332
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 98,254
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 82,314
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - japanese-gpt-neox-3.6b-instruction-sft-v2 Overview
  - Downloads: 72,618
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This is a Japanese sentence-BERT model.
  - Downloads: 72,405
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 70,927
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-ner-wikipedia-dataset „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨6Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãÂõ∫ÊúâË°®ÁèæË™çË≠ò„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67,060
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT
  - Downloads: 55,223
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT
  - Downloads: 54,188
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - luke-japanese-large luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 47,931
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japanese Êó•Êú¨Ë™û„ÅÆREADME/Japanese README GLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
  - Downloads: 44,915
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)
  - Downloads: 39,188
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - Model Card for Japanese DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 33,292
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese This is a DistilBERT model pre-trained on 131 GB of Japanese web text.
  - Downloads: 31,708
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - bert-finetuned-japanese-sentiment This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
  - Downloads: 31,455
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 28,496
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - llm-jp-3-13b-instruct This repository provides large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 27,536
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - rinna/japanese-clip-vit-b-16
  - Downloads: 27,119
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE Model description Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
  - Downloads: 27,027
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 24,343
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2
  - Downloads: 19,681
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 19,311
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-14B-Japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 17,529
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - alabnii/jmedroberta-base-sentencepiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 16,927
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,511
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 16,451
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - japanese-sentiment-analysis This model was trained from scratch on the chABSA dataset.
  - Downloads: 14,882
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese License MIT License üëâ DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf „Åì„Å£„Å°„ÅÆ„Åå„ÅÑ„ÅÑ„Åã„ÇÇüëâ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf
  - Downloads: 14,294
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - clip-japanese-base This is a Japanese CLIP (Contrastive Language-Image Pre-training) model developed by LY Corporation.
  - Downloads: 13,024
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 12,776
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - Êó•Êú¨Ë™ûT5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 12,228
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - japanese-roberta-base This repository provides a base-sized Japanese RoBERTa model.
  - Downloads: 11,125
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 10,998
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - japanese-gpt2-medium This repository provides a medium-sized Japanese GPT-2 model.
  - Downloads: 10,655
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2 reazonspeech-nemo-v2 is an automatic speech recognition model trained on ReazonSpeech v2.0 corpus.
  - Downloads: 10,296
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers fugashi sentencepiece unidic-lite Then you can load this model and run inference.
  - Downloads: 9,603
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-32B-Japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 8,975
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - bert-base-japanese-upos Model Description
  - Downloads: 8,258
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - sbert-jsnli-luke-japanese-base-lite This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
  - Downloads: 8,204
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 6,867
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1 Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü Reranker (CrossEncoder) „Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 6,607
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.
  - Downloads: 6,582
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 6,516
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese License MIT License üëâ DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf „Åì„Å£„Å°„ÅÆ„Åå„ÅÑ„ÅÑ„Åã„ÇÇüëâ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf
  - Downloads: 6,374
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri: Japanese General Text Embeddings Usage Direct Usage (Sentence Transformers)
  - Downloads: 6,336
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B Model Description PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese open datasets, developed by Preferred Networks, Inc.
  - Downloads: 6,307
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - japanese-gpt-neox-3.6b Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 6,010
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023 „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄéCreativeML Open RAIL-M„Äè„ÅßLicense„Åù„ÅÆ„ÇÇ„ÅÆ„Å´Â§âÊõ¥„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ
  - Downloads: 5,630
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper (v2.0)
  - Downloads: 5,521
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - rinna/japanese-hubert-base Overview This is a Japanese HuBERT Base model trained by rinna Co.
  - Downloads: 5,517
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - japanese-large-lm-1.7b This repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
  - Downloads: 5,290
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 Kotoba-Whisper-v2.2 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 5,189
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - WRIME-fine-tuned BERT base Japanese This model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
  - Downloads: 5,161
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - rinna/japanese-wav2vec2-base Overview This is a Japanese wav2vec 2.0 Base model trained by rinna Co.
  - Downloads: 5,148
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B (CALM2-7B)
  - Downloads: 4,961
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 4,828
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.
  - Downloads: 4,609
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B-GGUF Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 4,550
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri: Japanese General Text Embeddings Usage Direct Usage (Sentence Transformers)
  - Downloads: 4,323
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - Mistral-Nemo-Japanese-Instruct-2408 Model Description
  - Downloads: 4,294
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - japanese-gpt2-small This repository provides a small-sized Japanese GPT-2 model.
  - Downloads: 4,166
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô /
  - Downloads: 3,944
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - „ÄêÂëäÁü•„Äëchilled_remixÂèä„Å≥reversemix„ÅØ2023Âπ¥5Êúà21Êó•„Å´VersionÂ§âÊõ¥„ÇíË°å„ÅÑ„ÄÅv2„Å∏ÁßªË°å„ÅÑ„Åü„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 3,788
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØLuke-japanese-large-lite„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 3,692
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 3,617
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - hotchpotch/japanese-reranker-cross-encoder-small-v1 Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü Reranker (CrossEncoder) „Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 3,557
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - BERT„Å´„Çà„ÇãÊó•Êú¨Ë™ûÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„ÅÆ„É¢„Éá„É´ BertForTokenClassification„ÇíÁî®„ÅÑ„Å¶„ÄÅÊó•Êú¨Ë™û„ÅÆÊñá„Åã„ÇâÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫„Åó„Åæ„Åô„ÄÇ
  - Downloads: 3,480
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This is for (private) DEMO only.
  - Downloads: 3,420
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 3,387
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-gguf deepseek-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-14B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,383
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - Model Card for Japanese DeBERTa V3 base Model description This is a Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0.
  - Downloads: 3,375
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - bilingual-gpt-neox-4b Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 3,372
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 3,102
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM-Small Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 3,083
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - japanese-large-lm-3.6b-instruction-sft
  - Downloads: 3,077
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - japanese-gpt-neox-3.6b-instruction-ppo Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 3,024
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - Ninja-v1-NSFW-128k-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-NSFW-128k„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,947
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - rinna/japanese-hubert-large Overview This is a Japanese HuBERT Large model trained by rinna Co.
  - Downloads: 2,871
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - Model Description llava-calm2-siglip is an experimental Vision Language Model that can answer questions in Japanese about images.
  - Downloads: 2,812
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-gguf deepseek-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-32B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,804
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - DeepSeek-R1-Distill-Llama-8B-gguf deepseek-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Llama-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,804
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-instruct-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-7b-fast-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,740
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - Japanese SimCSE (BERT-base)
  - Downloads: 2,729
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 2,631
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - DeepSeek-R1-Distill-Qwen-7B-gguf deepseek-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,620
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 2,589
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - rinna/youri-7b Overview We conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
  - Downloads: 2,583
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - lightblue-suzume-llama-3-8B-japanese-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsuzume-llama-3-8B-japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,505
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunCho„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-novel-gpt-j-6b„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,467
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - luke-japanese-large-lite luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 2,464
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 2,460
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 2,458
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 2,445
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FINGU-AI/FinguAI-Chat-v1 Overview The FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
  - Downloads: 2,441
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-7B-Japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,440
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 2,431
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô Êú¨„É¢„Éá„É´„ÅØ DeepSeek-V3 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„ÅÆ‰æãÊñá„ÇíÂÖÉ„Å´È†ªÂá∫„Åô„Çã MoE (Mixture of Experts) „ÅÆÂêÑ„É¨„Ç§„É§„Éº„Åî„Å®„ÅÆexperts„ÇíÂé≥ÈÅ∏„Åó„Å¶ÂÜçÊßãÊàê„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 2,385
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 2,357
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - hotchpotch/japanese-reranker-cross-encoder-large-v1 Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü Reranker (CrossEncoder) „Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 2,327
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Tanuki-8B„ÅØ„ÄÅ„Éï„É´„Çπ„ÇØ„É©„ÉÉ„ÉÅ„ÅßÁ¥Ñ1.3T„Éà„Éº„ÇØ„É≥‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„ÅüÁ¥Ñ8B„Éë„É©„É°„Éº„Çø„ÅÆÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 2,266
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF This is quantized version of cyberagent/Mistral-Nemo-Japanese-Instruct-2408 created using llama.cpp Original Model Card Mistral-Nemo-Japanese-Instruct-2408 Model Description
  - Downloads: 2,213
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 Kotoba-Whisper-v2.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 2,200
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - llm-jp-3-172b-instruct3
  - Downloads: 2,192
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 2,187
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - japanese-gpt2-xsmall
  - Downloads: 2,150
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper ü§óüé§üìù
  - Downloads: 2,111
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - DeepSeek-R1-Distill-Qwen-1.5B-gguf deepseek-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãDeepSeek-R1-Distill-Qwen-1.5B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,038
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B Model Description
  - Downloads: 2,029
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM-Large Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 1,964
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,885
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - gpt-neox-japanese-2.7b
  - Downloads: 1,795
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,774
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãphi-4-open-R1-Distill-EZOv1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,771
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - DeepSeek-V3-slice-jp64 Êú¨„É¢„Éá„É´„ÅØ DeepSeek-V3 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„ÅÆ‰æãÊñá„ÇíÂÖÉ„Å´È†ªÂá∫„Åô„Çã MoE (Mixture of Experts) „ÅÆÂêÑ„É¨„Ç§„É§„Éº„Åî„Å®„ÅÆexperts„ÇíÂé≥ÈÅ∏„Åó„Å¶ÂÜçÊßãÊàê„Åó„Åü„É¢„Éá„É´„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,756
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-7b-fast„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,745
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b „ÅØ„ÄÅ Llama 2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,741
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - „Åì„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆÊó•Êú¨Ë™ûÁâà„ÅØ„Åæ„Å†‰ΩúÊàê‰∏≠„Åß„Åô„ÄÇ
  - Downloads: 1,723
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp Model description
  - Downloads: 1,691
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - DeBERTa V2 small Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 1,667
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,651
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
  - Downloads: 1,640
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - stockmark/stockmark-13b Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
  - Downloads: 1,632
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - BERT base Japanese (character tokenization, whole word masking enabled)
  - Downloads: 1,606
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 1,603
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - roberta-small-japanese-luw-upos Model Description
  - Downloads: 1,594
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B "A parrot able to speak Japanese, ukiyoe, edo period" ‚Äî Stable Diffusion XL Model Description japanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 1,573
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - What‚Äôs this?
  - Downloads: 1,568
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-with-auto-jumanpp Model description
  - Downloads: 1,559
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-13b-fast-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,539
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - What‚Äôs this?
  - Downloads: 1,531
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT Model The RetrievaBERT is the pre-trained Transformer Encoder using Megatron-LM.
  - Downloads: 1,508
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - About static quants of https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 1,507
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-instruct-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-7b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,488
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B (rinna/llama-3-youko-8b)
  - Downloads: 1,483
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation on MIRACL japanese These models don't train on the MIRACL training data.
  - Downloads: 1,462
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - japanese-large-lm-3.6b
  - Downloads: 1,462
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 1,461
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLM-13B-instruct-gguf Fugaku-LLM„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãFugaku-LLM-13B-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,435
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - mt5_summarize_japanese (Japanese caption : Êó•Êú¨Ë™û„ÅÆË¶ÅÁ¥Ñ„ÅÆ„É¢„Éá„É´)
  - Downloads: 1,428
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - nlp-waseda/roberta-base-japanese Model description This is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 1,423
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - ‰∏≠Êñá | ÌïúÍµ≠Ïñ¥ | Êó•Êú¨Ë™û | –†—É—Å—Å–∫–∏–π | Deutsch | Fran√ßais | Espa√±ol | Portugu√™s | T√ºrk√ße | Ti·∫øng Vi·ªát | ÿßŸÑÿπÿ±ÿ®Ÿäÿ© Ultralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.
  - Downloads: 1,403
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,393
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - hubert-base-jtube This repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.
  - Downloads: 1,389
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)
  - Downloads: 1,373
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - (ÁÆÄ‰Ωì‰∏≠Êñá|English|Êó•Êú¨Ë™û) Introduction github repo : https://github.com/FunAudioLLM/SenseVoice SenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED).
  - Downloads: 1,362
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf Google„Åï„Çì„ÅÆgemma-2-2b-jpn-it„ÇíÈáèÂ≠êÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åü„Å°„Åß„Åô„ÄÇ
  - Downloads: 1,357
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 1,338
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - bert-base-japanese-v3-jnli „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(Ëá™ÁÑ∂Ë®ÄË™ûÊé®Ë´ñ)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,325
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 1,322
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 1,321
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - Model card for model ID
  - Downloads: 1,298
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - Model Card for Japanese BART base Model description This is a Japanese BART base model pre-trained on Japanese Wikipedia.
  - Downloads: 1,291
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin-inst-merge„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,273
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - recruit-jp/japanese-typo-detector-roberta-base „É¢„Éá„É´„ÅÆÊ¶ÇË¶Å Êó•Êú¨Ë™û„ÅÆÊñáÁ´†„ÇíÂÖ•Âäõ„Åô„Çã„Å®ÂêÑÊñáÂ≠ó„Åî„Å®„Å´Ë™§Â≠óËÑ±Â≠ó„Åß„ÅÇ„ÇãÁ¢∫Áéá„ÇíÂá∫Âäõ„Åó„Åæ„Åô ÂêÑ„É©„Éô„É´„ÅÆÊÑèÂë≥„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô id label meaning 0 OK Ë™§Â≠ó„Å™„Åó 1 deletion 1ÊñáÂ≠ó„ÅÆÊäú„Åë 2 insertion_a ‰ΩôÂàÜ„Å™1ÊñáÂ≠ó„ÅÆÊåøÂÖ• 3 insertion_b Áõ¥Ââç„ÅÆÊñáÂ≠óÂàó„Å®‰∏ÄËá¥„Åô„ÇãÔºíÊñáÂ≠ó‰ª•‰∏ä„ÅÆ‰ΩôÂàÜ„Å™ÊñáÂ≠ó„ÅÆÊåøÂÖ• 4 kanji-conversion_a Âêå‰∏Ä„ÅÆË™≠„Åø„ÇíÊåÅ„Å§Êº¢Â≠ó„ÅÆÂÖ•„ÇåÊõø„ÅàÔºàË™§Â§âÊèõÔºâ 5 kanji-conversion_b Ëøë„ÅÑË™≠„Åø„ÇíÊåÅ„Å§Êº¢Â≠ó„ÅÆÂÖ•„ÇåÊõø„ÅàÔºàË™§Â§âÊèõÔºâ 6 substitution 1ÊñáÂ≠ó„ÅÆÂÖ•„ÇåÊõø„Åà 7 transposition Èö£Êé•„Åô„ÇãÔºíÊñáÂ≠óÈñì„ÅÆËª¢ÁΩÆ 8 others „Åù„ÅÆ‰ªñ„ÅÆÂÖ•ÂäõË™§„Çä Ë™§„ÇäÁ®ÆÈ°û„ÅÆË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØÂ≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂÖÉË´ñÊñá„Çí„ÅîÂèÇÁÖß„Åè„Å†„Åï„ÅÑ Êó•Êú¨Ë™û Wikipedia „ÅÆÁ∑®ÈõÜÂ±•Ê≠¥„Å´Âü∫„Å•„Åè ÂÖ•ÂäõË™§„Çä„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®Ë®ÇÊ≠£„Ç∑„Çπ„ÉÜ„É†„ÅÆÊîπËâØ „Åù„ÅÆ‰ªñ„ÄÅ„É¢„Éá„É´„ÅÆË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØÂΩìÁ§æ„Éñ„É≠„Ç∞Ë®ò‰∫ã„Çí„ÅîÂèÇÁÖß„Åè„Å†„Åï„ÅÑ Ë™§Â≠óËÑ±Â≠óÊ§úÂá∫„É¢„Éá„É´„ÇíHugging Face Hub„Å´ÂÖ¨Èñã„Åó„Åæ„Åó„Åü (Re
  - Downloads: 1,262
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
  - Downloads: 1,257
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - DeBERTa V2 base Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 1,253
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - weblab-10b-instruction-sft-GPTQ Original model weblab-10b-instruction-sft which is a Japanese-centric multilingual GPT-NeoX model of 10 billion parameters created by matsuo-lab Takeshi Kojima.
  - Downloads: 1,247
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 1,247
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b „ÅØ„ÄÅ Llama 2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,245
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA-japanese-CodeLlama-7b-instruct-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-CodeLlama-7b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,245
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1 shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 1,239
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet TDT-CTC 0.6B (ja) | | parakeet-tdt_ctc-0.6b-ja is an ASR model that transcribes Japanese speech with Punctuations.
  - Downloads: 1,218
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instruct Model Description ELYZA-japanese-Llama-2-13b „ÅØ„ÄÅ Llama 2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,205
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,204
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - hotchpotch/japanese-reranker-cross-encoder-base-v1 Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü Reranker (CrossEncoder) „Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 1,197
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fast Model Description ELYZA-japanese-Llama-2-13b „ÅØ„ÄÅ Llama 2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,172
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - (English part follows Japanese one.
  - Downloads: 1,157
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - bert-base-japanese-v3-unsup-simcse-jawiki „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨8Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãÊïôÂ∏´„Å™„ÅóSimCSE„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,140
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - Polyglot-math-4x7b-24b Polyglot-4x7b is a Mixture of Experts approach to a multilingual model.
  - Downloads: 1,133
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Tanuki-8x8B„ÅØ„ÄÅ„Éï„É´„Çπ„ÇØ„É©„ÉÉ„ÉÅ„ÅßÁ¥Ñ1.7T„Éà„Éº„ÇØ„É≥‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü8x8B„Éë„É©„É°„Éº„ÇøÔºàÁ∑è„Éë„É©„É°„Éº„ÇøÁ¥Ñ47B„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÁ¥Ñ13BÔºâ„ÅÆÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,130
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - Japanese Parler-TTS Mini „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅparler-tts/parler-tts-mini-v1„ÇíÂü∫„Å´„ÄÅÊó•Êú¨Ë™û„Åß„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàË™≠„Åø‰∏ä„Åí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Çà„ÅÜÂÜçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 1,123
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - hotchpotch/japanese-bge-reranker-v2-m3-v1 Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü Reranker (CrossEncoder) „Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 1,108
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM
  - Downloads: 1,067
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 1,061
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 996
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) CoolJapanDiffusion 2.1.1„Å®WaifuDiffusion 1.4 anime epoch2„ÅÆ„Éû„Éº„Ç∏„ÄÇ
  - Downloads: 972
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Japanese-Instruct-2408„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 969
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - Êó•Êú¨Ë™ûT5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 930
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf rinna„Åï„Çì„ÅÆgemma-2-baku-2b-it„ÇíÈáèÂ≠êÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åü„Å°„Åß„Åô„ÄÇ
  - Downloads: 918
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - llm-jp-3-3.7b
  - Downloads: 913
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA-japanese-Llama-2-7b-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-7b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 885
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-1.7b-instruction-sft line-corporation„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-large-lm-1.7b-instruction-sft„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 884
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - llm-book/t5-base-long-livedoor-news-corpus „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨7Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãË¶ÅÁ¥ÑÁîüÊàê„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 884
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 874
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - alabnii/jmedroberta-base-manbyo-wordpiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 874
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 871
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Japanese-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 861
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - gpt2-large-japanese This repository provides a large sized Japanese GPT-2 model.
  - Downloads: 854
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
  - Downloads: 844
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãJapanese-Chat-Umievo-itr001-7b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 830
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplpl„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Instruct-Ja„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 823
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - calm3-22b-RP-v2-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 820
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - Êó•Êú¨Ë™ûÁâàCLIP„É¢„Éá„É´ This is a CLIP text/image encoder model for Japanese.
  - Downloads: 811
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuiseki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãYuisekinAIEvol-Mistral-7B-ja-math-v0.1.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 811
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - japanese-gpt-neox-3.6b-instruction-sft Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 794
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-Llama-2-13b-fast„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 780
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - line-corporation/japanese-large-lm-1.7b line-corporation„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-large-lm-1.7b„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 778
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - stockmark-gpt-neox-japanese-1.4b-gguf stockmark„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgpt-neox-japanese-1.4b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 773
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual (v1.0)
  - Downloads: 760
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - karakuri-lm-32b-thinking-2501-exp-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-32b-thinking-2501-exp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 748
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ‰ΩïÔºü
  - Downloads: 740
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 725
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - Model card for model ID
  - Downloads: 723
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T Base Model Description This is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 706
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - bilingual-gpt-neox-4b-instruction-ppo Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 702
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 695
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 695
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - japanese-gpt-neox-small This repository provides a small-sized Japanese GPT-NeoX model.
  - Downloads: 680
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - Japanese StableLM-3B-4E1T Instruct Model Description
  - Downloads: 676
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãc4ai-command-r-plus„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 675
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 static quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF Usage
  - Downloads: 671
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-4k-instruct-dpo„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 664
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 646
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - AXCXEPT-EZO-phi-4-v2_900-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-phi-4-v2_900„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 619
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 617
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
  - Downloads: 590
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf abeja„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãABEJA-Qwen2.5-32b-Japanese-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 587
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - Model card for model ID
  - Downloads: 586
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwm Model description This is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 577
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 572
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - albert-base-japanese-v1 Êó•Êú¨Ë™û‰∫ãÂâçÂ≠¶ÁøíÊ∏à„ÅøALBERT„É¢„Éá„É´„Åß„Åô
  - Downloads: 569
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - Japanese GPT2 Lyric Model Model description
  - Downloads: 561
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b „ÅØ„ÄÅ Code Llama„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 558
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - alabnii/jmedroberta-base-sentencepiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 544
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - hubert-base-asr
  - Downloads: 537
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãQwen1.5-110B-Chat„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 518
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - Additional pretrained BERT base Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 518
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF Original Model elyza/ELYZA-japanese-Llama-2-13b-fast-instruct Run with LlamaEdge LlamaEdge version: v0.2.8 and above Prompt template Prompt type: llama-2-chat Prompt string &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ user_msg_1 }}
  - Downloads: 508
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - deberta-base-japanese-aozora-ud-head Model Description
  - Downloads: 507
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - Japanese-Starling-ChatV-7B-GGUF GGUF conversion of "Japanese-Starling-ChatV-7B" "Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.
  - Downloads: 499
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 486
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-base-japanese-with-auto-jumanpp Model description
  - Downloads: 450
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - BERT Base Japanese for Irony
  - Downloads: 447
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - Model card for model ID
  - Downloads: 434
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - japanese-large-lm-1.7b-instruction-sft This repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
  - Downloads: 424
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - SpeechT5 (TTS task) for Japanese SpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech)
  - Downloads: 409
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - Sentence BERT base Japanese model This repository contains a Sentence BERT base model for Japanese.
  - Downloads: 408
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - Wav2Vec2-Large-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
  - Downloads: 403
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - wav2vec2-base-asr
  - Downloads: 399
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-A-v1-7B-gguf SakanaAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEvoLLM-JP-A-v1-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 397
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-v1-7B-gguf SakanaAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEvoLLM-JP-v1-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 392
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha Model Details Japanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
  - Downloads: 389
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B static quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF Usage
  - Downloads: 369
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
  - Downloads: 367
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - Êó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese Deepseek's R1 models are excellent, state-of-the-art reasoning models which have been trained to work bilingually, with English and Chinese.
  - Downloads: 365
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - oldÔºü
  - Downloads: 361
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - rinna/japanese-gpt-neox-3.6b-instruction-ppo rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-gpt-neox-3.6b-instruction-ppo„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 359
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese Considering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
  - Downloads: 357
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - About static quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF Usage
  - Downloads: 354
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - One more step before getting this model.
  - Downloads: 351
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - hubert-large-asr
  - Downloads: 338
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 338
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 333
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - bert-base-japanese-wikipedia-ud-head Model Description
  - Downloads: 330
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - QwQ-32B-Preview-gguf Qwen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãQwQ-32B-Preview„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 329
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - XLNet-japanese Model description This model require Mecab and senetencepiece with XLNetTokenizer.
  - Downloads: 327
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - ELYZA-japanese-CodeLlama-7b-gguf ELYZA„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-CodeLlama-7b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 325
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - Japanese to Korean translator Japanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)
  - Downloads: 315
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 312
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - ascktgcc/Mistral-nemo-ja-rp-v0.2„ÅÆGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - bert-base-japanese-v3-jcommonsenseqa „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(Â§öËÇ¢ÈÅ∏ÊäûÂºèË≥™ÂïèÂøúÁ≠î)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 311
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - rinna/japanese-gpt-neox-3.6b rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-gpt-neox-3.6b„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 304
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.
  - Downloads: 295
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - stockmark/gpt-neox-japanese-1.4b This repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
  - Downloads: 294
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 tiny Model description This is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 285
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - Model Card for Japanese character-level GPT-2 Medium Model description This is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 282
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 281
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 ja Finetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
  - Downloads: 279
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 277
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 277
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 266
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - ÂõûÁ≠î„Å®ÂõûÁ≠î„ÅåÂá∫„Å¶„Åè„Çã„Éë„É©„Ç∞„É©„Éï„Çí‰∏é„Åà„Çã„Å®Ë≥™ÂïèÊñá„ÇíÁîüÊàê„Åô„Çã„É¢„Éá„É´ SEE: https://github.com/sonoisa/deep-question-generation Êú¨„É¢„Éá„É´„ÅÆ‰ΩúÊàê„Çπ„ÉÜ„ÉÉ„ÉóÊ¶ÇË¶Å SQuAD 1.1„ÇíÊó•Êú¨Ë™û„Å´Ê©üÊ¢∞ÁøªË®≥„Åó„ÄÅ‰∏çÊ≠£„Å™„Éá„Éº„Çø„Çí„ÇØ„É¨„É≥„Ç∏„É≥„Ç∞ÔºàÊúâÂäπ„Å™„Éá„Éº„Çø„ÅØÁ¥ÑÂçäÂàÜÔºâ„ÄÇ
  - Downloads: 265
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 ABEJA-Qwen2.5-32b-Japanese-v0.1„ÅØQwen/Qwen2.5-32B-Instruct„Çí„Éô„Éº„Çπ„Å´Êó•Êú¨Ë™û‰∏≠ÂøÉ„Å®„Åó„ÅüÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÂÆüÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 264
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - Stanza model for Japanese (ja)
  - Downloads: 260
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-small Fine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 255
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - nlp-waseda/bigbird-base-japanese Model description This is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 255
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - t5-base-japanese-web (with Byte-fallback, 32K) Description megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 246
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - Êó•Êú¨Ë™ûT5 Prefix Language Model
  - Downloads: 240
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - Êó•Êú¨Ë™ûT5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 236
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023 „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄéCreativeML Open RAIL-M„Äè„ÅßLicense„Åù„ÅÆ„ÇÇ„ÅÆ„Å´Â§âÊõ¥„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ
  - Downloads: 235
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository contains some GGUF quantizations of the merge of the VNTL LLaMA 3 8B qlora.
  - Downloads: 226
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Finetuned Waseda RoBERTa to evaluate the generated answers on JTruthfulQA.
  - Downloads: 220
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-base Fine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 218
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - shisa-7b-v1-gguf augmxnt„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãshisa-7b-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 217
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - Original Model Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 213
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 "A parrot able to speak Japanese, ukiyoe, edo period" ‚Äî Stable Diffusion XL Model Description japanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
  - Downloads: 213
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - Ëã±Ë™û+Êó•Êú¨Ë™ûT5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
  - Downloads: 213
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 211
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅMARC-ja(positive or negative„ÅÆ‰∫åÂÄ§ÂàÜÈ°û)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 209
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - About static quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B weighted/imatrix quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 208
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for VecTeus-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 VecTeus has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 206
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - Japanese Natural Language Inference Model
  - Downloads: 206
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 204
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b „ÅØ„ÄÅ Code Llama„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - RoBERTa base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
  - Downloads: 202
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - Japanese BERT-base (MeCab + BPE) How to load the tokenizer Please download the dictionary file for MeCab + BPE from our GitHub repository.
  - Downloads: 200
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - Model Card for Japanese BART large Model description
  - Downloads: 199
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - Japanese BERT-base (Nothing + BPE) How to load the tokenizer Please download the dictionary file for Nothing + BPE from our GitHub repository.
  - Downloads: 197
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - Japanese BERT-base (Vaporetto + Unigram)
  - Downloads: 197
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - Japanese BERT-base (Nothing + Unigram)
  - Downloads: 195
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - Japanese BERT-base (Vaporetto + BPE) How to load the tokenizer Please download the dictionary file for Vaporetto + BPE from our GitHub repository.
  - Downloads: 195
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - „É¢„Éá„É´Ê¶ÇË¶Å AWS„ÅÆtrn1„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÁî®„ÅÑ„Å¶ÈñãÁô∫„Åó„ÅüÂ§ßÂñúÂà©Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 195
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - Japanese BERT-base (Juman++ + WordPiece) How to load the tokenizer Please download the dictionary file for Juman++ +
  - Downloads: 195
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - Japanese BERT-base (Sudachi + WordPiece) How to load the tokenizer Please download the dictionary file for Sudachi + WordPiece from our GitHub repository.
  - Downloads: 194
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - Japanese BERT-base (Sudachi + BPE) How to load the tokenizer Please download the dictionary file for Sudachi + BPE from our GitHub repository.
  - Downloads: 194
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - Japanese BERT-base (Juman++ + Unigram)
  - Downloads: 194
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - Japanese BERT-base (MeCab + WordPiece) How to load the tokenizer Please download the dictionary file for MeCab + WordPiece from our GitHub repository.
  - Downloads: 194
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - Japanese BERT-base (MeCab + Unigram)
  - Downloads: 194
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - AXCXEPT/phi-4-open-R1-Distill-EZOv1 Model Details This model is a Reasoner version of the phi-4 model by employing open-r1, which mimics the Distill methodology of Deepseek-R1.
  - Downloads: 193
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - Japanese BERT-base (Vaporetto + WordPiece) How to load the tokenizer Please download the dictionary file for Vaporetto + WordPiece from our GitHub repository.
  - Downloads: 193
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - bert-base-japanese-luw-upos Model Description
  - Downloads: 190
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 187
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - albert-base-japanese-v1-with-japanese Êó•Êú¨Ë™û‰∫ãÂâçÂ≠¶ÁøíÊ∏à„ÅøALBERT„É¢„Éá„É´„Åß„Åô„Åì„ÅÆ„É¢„Éá„É´„Åß„ÅØTokenizer„Å´BertJapaneseTokenizer„ÇØ„É©„Çπ„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åôalbert-base-japanese-v1„Çà„Çä„Éà„Éº„ÇØ„Éä„Ç§„Ç∫Âá¶ÁêÜ„ÅåÊ•Ω„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô How to use „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Åì„ÅÆ„É¢„Éá„É´„ÅØPreTrained„É¢„Éá„É´„Åß„ÅôÂü∫Êú¨ÁöÑ„Å´„ÅØÂêÑÁ®Æ„Çø„Çπ„ÇØÁî®„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶‰ΩøÁî®„Åï„Çå„Çã„Åì„Å®„ÇíÊÉ≥ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô Fill-Mask for PyTorch from transformers import ( AutoModelForMaskedLM, AutoTokenizer ) tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")
  - Downloads: 183
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP Model Card Model detail Model type: LLaVA-JP is a vision-language model that can converse about input images.
  - Downloads: 183
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - Japanese BERT-base (Nothing + WordPiece) How to load the tokenizer Please download the dictionary file for Nothing + WordPiece from our GitHub repository.
  - Downloads: 182
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - Êõ¥Êñ∞Â±•Ê≠¥ 2023Âπ¥5Êúà7Êó• „Äåoasst1-89k-ja„Äç„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíËøΩÂä†„Åó„Å¶ÂØæË©±„Ç∑„Çπ„ÉÜ„É†„Å´ÂØæÂøú„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 182
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - Japanese BERT-base (Sudachi + Unigram)
  - Downloads: 181
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This is a Japanese sentence-T5 model.
  - Downloads: 180
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - kakuyomu-genre-bert Â∞èË™¨„ÅÆ„Çø„Ç§„Éà„É´„ÇÑÁ¥π‰ªãÊñá„Åã„Çâ„Ç∏„É£„É≥„É´„ÇíÂàÜÈ°û„Åô„Çã BERT Êù±ÂåóÂ§ß„ÅÆ cl-tohoku/bert-base-japanese-char-v3 „Çí„Éô„Éº„Çπ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„É≥„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 180
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - Êó•Êú¨Ë™û gpt2 Ëí∏Áïô„É¢„Éá„É´ „Åì„ÅÆ„É¢„Éá„É´„ÅØrinna/japanese-gpt2-meduim„ÇíÊïôÂ∏´„Å®„Åó„Å¶Ëí∏Áïô„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 179
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - üéà FlexDreamHK FlexDreamHK„ÅØ„É™„Éº„ÇØ„Åï„Çå„ÅüNovelAI„É¢„Éá„É´„ÅÆÂÖ•„Å£„Å¶„ÅÑ„Å™„ÅÑ„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØ„Åù„ÅÆ„É™„Çπ„ÇØ„ÇíÂèØËÉΩ„Å™Èôê„Çä‰Ωé„Åè„Åó„Åü„É¢„Éá„É´„ÇíÁõÆÊåá„Åó„Å¶‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 179
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - Japanese BERT-base (Juman++ + BPE) How to load the tokenizer Please download the dictionary file for Juman++ + BPE from our GitHub repository.
  - Downloads: 179
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
  - Downloads: 175
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm Model description This is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 173
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - sonoisa/t5-base-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅ„Çø„Ç§„Éà„É´ÁîüÊàê„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 172
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM
  - Downloads: 172
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-instruct - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-instruct StableLM
  - Downloads: 171
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - Model card for model ID
  - Downloads: 171
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨6Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãÂõ∫ÊúâË°®ÁèæË™çË≠ò„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 170
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-large-lite„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅQuestion-Answering„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 166
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False
  - Downloads: 164
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - Feedback and support: TensorBlock's Twitter/X, Telegram Group and Discord server cyberagent/Mistral-Nemo-Japanese-Instruct-2408 - GGUF
  - Downloads: 163
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - bert-japanese_finetuned-sentiment-analysis This model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
  - Downloads: 163
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True
  - Downloads: 162
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False
  - Downloads: 159
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct Stockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
  - Downloads: 158
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 ‰∫ãÂâçÂ≠¶Áøí„Åã„ÇâÂÖ®ÈÉ®Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´„ÅÆ„Éê„Éº„Ç∏„Éß„É≥2„Åß„Åô„ÄÇ
  - Downloads: 156
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base-lite„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅQuestion-Answering„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 154
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1
  - Downloads: 150
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - „Çø„Ç§„Éà„É´„Åã„ÇâË®ò‰∫ãÊú¨Êñá„ÇíÁîüÊàê„Åô„Çã„É¢„Éá„É´ SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 149
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - nlp-waseda/roberta-large-japanese-seq512 Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.
  - Downloads: 148
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - Whisper Large V3 Japanese Phone Accent
  - Downloads: 147
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - Ë®ò‰∫ãÊú¨Êñá„Åã„Çâ„Çø„Ç§„Éà„É´„ÇíÁîüÊàê„Åô„Çã„É¢„Éá„É´ SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 146
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - roberta_qa_japanese (Japanese caption : Êó•Êú¨Ë™û„ÅÆ (ÊäΩÂá∫Âûã) Ë≥™ÂïèÂøúÁ≠î„ÅÆ„É¢„Éá„É´)
  - Downloads: 145
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - Êó•Êú¨Ë™ûByT5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 141
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 140
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
  - Downloads: 140
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT
  - Downloads: 140
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - „É¢„Éá„É´Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ sonoisa/sentence-luke-japanese-base-lite „ÇíSNS‰∏ä„ÅÆ„Ç≥„É°„É≥„Éà„Å´‰∫∫Êâã„ÅßÊîªÊíÉÊÄßË©ï‰æ°„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßFine-tuning„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 139
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 ‰∫ãÂâçÂ≠¶Áøí„Åã„ÇâÂÖ®ÈÉ®Êó•Êú¨Ë™û„ÅßÂ≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - WS TCG Card Text Translator A Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
  - Downloads: 135
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - roberta-large-japanese-aozora Model Description
  - Downloads: 134
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 133
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - Model card for model ID
  - Downloads: 133
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 133
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - bert-base-japanese-v3-bpr-passage-aio „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨9Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãÊñáÊõ∏Ê§úÁ¥¢„É¢„Éá„É´BPR„ÅÆ„Éë„ÉÉ„Çª„Éº„Ç∏„Ç®„É≥„Ç≥„Éº„ÉÄ„Åß„Åô„ÄÇ
  - Downloads: 132
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush ‚Äî Stable Diffusion XL Model Description japanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 128
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅÂõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 128
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - roberta-base-japanese-aozora-ud-head Model Description
  - Downloads: 128
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
  - Downloads: 127
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID ÊñôÁêÜ„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„ÅÆË≥™ÂïèÊñá„Åã„Çâ„ÄÅÊ§úÁ¥¢Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ„Åß„ÅÇ„ÇãÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫„Åó„Åæ„Åô Model Details Model Description ‰æã„Åà„Å∞„ÄÅ„ÄåÊù±‰∫¨„ÅÆËÇâÊñôÁêÜ„Åß„ÄÅÊò•„Å´È£ü„Åπ„Çâ„Çå„Çã„ÄÅÈ∂èËÇâ„Çí‰Ωø„Å£„ÅüÊñôÁêÜ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊñáÁ´†„ÇíÂÖ•Âäõ„Åô„Çã„Å®„ÄÅ „ÄåÊù±‰∫¨ ‚Üí ÈÉΩÈÅìÂ∫úÁúå/Âú∞Êñπ(AREA)„Äç „ÄåËÇâÊñôÁêÜ ‚Üí Á®ÆÈ°û(TYPE)„Äç „ÄåÊò• ‚Üí Â≠£ÁØÄ(SZN)
  - Downloads: 127
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - bert-large-japanese-wikipedia-ud-head Model Description
  - Downloads: 127
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 125
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 124
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 123
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - c4ai-command-r-v01-japanese-instruct GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å CohereForAI/c4ai-command-r-v01„Çí„ÄÅichikara-instruction„Çí‰Ωø„Å£„Å¶ËøΩÂä†„ÅßÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - bert-base-japanese-char-extended Model Description
  - Downloads: 121
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - deberta-large-japanese-aozora-ud-goeswith Model Description
  - Downloads: 120
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - Ë®ò‰∫ãÊú¨Êñá„Åã„Çâ„Çø„Ç§„Éà„É´„ÇíÁîüÊàê„Åô„Çã„É¢„Éá„É´ SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
  - Downloads: 120
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - Bloom model trained on Japanese corpus.
  - Downloads: 119
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - japanese-gpt-1b-PII-masking Model Description japanese-gpt-1b-PII-masking „ÅØ„ÄÅ Êó•Êú¨Ë™û‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø1B GPT„É¢„Éá„É´„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶„ÄÅÊó•Êú¨Ë™û„ÅÆÊñáÁ´†„Åã„ÇâÂÄã‰∫∫ÊÉÖÂ†±„Çí„Éû„Çπ„Ç≠„É≥„Ç∞„Åô„Çã„Çà„ÅÜ„Å´Â≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 119
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - deberta-base-japanese-aozora-ud-goeswith Model Description
  - Downloads: 118
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 118
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - transformers-ud-japanese-electra-ginza-520 (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 118
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - bert-base-sudachitra-v11
  - Downloads: 117
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B-GGUF GGUF conversion of "Japanese-WizardLM2-ChatV-7B" This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.
  - Downloads: 116
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
  - Downloads: 116
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - bert-large-japanese-char-extended Model Description
  - Downloads: 116
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 115
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - t5-base-japanese-web-8k (with Byte-fallback, 8K) Description megagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 115
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - „Å§„Åè„Çà„Åø„Å°„ÇÉ„Çì„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁî®„ÅÑ„Å¶ calm-2-7b-chat „Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 115
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 115
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 115
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - roberta-base-japanese-aozora Model Description
  - Downloads: 115
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 114
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 113
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - roberta-large-japanese-aozora-ud-goeswith Model Description
  - Downloads: 113
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base-lite„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅQuestion-Answering„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 113
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - Model overview This model is the baseline model for awesome-japanese-nlp-classification-dataset.
  - Downloads: 113
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - Model Card for Japanese character-level GPT-2 Small Model description This is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 113
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - deberta-base-japanese-juman-ud-goeswith Model Description
  - Downloads: 113
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct Model Description PLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 112
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
  - Downloads: 112
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - INPUT: Japanese name in ROMAJI FORM OUTPUT:
  - Downloads: 112
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - roberta-large-japanese-aozora-ud-head Model Description
  - Downloads: 112
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - deberta-base-japanese-luw-upos Model Description
  - Downloads: 112
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - bert-base-japanese-unidic-luw-upos Model Description
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - roberta-base-japanese-aozora-ud-goeswith Model Description
  - Downloads: 111
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - rinna/nekomata-7b Overview We conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
  - Downloads: 111
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - deberta-large-japanese-wikipedia-ud-goeswith Model Description
  - Downloads: 111
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - deberta-base-japanese-wikipedia-ud-goeswith Model Description
  - Downloads: 111
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)
  - Downloads: 110
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - bert-base-japanese-v3-bpr-question-aio „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨9Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„ÇãÊñáÊõ∏Ê§úÁ¥¢„É¢„Éá„É´BPR„ÅÆË≥™Âïè„Ç®„É≥„Ç≥„Éº„ÉÄ„Åß„Åô„ÄÇ
  - Downloads: 110
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - Finance Sentiment JA (base) Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
  - Downloads: 110
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - japanese-reversed-gpt2-medium-unidic This is a medium-sized Japanese reversed GPT-2 model using BERT-like tokenizer.
  - Downloads: 110
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-base-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶CommonsenseQA(ÈÅ∏ÊäûÂºè„ÅÆË≥™Âïè)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 110
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 109
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 109
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - rinna/nekomata-14b Overview We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
  - Downloads: 109
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - Electra Base Japanese Irony
  - Downloads: 109
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - deberta-large-japanese-unidic-ud-head Model Description
  - Downloads: 109
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - bert-large-japanese-unidic-luw-upos Model Description
  - Downloads: 109
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - deberta-base-japanese-unidic-luw-upos Model Description
  - Downloads: 108
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - output Á≠ëÊ≥¢ 2.0035860538482666 „Å§„Åè„Å∞ 1.6586617231369019 Á†îÁ©∂ 1.6227693557739258 Â§ßÂ≠¶ 1.3798155784606934 ÂÆüÈ®ì 0.5522942543029785 Â≠¶Áîü 0.42351895570755005 ÂàÜÊûê 0.37844282388687134 ÂõΩÁ´ã 0.3685397505760193 „Ç≠„É£„É≥„Éë„Çπ 0.36495038866996765 Ëå®Âüé 0.3056415021419525 ÁßëÂ≠¶ 0.2876652181148529 Èñ¢Êù± 0.24301066994667053 Âú∞Âüü 0.21340851485729218 ÂÆüÊñΩ 0.1976248174905777 ÂÖàÁ´Ø 0.192025288939476 „Çµ„Ç§„Éà 0.11629197001457214 Ë™øÊüª 0.09159307181835175 „Éó„É≠„Ç∏„Çß„ÇØ„Éà 0.08552580326795578 Ë≠∞Ë´ñ 0.07484486699104309 Ê§úË®é 0.007034890353679657
  - Downloads: 108
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - deberta-large-japanese-upos Model Description
  - Downloads: 108
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - deberta-small-japanese-upos Model Description
  - Downloads: 108
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - bert-large-japanese-upos Model Description
  - Downloads: 108
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØcl-tohoku/bert-large-japanese-v2„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅÂõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 107
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - deberta-base-japanese-wikipedia-ud-head Model Description
  - Downloads: 107
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - roberta-small-japanese-char-luw-upos Model Description
  - Downloads: 106
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 106
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - roberta-base-japanese-juman-ud-goeswith Model Description
  - Downloads: 106
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - deberta-large-japanese-juman-ud-goeswith Model Description
  - Downloads: 106
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - deberta-large-japanese-wikipedia-luw-upos Model Description
  - Downloads: 106
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - deberta-base-japanese-upos Model Description
  - Downloads: 106
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - deberta-small-japanese-luw-upos Model Description
  - Downloads: 106
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - deberta-base-japanese-unidic-ud-head Model Description
  - Downloads: 105
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - „É¢„Éá„É´Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ Twitter/twhin-bert-large „ÇíSNS‰∏ä„ÅÆ„Ç≥„É°„É≥„Éà„Å´‰∫∫Êâã„ÅßÊîªÊíÉÊÄßË©ï‰æ°„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßFine-tuning„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åæ„Åó„Åü
  - Downloads: 105
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - Japanese Stock Comment Sentiment Model
  - Downloads: 105
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-large-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 105
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - roberta-base-japanese-char-luw-upos Model Description
  - Downloads: 105
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-3.6b-instruction-sft line-corporation„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-large-lm-3.6b-instruction-sft„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 104
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - roberta-large-japanese-juman-ud-goeswith Model Description
  - Downloads: 104
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - deberta-base-japanese-wikipedia-luw-upos Model Description
  - Downloads: 104
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØcl-tohoku/bert-large-japanese-v2„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶CommonsenseQA(ÈÅ∏ÊäûÂºè„ÅÆË≥™Âïè)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 103
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - „É¢„Éá„É´Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ studio-ousia/luke-japanese-large-lite „ÇíSNS‰∏ä„ÅÆ„Ç≥„É°„É≥„Éà„Å´‰∫∫Êâã„ÅßÊîªÊíÉÊÄßË©ï‰æ°„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßFine-tuning„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 103
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - „É¢„Éá„É´Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ Twitter/twhin-bert-base „ÇíSNS‰∏ä„ÅÆ„Ç≥„É°„É≥„Éà„Å´‰∫∫Êâã„ÅßÊîªÊíÉÊÄßË©ï‰æ°„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßFine-tuning„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åæ„Åó„Åü
  - Downloads: 103
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-base - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-base StableLM
  - Downloads: 103
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-base-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶CommonsenseQA(ÈÅ∏ÊäûÂºè„ÅÆË≥™Âïè)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 103
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - nlp-waseda/roberta-large-japanese Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 103
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 103
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - deberta-large-japanese-unidic-luw-upos Model Description
  - Downloads: 102
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - llm-jp-1.3b-v1.0-aya llm-jp's llm-jp-1.3b-v1.0 model fine-tuned on the Japanese examples from Cohere's aya dataset Model llm-jp-eval AVG kcoopermiller/llm-jp-1.3b-v1.0-aya 0.0698 llm-jp/llm-jp-1.3b-v1.0 0.047 How to use import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("kcoopermiller/llm-jp-1.3b-v1.0-aya")
  - Downloads: 102
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-tiny-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶CommonsenseQA(ÈÅ∏ÊäûÂºè„ÅÆË≥™Âïè)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 101
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 100
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - studio-ousia/luke-japanese-base„Å´ÂØæ„Åó„Å¶Ê¨°„ÅÆÂ§âÊõ¥„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 98
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.
  - Downloads: 96
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ"chatntq-ja-7b-v1.0"„Çí„Éô„Éº„Çπ„Å´„Åó„Åü7B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 96
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅJCommonsenseQA(ÈÅ∏ÊäûÂºèÂøúÁ≠î)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 94
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅJNLI(ÊñáÁ´†„ÅÆÈñ¢‰øÇÊÄßÂà§Âà•)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 91
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - Model Card For llm-jp-3-3.7b-instruct-gguf LLM-jp„Åï„Çì„ÅÆllm-jp-3-3.7b-instruct„ÇíÈáèÂ≠êÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åü„Å°„Åß„Åô„ÄÇ
  - Downloads: 91
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 Model Description
  - Downloads: 89
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - bart-base-japanese-news(base-sized model)
  - Downloads: 88
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunCho„ÅßÂà©Áî®„Åó„Å¶„ÅÑ„Çã„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 88
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - rinna/japanese-data2vec-audio-base Overview This is a Japanese data2vec Audio Base model trained by rinna Co.
  - Downloads: 88
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling: Multilingual Gemma Update @ 2024.04.15: First release of Gemma-Mling 7B model Original Gemma Model Page:
  - Downloads: 87
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - recruit-jp/japanese-clip-vit-b-32-roberta-base Overview Developed by: Recruit Co.
  - Downloads: 86
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 84
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - GPT-2 small Japanese model This repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
  - Downloads: 83
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØÂº∑Âäõ„Å™Ôºî„Å§„ÅÆÊó•Êú¨Ë™û„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 83
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - bilingual-gpt-neox-4b-8k Overview Notice: This model requires transformers&gt;=4.31.0 to work properly.
  - Downloads: 82
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - BERT base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
  - Downloads: 80
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fined_Tuned_Whisper_Model
  - Downloads: 80
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True
  - Downloads: 79
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 78
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - Êó•Êú¨Ë™ûÂêë„Åë Llama 3 8B „ÅØ„Åò„ÇÅ„Å´ „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØLlama 3„ÇíÊó•Êú¨Ë™ûÂåñ„Åó„Çà„ÅÜ„Å®„Åó„Åü„É¢„Éá„É´„ÅÆ„É™„Éù„Ç∏„Éà„É™„Åß„Åô„ÄÇ
  - Downloads: 78
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - Model Card for Japanese DeBERTa V2 tiny Model description
  - Downloads: 77
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False
  - Downloads: 77
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - uniTKU-hubert-japanese-asr
  - Downloads: 77
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - Kendamarron/LongWriter-llm-jp-3-3.7b-instruct llm-jp/llm-jp-3-3.7b-instruct„ÇíÈï∑ÊñáÂá∫Âäõ„Åå„Åß„Åç„Çã„Çà„ÅÜ„Å´SFT„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 77
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 76
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False
  - Downloads: 75
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC Model Description PLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 75
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - Japanese-Alpaca-2-13B-GGUF Japanese-Alpaca-2-13B-GGUF„ÅØJapanese-Alpaca-2-13B„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 75
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.
  - Downloads: 74
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - Summary This is a text classifier for assigning a JLPT level.
  - Downloads: 69
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - nlp-waseda/gpt2-xl-japanese This is Japanese GPT2 with approximately 1.5B parameters pretrained on Japanese Wikipedia and CC-100
  - Downloads: 68
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset ) „Éì„Ç∏„É•„Ç¢„É´„Éé„Éô„É´È¢®ÁîªÂÉè„ÅÆÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßnaver-clova-ix/donut-base„ÇíË®ìÁ∑¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 68
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - line-corporation/japanese-large-lm-3.6b line-corporation„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-large-lm-3.6b„ÅÆggufÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 67
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 66
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - reazonspeech-espnet-next ReazonSpeech is a project to maintain freely-available Japanese audio datasets and ML models.
  - Downloads: 66
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - ES„ÇíÊõ∏„ÅèAI Japanese GPT-2 model„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åæ„Åó„Åü „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å´„ÅØ„ÄÅ„ÅÇ„Çâ„ÇÜ„ÇãÂàÜÈáé„Åã„Çâ140,000‰ª∂„Åª„Å©„ÅÆES„ÇíÁî®„ÅÑ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 65
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - rinna/nekomata-14b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-14b-instruction.
  - Downloads: 63
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This model is traned with guanaco dataset.
  - Downloads: 62
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba Barba is a multilingual natural language inference model for textual entailment and zero-shot text classification, available as an end-to-end service through TensorFlow Serving.
  - Downloads: 61
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 61
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - Swallow Education Classifier Japanese README Model summary This repository contains fastText classifiers for judging the educational value of Japanese web pages.
  - Downloads: 60
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - Japanese-Alpaca-2-13B Japanese-Alpaca-2-13B„ÅØÊåáÁ§∫ÂÆüË°å„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 60
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - bert-base-japanese-jsnli This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
  - Downloads: 59
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - yacis-electra-small-cyberbullying
  - Downloads: 59
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
  - Downloads: 59
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - Fugaku-LLMÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂØåÂ£´ÈÄöÊ†™Âºè‰ºöÁ§æ„ÄÅÂõΩÁ´ãÁ†îÁ©∂ÈñãÁô∫Ê≥ï‰∫∫ÁêÜÂåñÂ≠¶Á†îÁ©∂ÊâÄ„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±ÂåóÂ§ßÂ≠¶„ÄÅÊ†™Âºè‰ºöÁ§æ„Çµ„Ç§„Éê„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±Êµ∑ÂõΩÁ´ãÂ§ßÂ≠¶Ê©üÊßã„ÄÅÂèä„Å≥Ê†™Âºè‰ºöÁ§æKotoba Technologies Japan (‰ª•‰∏ã„ÄåÈñãÁô∫ËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„Åô)„Å´„Çà„Çã„ÄÅ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„Çø„ÄåÂØåÂ≤≥„ÄçÊîøÁ≠ñÂØæÂøúÊû†„Å´„Åä„Åë„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂàÜÊï£‰∏¶ÂàóÂ≠¶ÁøíÊâãÊ≥ï„ÅÆÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´Ôºà‰ª•‰∏ã„ÄåFugaku-LLM„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 58
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 58
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 58
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 large Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 58
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 57
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - Japanese-LLaMA-2-13B Japanese-LLaMA-2-13B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 56
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - Fugaku-LLMÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂØåÂ£´ÈÄöÊ†™Âºè‰ºöÁ§æ„ÄÅÂõΩÁ´ãÁ†îÁ©∂ÈñãÁô∫Ê≥ï‰∫∫ÁêÜÂåñÂ≠¶Á†îÁ©∂ÊâÄ„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±ÂåóÂ§ßÂ≠¶„ÄÅÊ†™Âºè‰ºöÁ§æ„Çµ„Ç§„Éê„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±Êµ∑ÂõΩÁ´ãÂ§ßÂ≠¶Ê©üÊßã„ÄÅÂèä„Å≥Ê†™Âºè‰ºöÁ§æKotoba Technologies Japan (‰ª•‰∏ã„ÄåÈñãÁô∫ËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„Åô)„Å´„Çà„Çã„ÄÅ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„Çø„ÄåÂØåÂ≤≥„ÄçÊîøÁ≠ñÂØæÂøúÊû†„Å´„Åä„Åë„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂàÜÊï£‰∏¶ÂàóÂ≠¶ÁøíÊâãÊ≥ï„ÅÆÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´Ôºà‰ª•‰∏ã„ÄåFugaku-LLM„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 56
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - Model Card for Japanese character-level GPT-2 Large Model description
  - Downloads: 55
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 55
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - Model card for model ID
  - Downloads: 55
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 54
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 54
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-base„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅJSTS(ÊñáÁ´†„ÅÆÈ°û‰ººÂ∫¶Ë®àÁÆó)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 53
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 53
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - „Éô„Éº„Çπ„É¢„Éá„É´Ôºöcl-tohoku/bert-base-japanese-whole-word-masking „Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-book/wrime-sentiment „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂: adafactor Optuna„Åß„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊé¢Á¥¢ Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É´„ÅÆ„Çø„Ç§„Éó(lr_scheduler_type):
  - Downloads: 53
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - nagisa_bert A BERT model for nagisa.
  - Downloads: 52
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech-v0.1 Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
  - Downloads: 52
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - luke-large-defamation-detection-japanese Êó•Êú¨Ë™ûË™πË¨ó‰∏≠ÂÇ∑Ê§úÂá∫Âô®
  - Downloads: 51
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - c4ai-command-r-v01-japanese-instruct-GGUF Ê¶ÇË¶Å Aratako/c4ai-command-r-v01-japanese-instruct„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 49
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - transformer-lm-japanese-0.1b
  - Downloads: 48
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - Model card for model ID
  - Downloads: 46
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 46
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - Model card for model ID
  - Downloads: 45
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
  - Downloads: 43
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 43
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - Model Card for Tanrei/GPTSAN-japanese General-purpose Swich transformer based Japanese language model GPTSAN has some unique features.
  - Downloads: 43
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - NLLB 1.3B fine-tuned on Japanese to English Light Novel translation This model was fine-tuned on light and web novel for Japanese to English translation.
  - Downloads: 43
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59363 Validation Metrics Loss: 0.12651239335536957 Accuracy: 0.9532079853817648 Precision: 0.9729688278823665 Recall: 0.9744633462616643 AUC: 0.9717333684823413 F1: 0.9737155136027014 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
  - Downloads: 41
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - The English document is here.
  - Downloads: 41
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - Êó•Êú¨Ë™ûÂåªÁôÇÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„É¢„Éá„É´ Ê¶ÇË¶Å „ÇΩ„Éº„Ç∑„É£„É´„Éª„Ç≥„É≥„Éî„É•„Éº„ÉÜ„Ç£„É≥„Ç∞Á†îÁ©∂ÂÆ§„Åï„Åæ„Çà„ÇäÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãMedTxt-CR„ÇíÁî®„ÅÑ„Å¶„ÄÅalabnii„Åï„Åæ„Çà„ÇäÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãRoBERTa„Çífine-tuning„Åó„ÅüÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - yacis-electra-small
  - Downloads: 40
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Êó•Êú¨Ë™û„É¢„Éá„É´„Ç´„Éº„Éâ/Japanese model card Êó•Êú¨Ë™û„ÅÆ„Éñ„É≠„Ç∞/Full Japanese dev blog Development source code/ÈñãÁô∫„ÇΩ„Éº„Çπ„Ç≥„Éº„Éâ Karasu-DPO-7B
  - Downloads: 37
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 36
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - Model Card for Model ID Fine tunned ASR model from distil-whisper/distil-large-v2.
  - Downloads: 35
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - roberta-base-japanese-aozora-char Model Description
  - Downloads: 35
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - bert-japanese-ner „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„ÅÆÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„Çø„Çπ„ÇØ„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÄÅ‰∫¨ÈÉΩÂ§ßÂ≠¶ ÈªíÊ©ã„ÉªË§ö„ÉªÊùëËÑáÁ†îÁ©∂ÂÆ§„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBERTÊó•Êú¨Ë™ûPretrained„É¢„Éá„É´„Çí„Éô„Éº„Çπ„Å´„Çπ„Éà„ÉÉ„ÇØ„Éû„Éº„ÇØÊ†™Âºè‰ºöÁ§æ„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãner-wikipedia-dataset„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 34
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - t5-base-xlsum-ja
  - Downloads: 34
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B pre-trained model for Japanese Model Description GPT2/GPT3 like model trained on Japanese.corpus.
  - Downloads: 33
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1 Êó•Êú¨Ë™ûÁâà„ÅØËøëÊó•ÂÖ¨Èñã‰∫àÂÆö„Åß„ÅôÔºàÊó•Êú¨Ë™û„ÇíÂãâÂº∑‰∏≠„Å™„ÅÆ„Åß„ÄÅÈñìÈÅï„ÅÑ„ÅØ„ÅîÂÆπËµ¶„Åè„Å†„Åï„ÅÑÔºÅ
  - Downloads: 33
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - Convert from: drewschaub/whisper-large-v3-japanese-4k-steps Whisper large-v3 model for CTranslate2 This repository contains the conversion of drewschaub/whisper-large-v3-japanese-4k-steps to the CTranslate2 model format.
  - Downloads: 32
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-base-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶QA„Çø„Çπ„ÇØ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 32
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - roberta-base-japanese-luw-upos Model Description
  - Downloads: 32
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - whisper-large-v2-mix-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
  - Downloads: 31
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM
  - Downloads: 31
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 31
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - whisper-large-v2-japanese-5k-steps This model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
  - Downloads: 31
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - doc2query/msmarco-japanese-mt5-base-v1 This is a doc2query model based on mT5 (also known as docT5query).
  - Downloads: 30
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - Japanese-LLaMA-2-13B-GGUF Japanese-LLaMA-2-13B-GGUF„ÅØJapanese-LLaMA-2-13B„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 30
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - rinna/nekomata-7b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-7b-instruction.
  - Downloads: 29
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 29
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0 ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
  - Downloads: 28
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - BERT for Japanese Twitter
  - Downloads: 27
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - Mixtral-8x7B-v0.1-japanese Mixtral-8x7B-v0.1-japanese„ÅØMixtral-8x7B-v0.1„Çí„Éô„Éº„Çπ„Å´Êó•Êú¨Ë™û„ÅÆË™ûÂΩôÊã°ÂºµÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÂÆüÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - Fine-tuned XLSR-53 large model for speech diarization in Japanese phone-call 2 speakers diarization model which was fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using phone-call data CallHome.
  - Downloads: 26
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - „Åì„Å°„Çâ„Åß„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åß„Åç„Å™„ÅÑ„ÅÆ„Åß„ÄÅcivitai„Å´„Å¶ÂÖà„Å´ÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 26
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - calm3-22b-RP-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version „Åæ„Åü„ÄÅ„Åì„Å°„Çâ„ÅßÊú¨„É¢„Éá„É´„ÅÆ„Éá„É¢„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 25
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B Please check our blog post for more details, samples, evaluations and more: Blogpost Model Description Genji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
  - Downloads: 25
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - Model description This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on my collection of Public Japanese Voice datasets for research Common Voice 7.0, JUST (Japanese speech corpus of Saruwatari-lab.
  - Downloads: 25
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 25
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - Japanese GPT2 Lyric Model Model description
  - Downloads: 25
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 25
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B Model Details Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model that can converse about input images.
  - Downloads: 24
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - We initialize SPLADE-japanese from tohoku-nlp/bert-base-japanese-v2.
  - Downloads: 24
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - About This model is Lightblue's QLoRA finetune of OpenOrca's Open-Orca/OpenOrcaxOpenChat-Preview2-13B model on Japanese fine-tuning datasets.
  - Downloads: 24
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 23
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-base-gamma-7b üß© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
  - Downloads: 23
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM
  - Downloads: 23
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - Japanese-LLaMA-2-7B-GGUF Japanese-LLaMA-2-7B-GGUF„ÅØJapanese-LLaMA-2-7B„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 23
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-instruct-gamma-7b üß© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
  - Downloads: 23
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - nlp-waseda/gpt2-small-japanese This model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.
  - Downloads: 23
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - nlp-waseda/gpt2-small-japanese-wikipedia This model is Japanese GPT-2 pretrained on Japanese Wikipedia.
  - Downloads: 23
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - whisper-large-v3-japanese-4k-steps This model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
  - Downloads: 22
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - ebisuke/liz-nojaloli-ja License MIT License„Éô„Éº„Çπ„Å®„Åó„Å¶rinna/japanese-gpt-neox-3.6b„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 22
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - deberta-large-japanese-aozora Model Description
  - Downloads: 22
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This is a Japanese+English sentence-BERT model.
  - Downloads: 21
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 21
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - Details: https://spacy.io/models/ja#ja_core_news_lg Japanese pipeline optimized for CPU.
  - Downloads: 21
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - Êó•Êú¨Ë™ûVL-T5‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´
  - Downloads: 21
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - bert-large-japanese-luw-upos Model Description
  - Downloads: 21
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 20
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - rinna/nekomata-14b-gguf Overview The model is the GGUF version of rinna/nekomata-14b.
  - Downloads: 20
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - ebisuke/liz-nojaloli-nxja-ja License MIT „Éô„Éº„Çπ„Å®„Åó„Å¶abeja/gpt-neox-japanese-2.7b„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 20
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - jpn-heb source group: Japanese target group:
  - Downloads: 19
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This model is for transcribing audio into Hiragana, one format of Japanese language.
  - Downloads: 19
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2 Japanese base model version 2 Prerequisites transformers==4.19.2 Model architecture This model uses GPT2 base setttings except vocabulary size.
  - Downloads: 19
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - Swallow-MX-8x7b-NVE-v0.1„Å´ÂØæ„Åó„ÄÅ Mixtral-8x7B-Instruct-v0.1„Å®Mixtral-8x7B-v0.1„ÅÆÂ∑ÆÂàÜ„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 19
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 19
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 19
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
  - Downloads: 19
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - ‚óÜREV-Mix "„É¨„Éú„É™„É•„Éº„Ç∑„Éß„É≥"„Å™„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 19
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - MobileBERT Êó•Êú¨Ë™û‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ÁàÜË™ïÔºÅÔºÅ
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - wav2vec2-live-japanese https://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using the common_voice JSUT CSS10
  - Downloads: 18
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - Tanuki-ZeRo-gguf kanhatakeyama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãTanuki-ZeRo„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 18
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - „É¢„Éá„É´„ÅÆÊ¶ÇÁï• Êù±ÊñπProject„ÅÆ„Ç≠„É£„É©„ÇØ„Çø„Éº„Åß„ÅÇ„ÇãÈúßÈõ®È≠îÁêÜÊ≤ô„Å®„Åä„Åó„ÇÉ„Åπ„Çä„Åß„Åç„Çã„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 18
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT dataset{s}.
  - Downloads: 18
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - VITS TTS Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - rinna/nekomata-7b-gguf Overview The model is the GGUF version of rinna/nekomata-7b.
  - Downloads: 17
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 17
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B „Äå„Å©„ÅÜ„Åã„ÅäÊÖàÊÇ≤„Çí „ÇÇ„ÅÜ Áñ≤„ÇåÊûú„Å¶„Åæ„Åó„Åü„Äç ÁîüÊàê‰æã [Â§™Â≠ó‰ª•Èôç„ÅåAIÁîüÊàê] „Äå„Å©„ÅÜ„Åã„Äç ‚Äù„Åù„Çå‚Äù„ÅØÊááÈ°ò„Åó„Åü„ÄÇ
  - Downloads: 17
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged„ÅØMixtral-8x7B-Instruct-v0.1„Çí„Éô„Éº„Çπ„Å´Êó•Êú¨Ë™û„ÅÆË™ûÂΩôÊã°ÂºµÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÂÆüÊñΩ„Åó„ÅüÂ≠¶ÁøíÈÄî‰∏≠„ÅÆ„É¢„Éá„É´„Å´ÂØæ„Åó„Å¶„ÄÅÂ∑ÆÂàÜ„Éû„Éº„Ç∏„ÇíÂÆüÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 17
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - zenz-v2.5-small zenz-v2.5„ÅØ„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüGPT-2„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊù°‰ª∂‰ªò„ÅçË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 16
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2 V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÇÑInstaÁ≥ª„É¢„Éá„É´„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„ÉªInstaÁ≥ª„É¢„Éá„É´„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 16
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fine_Tuned_Whisper_Model This model is a fine-tuned version of openai/whisper-tiny on the Common Voice dataset.
  - Downloads: 16
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
  - Downloads: 16
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - Mixtral-8x7B-Instruct-v0.1-japanese Mixtral-8x7B-Instruct-v0.1-japanese„ÅØMixtral-8x7B-Instruct-v0.1„Çí„Éô„Éº„Çπ„Å´Êó•Êú¨Ë™û„ÅÆË™ûÂΩôÊã°ÂºµÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÂÆüÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha Mixtral-8x7B-Instruct-v0.1-japanese-alpha„ÅØMixtral-8x7B-Instruct-v0.1„Çí„Éô„Éº„Çπ„Å´Êó•Êú¨Ë™û„ÅÆË™ûÂΩôÊã°ÂºµÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÂÆüÊñΩ„Åó„ÅüÂ≠¶ÁøíÈÄî‰∏≠„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 16
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - NLLB-200 1.3B fine-tuned on Ascendance of a Bookworm
  - Downloads: 16
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - zenz-v2.5-small zenz-v2.5„ÅØ„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüGPT-2„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊù°‰ª∂‰ªò„ÅçË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - zenz-v2.5-small zenz-v2.5„ÅØ„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüGPT-2„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊù°‰ª∂‰ªò„ÅçË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU-Model (T5 fine-tuned model) JAINU is a Japanese - Ainu language machine translation model.
  - Downloads: 15
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - Japanese DialoGPT trained with Aozora (ja) ÈùíÁ©∫ÊñáÂ∫´„ÅÆ„Çª„É™„Éï„ÅßÂ≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„ÅÆDialoGPT Small„Åß„Åô(en) Japanese DialoGPT Small trained on Aozora Bunko.
  - Downloads: 15
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - Model Card Summary This model was trained using H2O LLM Studio.
  - Downloads: 15
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese This model is Llama-2-Chat 70B fine-tuned with a part of the Japanese instruction dataset named izumi-lab/llm-japanese-dataset.
  - Downloads: 15
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - „É¢„Éá„É´„ÅÆÊ¶ÇÁï• ÈúßÈõ®È≠îÁêÜÊ≤ô„Å®„Åä„Åó„ÇÉ„Åπ„Çä„Åß„Åç„Çã„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 15
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 14
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This model is a voice clone of myself created specifically for Style Bert VITS2.
  - Downloads: 14
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 14
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 14
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3„ÄÇ
  - Downloads: 13
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - japanese-gpt2-medium-unidic This is a medium-sized Japanese GPT-2 model using BERT-like tokenizer.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 13
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This pre-trained model is work in progress!
  - Downloads: 13
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59362 Validation Metrics Loss: 0.13092292845249176 Accuracy: 0.9527127414314258 Precision: 0.9634070704982427 Recall: 0.9842171959602166 AUC: 0.9667289746092403 F1: 0.9737009564152002 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
  - Downloads: 12
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_full_band_vits_prosody ‚ôª
  - Downloads: 12
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - yuyuyui-chatbot
  - Downloads: 12
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 ja Finetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 12
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - jvnv„Ç≥„Éº„Éë„Çπ„ÅÆF2„Åã„ÇâÂ≠¶Áøí„Åó„Å¶‰ΩúÊàê„Åó„Åübert-vits2„ÅÆÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZA-japanese-Llama-2-13b-fast-instruct„ÅÆ GGUF Â§âÊèõ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base SambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
  - Downloads: 12
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - Unihan LM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database Model description Chinese and Japanese share many characters with similar surface morphology.
  - Downloads: 12
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 12
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - BERT base Japanese model This repository contains a BERT base model trained on Japanese Wikipedia dataset.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 Japanese-LLaMA-3-8B-Instruct-v2„ÅØÊåáÁ§∫ÂÆüË°å„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - deberta-base-japanese-wikipedia Model Description
  - Downloads: 12
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This model is traned with guanaco dataset.
  - Downloads: 11
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - Only for Japanese Please use AutoTokenizer and AutoModelForCausalLM And must use Unifine format to input and output.
  - Downloads: 11
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0
  - Downloads: 11
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - „Éâ„Éü„Éã„Ç™„É≥Êó•Êú¨Ë™ûLLM for WhisperÔºà2023/12/19 1.0ÁâàÔºâ Ê¶ÇË¶Å Whisper„Åß„Éâ„Éü„Éã„Ç™„É≥Ôºà„Éú„Éº„Éâ„Ç≤„Éº„É†Ôºâ„ÅÆ„Ç´„Éº„ÉâÁî®Ë™û„Å™„Å©„ÇíÂê´„Çì„Å†Èü≥Â£∞„ÇíÊñáÂ≠óËµ∑„Åì„ÅóÂá∫Êù•„Çã„Åì„Å®„ÇíÁõÆÊ®ô„Å´„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüLLM„Åß„Åô„ÄÇ
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - „ÅäÁü•„Çâ„Åõ „Çà„ÇäÂõûÁ≠î„ÅåÈÅ©Âàá„Å´„Å™„Çã„Çà„ÅÜ„Å´Â≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´„ÄÅhttps://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq „ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ
  - Downloads: 11
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - Japanese Stable Diffusion Pokemon Model Card Stable-Diffusion-Pokemon-ja is a Japanese-specific latent text-to-image diffusion model capable of generating Pokemon images given any text input.
  - Downloads: 11
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,758
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãVecteus-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 5,901
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B Êó•Êú¨Ë™û„ÅÆREADME/Japanese README "Sarashina-Embedding-v1-1B" is a Japanese text embedding model, based on the 1.2B-parameter Japanese LLM "Sarashina2.1-1B".
  - Downloads: 4,952
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsuzume-llama-3-8B-multilingual„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,495
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - japanese-stablelm-2-instruct-1_6b-gguf stabilityai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-stablelm-2-instruct-1_6b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,241
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KUJIRA„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,111
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,101
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-13b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,057
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-MS-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,038
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 1,004
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - Ninja-v1-NSFW-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-NSFW„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 941
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8b-Cosmopedia-japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 919
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-8b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-70b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 732
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-RobinHood„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 672
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 628
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 602
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-128k„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 408
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãKarasu-Mixtral-8x22B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 211
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteus„ÅÆGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 179
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
  - Downloads: 178
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„Å´chat vector„ÅßÂØæË©±ËÉΩÂäõ„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 150
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 141
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„ÇíÊó•Êú¨Ë™ûinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 139
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Assistance „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 122
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLMÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂØåÂ£´ÈÄöÊ†™Âºè‰ºöÁ§æ„ÄÅÂõΩÁ´ãÁ†îÁ©∂ÈñãÁô∫Ê≥ï‰∫∫ÁêÜÂåñÂ≠¶Á†îÁ©∂ÊâÄ„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±ÂåóÂ§ßÂ≠¶„ÄÅÊ†™Âºè‰ºöÁ§æ„Çµ„Ç§„Éê„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÅÂõΩÁ´ãÂ§ßÂ≠¶Ê≥ï‰∫∫Êù±Êµ∑ÂõΩÁ´ãÂ§ßÂ≠¶Ê©üÊßã„ÄÅÂèä„Å≥Ê†™Âºè‰ºöÁ§æKotoba Technologies Japan (‰ª•‰∏ã„ÄåÈñãÁô∫ËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„Åô)„Å´„Çà„Çã„ÄÅ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„Çø„ÄåÂØåÂ≤≥„ÄçÊîøÁ≠ñÂØæÂøúÊû†„Å´„Åä„Åë„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂàÜÊï£‰∏¶ÂàóÂ≠¶ÁøíÊâãÊ≥ï„ÅÆÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´Ôºà‰ª•‰∏ã„ÄåFugaku-LLM„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 95
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 91
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-KUJIRA „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 80
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 64
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "ÂΩ±" in Japanese or "Shadow" in English.
  - Downloads: 55
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-RobinHood „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 50
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 44
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 39
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„Å´Áï∞„Å™„ÇãÊ®©Èôê„ÇíË®≠ÂÆö„Åô„Çã Have different permissions when sharing merges üñºÔ∏è ‰æã / Examples(‚Äª‰ªñ„ÅÆ‰∫∫„ÅåÁîüÊàê„Åó„ÅüÁâ©„ÇíË°®Á§∫„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÊú¨‰∫∫„ÅÆË®±Ë´æ„ÇíÂæó„Å¶
  - Downloads: 35
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest „Åì„ÅÆ„É¢„Éá„É´„ÅØÁîüÁâ©Â≠¶„ÉªÂåªÂ≠¶„Å´Á≤æÈÄö„Åó„ÅüOpenBioLLM-8B„Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™ûÂØæÂøú„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´Llama-3-youko-8b-instruct-chatvector„Å®„Éû„Éº„Ç∏„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - zenz-v1 zenz-v1„ÅØGPT-2„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Å´Âü∫„Å•„Åè„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.2 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 21
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 20
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 19
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - Model Card for Model ID „Åì„ÅÆ„É¢„Éá„É´„ÅØrinna/japanese-gpt-1b„Çí„Éô„Éº„Çπ„É¢„Éá„É´„Å®„Åó„Å¶„ÄÅ „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ„ÅÆÊäΩÂá∫ÂûãQA„Å®„ÄÅËß£Á≠î„ÇíÊñ∞„Åü„Å™„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Åß„É™„Éï„Ç°„Ç§„É≥„Åô„Çã„Åü„ÇÅ„ÅÆÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 18
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - alpaca-guanaco-japanese-gpt-1b 1.3B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™ûGPT„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„ÅüÂØæË©±AI„Åß„Åô„ÄÇ
  - Downloads: 17
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 17
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
  - Downloads: 16
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - Swallow-MoE-2x13B-v0.1 English description here Ê¶ÇË¶Å Llama-2„Éô„Éº„Çπ„ÅÆÂ≠¶ÁøíÊ∏à„ÅøÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÇ„Çãtokyotech-llm/Swallow-13b-instruct-hf„Å®„ÄÅ„Åù„Çå„ÇíÂà©Áî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„ÅÇ„Çãnitky/Superswallow-13b-v0.2 „Çí„ÄÅmergekit„Çí‰Ωø„Å£„Å¶MoE„ÇíË°å„ÅÑ‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 16
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteus„Çí„Éô„Éº„Çπ„Å´LLava„Å´ÂØæÂøú„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 15
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion Model Card SFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
  - Downloads: 13
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - Êó•Êú¨Ë™û„Åßtraining„Åó„Åüllama2„ÇíinstructionÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßsft„Åó„Åü„ÇÇ„ÅÆ„Å´„Å™„Çä„Åæ„Åô base: https://huggingface.co/if001/llama2_ja_small training„ÅØ‰ª•‰∏ã„ÅÆscriptÂèÇÁÖß https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
  - Downloads: 13
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - japanese-sexual-moderation-v2„ÅØ„ÄÅstudio-ousia/luke-japanese-large-lite„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 „Åì„ÅÆ„É¢„Éá„É´„ÅØtokyotech-llm/Swallow-MS-7b-instruct-v0.1„ÅÆtokenizer.chat_template„Çí‰ª•‰∏ã„Å´Â§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - jvnv„Ç≥„Éº„Éë„Çπ„ÅÆF2„Åã„ÇâÂ≠¶Áøí„Åó„Å¶‰ΩúÊàê„Åó„Åübert-vits2„ÅÆÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - MPT-7B-base „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMosaicML„ÅÆllm-foundry„É™„Éù„Ç∏„Éà„É™„Çí‰ΩøÁî®„Åó„Å¶mosaicml/mpt-7b„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - roberta-large-japanese-char-luw-upos Model Description
  - Downloads: 12
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - whisper-large-v2-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-jp to the CTranslate2 model format.
  - Downloads: 12
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This model is traned with llm-japanese-dataset dataset.
  - Downloads: 12
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard Model Description Deepreneur-blue-lizard„ÅØ„ÄÅMeta„ÅÆLlama-2-7b„Å´ÂØæ„Åó„Å¶„ÄÅWikipedia„ÇÑÊõ∏Á±çÁ≠â„ÅÆÊó•Êú¨Ë™û„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„Å®Áã¨Ëá™„Éá„Éº„Çø„Å´„Çà„Çã„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÂÆüÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1 English description here Ê¶ÇË¶Å Llama-2„Éô„Éº„Çπ„ÅÆÂ≠¶ÁøíÊ∏à„ÅøÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-7b-fast„Å®„ÄÅ„Åù„ÅÆinstruction tuning„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-7b-fast-instruct „Çí„ÄÅmergekit„Çí‰Ωø„Å£„Å¶MoE„ÇíË°å„ÅÑ‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja
  - Downloads: 12
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - The English document is here „É¢„Éá„É´Ê¶ÇË¶Å Watashiha-Llama-2-13B-Ogiri-sft„ÇíAWS„ÅÆinf2„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅßÂãï‰Ωú„Åô„Çã„Çà„ÅÜ„Å´„Ç≥„É≥„Éë„Ç§„É´„Åï„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - ES„ÇíÊõ∏„ÅèAI Japanese GPT-2 model„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 11
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-medium-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 669
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHonyaku-13b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 604
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 529
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 210
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - Deepreneur-blue-lizard-gguf Deepreneur„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãblue-lizard„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 176
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmark„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãstockmark-100b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 137
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT ElanMT-BT-ja-en is a Japanese to English translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 132
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
  - Downloads: 125
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
  - Downloads: 108
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 99
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 93
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP Ê¶ÇË¶Å Local-Novel-LLM-project/Ninja-v1-NSFW„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 71
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8b„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂öÂ≠¶Áøí„É¢„Éá„É´„Å´ChatVector„ÇíÈÅ©Áî®„Åó„ÄÅ„Åï„Çâ„Å´QLora„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
  - Downloads: 43
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiÊßò„ÅÆ Japanese-Chat-Umievo-itr004-7b „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 38
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 37
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 36
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - ELECTRA base Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 29
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Japanese CLIP ViT-H/14 (Wider) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
  - Downloads: 29
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - Ninja-v1-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 27
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card Ëã±Êó•„ÄÅÊó•Ëã±ÁøªË®≥Áî®„É¢„Éá„É´C3TR-Adapter„ÅÆGPTQ4bitÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 23
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - isekai-bert-v1
  - Downloads: 21
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - swallow-hermes-st-v1 Áâ©Ë™û‰ΩúÊàê„Å´Âº∑„ÇÅ„Å™„É¢„Éá„É´„ÅåÂá∫Êù•„Å™„ÅÑ„Åã„Å®ËÄÉ„Åà„Å¶‰Ωú„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 16
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - MPT-7B-inst „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMosaicML„ÅÆllm-foundry„É™„Éù„Ç∏„Éà„É™„Çí‰ΩøÁî®„Åó„Å¶mosaicml/mpt-7b-instruct„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 16
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãELYZA-japanese-CodeLlama-7b-instruct„Çí Êó•Êú¨Ë™û„ÅÆ„Ç≠„É£„É™„Éñ„É¨„Éº„Ç∑„Éß„É≥„Çª„ÉÉ„Éà„ÅßÁîüÊàê„Åó„ÅüGPTQ„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 15
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - Japanese Stable LM Instruct Gamma 7B +
  - Downloads: 15
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - Model description CyberagentÊßò„ÅÆcyberagent/calm2-7b-chat„ÇíËøΩÂä†Â≠¶Áøí„Åó„Åü„ÄÅ‰ΩúÂÆ∂„Åï„ÇìÁî®„Ç¢„Ç∑„Çπ„Çø„É≥„ÉàAI„ÅÆ„Ç¢„É´„Éï„Ç°Áâà„Åß„Åô„ÄÇ
  - Downloads: 15
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - ES„ÇíÊõ∏„ÅèAI Japanese GPT-2 model„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åæ„Åó„Åü „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å´„ÅØ„ÄÅÂÜÖÂÆöËÄÖ„ÅÆ‰∫å‰∏á‰ª∂‰ª•‰∏ä„ÅÆES„ÇíÁî®„ÅÑ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 15
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 14
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 13
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 Known Performance Issues Two potential bugs have been found in this model: NEED repetition_penalty NEED high temperature Reference: Japanese LLM benchmark results at Nejumi LLM Leaderboad Neo
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - „ÅØ„Åò„ÇÅ„Å´ Google„ÅÆGemma-2B„ÇíÊó•Êú¨Ë™û„Åß‰Ωø„Åà„Çã„Çà„ÅÜ„Å´Á∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„ÇíÊñΩ„Åó„Åü„ÄÅÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 13
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - roberta-small-japanese-aozora-char Model Description
  - Downloads: 12
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - ELECTRA base Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 12
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 12
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m üêç MambaSan-370m is the first chat Japanese language model based on a state-space model architecture (Mamba).
  - Downloads: 12
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 12
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - In-progess long-context Japanese-English translation model based on tinyllama.
  - Downloads: 12
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - japanese-novel-gpt-j-6b https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b" „Å´ÂêàË®à216ÂÄã„ÅÆË©ï‰æ°„ÅÆÈ´ò„ÅÑ„Å™„Çç„ÅÜÂ∞èË™¨„ÄÅÈùíÁ©∫ÊñáÂ∫´„ÄÅ„Ç¶„Ç£„Ç≠„Éö„Éá„Ç£„Ç¢„Å™„Å©„ÅÆÊñáÁ´†„ÇíQLoRAÂ≠¶Áøí„Åï„Åõ„ÅüÂ∞èË™¨ÁîüÊàêÁî®„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - Reproduced Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 12
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - „ÅäÁü•„Çâ„Åõ „Çà„ÇäÂõûÁ≠î„ÅåÈÅ©Âàá„Å´„Å™„Çã„Çà„ÅÜ„Å´Â≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´„ÄÅhttps://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq „ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ
  - Downloads: 12
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 Model Application
  - Downloads: 12
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This model learned the proceedings of the Japanese parliament in 2022.
  - Downloads: 12
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - ÂêçË®ÄÊé®Ë´ñ„É¢„Éá„É´
  - Downloads: 11
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - japanese-soseki-gpt2-1b
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - deberta-base-japanese-aozora Model Description
  - Downloads: 11
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - Êó•Êú¨Ë™û„Åßtraining„Åó„Åüllama2 model size: 130.78M training„ÅØ‰ª•‰∏ã„ÅÆscriptÂèÇÁÖß https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 11
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - Êõ¥Êñ∞ÊÉÖÂ†± Êó•Êú¨Ë™ûÊ©üËÉΩ„Å®instruct„Éô„ÇØ„Éà„É´„ÅÆ„Éê„É©„É≥„ÇπË™øÊï¥„Åó„Åüver.2„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Åæ„Åó„ÅüSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2 „É¢„Éá„É´Ê¶ÇË¶Å Swallow-MX-8x7b-NVE-v0.1„Å´ÂØæ„Åó„ÄÅ Mixtral-8x7B-Instruct-v0.1„Å®Mixtral-8x7B-v0.1„ÅÆÂ∑ÆÂàÜ„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - Llama-3.1-70B-Japanese-Instruct-2407 Model Description This is a Japanese continually pre-trained model based on meta-llama/Meta-Llama-3.1-70B-Instruct.
  - Downloads: 5,149
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - Model Card for gemma-2-2b-jpn-it-translate-gguf gemma-2-2b-jpn-it-translate-gguf„ÅØ„ÄÅÊó•Ëã±„ÉªËã±Êó•ÁøªË®≥„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüSLMÔºàSmall Language ModelÔºâ„Åß„Åô„ÄÇ
  - Downloads: 2,007
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta RoSEtta (RoFormer-based Sentence Encoder through Distillation) is a general Japanese text embedding model, excelling in retrieval tasks.
  - Downloads: 1,528
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ about this model.
  - Downloads: 964
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - umiyuki-Umievo-itr012-Gleipnir-7B-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãUmievo-itr012-Gleipnir-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 607
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - sarashina2.1-1b-sft-gguf Aratako„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsarashina2.1-1b-sft„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 200
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallow-70B-Instruct-v0.1 This is a merge of pre-trained language models created using mergekit.
  - Downloads: 176
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Model Card for Wabisabi-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 wabisabi has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 152
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - bert-base-irony
  - Downloads: 123
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Llama 3 Youko 70B (rinna/llama-3-youko-70b)
  - Downloads: 39
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - GGUFÁâà„ÅØ„Åì„Å°„Çâascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF Ê¶ÇË¶Å Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô Êó•Êú¨Ë™û„ÇíÂê´„ÇÅ„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰ΩøÁî®„Åó„Å¶„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„Åü„ÇÅmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™ûÂäõ„Åå‰∏ä„Åå„Å£„Å¶„ÅÑ„Çã„ÅØ„Åö Mistral-Nemo„Éô„Éº„Çπ„Å™„ÅÆ„ÅßTemperature„ÅØ0.3„ÇíÂü∫Ê∫ñ„Å´Ë™øÊï¥„Åô„Çã„Åì„Å®„ÇíÊé®Â•® v0.1„Åã„Çâ„ÅÆÂ§âÊõ¥ÁÇπ „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆËøΩÂä† „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆsystem prompt„Å´&lt;„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË®ÄË™û&gt;„ÅßÂá∫Âäõ„Åô„ÇãÊåáÁ§∫„ÇíËøΩÂä† „Ç®„Éù„ÉÉ„ÇØ„Çí9ÂÄç„Å´Â¢óÂä† ‰ΩøÁî®„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„ÅÑ„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà kalomaze/Opus_Instruct_25k Nopm/Opus_WritingStruct anthracite-org/kalo-opus-instruct-22k-no-refusal Aratako/Synthetic-Japanese-Roleplay-NSFW-Claude-3.5s-15.3k-formatted Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini
  - Downloads: 34
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - dolly-japanese-gpt-1b-clone Ê¶ÇË¶Å rinnaÁ§æ„ÅÆ„Äåjapanese-gpt-1b„Äç„Çí„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Äådatabricks-dolly-15k-ja„Äç„Çí‰ΩøÁî®„Åó„Å¶Â≠¶Áøí„Åï„Åõ„ÅüÊé®Ë´ñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 21
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - deberta-large-japanese-luw-upos Model Description
  - Downloads: 21
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - roberta-large-japanese-luw-upos Model Description
  - Downloads: 18
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - roberta-small-japanese-aozora Model Description
  - Downloads: 17
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - „É¢„Éá„É´„ÅÆÊ¶ÇË¶Å line-corporation/japanese-large-lm-1.7b„ÅÆ„Éô„Éº„Çπ„É¢„Éá„É´„Å´ÂØæ„ÅóÔºåsft„Å´„Çà„Çãfull instruction tuning„ÇíË°å„ÅÑ„Åæ„Åó„ÅüÔºé
  - Downloads: 17
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 English description here Ê¶ÇË¶Å Llama-2„Éô„Éº„Çπ„ÅÆÂ≠¶ÁøíÊ∏à„ÅøÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-13b„Å®„ÄÅ„Åù„ÅÆinstruction tuning„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-13b-instruct „Çí„ÄÅmergekit„Çí‰Ωø„Å£„Å¶MoE„ÇíË°å„ÅÑ‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 16
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - „É¢„Éá„É´Ê¶ÇË¶Å Watashiha-Llama-2-13B-Ogiri-sft„ÇíLLaVA„ÅßÂ≠¶Áøí„Åó„ÄÅÁîªÂÉè„Å´ÂØæÂøú„Åó„ÅüÂ§ßÂñúÂà©Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero Base model: llm-jp/llm-jp-13b-v1.0 Instruction data: Randomly sampled, 15k Jaster dataset (train) Code is here.
  - Downloads: 13
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - deberta-small-japanese-aozora Model Description
  - Downloads: 13
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 xl on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 12
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - Japanese Stable LM Instruct Gamma 7B +
  - Downloads: 12
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 11
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - Êó•Êú¨Ë™û„Åßtraining„Åó„Åüllama2 model size: 417.12M training„ÅØ‰ª•‰∏ã„ÅÆscriptÂèÇÁÖßhttps://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
  - Downloads: 11
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - Model Card for Model ID
  - Downloads: 11
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - llm-jp-3-1.8b
  - Downloads: 25,694
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - ÂçÅÊù°ËõçÔºàHotaru JujoÔºâ„ÅÆ‰ΩúÊàê„Åó„ÅüLoRA„ÇíÈÖçÂ∏É„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 6,020
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 1,455
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B
  - Downloads: 791
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 377
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 152
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM
  - Downloads: 121
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - 4-bit ÈáèÂ≠êÂåñÁâà llm-jp-3-172b-instruct3 Êú¨„É™„Éù„Ç∏„Éà„É™„Åß„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåNII„ÄçÔºâ„ÅåÊèê‰æõ„Åô„Çã„Äållm-jp-3-172b-instruct3„Äç(‰ª•‰∏ã„ÄåÊú¨„É¢„Éá„É´„Äç) „Çí 4-bit ÈáèÂ≠êÂåñ„Åó„ÅüÊ¥æÁîü„É¢„Éá„É´ (‰ª•‰∏ã„ÄåÊú¨ÈáèÂ≠êÂåñ„É¢„Éá„É´„Äç) „ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 84
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer This model is a Phoneme Level Speech Recognition network, originally a fine-tuned version of openai/whisper-large-v3 on a mixture of Different Japanese datasets.
  - Downloads: 75
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - Japanese Parler-TTS Large (Œ≤Áâà) „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅparler-tts/parler-tts-large-v1„ÇíÂü∫„Å´„ÄÅÊó•Êú¨Ë™û„Åß„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàË™≠„Åø‰∏ä„Åí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Çà„ÅÜÂÜçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 48
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - Ê¶ÇË¶Å vecteus„ÅØ„ÄÅÈ´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™ûÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 41
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to solve error detection and correction task.
  - Downloads: 24
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - Byt5-small-ain-jpn-mt is a machine translation model pretrained with Google's ByT5-small and fine-tuned on bilingual datasets crawled from the Web.
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja Model
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 English description here Ê¶ÇË¶Å Llama-2„Éô„Éº„Çπ„ÅÆÂ≠¶ÁøíÊ∏à„ÅøÊó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-7b„Å®„ÄÅ„Åù„ÅÆinstruction tuning„É¢„Éá„É´„Åß„ÅÇ„Çãelyza/ELYZA-japanese-Llama-2-7b-instruct „Çí„ÄÅmergekit„Çí‰Ωø„Å£„Å¶MoE„ÇíË°å„ÅÑ‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 14
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This model is a merged version of qwen-14b-vntl and Qwen1.5-14B-Chat , aiming for the translation of Japanese context into Chinese.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 12
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - deberta-large-japanese-aozora-ud-head Model Description
  - Downloads: 12
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - Wav2Vec2-XLS-R-300M-Japanese-Hiragana Fine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using the Common Voice and JSUT.
  - Downloads: 12
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 12
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B Japanese-LLaMA-2-7B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - „Éô„Éº„Çπ„É¢„Éá„É´Ôºöcl-tohoku/bert-base-japanese-whole-word-masking „Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-book/wrime-sentiment „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂: adamw Optuna„Åß„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊé¢Á¥¢ Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É´„ÅÆ„Çø„Ç§„Éó(lr_scheduler_type):
  - Downloads: 12
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - roberta-base-japanese-jsnli This model is a fine-tuned version of nlp-waseda/roberta-base-japanese on the JSNLI dataset.
  - Downloads: 12
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - deberta-large-japanese-wikipedia-ud-head Model Description
  - Downloads: 12
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 12
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
  - Downloads: 12
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 11
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - Model card for model ID
  - Downloads: 11
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - fasttext-jp-embedding This model is experimental.
  - Downloads: 11
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - „ÅØ„Åò„ÇÅ„Å´ „Å™„Çì„ÅãÊó•Êú¨Ë™û„ÅåË©±„Åõ„ÇãÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™AI„Åß„Åô„ÄÇ
  - Downloads: 35
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 25
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - deberta-large-japanese-unidic Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - deberta-base-japanese-unidic Model Description
  - Downloads: 15
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - roberta-large-japanese-aozora-char Model Description
  - Downloads: 14
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - bart-large-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 14
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection„Å®„ÅØÔºü
  - Downloads: 13
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-base-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶Âõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - Description A Japanese-specialized SentencePiece tokenizer trained for AI Novelist's SuperTrin and Damsel 20B models.
  - Downloads: 13
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - friendly_JA-Model (T5 fine-tuned model) MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexicon Examples input output ÊúÄÈÅ©Âåñ„ÇíÂøúÁî®„Åó„ÅüÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„ÅØÈ´òÁ≤æÂ∫¶„Å† „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Çº„Éº„Ç∑„Éß„É≥„ÇíÂøúÁî®„Åó„Åü„Éû„Ç∑„É≥„Éà„É©„É≥„Çπ„É¨„Éº„Ç∑„Éß„É≥„É¢„Éá„É´„ÅØÈ´ò„ÅÑ„Ç¢„Ç≠„É•„É©„Ç∑„Éº„Å† ÂΩº„ÅØÊû∂Á©∫„ÅÆ‰∏ñÁïå„Å´‰Ωè„Çì„Åß„ÅÑ„Çã ÂΩº„ÅØ„Ç§„Éû„Ç∏„Éä„É™„Éº‰∏ñÁïå„Å´‰Ωè„Çì„Åß„ÅÑ„Çã Êñ∞Âûã„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ„Å´ÊÑüÊüì„Åó„Å¶„Åó„Åæ„Å£„Åü „Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ„Å´„Åã„Åã„Å£„Å¶„Åó„Åæ„Å£„Åü Ê∑±Â±§Â≠¶Áøí„ÅØÈõ£„Åó„ÅÑ „Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞„ÅØ„ÇÄ„Åö„Åã„Åó„ÅÑ Êñ∞„Åü„Å™Ê¶ÇÂøµ„ÇíÁ¥π‰ªã„Åô„Çã Êñ∞„Åó„ÅÑ„Ç≥„É≥„Çª„Éó„Éà„ÇíÁ¥π‰ªã„Åô„Çã Ê¥•Ê≥¢„ÅÆË≠¶Â†±„ÅåÊµÅ„Çå„Åü „ÉÑ„Éä„Éü„ÅÆ„Ç¢„É©„Éº„Éà„ÅåÊµÅ„Çå„Åü ÂçóÊµ∑„Éà„É©„Éï„ÅÆÁÅΩÂÆ≥„ÅØÈúáÊ∫êÂú∞„Å´„Çà„Çã ÂçóÊµ∑„Éà„É©„Éï„ÅÆ„Éá„Ç£„Ç∂„Çπ„Çø„Éº„ÅØ„Ç®„Éî
  - Downloads: 13
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 12
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - ku-accms/bert-base-japanese-ssuw Model description This is a pre-trained Japanese BERT base model for super short unit words (SSUW).
  - Downloads: 12
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - deberta-large-japanese-wikipedia Model Description
  - Downloads: 11
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - bart-base-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 11
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - ku-accms/roberta-base-japanese-ssuw Model description This is a pre-trained Japanese RoBERTa base model for super short unit words (SSUW).
  - Downloads: 11
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØdeberta-v2-tiny-japanese„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶QA„Çø„Çπ„ÇØ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - LINE DistilBERT Japanese (forked by liwii)
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 1,290
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - jpn-msa source group: Japanese target group: Malay (macrolanguage) OPUS readme: jpn-msa model: transformer-align source language(s): jpn jpn_Hani jpn_Hira jpn_Kana target language(s): ind
  - Downloads: 22
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA - Lorenzo Concina
  - Downloads: 18
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-large„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅÂõ∫ÊúâË°®ÁèæÊäΩÂá∫ÔºàNERÔºâ„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 17
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - roberta-small-hi-char-mlm Model Description
  - Downloads: 17
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - tiny_mixtral_ja„ÇíinstructionÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßtraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åôhttps://huggingface.co/if001/tiny_mixtral_ja
  - Downloads: 15
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - Wav2Vec2 Accent Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese accent dataset When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 15
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 14
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 14
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØluke-japanese-large„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Å¶„ÄÅJCommonsenseQA(ÈÅ∏ÊäûÂºèÂøúÁ≠î)„Å´Áî®„ÅÑ„Çå„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 12
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - VITS TTS Japanese Only Amitaro VITS TTS model finetuned using free voice data from amitaro free voice here „ÅÇ„Åø„Åü„Çç„ÅÆÂ£∞Á¥†ÊùêÂ∑•Êàø Finetuning code is from Plachtaa - VITS Fast Fine-tuning See sample usage Lycoris53/VITS-TTS-Japanese-Only-Amitaro Model Details 76 annotated wav file train for 600 epoch Êó•Êú¨Ë™û„ÅÆË™¨Êòé„Å™„Å©„Åì„Å°„Çâ„Å´ AiThinkso.net Developed by:
  - Downloads: 12
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - k-ush/xlm-roberta-base-ance-en-jp-warmup A XLM-RoBERTa-base model trained on mMARCO Japanese dataset with ANCE warmup script.
  - Downloads: 12
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - ELECTRA Base Japanese for Information Triage
  - Downloads: 12
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - roberta-small-hi-char Model Description
  - Downloads: 12
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 12
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - Details: https://spacy.io/models/ja#ja_core_news_md Japanese pipeline optimized for CPU.
  - Downloads: 11
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - What is this model?
  - Downloads: 11
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - „Éô„Éº„Çπ„É¢„Éá„É´Ôºöcl-tohoku/bert-base-japanese-whole-word-masking „Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºötyqiangz/multilingual-sentiments „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫: 16Âõ∫ÂÆö „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂: adamw Optuna„Åß„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊé¢Á¥¢ Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É´„ÅÆ„Çø„Ç§„Éó(lr_scheduler_type):
  - Downloads: 11
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - Japanese CLIP ViT-H/14 (Deeper) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation
  - Downloads: 11
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - Japanese CLIP ViT-H/14 (Base) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
  - Downloads: 11
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - distilhubert-ft-japanese-50k Fine-tuned (more precisely, continue trained)
  - Downloads: 11
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - „Ç¢„Éã„É°Â£∞„ÅÆ„Çà„ÅÜ„Å™„Çè„Åñ„Å®„Çâ„Åó„ÅÑÂ£∞„Åß„ÇÇ„Å™„Åè„ÄÅ„Éú„Ç´„É≠„Å™„Å©„ÅÆ„ÇΩ„Éï„Éà„Çí‰Ωø„Å£„Åü„ÅÑ„Åã„Å´„ÇÇÂêàÊàê„ÅÆÈü≥Â£∞„Åß„ÇÇ„Å™„Åè„ÄÅ„ÇØ„É©„Çπ„Å´‰∏Ä‰∫∫„Åè„Çâ„ÅÑ„ÅÑ„Åù„ÅÜ„Å™„ÄÅËá™ÁÑ∂„ÅßË¶™„Åó„Åø„ÇÑ„Åô„ÅÑÁæéÂ∞ëÂ•≥„ÅÆÂ£∞„Çí‚Ä¶„ÄÇ
  - Downloads: 11
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 11
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 16
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 14
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g.
  - Downloads: 14
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - Japanese Parler-TTS Mini (Œ≤Áâà) „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅparler-tts/parler-tts-mini-v1„ÇíÂü∫„Å´„ÄÅÊó•Êú¨Ë™û„Åß„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàË™≠„Åø‰∏ä„Åí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Çà„ÅÜÂÜçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 1,039
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - spekulatius „Éû„Éº„Ç∏„Åó„Å¶„ÅÑ„Çã„Å®„Åü„Åæ„Å´Âá∫„Å¶„Åè„Çã„ÄåÁõÆÁöÑ„ÅÆÊÑèÂõ≥„Å®„ÅØÈÅï„ÅÜ„ÅÆ„Å†„Åë„Å©„Å™„Çì„Å†„ÅãÊ∂à„Åô„Å´„ÅØ„ÇÇ„Å£„Åü„ÅÑ„Å™„ÅÑ„É¢„Éá„É´„Äç„Çí„Åä„Åô„ÅùÂàÜ„Åë„Åô„Çã„Ç∑„É™„Éº„Ç∫„Åß„Åô„ÄÇ
  - Downloads: 14
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese for Irony
  - Downloads: 13
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 25
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - Model Card For llm-jp-3-1.8b-instruct-gguf LLM-jp„Åï„Çì„ÅÆllm-jp-3-1.8b-instruct„ÇíÈáèÂ≠êÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åü„Å°„Åß„Åô„ÄÇ
  - Downloads: 1,466
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - (English part follows Japanese one.
  - Downloads: 36
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - llm-jp-13b-instruct-lora-jaster-v1.0
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: Âº∑ÂåñÂ≠¶Áøí„ÇíÁî®„ÅÑ„Å¶„Ç≠„É£„É©„ÇØ„Çø„Çâ„Åó„Åï„Çí‰ªò‰∏é„Åó„ÅüÈõëË´áÂøúÁ≠î„ÅÆÁîüÊàê
  - Downloads: 11
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - Llama-3-ELYZA-JP-8B-AWQ Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 1,668
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository contains some GGUF quantizations of the VNTL Gemma 2 27B model.
  - Downloads: 134
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 üö® If you want to avoid outputs that appear to be literal translations, please prompt this model to role-play as a Japanese person.
  - Downloads: 62
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - (English part follows Japanese one.
  - Downloads: 21
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct üêç MambaSan-instruct is the first chat Japanese language model based on a state-space model architecture (Mamba), not a transformer.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - Kokuwa lametta„ÅÆÊîπËâØ„Åß„Éû„Éº„Ç∏„Åï„Åõ„Çã„É¢„Éá„É´Êé¢„Åó„Çí„Åó„Å¶„ÅÑ„Åü„ÇâKiwiMix„Å®„ÅÑ„ÅÜÈù¢ÁôΩ„Åù„ÅÜ„Å™„É¢„Éá„É´„ÇíË¶ã„Å§„Åë„Åæ„Åó„Åü„ÄÇ
  - Downloads: 12
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 23
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - Details: https://spacy.io/models/ja#ja_core_news_trf Japanese transformer pipeline (Transformer(name='cl-tohoku/bert-base-japanese-char-v2', piece_encoder='char', stride=160, type='bert', width=768, window=216, vocab_size=6144)).
  - Downloads: 13
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 13
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - ELECTRA small Japanese discriminator for Irony
  - Downloads: 11
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 11
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,932
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-7B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,413
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 568
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 29
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ"chatntq-ja-7b-v1.0"„Çí„Éô„Éº„Çπ„Å´„Åó„Åü7B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - zenz-v2 zenz-v2„ÅØGPT-2„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Å´Âü∫„Å•„Åè„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - Japanese transformer pipeline (bert-base).
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-gguf matsuo-lab„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãweblab-10b-instruction-sft„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 59
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAE„ÅÆÂÜÖËáì„ÅØ„Å™„ÅÑ„ÅûÔºÅ„Å®Ë®Ä„Çè„Åõ„Å™„ÅÑ„ÅûÔºÅÔºÅÔºÅÔºÅ
  - Downloads: 21
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakuten„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãRakutenAI-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 85
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - SakuraMixSeries ËÉåÊôØ„Å®„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇØ„Ç™„É™„ÉÜ„Ç£„Éº„Çí‰∏°Á´ã„Åï„Åõ„ÅüVAEÂÜÖËîµÂûã„É¢„Éá„É´ Model with built-in VAE for both background and character quality üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá
  - Downloads: 65
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 13
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„Å´Áï∞„Å™„ÇãÊ®©Èôê„ÇíË®≠ÂÆö„Åô„Çã Have different permissions when sharing merges
  - Downloads: 45
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - Ê¶ÇË¶Å „ÄåLOCAL AI HACKATHON„Äç„Å´„Åä„Åë„Çã„ÄÅ„ÉÅ„Éº„É†DataPilot,4„Å§„ÇÅ„ÅÆÊàêÊûúÂìÅ„Åß„Åô„ÄÇ
  - Downloads: 11
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
## Datasets

This list is sorted by downloads as of February 04, 2025.
435 datasets are listed.

- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - „Éã„Ç≥„Éã„Ç≥ÂÆüÊ≥Å ÈÅéÂéª„É≠„Ç∞„Ç¢„Éº„Ç´„Ç§„Éñ „Éã„Ç≥„Éã„Ç≥ÂÆüÊ≥Å ÈÅéÂéª„É≠„Ç∞„Ç¢„Éº„Ç´„Ç§„Éñ„ÅØ„ÄÅ„Éã„Ç≥„Éã„Ç≥ÂÆüÊ≥Å „ÅÆ„Çµ„Éº„Éì„ÇπÈñãÂßã„Åã„ÇâÁèæÂú®„Åæ„Åß„ÅÆ„Åô„Åπ„Å¶„ÅÆÈÅéÂéª„É≠„Ç∞„Ç≥„É°„É≥„Éà„ÇíÂèéÈõÜ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 326,149
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU Japanese Massive Multitask Language Understanding Benchmark JMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
  - Downloads: 128,635
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - The Cauldron is a massive collection of 50 vision-language datasets (training sets only) that were used for the fine-tuning of the vision-language model Idefics2.
  - Downloads: 6,747
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023:
  - Downloads: 5,932
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - Please feel free to open an issue or pull request.
  - Downloads: 3,527
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100: Êó•Êú¨Ë™ûinstruction„É¢„Éá„É´Ë©ï‰æ°„Éá„Éº„Çø„Çª„ÉÉ„Éà Data Description Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØinstruction-tuning„ÇíË°å„Å£„Åü„É¢„Éá„É´„ÅÆË©ï‰æ°Áî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 2,285
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - Please feel free to open an issue or pull request.
  - Downloads: 1,449
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023:
  - Downloads: 1,384
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 Êó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ japanese-anime-speech-v2 is an audio-text dataset designed for training automatic speech recognition models.
  - Downloads: 1,378
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - Maintainers Junfeng Jiang@Aizawa Lab: jiangjf (at) is.s.u-tokyo.ac.jp Jiahao Huang@Aizawa Lab: jiahao-huang (at) g.ecc.u-tokyo.ac.jp
  - Downloads: 1,329
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
  - Downloads: 1,281
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
  - Downloads: 1,242
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - We provide an Amazon product reviews dataset for multilingual text classification.
  - Downloads: 1,032
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - Overview This dataset provides a convenient and user-friendly format of data from Aozora Bunko (ÈùíÁ©∫ÊñáÂ∫´), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.
  - Downloads: 948
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - Dataset.
  - Downloads: 925
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB:
  - Downloads: 897
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - VNTL Leaderboard
  - Downloads: 867
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - Êó•Êú¨Ë™û„Ç¶„Çß„Éñ„Ç≥„Éº„Éë„Çπ2010 „Åì„Å°„Çâ„ÅÆ„Éá„Éº„Çø„Çíhuggingface„Å´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° 2009 Âπ¥Â∫¶„Å´„Åä„Åë„ÇãËëó‰ΩúÊ®©Ê≥ï„ÅÆÊîπÊ≠£ÔºàÂπ≥Êàê21Âπ¥ÈÄöÂ∏∏ÂõΩ‰ºö Ëëó‰ΩúÊ®©Ê≥ïÊîπÊ≠£Á≠â„Å´„Å§„ÅÑ„Å¶ | ÊñáÂåñÂ∫ÅÔºâ„Å´Âü∫„Å•„ÅçÔºåÊÉÖÂ†±Ëß£ÊûêÁ†îÁ©∂„Å∏„ÅÆÂà©Áî®„Å´Èôê„Å£„Å¶Âà©Áî®ÂèØËÉΩ„Åß„ÅôÔΩ° ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„ÇíÁî®„ÅÑ„Å¶ÔΩ§Ëá™Âãï„ÅßÂè•ÁÇπ„Çí„Å§„Åë„Åæ„Åó„ÅüÔΩ° Â§âÊèõ„Ç≥„Éº„Éâ Â§âÊèõ„Çπ„ÇØ„É™„Éó„Éà ÂΩ¢ÊÖãÁ¥†Ëß£Êûê„Å™„Å©
  - Downloads: 839
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - llm-japanese-dataset LLMÊßãÁØâÁî®„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥(„ÉÅ„É£„ÉÉ„Éà)„Éá„Éº„Çø„Çª„ÉÉ„Éà ‰∏ª„Å´ÔºåËã±Ë™û„ÅßÊßãÁØâ„Åï„Çå„ÅüLLM„É¢„Éá„É´„Å™„Å©„Å´ÂØæ„Åó„Å¶Ôºå„ÉÅ„É£„ÉÉ„Éà(Instruction)ÂøúÁ≠î„Çø„Çπ„ÇØ„Å´Èñ¢„Åó„Å¶LoRA„Å™„Å©„Åß„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åß„Åç„Åæ„ÅôÔºé
  - Downloads: 829
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese Anime Speech Dataset Êó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
  - Downloads: 796
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This is the filtered Japanese subset of XL-Sum followed by PaLM 2 filters 15-gram overlap * code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86a number of examples train: 4215 (before: 7113) validation: 758 (before: 889) test: 766 (before: 889)
  - Downloads: 778
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - CC-MAIN-2019-30„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 766
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - Elite Voice Project „Åì„Çå„ÅØ„Éõ„É≠„É©„Ç§„ÉñÊâÄÂ±ûVtuber„Åï„Åè„Çâ„Åø„ÅìÊ∞è„ÅÆÂ£∞„Çí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂåñ„ÅóÈü≥Â£∞Ë™çË≠ò„Å™„Å©„ÅßÊ¥ªÁî®„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åô„Çã‰∫ã„ÇíÁõÆÁöÑ„Å®„Åó„ÅüÈùûÂÖ¨Âºè„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß„Åô„ÄÇ
  - Downloads: 701
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 681
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - VOICEVOX„Çí‰Ωø„Å£„Åü‰∫∫Â∑•Èü≥Â£∞„Éú„Ç§„Çπ„Éá„Éº„Çø„Çª„ÉÉ„Éà ‰ΩøÁî®„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Ç≥„Éº„Éë„Çπ ITA„Ç≥„Éº„Éë„Çπ „Å§„Åè„Çà„Åø„Å°„ÇÉ„Çì„Ç≥„Éº„Éë„Çπ ROHAN„Ç≥„Éº„Éë„Çπ „Éá„Éº„Çø„Çª„ÉÉ„ÉàÈáèÊÉÖÂ†± „Éï„Ç©„É´„ÉÄÂÜÖ„ÅÆ.
  - Downloads: 662
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - In this study, we introduce a new dataset, WRIME, for emotional intensity estimation.
  - Downloads: 593
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - This is a Japanese translated version of HumanEval, an evaluation harness for the HumanEval problem solving dataset described in the paper "Evaluating Large Language Models Trained on Code".
  - Downloads: 545
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - This dataset contains a diverse set of natural Japanese speech, collected from terrestrial television streams.
  - Downloads: 543
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - wikipedia Êó•Êú¨Ë™û„ÅÆÊñá„Çí„ÄÅÂêÑÁ®ÆÊó•Êú¨Ë™û„ÅÆ embeddings „ÇÑ faiss index „Å∏„Å®Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 517
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - Japan Diverse Images Dataset Overview This dataset is a comprehensive collection of high-quality images capturing the diverse aspects of Japan, including urban landscapes, natural scenery, historical sites, contemporary art, everyday life, and culinary experiences.
  - Downloads: 515
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÁî®„ÅÑ„Å¶Ëá™ÂãïÁîüÊàê„Åï„Çå„ÅüÊó•Êú¨Ë™û„ÅÆÊåáÁ§∫„Å®„Åù„Çå„Å´ÂØæ„Åô„ÇãÂøúÁ≠î„ÅÆ„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„Åß„Åô„ÄÇ
  - Downloads: 503
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU:
  - Downloads: 482
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - GitHub „É™„Éù„Ç∏„Éà„É™ ids-cv/wrime „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 480
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 Description Malum-230 is a meticulously handcrafted Japanese dataset featuring multi-turn conversations and passages, specifically designed for logical reasoning tasks.
  - Downloads: 471
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA : Japanese Question Answering with Retrieval Augmentation - Ê§úÁ¥¢Êã°Âºµ(RAG)Ë©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û Q&amp;A „Éá„Éº„Çø„Çª„ÉÉ„Éà È´òÊÄßËÉΩ„Å™ LLM „ÅÆÂè∞È†≠„Å´‰º¥„ÅÑ„ÄÅLLM „ÇíÁî®„ÅÑ„ÅüË≥™ÁñëÂøúÁ≠î„ÅÆ„É¶„Éº„Çπ„Ç±„Éº„Çπ„ÅåÂ¢óÂä†„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 457
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - CC-MAIN-2019-39„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 455
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - databricks-dolly-15k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 438
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
  - Downloads: 410
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - AutoWikiQA Êù±Â∑•Â§ß„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-MX„ÇíÁî®„ÅÑ„Å¶„ÄÅWikipedia‰∏≠„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂÖ•Âäõ„Å®„Åó„Å¶„ÄåË≥™Âïè(query)„Äç„Å®„ÄåÂõûÁ≠î(answer)„Äç„ÇíÁîüÊàê„Åó„ÄÅÁîüÊàê„Åï„Çå„ÅüË≥™Âïè„Å®ÂõûÁ≠î„Å´„Å§„ÅÑ„Å¶„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 389
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - Êó•Êú¨Ë™û„Ç≥„Éº„Éë„Çπ mc4-ja„Å™„Å©„ÅÆweb„Ç≥„Éº„Éë„Çπ„Çí„ÇØ„É™„Éº„Éã„É≥„Ç∞ÂæåÔΩ§ÊïôÂ∏´„Å™„ÅóÂ≠¶Áøí„É¢„Éá„É´„Åß„ÉÜ„Ç≠„Çπ„Éà„ÇíÁ¥Ñ1‰∏á‰ª∂„Å´„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Åó„Åü„Ç≥„Éº„Éë„Çπ„Åß„ÅôÔΩ° Ëëó‰ΩúÊ®©Ê≥ï„ÅßË™ç„ÇÅ„Çâ„Çå„ÅüÊÉÖÂ†±Ëß£ÊûêÁõÆÁöÑ„Åß‰ΩøÁî®„Åß„Åç„Åæ„ÅôÔΩ° ‰∏ÄÈÉ®„ÅÆ„Éï„Ç°„Ç§„É´„Åó„ÅãparquetÂåñ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÅÆ„ÅßÔΩ§„ÅîÊ≥®ÊÑè„Åè„Å†„Åï„ÅÑÔΩ°„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÅØout„Éï„Ç©„É´„ÉÄÂÜÖ„Å´„ÅÇ„Çä„Åæ„Åô git lfs„Å™„Å©„Åß„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åè„Å†„Åï„ÅÑÔΩ°
  - Downloads: 378
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - Êó•Êú¨Ë™û„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí SentenceTransformes „ÅßÂ≠¶Áøí„Åó„ÇÑ„Åô„ÅÑ„Ç´„É©„É†Âêç„Å®ÊßãÈÄ†„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 368
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST Dataset lassify images from the KMNIST dataset into one of the 10 classes, representing different Japanese characters.
  - Downloads: 326
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶ „Åì„ÅÆ„Éá„Éº„Çø„ÅØ„ÄÅÊó•Êú¨„ÅÆÂÆòÂÖ¨Â∫Å„ÅÆWeb„Çµ„Ç§„Éà„Å´Êé≤Ëºâ„Åï„Çå„Å¶„ÅÑ„Çã„Äå„Çà„Åè„ÅÇ„ÇãË≥™Âïè„Äç„ÇíÊâã‰ΩúÊ•≠„ÅßÊäΩÂá∫„Åó„ÄÅ„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥Áî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 321
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - cc100-ja-documents HuggingFace „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã cc100 / cc100-ja „ÅØ line Âçò‰Ωç„ÅÆÂàÜÂâ≤„ÅÆ„Åü„ÇÅ„ÄÅdocument Âçò‰Ωç„Å´ÁµêÂêà„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 319
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - ü•ï Â¶ÇÊûúÂÖîÂÖîÁöÑ‰ªìÂ∫ìÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÁöÑËØùÁÇπ‰∏™‚≠êÂñµ~ If Tutu's repository is helpful to you, please give it a ‚≠ê meow~ „ÇÇ„Åó„ÅÜ„Åï„Åé„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅåÂΩπ„Å´Á´ã„Å£„ÅüÂ†¥Âêà„ÅØ„ÄÅ‚≠ê„Çí„ÅΩ„Å°„Å£„Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„Å´„ÇÉ„Çì~ üçâ ‰ªª‰Ωï ‚ùì
  - Downloads: 305
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth: Japanese/English Synthetic Conversation Dataset Derived from LMSYS-Chat-1M
  - Downloads: 304
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - „Ç∑„É≥„Éó„É´„Åö„Çì„Å†„ÇÇ„Çì„Éá„Éº„Çø„Çª„ÉÉ„Éà „ÅØ„Åò„ÇÅ„Å´ „Åö„Çì„Å†„ÇÇ„Çì„ÅÆË®≠ÂÆö„ÅåË©∞„Åæ„Å£„Åü„Ç∑„É≥„Éó„É´„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 287
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - Anime with caption CC-0 dataset „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„Ç§„É©„Çπ„Éà„Å´ÂØæ„Åô„ÇãÊó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥„Çí ÂÄ´ÁêÜÁöÑ„Å´Â≠¶Áøí„Åó„ÇÑ„Åô„Åè„Åô„Çã„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 275
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - Github„É™„Éù„Ç∏„Éà„É™stockmarkteam/ner-wikipedia-dataset„ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 251
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - Dataset details: Each entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
  - Downloads: 249
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - [Under Construction]
  - Downloads: 247
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench Dataset Description Japanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
  - Downloads: 242
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
  - Downloads: 240
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") Ê¶ÇË¶Å Â∑ùÊü≥ÊäïÁ®ø„Çµ„Ç§„Éà„ÅÆ„ÄéÂÜôÁúüÂ∑ùÊü≥„Äè„Å®„ÄéÂ∑ùÊü≥ÊäïÁ®ø„Åæ„Çã„Åõ„Çì„Äè„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„ÄÅ„Åä„Çà„Å≥ YANS ÂßîÂì°„Åå‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„ÇíÂê´„Åø„Åæ„Åô„ÄÇ
  - Downloads: 229
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - Washi (a kind of traditional Japanese paper)
  - Downloads: 227
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - CC-news-2024-July-October-cleaned „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommon Crawl„ÅÆnews„Çµ„Éñ„Çª„ÉÉ„Éà„Åã„Çâ‰ΩúÊàê„Åó„Åü2024Âπ¥7Êúà„Åã„Çâ10Êúà„ÅÆÊó•Êú¨Ë™û„ÅÆ„Éã„É•„Éº„Çπ„ÅÆÊñáÁ´†„ÅåÂèéÈå≤„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 217
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - llm-japanese-dataset-vanilla LLMÊßãÁØâÁî®„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà izumi-lab/llm-japanese-dataset „Åã„ÇâÔºåÊó•Ëã±ÁøªË®≥„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠â„ÇíÊäú„ÅÑ„Åü„ÇÇ„ÅÆ„Åß„ÅôÔºé
  - Downloads: 208
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - „Ç™„É™„Ç∏„Éä„É´„ÅÆ„Çµ„Ç§„Éà„Å®Âêå„Åò„ÇÇ„ÅÆ„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 201
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - ‰ª•‰∏ã„ÅÆ„Éá„Éº„ÇøÊ∫ê„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÊäΩÂá∫„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇÇ„Å®„Å´ÔΩ§phi3„ÅßÂÜçÁîüÊàê„Åó„ÅüÊñáÁ´†„Åß„ÅôÔΩ° Wikibooks Wikipedia Cosmopedia Âà§‰æã„Éá„Éº„Çø „Éá„Éº„Çø parquet„Éï„Ç°„Ç§„É´„ÅåÊï∞ÂçÅGBÁ®ãÂ∫¶„ÅÇ„Çä„Åæ„Åô datasets„É©„Ç§„Éñ„É©„É™„Åã„Çâ„Åß„ÅØÔΩ§„ÅØ„Åò„ÇÅ„ÅÆÊï∞GBÁ®ãÂ∫¶„Åó„ÅãË™≠„ÅøËæº„ÇÅ„Å™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„ÅôÔΩ°git lfs„Å™„Å©„Åß„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åù„ÅÜ„Åß„ÅôÔΩ° „Ç≥„Éº„Éâ „Åì„Å°„Çâ ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ°
  - Downloads: 201
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda - Questions for Japanese models Repository:
  - Downloads: 199
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - range3/cc100-ja This dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
  - Downloads: 195
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - Dataset Summary This is the Business Scene Dialogue (BSD) dataset, a Japanese-English parallel corpus containing written conversations in various business scenarios.
  - Downloads: 191
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard „Å®„ÅØ Allganize RAG Leaderboard „ÅØ„ÄÅ5„Å§„ÅÆÊ•≠Á®Æ„Éâ„É°„Ç§„É≥ÔºàÈáëËûç„ÄÅÊÉÖÂ†±ÈÄö‰ø°„ÄÅË£ΩÈÄ†„ÄÅÂÖ¨ÂÖ±„ÄÅÊµÅÈÄö„ÉªÂ∞èÂ£≤Ôºâ„Å´„Åä„ÅÑ„Å¶„ÄÅÊó•Êú¨Ë™û„ÅÆRAG„ÅÆÊÄßËÉΩË©ï‰æ°„ÇíÂÆüÊñΩ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 191
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - „Ç™„É™„Ç∏„Éä„É´„ÅÆ„Çµ„Ç§„Éà„Å®Âêå„Åò„ÇÇ„ÅÆ„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 191
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - CC-MAIN-2019-35„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 191
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset is a clarified version of the image, context, and question set included in the Japanese-Heron-Bench for the construction of the Japanese evaluation benchmark suite.
  - Downloads: 179
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - „Çã„Çä„ÅÆ„Çπ„ÉÜ„ÉÉ„Ç´„Éº just for fun.
  - Downloads: 173
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅLLM„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§ËÉΩÂäõ„ÇíË®àÊ∏¨„Åô„Çã„Éô„É≥„ÉÅ„Éû„Éº„ÇØJapanese-RP-BenchÁî®„ÅÆË©ï‰æ°„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 160
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - Dataset Summary SNOW T15:The simplified corpus for the Japanese language.
  - Downloads: 156
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset A voice dataset collected from Project Sekai charactor Emu Otori Introduction Size: 2735, all WAV format.
  - Downloads: 155
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR: Japanese Casual Web IR - Êó•Êú¨Ë™ûÊÉÖÂ†±Ê§úÁ¥¢Ë©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆÂ∞èË¶èÊ®°„Åß„Ç´„Ç∏„É•„Ç¢„É´„Å™Web„Çø„Ç§„Éà„É´„Å®Ê¶ÇË¶Å„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà ËøëÂπ¥„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÅÆÂè∞È†≠„Å´„Çà„Çä„ÄÅ‰∏ÄËà¨ÁöÑ„Å™Êó•Êú¨Ë™û„ÇíÁî®„ÅÑ„ÅüËá™ÁÑ∂„Å™Ê§úÁ¥¢„ÇØ„Ç®„É™„ÅßË≥™Âïè„Åô„Çã„É¶„Éº„Çπ„Ç±„Éº„Çπ„ÅåÂ¢ó„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 146
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - üì∞ News
  - Downloads: 145
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - Ë©ï‰æ°„Çπ„Ç≥„Ç¢„ÅÆÂÜçÁèæÊÄßÁ¢∫‰øù„Å® SB Intuitions ‰øÆÊ≠£Áâà„ÅÆÂÖ¨ÈñãÁî®„ÇØ„É≠„Éº„É≥ „ÇΩ„Éº„Çπ: yahoojapan/JGLUE on GitHub JSQuAD JSQuAD is a Japanese version of SQuAD (Rajpurkar+, 2016), one of the datasets of reading comprehension.
  - Downloads: 144
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - Synthetic-JP-EN-Coding-Dataset-801k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà801262‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 143
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
  - Downloads: 138
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023:
  - Downloads: 138
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences Dataset „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÊó•Êú¨Ë™ûÁâàWikipedia„ÅÆË®ò‰∫ã„ÇíÂÖÉ„Å´‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 135
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset Êó•Êú¨Ë™ûÊúâÂÆ≥ÊñáÊõ∏„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄåLLM-jp Toxicity Dataset„Äç See https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset
  - Downloads: 134
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This is a Japanese portion of the Guanaco dataset.
  - Downloads: 133
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - „Éû„É´„ÉÅ„É™„É≥„Ç¨„É´„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 132
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja This repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 132
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - J-ResearchCorpus Update: 2024/3/16Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨30ÂõûÂπ¥Ê¨°Â§ß‰ºö(NLP2024)„ÇíÂê´„ÇÄ„ÄÅË´ñÊñá 1,343 Êú¨„ÅÆ„Éá„Éº„Çø„ÇíËøΩÂä† 2024/2/25Ë®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöË™å„ÄåËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Äç„ÅÆ„ÅÜ„Å° CC-BY-4.0 „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãË´ñÊñá 360 Êú¨„ÅÆ„Éá„Éº„Çø„ÇíËøΩÂä† Ê¶ÇË¶Å CC-BY-* „É©„Ç§„Çª„É≥„Çπ„ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãÊó•Êú¨Ë™ûË´ñÊñá„ÇÑÂ≠¶‰ºöË™åÁ≠â„Åã„ÇâÊäúÁ≤ã„Åó„ÅüÈ´òÂìÅË≥™„Å™„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 130
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - Dataset 5M (5121625) clean Japanese full sentence with the context.
  - Downloads: 128
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - range3/wiki40b-ja This dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
  - Downloads: 128
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - en-ja-align Êó•Ëã±ÂØæË®≥ÊñáÂØæÂøú‰ªò„Åë„Éá„Éº„Çø(ÂÜÖÂ±±„Çâ, 2003)„Å®„Åó„Å¶ÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãÊó•Ëã±ÂØæË®≥Êñá„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 128
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - Dataset overview This dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).
  - Downloads: 126
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - Umamusume-voice-transcription Total charcters: 77 Comes with transcription.
  - Downloads: 125
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct Update: 2023/12/27„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ JaxTon , „Éó„É≠„Å´„Å™„ÇãJava „ÅÆ„Ç≥„Éº„Éâ„Éá„Éº„Çø 180 „É¨„Ç≥„Éº„Éâ„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 123
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - ‰ª•‰∏ã„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰∏≠„ÅÆ256ÊñáÂ≠ó‰ª•‰∏ã„ÅÆË°å„ÇíÊäΩÂá∫„Åó„Éû„Éº„Ç∏„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 122
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - ebisuke/liz-nojaloli-ja-ds License MIT License Description ebisuke/liz-nojaloli-ja„ÅÆÂ≠¶ÁøíÂÖÉ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 121
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki Wikipedia„ÅÆHTMLÂΩ¢Âºè„ÅÆ„ÉÄ„É≥„Éó„Éï„Ç°„Ç§„É´„Åã„ÇâÊäΩÂá∫„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 120
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - Dataset Details Dataset Sources Repository: Helsinki-NLP/Tatoeba-Challenge Detail: Japanese - Korean jpn-kor Uses The dataset can be used to train the translation model that translates Japanese sentence to Korean.
  - Downloads: 120
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - Êó•Êú¨Ë™û„Ç¶„Ç£„Ç≠„Éö„Éá„Ç£„Ç¢ÂÖ•ÂäõË™§„Çä„Éá„Éº„Çø„Çª„ÉÉ„Éà (Êº¢Â≠óË™§Â§âÊèõÊäΩÂá∫Áâà) Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØÔºå‰∫¨ÈÉΩÂ§ßÂ≠¶ Ë®ÄË™û„É°„Éá„Ç£„Ç¢Á†îÁ©∂ÂÆ§„Å´„Çà„Å£„Å¶ÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíHuggingFace„Åß‰ΩøÁî®„Åß„Åç„Çã„Çà„ÅÜÂ§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔºé
  - Downloads: 119
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - Dataset containing ~7000 synthetically generated (by GPT-4o-mini) children's stories in Japanese that only use simple words.
  - Downloads: 116
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - OpenMathInstruct-1 „ÇíÊó•Êú¨Ë™û„Å´Ëá™ÂãïÁøªË®≥„Åó„ÅüÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™180‰∏á‰ª∂„ÅÆÊåáÁ§∫„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 112
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-bokete", split="train") Ê¶ÇË¶Å Â§ßÂñúÂà©ÊäïÁ®ø„Çµ„Ç§„ÉàBokete„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 111
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - Dataset Information „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅThePioneer/japanese-photos„ÅÆÂÜôÁúü„Çí„ÅäÂÄü„Çä„Åó„Å¶„ÄÅ Qwen/Qwen2-VL-7B-Instruct Qwen/Qwen2.5-32B-Instruct-AWQ „ÅÆ2„Å§„ÅÆ„É¢„Éá„É´„Çí‰Ωø„Å£„Å¶ÁîüÊàê„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 110
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - CABank Japanese Sakura Corpus Susanne Miyata Department of Medical Sciences Aichi Shukotoku University smiyata@asu.aasa.ac.jp website: https://ca.talkbank.org/access/Sakura.html Important
  - Downloads: 107
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA This dataset is hf mirror of https://registry.opendata.aws/abeja-cc-ja/ Please Refer to https://tech-blog.abeja.asia/entry/abeja-cc-ja-202409 „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØhttps://registry.opendata.aws/abeja-cc-ja/„ÅÆHF„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 106
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØOpen_o1_sft_Pro„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíQwenÁ§æ„ÅÆQwen2.5-14B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Åü„ÇÇ„ÅÆ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 105
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - Dataset Summary RealPersonaChat „ÅØÔºåË©±ËÄÖÊú¨‰∫∫„ÅÆ„Éö„É´„ÇΩ„Éä„Å®ÊÄßÊ†ºÁâπÊÄß„ÇíÂê´„ÇÄÔºåÁ¥Ñ14,000‰ª∂„ÅÆÊó•Êú¨Ë™ûÈõëË´áÂØæË©±„Åã„Çâ„Å™„Çã„Ç≥„Éº„Éë„Çπ„Åß„ÅôÔºé
  - Downloads: 103
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà„ÇíHF„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åó„ÄÅÂêÑ„Éá„Éº„Çø„Å´‰ªò‰∏é„Åï„Çå„ÅüURL„Åã„ÇâÂÖÉË®ò‰∫ã„ÅÆ„Çø„Ç§„Éà„É´„ÇíÂèñÂæóÂèØËÉΩ„Å™„ÇÇ„ÅÆ„Å´„Å§„ÅÑ„Å¶„ÅØÂèñÂæó„Åó„Å¶‰ªò‰∏é„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 103
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - Dataset Summary 53,640 Japanese tweets with annotation if a tweet is related to COVID-19 or not.
  - Downloads: 101
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - fungi_indexed_mycological_papers_japanese Â§ßËèåËº™„ÄåË´ñÊñá3Ë°å„Åæ„Å®„ÇÅ„Äç„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊúÄÁµÇÊõ¥Êñ∞Êó•Ôºö2024/9/28ÔºàR3-12108„Åæ„ÅßÔºâ Languages Japanese This dataset is available in Japanese only.
  - Downloads: 101
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - GitHub „É™„Éù„Ç∏„Éà„É™ singletongue/wikipedia-utils „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 100
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Data created manually
  - Downloads: 98
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - Anime Songs Lyrics Dataset ‚Äï „Ç¢„Éã„É°„ÇΩ„É≥„Ç∞„ÅÆÊ≠åË©û„Éá„Éº„Çø„Çª„ÉÉ„Éà Welcome to the Anime Songs Lyrics Dataset Overview This dataset compiles a diverse collection of lyrics from various anime songs, providing a rich resource for enthusiasts and researchers alike.
  - Downloads: 96
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/bokete-ogiri-test", split="test") Ê¶ÇË¶Å Â§ßÂñúÂà©ÊäïÁ®ø„Çµ„Ç§„ÉàBokete„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 96
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - Tengentoppa corpus for sft (Combined Japanese Instruction Dataset) Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÊó•Êú¨Ë™û„ÅÆ instruction-following „Éá„Éº„Çø„Çª„ÉÉ„Éà16ÂÄã„ÇíÁµ±Âêà„Åó„Å¶‰ΩúÊàê„Åï„Çå„ÅüÂ§ßË¶èÊ®°„Å™ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶ÁøíÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 96
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - Dataset Summary This dataset is a Japanese-translated version of the OpenO1-SFT dataset, containing Chain of Thought (CoT) reasoning examples designed for fine-tuning language models.
  - Downloads: 95
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIO with extended answers AIO (AIÁéã) is a Japanese quiz dataset.
  - Downloads: 95
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - HF Datasets version of Tanaka Corpus.
  - Downloads: 95
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - llm-book/aio-passages „ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶„ÄÅllm-book/bert-base-japanese-v3-bpr-passage-encoder „Å´„Çà„Çã„Éë„ÉÉ„Çª„Éº„Ç∏„ÅÆ„Éê„Ç§„Éä„É™„Éô„ÇØ„Éà„É´„Åå embeddings „Éï„Ç£„Éº„É´„Éâ„Å´ËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 94
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - alpaca_jp_python alpaca_jp_python„ÅØ„ÄÅ Stanford Alpaca„ÅÆÊâãÊ≥ï mistralai/Mixtral-8x22B-Instruct-v0.1 „Åß‰Ωú„Å£„ÅüÂêàÊàê„Éá„Éº„Çø(Synthetic data)„Åß„Åô„ÄÇ
  - Downloads: 93
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - Lurunchik/WikiHowNFQA„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„ÄÅ‰∫∫Êâã„Åß„ÇØ„É™„Éº„Éã„É≥„Ç∞„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 93
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - Ë©ï‰æ°„Çπ„Ç≥„Ç¢„ÅÆÂÜçÁèæÊÄßÁ¢∫‰øù„Å® SB Intuitions ‰øÆÊ≠£Áâà„ÅÆÂÖ¨ÈñãÁî®„ÇØ„É≠„Éº„É≥ „ÇΩ„Éº„Çπ: aiishii/JEMHopQA on GitHub JEMHopQA JEMHopQA (Japanese Explainable Multi-hop Question Answering) is a Japanese multi-hop QA dataset that can evaluate internal reasoning.
  - Downloads: 92
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - Ë©¶È®ìÁöÑ„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Å™„Çä„Åæ„Åô Ë©ï‰æ°Áî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúÊàêÊñπÊ≥ï ChatGPT-4o„ÅßÂºÅÁêÜÂ£´Á¥π‰ªã„ÇíÂê´„ÇÄ5„Å§„ÅÆË¶≥ÁÇπ„Åã„Çâ50„ÅÆ„ÇØ„Ç®„É™„ÇíÁîüÊàê ChatGPT-4o„Åß„ÇØ„Ç®„É™„Å´ÂØæ„Åô„ÇãÂõûÁ≠î„Å®Ë©ï‰æ°„Éù„Ç§„É≥„Éà„ÇíÁîüÊàê‚ÄªÂºÅÁêÜÂ£´„ÇíÁõ¥Êé•Á¥π‰ªã„Åô„Çã10Âïè„ÅØÈô§„Åè Èô§Â§ñ„Åó„Åü10Âïè„Å´Èñ¢„Åó„Å¶„ÅØ„ÄÅÈñãÊîæÁâπË®±ÊÉÖÂ†±„Éá„Éº„Çø„Éô„Éº„ÇπÂèä„Å≥j-platpat„ÅÆÂÖ¨Èñã„Éá„Éº„Çø„Çà„ÇäÂõûÁ≠î„ÇíÊâãÂãï‰ΩúÊàê
  - Downloads: 92
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - Dataset Summary This dataset is a Japanese-translated subset of the NuminaMath CoT dataset, containing the first 100k samples from the original dataset.
  - Downloads: 91
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 91
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - Overview This dataset is of conversations extracted from Aozora Bunko (ÈùíÁ©∫ÊñáÂ∫´), which collects public-domain books in Japan, using a simple heuristic approach.
  - Downloads: 90
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset ÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™Ë∂ÖÂ∞èË¶èÊ®°È´òÂìÅË≥™Êó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 90
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - mqa„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆquery--passage„ÅÆ„Éö„Ç¢„Å´„Å§„ÅÑ„Å¶ÈáçË§á„ÇíÂâäÈô§„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 89
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - Ëá™ÂãïÁîüÊàê„ÅÆ„Éû„É´„ÉÅ„Çø„Éº„É≥„Éá„Éº„Çø„Çª„ÉÉ„Éà „Ç™„Éº„Éó„É≥„Å™„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÔΩ§Calm3-22b„Çí‰Ωø„Å£„Å¶Q&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ° „Éá„Éº„Çø„ÇΩ„Éº„Çπ „ÅØ„Åò„ÇÅ„ÅÆË≥™Âïè(q1)„ÇíÔΩ§Á®Æ„ÄÖ„ÅÆ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÂèéÈõÜ„Åó„Åæ„Åó„ÅüÔΩ°„Åù„ÅÆÂæå„ÅÆ„ÇÑ„Çä„Å®„Çä„ÅØ„Åô„Åπ„Å¶ÔΩ§Calm„ÅåÁîüÊàê„Åó„Åæ„Åó„ÅüÔΩ°Ë≥™ÂïèÊñá„Å´„Å§„ÅÑ„Å¶„ÅØÔΩ§ÂÖÉ„Éá„Éº„Çø„ÅÆ„É©„Ç§„Çª„É≥„Çπ„Å´Ê∫ñÊã†„Åó„Åæ„ÅôÔΩ° oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
  - Downloads: 89
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec: Gender Dection from Japanese Names with Machine Learning
  - Downloads: 89
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M: Japanese Light Novel Character Names Corpus Overview This dataset extracts fictional character names from the publicly available text of novels on the Japanese light novel platform "Sh≈çsetsuka ni Nar≈ç" (syosetu.com),
  - Downloads: 86
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench: A Benchmark for Advanced Japanese Reasoning Capabilities KUM-Bench (Kyoto University Math Entrance Exam Benchmark) is designed to evaluate advanced Japanese reasoning capabilities by leveraging mathematics entrance exam questions from Kyoto University‚Äîone of the most prestigious universities in Japan.
  - Downloads: 85
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - abc-multiple-choice Dataset abc-multiple-choice „ÅØ„ÄÅÁ´∂ÊäÄ„ÇØ„Ç§„Ç∫„ÅÆÂ§ß‰ºö„Äåabc„Äç„Åß‰ΩøÁî®„Åï„Çå„Åü4ÊäûÂïèÈ°å„ÇíÂÖÉ„Å´‰ΩúÊàê„Åï„Çå„Åü„ÄÅÂ§öËÇ¢ÈÅ∏ÊäûÂºè„ÅÆË≥™ÂïèÂøúÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 85
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted 20240907 „Éá„Éº„ÇøÂ¢óÈáèÔºàÁ¥Ñ19800‰ª∂‚ÜíÁ¥Ñ39600‰ª∂Ôºâ Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 84
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - Windows„ÅÆÊñπ„ÅØggml-japanese-gpt2„ÅÆÂÆüË°å„Éï„Ç°„Ç§„É´„ÅßÂãï„Åè„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 84
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - , 2023) was trained on.
  - Downloads: 84
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - Âõ∫ÊúâË°®Áèæ„É©„Éô„É´„ÅØllm-book/ner-wikipedia-dataset„Å®ÂêåÊßò„ÅÆ„ÇÇ„ÅÆ„ÇíÊé°Áî®„Åó„Å¶„Åä„Çä„ÄÅÂÖ®ÈÉ®„Åß8Á®ÆÈ°û (‰∫∫Âêç„ÄÅÊ≥ï‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅË£ΩÂìÅÂêç„ÄÅÊîøÊ≤ªÁöÑÁµÑÁπîÂêç„ÄÅÊñΩË®≠Âêç„ÄÅ„Åù„ÅÆ‰ªñ„ÅÆÁµÑÁπîÂêç„ÄÅ„Ç§„Éô„É≥„ÉàÂêç)„ÅÇ„Çä„Åæ„Åô„ÄÇ
  - Downloads: 83
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - GitHub „É™„Éù„Ç∏„Éà„É™ cl-tohoku/quiz-datasets „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 81
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - The JaNLI (Japanese Adversarial NLI) dataset, inspired by the English HANS dataset, is designed to necessitate an understanding of Japanese linguistic phenomena and to illuminate the vulnerabilities of models.
  - Downloads: 80
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - Ë©ï‰æ°„Çπ„Ç≥„Ç¢„ÅÆÂÜçÁèæÊÄßÁ¢∫‰øù„Å® SB Intuitions ‰øÆÊ≠£Áâà„ÅÆÂÖ¨ÈñãÁî®„ÇØ„É≠„Éº„É≥ „ÇΩ„Éº„Çπ: yahoojapan/JGLUE on GitHub datasets/jcommonsenseqa-v1.1 JCommonsenseQA JCommonsenseQA is a Japanese version of CommonsenseQA (Talmor+, 2019), which is a multiple-choice question answering dataset that requires commonsense reasoning ability.
  - Downloads: 80
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - [github].
  - Downloads: 78
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 77
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - mbpp-ja
  - Downloads: 77
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - OpenOrca„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊó•Êú¨Ë™ûÁøªË®≥Áâà„Åß„Åô https://huggingface.co/datasets/Open-Orca/OpenOrca ÁèæÂú®ÁøªË®≥‰ΩúÊ•≠„ÅåÁ∂öË°å‰∏≠„Åß„ÄÅOpenOrcaÂÖ®‰Ωì„ÅÆ1/5Á®ãÂ∫¶„ÅÆÁøªË®≥„ÅåÁµÇ„Çè„Å£„ÅüÁä∂ÊÖã„Åß„Å≤„Å®„Åæ„ÅöÂÖ¨Èñã„Åó„Åæ„Åô„ÄÇ
  - Downloads: 76
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - Ëá™ÂãïÁîüÊàêQ&amp;A „Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÔΩ§MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF„Çí‰Ωø„Å£„Å¶Q&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° „ÉÅ„Éº„É†„Åß‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Åä„Çà„Å≥„ÄåCommon Crawl„Çí„ÇÇ„Å®„Å´ÁîüÊàê„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 76
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - Asian Language Treebank (ALT) Project ALT Parallel Corpus„ÅÆ„ÅÜ„Å°„ÄÅÊó•Ëã±ÂØæË®≥ÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 75
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - japanese-stackexchange Ëã±Ë™û„Å´„Çà„ÇãÊó•Êú¨Ë™û„Å´Èñ¢„Åô„ÇãË≥™Âïè„Åå„Åß„Åç„Çã Japanese Stack Exchange „ÅÆ„Éá„Éº„Çø„ÉÄ„É≥„Éó „Çí„ÇÇ„Å®„Å´„Éá„Éº„Çø„ÇíÂä†Â∑•„Åó„ÄÅË≥™ÂïèÊñá„Å®ÂõûÁ≠îÊñá„ÅÆ„Éö„Ç¢„Å´„Å™„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü QA „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 75
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") Ê¶ÇË¶Å Â∑ùÊü≥ÊäïÁ®ø„Çµ„Ç§„Éà„ÅÆ„ÄéÂÜôÁúüÂ∑ùÊü≥„Äè„Å®„ÄéÂ∑ùÊü≥ÊäïÁ®ø„Åæ„Çã„Åõ„Çì„Äè„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 74
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - Ëã±Ë™ûWikipediaË®ò‰∫ã„ÅÆÂÜíÈ†≠Ë§áÊï∞Êñá„ÇíÊäΩÂá∫„Åó„ÄÅ‰∫∫Êâã„ÅßÊó•Êú¨Ë™ûÁøªË®≥„Åó„ÅüÊñáÁ´†„É¨„Éô„É´ÂØæË®≥„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 74
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - japanese-asr/whisper_transcriptions.reazon_speech_all without audio
  - Downloads: 74
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - recruit-jp/japanese-image-classification-evaluation-dataset Overview Developed by: Recruit Co.
  - Downloads: 74
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - Japanese stopwords for nagisa
  - Downloads: 74
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - „Çµ„É®Â≠ê Èü≥Â£∞„Ç≥„Éº„Éë„Çπ „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊñπÊ≥ï „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂúßÁ∏Æ„Åó„Åüzip„Éï„Ç°„Ç§„É´„Çí„ÄÅgdrive„Å´ÁΩÆ„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 72
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
  - Downloads: 72
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - The images were sourced from https://huggingface.co/datasets/ThePioneer/japanese-photos.
  - Downloads: 72
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - Introduction This is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
  - Downloads: 72
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - Anime Quotes Dataset ‚Äï „Ç¢„Éã„É°„ÅÆÂêçË®Ä„Éá„Éº„Çø„Çª„ÉÉ„Éàüéê Welcome to Anime Quotes Dataset Overview This dataset contains a curated collection of inspiring and memorable quotes from various anime series, sourced from the Anime Motivation website.
  - Downloads: 71
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - ms_marco_japanese ms_marco „ÅÆÊó•Êú¨Ë™ûÁøªË®≥„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 71
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 Dataset Description JA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
  - Downloads: 71
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - JSNLI Version 1.1 „ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„ÅÜ„Å°„ÄÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆË®ìÁ∑¥„Çª„ÉÉ„Éà (train_w_filtering)
  - Downloads: 70
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-shashin", split="train") Ê¶ÇË¶Å Ê†™Âºè‰ºöÁ§æÊù±Âª∫„Ç≥„Éº„Éù„É¨„Éº„Ç∑„Éß„É≥„ÅåÈÅãÂñ∂„Åô„Çã„Éõ„Éº„É†„É°„Ç§„Éà„Éª„É™„Çµ„Éº„ÉÅ„Å´„Çà„Çã„Äé„Éõ„Éº„É†„É°„Ç§„ÉàÂ∑ùÊü≥Â§ßË≥û„Äè„ÅÆ„ÅÜ„Å°„ÄÅ„ÅäÈ°å„ÅåÁîªÂÉèÂΩ¢Âºè„ÅßÊèê‰æõ„Åï„Çå„Çã„ÄéÂÜôÁúüÂ∑ùÊü≥„Äè„Å´Èñ¢„Åô„Çã„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 70
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - dialogsum-ja „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØdialogsum„ÄÅCSDS„Å™„Å©„ÇíÁøªË®≥„Åó„ÅüÊó•Êú¨Ë™ûÂØæË©±Ë¶ÅÁ¥Ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 70
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - The dataset contains (almost) the entire OpenSubtittles database for Japanese: Over 7000 tv shows and/or movies.
  - Downloads: 68
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
  - Downloads: 68
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - neody/oscar-ja-cleaned„ÅÆ„Éá„Éº„Çø„ÅÆ‰∏≠„Åã„Çâ256ÊñáÂ≠ó‰ª•‰∏ã„ÅÆ„ÇÇ„ÅÆ„ÇíÊäΩÂá∫„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 68
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - „É™„Ç¢„É´Á≥ª„É¢„Éá„É´„Å´ÁâπÊúâ„ÅÆËÇñÂÉèÊ®©„ÅÆÂïèÈ°å„Å´„Å§„ÅÑ„Å¶ÊØîËºÉÁöÑ„ÇØ„É™„Ç¢„Å™„É¢„Éá„É´„Çí‰Ωú„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å™„Çà„ÅÜ„Å´„ÄÅÁßÅ„ÅåÁßÅËá™Ë∫´„Åã„Çâ‰Ωú„ÇäÂá∫„Åó„Åü‰∫∫Â∑•Ë∂ÖÂΩºÂ•≥Ôºàver 2.1Á≥ª„ÄÅver 2.6Á≥ªÔºâ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàÁ¥Ñ2800ÊûöÔºâ„Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 68
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - Ëá™ÂãïÁîüÊàêQ&amp;A Á®Æ„ÄÖ„ÅÆ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÔΩ§MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF„Çí‰Ωø„Å£„Å¶Q&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° CC-BYÁ≥ª„Åæ„Åü„ÅØApatch-2.0„ÅÆ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÇíÊîπÂ§â„Åó„Å¶ÁîüÊàê„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 67
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - Êó•Êú¨Ë™ûWikipediaÂÖ•ÂäõË™§„Çä„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„Çå„ÅØ‰∫¨ÈÉΩÂ§ßÂ≠¶ Ë®ÄË™û„É°„Éá„Ç£„Ç¢Á†îÁ©∂ÂÆ§„Å´„Çà„Å£„Å¶ÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíHuggingFace„Åß‰ΩøÁî®„Åß„Åç„Çã„Çà„ÅÜÂ§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔºé
  - Downloads: 67
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - def prompt(japanese, english):
  - Downloads: 67
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - Êù±‰∫¨Â§ßÂ≠¶ÊùæÂ∞æ„ÉªÂ≤©Êæ§Á†îÁ©∂ÂÆ§‰∏ªÂÇ¨„ÅÆLLMË¨õÂ∫ß2024„ÅÆÁ¨¨5Âõû„ÄåSFT„ÄçÊºîÁøí„Åß‰ΩøÁî®„Åô„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 67
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - CABank Japanese CallHome Corpus Participants: 120 Type of Study: phone call Location: United States Media type: audio DOI: doi:10.21415/T5H59V Web: https://ca.talkbank.org/access/CallHome/jpn.html Citation information Some citation here.
  - Downloads: 66
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This is a handmade dataset for making a Japanese chatbot.
  - Downloads: 66
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent„ÅØ‰∫∫Êâã„Åß‰ΩúÊàê„Åï„Çå„ÅüÈ´òÂìÅË≥™„Åß„ÇØ„É™„Éº„É≥„Å™100„Çª„ÉÉ„Éà„ÅÆÊó•Êú¨Ë™ûCoTÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 66
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - Ëá™ÂãïÁîüÊàê„ÅÆ„Éû„É´„ÉÅ„Çø„Éº„É≥„Éá„Éº„Çø„Çª„ÉÉ„Éà „Ç™„Éº„Éó„É≥„Å™„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÔΩ§MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF„Çí‰Ωø„Å£„Å¶Q&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° Èñ¢ÈÄ£„Ç≥„Éº„Éâ ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ° „Éá„Éº„Çø„ÇΩ„Éº„Çπ „ÅØ„Åò„ÇÅ„ÅÆË≥™Âïè(q1)„ÇíÔΩ§Á®Æ„ÄÖ„ÅÆ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÂèéÈõÜ„Åó„Åæ„Åó„ÅüÔΩ°„Åù„ÅÆÂæå„ÅÆ„ÇÑ„Çä„Å®„Çä„ÅØ„Åô„Åπ„Å¶ÔΩ§Mixtral„ÅåÁîüÊàê„Åó„Åæ„Åó„ÅüÔΩ°Ë≥™ÂïèÊñá„Å´„Å§„ÅÑ„Å¶„ÅØÔΩ§ÂÖÉ„Éá„Éº„Çø„ÅÆ„É©„Ç§„Çª„É≥„Çπ„Å´Ê∫ñÊã†„Åó„Åæ„ÅôÔΩ° oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
  - Downloads: 66
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 66
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - TALPCo„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊó•Ëã±ÁøªË®≥„Éö„Ç¢„ÇíHuggingFaceÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 66
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - Self-Instruct-Qwen2.5-72B-Instruct-60k Ê¶ÇË¶Å ‰ª•‰∏ã„ÅÆÊâãÈ†Ü„Åß‰ΩúÊàê„Åó„ÅüÁ¥Ñ6‰∏á‰ª∂„ÅÆÊó•Êú¨Ë™û„ÅÆÂêàÊàêinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 66
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - Magpie-Tanuki-8B-annotated-96k Magpie„ÅÆÊâãÊ≥ï„Çíweblab-GENIAC/Tanuki-8B-dpo-v1.0„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Magpie-Tanuki-8B-97k„Å´ÂØæ„Åó„Å¶„ÄÅcyberagent/calm3-22b-chat„ÇíÁî®„ÅÑ„Å¶instruction„Å´ÂØæ„Åó„Å¶Èõ£ÊòìÂ∫¶„ÄÅ„ÇØ„Ç™„É™„ÉÜ„Ç£„ÄÅ„Ç´„ÉÜ„Ç¥„É™„Çí„Ç¢„Éé„ÉÜ„Éº„Ç∑„Éß„É≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 66
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - Japanese Prompt of GuanacoDataset extracted using langdetect.
  - Downloads: 65
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - magpie-sft-v1.0-dpo-judged Ê¶ÇË¶Å llm-jp/magpie-sft-v1.0„ÇíÂÖÉ„Å´‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÊîπÂ§â„ÇíÂä†„Åà„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûPreference„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 65
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - Ê¶ÇË¶Å Common Voice Corpus 17.0„ÇíspeechMOS„Å´„Å¶Èü≥Â£∞ÂìÅË≥™„ÅÆÂàÜÊûê„ÇíË°å„Å£„ÅüÁµêÊûú„Åß„Åô„ÄÇ
  - Downloads: 64
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - iterative-dpo-data-for-SimPO-iter2 Ê¶ÇË¶Å ÂêàÊàêinstruction„Éá„Éº„Çø„Åß„ÅÇ„ÇãAratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k„ÇíÂÖÉ„Å´‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÊâãÈ†Ü„Åß‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûPreference„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 64
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Conversations-Magpie-Nemotron-4-10k Magpie„ÅÆÊâãÊ≥ï„Çínvidia/Nemotron-4-340B-Instruct„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ10000‰ª∂„ÅÆÊó•Êú¨Ë™ûinstruction tuningÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 64
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - Japanese version of MMLU dataset tranlasted by gpt-3.5-turbo.
  - Downloads: 63
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - cosmopedia-japanese-20k„ÅÆ„Éá„Éº„Çø„Å´„ÄÅkunishouÊßò„Åã„Çâ20k-100k„Çí„ÅîÊèê‰æõ„ÅÑ„Åü„Å†„Åë„Çã„Åì„Å®„Å´„Å™„Çä100k„Åæ„ÅßÊã°Â§ß„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 63
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun Dataset Description ChouBun is a benchmark for assessing LLMs' performance in long-context tasks in the Japanese language.
  - Downloads: 63
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - Wikipedia„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™û„ÅÆÂõ∫ÊúâË°®ÁèæÊäΩÂá∫„Éá„Éº„Çø„Çª„ÉÉ„Éà GitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/ LICENSE: CC-BY-SA 3.0 Developed by Stockmark Inc.
  - Downloads: 63
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - ‰ª•‰∏ã„ÅÆ„Éá„Éº„ÇøÊ∫ê„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÊäΩÂá∫„Åó„ÅüÊó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇÇ„Å®„Å´ÔΩ§Phi-3„Åß‰ΩúÊñá„Åó„Åü„Ç≥„Éº„Éë„Çπ„Åß„ÅôÔΩ° OpenMathInstruct-1-1.8m-ja „Ç≥„Éº„Éâ „Åì„Å°„Çâ ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ°
  - Downloads: 62
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSEC„Éõ„Éº„É†„Éö„Éº„Ç∏
  - Downloads: 62
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - ShareGPT-Processed The RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.
  - Downloads: 62
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - Filtered and modified version of Japanese/Chinese language pair data from WikiMatrix v1.
  - Downloads: 61
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 61
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - „Ç™„É™„Ç∏„Éä„É´„ÅÆ„Çµ„Ç§„Éà„Å®Âêå„Åò„ÇÇ„ÅÆ„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 61
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - danbooru-ja-tag-pair-20241015 2024/10/15„Å´‰ΩúÊàê„Åó„Åüdanbooru„Çø„Ç∞„Å®Êó•Êú¨Ë™û„Çø„Ç∞„ÅÆ„Éö„Ç¢„Éá„Éº„Çø„Çª„ÉÉ„Éà(Á¥Ñ15‰∏á‰ª∂) p1atdev/danbooru-ja-tag-pair-20240715 „Å®„ÅÆÈÅï„ÅÑ„ÅØ„ÄÅ „Éô„Éº„Çπ„ÅÆwiki„Éá„Éº„Çø„ÅåÂ¢ó„Åà„Åü„ÅÆ„Åß„Åù„ÅÆÂàÜÂØæÂøú„Çø„Ç∞„ÇÇÂ¢ó„Åà„Åü fasttext„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÇíÊåü„ÇÄ„Çà„ÅÜ„Å´„Åó„Åü „ÄåÊòé„Çâ„Åã„Å´‰ªñË®ÄË™û„ÅÆ„Çø„Ç∞„Äç„ÅåÊ∑∑„Åò„ÇãÈ†ªÂ∫¶„ÅØ„Å°„Çá„Å£„Å®Ê∏õ„Å£„ÅüÊ∞ó„Åå„Åô„Çã„Åë„Å©„ÄÅÂÆåÂÖ®„Åß„ÅØ„Å™„ÅÑ (calm3„Åè„Çì„ÅÆÂá¶ÁêÜ„Å´)„Éü„Çπ„Åå„Å™„Åë„Çå„Å∞„ÄÅÊúÄ‰Ωé‰∏Ä„Å§‰ª•‰∏ä„ÅÆÊó•Êú¨Ë™û„Çø„Ç∞ (other_names „Éï„Ç£„Éº„É´„Éâ) „ÅåÂ≠òÂú®„Åô„Çã„ÅØ„Åö ‰ΩúÊàêÈÅéÁ®ã isek-ai/danbooru-wiki-2024 „ÅÆ #202408-at20240906 revision „ÇíÂÖÉ„Å´„ÄÅ other_names (Âü∫Êú¨ÁöÑ„Å´Pixiv„ÅÆ„Çø„Ç∞)„Åå„Å§„ÅÑ„Å¶„ÅÑ„Çã„ÇÇ„ÅÆ„Åã„Çâ„ÄÅÊó•Êú¨Ë™û„Åò„ÇÉ„Å™„ÅÑ„ÇÇ„ÅÆ„ÉªÊõñÊòß„ÉªÊÑèÂë≥„ÅÆÈÅé‰∏çË∂≥„ÅåÂ§ß„Åç„ÅÑ„Çø„Ç∞„ÇíÈô§Âéª„ÄÇ
  - Downloads: 61
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - llm-jp-corpus-v3„ÅÆwarp_html„ÅÆ„ÅÜ„Å°level2„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åï„Çå„Åü„Éá„Éº„Çø„ÇíHF„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åó„ÄÅÂêÑ„Éá„Éº„Çø„Å´‰ªò‰∏é„Åï„Çå„ÅüURL„Åã„ÇâÂÖÉË®ò‰∫ã„ÅÆ„Çø„Ç§„Éà„É´„ÇíÂèñÂæóÂèØËÉΩ„Å™„ÇÇ„ÅÆ„Å´„Å§„ÅÑ„Å¶„ÅØÂèñÂæó„Åó„Å¶‰ªò‰∏é„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 61
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - Dataset Summary JMultiWOZ is a large-scale Japanese multi-domain task-oriented dialogue dataset.
  - Downloads: 61
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k 20240907 „Éá„Éº„ÇøÂ¢óÈáèÔºàÁ¥Ñ19800‰ª∂‚ÜíÁ¥Ñ39600‰ª∂Ôºâ Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ39600‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 60
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 60
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - Dataset origin: https://github.com/doc-analysis/XFUND XFUND:
  - Downloads: 60
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-clean filtered row["meta"]["ÊñáÂ≠óÈÅ£„ÅÑÁ®ÆÂà•"] == "Êñ∞Â≠óÊñ∞‰ªÆÂêç"
  - Downloads: 60
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - cyberagent/calm2-7b-chat„ÅÆÂá∫Âäõ„Çí‰∫∫Êâã„Åß„ÉÅ„Çß„ÉÉ„ÇØ„Éª‰øÆÊ≠£„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûInstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - „Éá„Éº„Çø„Çª„ÉÉ„ÉàÊ¶ÇË¶Å ÊâãÂãï„Åß‰ΩúÊàê„Åó„ÅüDatabricks„Å´Èñ¢„Åô„ÇãË≥™Âïè„Å®ÂõûÁ≠î„Éö„Ç¢„ÅÆÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - oasst2-33k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 59
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
  - Downloads: 58
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - Reasoning„ÄÅÁü•Ë≠ò„ÄÅ‰ºöË©±„ÅÆÊéõ„ÅëÂêà„ÅÑ„Å™„Å©„ÅÆÊÉÖÂ†±ÂØÜÂ∫¶„ÅåÈ´ò„ÅÑ„Éû„É´„ÉÅ„Çø„Éº„É≥„ÅÆ‰ºöË©±„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 58
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - iterative-dpo-data-for-ORPO-iter3 Ê¶ÇË¶Å ÂêàÊàêinstruction„Éá„Éº„Çø„Åß„ÅÇ„ÇãAratako/Self-Instruct-Qwen2.5-72B-Instruct-60k„ÇíÂÖÉ„Å´‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÊâãÈ†Ü„Åß‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûPreference„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 58
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - Magpie-Tanuki-Instruction-Selected-Evolved-26.5k Ê¶ÇË¶Å ‰ª•‰∏ã„ÅÆÊâãÈ†Ü„Åß‰ΩúÊàê„Åó„ÅüÁ¥Ñ2‰∏á6500‰ª∂„ÅÆÊó•Êú¨Ë™û„ÅÆÂêàÊàêinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 58
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - msmarco-ja-hard-negatives hpprc/msmarco-ja „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„ÇãMS MARCO„ÅÆÊó•Êú¨Ë™ûÁøªË®≥„Éá„Éº„Çø„Å´„ÄÅ‰ª•‰∏ã„ÅÆÂá¶ÁêÜ„ÇíÂä†„Åà„Åü„Éè„Éº„Éâ„Éç„Ç¨„ÉÜ„Ç£„Éñ„Éû„Ç§„Éã„É≥„Ç∞„Çí„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 58
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
  - Downloads: 57
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k Magpie„ÅÆÊâãÊ≥ï„Çíweblab-GENIAC/Tanuki-8B-dpo-v1.0„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅ97269‰ª∂„ÅÆÊó•Êú¨Ë™ûÂØæË©±„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 57
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - Dataset Summary From the official README.md: CAMERA (CyberAgent Multimodal Evaluation for Ad Text GeneRAtion) is the Japanese ad text generation dataset.
  - Downloads: 57
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - fungi_trait_circus_databaseÂ§ßËèåËº™„ÄåTrait Circus„Äç„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàÁµ±Âà∂ÂΩ¢Ë≥™ÔºâÊúÄÁµÇÊõ¥Êñ∞Êó•Ôºö2023/12/29 Languages Japanese and English Please do not use this dataset for academic purposes for the time being.
  - Downloads: 56
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - range3/wikipedia-ja-20230101
  - Downloads: 56
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Pretrain is a localized version of the original LLaVA Pretrain dataset.
  - Downloads: 56
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - bluemoon-fandom-1-1-rp-jp-translated A subset of Squish42/bluemoon-fandom-1-1-rp-cleaned translated to Japanese using command-r-08-2024.
  - Downloads: 56
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 56
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - Japanese Wikipedia Human Retrieval dataset This is a Japanese question answereing dataset with retrieval on Wikipedia articles by trained human workers.
  - Downloads: 56
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - ja-stackoverflow Êó•Êú¨Ë™ûÁâà Stack Overflow „ÅÆ „Çπ„Çø„ÉÉ„ÇØ„Éª„Ç™„Éº„Éê„Éº„Éï„É≠„Éº „ÅÆ„Éá„Éº„Çø„ÉÄ„É≥„Éó „Çí„ÇÇ„Å®„Å´„Éá„Éº„Çø„ÇíÂä†Â∑•„Åó„ÄÅË≥™ÂïèÊñá„Å®ÂõûÁ≠îÊñá„ÅÆ„Éö„Ç¢„Å´„Å™„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü QA „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 56
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja „ÅÆ‰∏≠„Åã„Çâ JGLUEÔºà JcommonsenseQA , MARC-ja , JSQuAD Ôºâ„ÅÆË¶≥ÁÇπ„ÅßÈ´òÂìÅË≥™„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Áµû„ÇäËæº„Çì„Å†„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 55
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - Japanese-Roleplay-Dialogues This is a dialogue corpus collected from Japanese role-playing forum (commonly known as "„Å™„Çä„Åç„Çä„ÉÅ„É£„ÉÉ„Éà(narikiri chat)").
  - Downloads: 55
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - The paper of GIELLM dataset.
  - Downloads: 55
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØÊÄùËÄÉ„É¢„Éá„É´„ÇíË£Ω‰Ωú„Åô„ÇãÈöõ„ÅÆ„ÇÇ„Å®„Å®„Å™„ÇãË≥™Âïè„Éá„Éº„Çø„ÇíÈõÜ„ÇÅ„Åü„ÇÇ„ÅÆ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 55
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - Bluemoon_Top50MB_Sorted_Fixed_ja SicariusSicariiStuff/Bluemoon_Top50MB_Sorted_Fixed„Çí„ÄÅGENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awq„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Åü„É≠„Éº„É´„Éó„É¨„Ç§Â≠¶ÁøíÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 55
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - Wikidata parallel descriptions en-ja Parallel corpus for machine translation generated from wikidata dump (2024-05-06).
  - Downloads: 55
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - It covers multiple fields such as tourism, medical treatment, daily life, news, etc.
  - Downloads: 54
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE[JNLI]: Japanese General Language Understanding Evaluation JNLI(yahoojapan/JGLUE)
  - Downloads: 54
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - fungi_diagnostic_chars_comparison_japaneseÂ§ßËèåËº™„ÄåË≠òÂà•ÂΩ¢Ë≥™„Åæ„Å®„ÇÅ„Äç„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊúÄÁµÇÊõ¥Êñ∞Êó• /
  - Downloads: 54
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - Dataset Preprocessing Supported Tasks and Leaderboards Languages Ê≥®Èáà„ÅØ„Åô„Åπ„Å¶Êó•Êú¨Ë™û„Çí‰∏ªË¶ÅË®ÄË™û„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 53
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella corpus : Japanese a cappella vocal ensemble corpus The jaCappella corpus is a corpus of Japanese a cappella vocal ensembles.
  - Downloads: 53
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - defamation_japanese_twitter TwitterÊó•Êú¨Ë™ûË™πË¨ó‰∏≠ÂÇ∑Ê§úÂá∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Dataset Summary SNS„Å´„Åä„Åë„ÇãË™πË¨ó‰∏≠ÂÇ∑Ê§úÂá∫„ÅÆ„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅôÔºé
  - Downloads: 53
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - For the English version, please click here.
  - Downloads: 53
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This dataset contains document-length Japanese-English parallel texts from various sources.
  - Downloads: 53
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - Ëá™ÂãïÁîüÊàêQ&amp;A „Éá„Éº„Çø„ÇΩ„Éº„Çπ„Åã„ÇâÔΩ§MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF„Çí‰Ωø„Å£„Å¶Q&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° Common Crawl„Çí„ÇÇ„Å®„Å´ÁîüÊàê„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 53
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - NVIDIA „ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çã SteerLM Âêë„Åë„ÅÆ„Éà„É©„Ç§„Ç¢„É´„Éá„Éº„Çø„Çª„ÉÉ„Éà HelpSteer2„ÇíÊó•Êú¨Ë™û„Å´Ëá™ÂãïÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 53
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - Derived from ÂÖ®ÂõΩÊõ∏Ë™å„Éá„Éº„Çø„Åã„Çâ‰ΩúÊàê„Åó„ÅüÊåØ„Çä‰ªÆÂêç„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàGitHubÔºâ
  - Downloads: 53
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 53
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØsakura_japanese_dataset„ÅÆË≥™Âïè„Å´ÂõûÁ≠î„Åô„ÇãÂΩ¢Âºè„Åß‰Ωú„Çâ„Çå„Åü„ÄÅ‰∏ÄÂïè‰∏ÄÁ≠îÂΩ¢Âºè„ÅÆÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 53
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - JP Voice-Text Dataset for
  - Downloads: 53
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - cosmopedia-100k „ÅÆindex 20k ÔΩû 100k „ÇíÊó•Êú¨Ë™û„Å´Ëá™ÂãïÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Å´„Å™„Çä„Åæ„ÅôÔºà„ÉÜ„Ç≠„Çπ„Éà„ÅåÈï∑„Åô„Åé„Å¶ÁøªË®≥„Ç®„É©„Éº„Å´„Å™„Å£„Åü„É¨„Ç≥„Éº„Éâ„ÅØÈô§Â§ñ„Åó„Å¶„ÅÑ„Åæ„ÅôÔºâ„ÄÇ
  - Downloads: 53
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - Dataset used to train Pok√©mon text to image model, add a Japanese Column of Pok√©mon BLIP captions BLIP generated captions for Pok√©mon images from Few Shot Pok√©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
  - Downloads: 53
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - JAQKET „Åã„Çâ CC-BY-SA „ÅÆ„Éá„Éº„Çø„ÅÆ„Åø„ÇíÂê´„ÇÅ„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà AIÁéã ÂÖ¨ÂºèÈÖçÂ∏É„Éá„Éº„Çø„Çª„ÉÉ„Éà(JAQKET) „ÅßÈÖçÂ∏É„Åï„Çå„Å¶„ÅÑ„Çã„ÇØ„Ç§„Ç∫„Éá„Éº„Çø„ÅÆ„ÅÜ„Å°„ÄÅ„É©„Ç§„Çª„É≥„Çπ„Åå CC-BY-SA-4.0„ÅÆ„Éá„Éº„Çø„ÅÆ„Åø„ÇíÂê´„ÇÅ„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 52
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - Corrected MT-Bench-ja Inflection AI„Å´„Çà„ÇãCorrected MT-Bench„ÅÆÊó•Êú¨Ë™ûË®≥„Åß„Åô„ÄÇ
  - Downloads: 52
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Dataset origin: https://jibiki.fr/data/ Description Les buts du projet Jibiki.fr sont de construire de mani√®re collaborative un dictionnaire fran√ßais-japonais de qualit√© et √† large couverture ainsi qu'un corpus bilingue align√©.
  - Downloads: 52
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - Magpie-Tanuki-Qwen2.5-72B-Answered Aratako/Magpie-Tanuki-8B-annotated-96k„Åã„Çâinput_quality„Ååexcellent„ÅÆ„ÇÇ„ÅÆ„ÇíÊäΩÂá∫„Åó„ÄÅ„Åù„Çå„Å´ÂØæ„Åó„Å¶Qwen/Qwen2.5-72B-Instruct„ÅßÂõûÁ≠î„ÅÆÂÜçÁîüÊàê„ÇíË°å„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 52
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - Description This is a collection of raw data from ~40 Japanese open source downstream task datasets.
  - Downloads: 52
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - Ê¶ÇË¶Å reazon-research/reazonspeech-v2[all]„ÇíWADA SNR„Å´„Å¶Èü≥Â£∞ÂìÅË≥™„ÅÆÂàÜÊûê„ÇíË°å„Å£„ÅüÁµêÊûú„Åß„Åô„ÄÇ
  - Downloads: 52
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - Japanese Laws This dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov.
  - Downloads: 52
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - LLMChat Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åó„Åü„É¢„Éá„É´„Çí‰∫∫ÊâãË©ï‰æ°„Åô„Çã„Åü„ÇÅ„Å´ÊßãÁØâ„Åó„ÅüLLMChat„Å®„ÅÑ„ÅÜ„Ç∑„Çπ„ÉÜ„É†„ÅßÂèéÈõÜ„Åï„Çå„ÅüË≥™Âïè„Å®LLM„ÅÆÂõûÁ≠î„ÄÅÂèä„Å≥‰∫∫ÊâãË©ï‰æ°„ÅÆ„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 51
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k Magpie„ÅÆÊâãÊ≥ï„Çínvidia/Nemotron-4-340B-Instruct„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅ20000‰ª∂„ÅÆÊó•‚áîËã±ÁøªË®≥„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - PubChem &amp; Wikipedia English-Japanese Paragraph Pair Classification This dataset is a multilingual extension of the PubChem &amp; Wikipedia Paragraphs Pair Classification dataset.
  - Downloads: 51
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models Jamp(tomo-vv/temporalNLI_dataset)
  - Downloads: 51
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025:
  - Downloads: 51
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 51
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - diet-members-voice-embeddings Êó•Êú¨„ÅÆÂõΩ‰ºöË≠∞Âì°„ÅÆÂ£∞„Çí speechbrain/spkrec-ecapa-voxceleb„Åß embedding „Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 50
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - ‰ª•‰∏ã„ÅÆ„Éá„Éº„ÇøÊ∫ê„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÊäΩÂá∫„Åó„ÅüÊó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíPhi-3„ÅßÂÜçÁîüÊàê„ÅóÔΩ§Êõ¥„Å´Ëá™ÂãïËã±Ë®≥„Åó„Åü„Ç≥„Éº„Éë„Çπ„Åß„ÅôÔΩ° Wikibooks Wikipedia „Ç≥„Éº„Éâ „Åì„Å°„Çâ ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ° „Éá„Éº„Çø parquet„Éï„Ç°„Ç§„É´„ÅåÊï∞ÂçÅGBÁ®ãÂ∫¶„ÅÇ„Çä„Åæ„Åô datasets„É©„Ç§„Éñ„É©„É™„Åã„Çâ„Åß„ÅØÔΩ§„ÅØ„Åò„ÇÅ„ÅÆÊï∞GBÁ®ãÂ∫¶„Åó„ÅãË™≠„ÅøËæº„ÇÅ„Å™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„ÅôÔΩ°git lfs„Å™„Å©„Åß„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åù„ÅÜ„Åß„ÅôÔΩ°
  - Downloads: 50
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - WikipediaÊó•Êú¨Ë™ûÁâà„Éá„Éº„Çø„Çª„ÉÉ„Éà(izumi-lab/wikipedia-ja-20230720)
  - Downloads: 50
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This dataset is a subset of the Open Assistant dataset, which contains Japanese conversations only.
  - Downloads: 50
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - zenz-v2.5-dataset zenz-v2.5-dataset„ÅØ„Åã„Å™Êº¢Â≠óÂ§âÊèõ„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüÊù°‰ª∂‰ªò„ÅçË®ÄË™û„É¢„Éá„É´„Äåzenz-v2.5„Äç„Ç∑„É™„Éº„Ç∫„ÅÆÂ≠¶Áøí„ÇíÁõÆÁöÑ„Å®„Åó„Å¶ÊßãÁØâ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 49
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØÊó•Êú¨Ë™ûLLM„ÅÆË©ï‰æ°Áî®„Å®„Åó„Å¶„Çà„ÅèÁî®„ÅÑ„Çâ„Çå„Çãelyza/ELYZA-tasks-100„Å´„Å§„ÅÑ„Å¶‰∫∫Èñì„ÅåÂõûÁ≠î„ÇíË°å„Å£„ÅüÁµêÊûú„Åß„Åô„ÄÇ
  - Downloads: 49
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - The dataset of SLG framework.
  - Downloads: 49
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - A JSON based anime dataset containing the most important meta data as well as cross references to various anime sites such as MAL, ANIDB, ANILIST, KITSU and more...
  - Downloads: 49
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN Dataset CT-RATE-JPN is a Japanese-translated version of radiology reports from the CT-RATE dataset, which contains chest CT volumes paired with corresponding radiology reports.
  - Downloads: 49
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM: Japanese semantic test suite (Japanese FraCaS and extensions) ÂèôËø∞ÊñáÈñì„ÅÆÂê´ÊÑèÈñ¢‰øÇ„ÅØ„ÄÅË®ÄË™ûÂ≠¶„Å´„Åä„ÅÑ„Å¶„ÅØÊÑèÂë≥Ë´ñ„ÅÆ‰∏≠ÂøÉÁöÑ„Å™Ë™¨ÊòéÂØæË±°„ÅÆ‰∏Ä„Å§„Åß„ÅÇ„Çã„Å®„Å®„ÇÇ„Å´„ÄÅÁêÜË´ñ„ÇíÊ§úË®º„Åô„Çã„Åü„ÇÅ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å®„Åó„Å¶Áî®„ÅÑ„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 49
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - Qwen/Qwen2.5-32B-Instruct-AWQ„ÅßÁîüÊàê„Åó„Åü3„Çø„Éº„É≥„ÅÆ„Éû„É´„ÉÅ„Çø„Éº„É≥instruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 49
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - LLMChat-Judge-Results team-hatakeyama-phase2/LLMChat„ÅÆ2„Å§„ÅÆ„É¢„Éá„É´„ÅÆÂøúÁ≠î„Å´ÂØæ„Åó„Å¶„ÄÅÊßò„ÄÖ„Å™„É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶PairwiseË©ï‰æ°„ÇíË°å„Å£„ÅüÁµêÊûú„ÅÆ„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 49
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - Negative Embedding / Textual Inversion NE4Mitsua is a Negative Embedding for Mitsua Diffusion One.
  - Downloads: 49
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - oasst2-135k-ja„Çí„ÉÅ„É£„ÉÉ„ÉàÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 48
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja Ê¶ÇË¶Å Â§öË®ÄË™ûÂåªÁôÇ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ ApolloCorpus „ÇíÊó•Êú¨Ë™û„Å´Ëá™ÂãïÁøªË®≥„Åó„Åü 525k „ÅÆÊåáÁ§∫„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 48
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - Derived from ÈùíÁ©∫ÊñáÂ∫´Âèä„Å≥„Çµ„Éî„Ç®„ÅÆÁÇπÂ≠ó„Éá„Éº„Çø„Åã„Çâ‰ΩúÊàê„Åó„ÅüÊåØ„Çä‰ªÆÂêç„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàGitHubÔºâ https://github.com/ndl-lab/huriganacorpus-aozora Certain mismatches in the original corpus were eliminated during validation (307 instances) Error: ÁÉà„Åó„ÅÑË™øÂ≠ê„Åß„ÅÇ„Çã„ÄÇ
  - Downloads: 48
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - AKU-d_ms-0.5B-v0.1_dataset Overview „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅÁßÅ„ÅÆÈñãÁô∫„Åó„Å¶„ÅÑ„ÇãAKU„Ç∑„É™„Éº„Ç∫„ÅÆ1„Å§ÁõÆ„Å®„Å™„Çã„ÄÅAKU-d_ms-0.5B-chat-v0.1„ÅÆ‰∫ãÂâçÂ≠¶Áøí„Å´‰ΩøÁî®„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÇíÈõÜ„ÇÅ„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 48
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset is a translation of https://huggingface.co/datasets/Abirate/english_quotes into Japanese using the llm-jp/llm-jp-3-3.7b-instruct model.
  - Downloads: 48
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - #Dataset Card for DancingPrismPJ/wikipedia-horse-dataset Wikipedia„ÅÆ„ÄåCategory:Êó•Êú¨Ë™øÊïô„ÅÆÁ´∂Ëµ∞È¶¨„ÄçÂÜÖ„ÅÆË®ò‰∫ã„Å´Âõ∫ÊúâË°®Áèæ„É©„Éô„É´„Çí‰ªò‰∏é„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 48
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - It covers multiple fields including general, IT, news, patent, and international engine.
  - Downloads: 48
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - Reference https://huggingface.co/datasets/mc4
  - Downloads: 48
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - Overview This dataset is edited from kunishou/databricks-dolly-15k-en.
  - Downloads: 48
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP This is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
  - Downloads: 47
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - oasst1-ja Description Based on OpenAssistant Conversations Dataset (OASST1)
  - Downloads: 47
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
  - Downloads: 47
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÄÅË©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 47
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - Derived from ÈùíÁ©∫ÊñáÂ∫´Âèä„Å≥„Çµ„Éî„Ç®„ÅÆÈü≥Â£∞„Éá„Ç§„Ç∏„Éº„Éá„Éº„Çø„Åã„Çâ‰ΩúÊàê„Åó„ÅüÊåØ„Çä‰ªÆÂêçÊ≥®Èáà‰ªò„ÅçÈü≥Â£∞„Ç≥„Éº„Éë„Çπ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà https://github.com/ndl-lab/hurigana-speech-corpus-aozora All text files in the original data were processed for 3361443 entries; duplicates and entries with no kanji were dropped post cleanup
  - Downloads: 47
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - For more information, see website below!
  - Downloads: 47
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - Description This is a templated version of data from ~40 Japanese open source downstream task datasets.
  - Downloads: 47
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka: A Journey Through Hyakumonogatari's Ghostly Tales Welcome to the Kaidan Nihonbunka Dataset About Name kaidan Nihonbunka translates to ÊÄ™Ë´áÊó•Êú¨ÊñáÂåñ in Japanese: ÊÄ™Ë´á (Kwaidan): Ghost story or supernatural tale.
  - Downloads: 47
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCohereForAI/aya_dataset„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 47
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
  - Downloads: 46
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - Èï∑ÊñáÁî®„ÅÆinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 46
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 46
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository contains the dataset used for the TaCo paper.
  - Downloads: 46
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - #Origin The name comes from "hachiwari/„ÅØ„Å°„Çè„Çå" (chiikawa/„Å°„ÅÑ„Åã„Çè).
  - Downloads: 46
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - NuminaMath Enhanced CoT Dataset (Japanese 50k Subset)
  - Downloads: 46
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - MagpieÊñπÂºè„Å´„Çà„ÇãpromptÊäΩÂá∫„Çírinna/llama-3-youko-8b„ÅßË°å„Å£„Å¶„Åø„Åæ„Åó„Åü„ÄÇ
  - Downloads: 46
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - Chatbot Arena Conversations„ÅÆË≥™ÂïèÊñá„Åã„Çâ„ÄÅaixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2„Çí‰ΩøÁî®„Åó„Å¶ÂøúÁ≠îÊñá„Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü Ë≥™ÂïèÊñá„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„É¢„Éá„É´„ÅÆPromptÈÉ®ÂàÜ„Çí‰ΩøÁî®„Åó„Åæ„Åó„Åü Chatbot Arena Conversations JA (calm2) ‰ª•‰∏ãÂºïÁî®„Åß„Åô„ÄÇ
  - Downloads: 46
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - GitHub „É™„Éù„Ç∏„Éà„É™ cl-tohoku/quiz-datasets „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 46
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - ÁîüÊàêAI„ÅÆÊó•Ëã±Â∞ÇÈñÄÁî®Ë™ûÈõÜ„Åß„Åô„ÄÇ
  - Downloads: 45
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
  - Downloads: 45
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - Èï∑Êñá„Åã„Çâ„ÅÆË¶ÅÁ¥Ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 45
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÄÅË©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 45
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - „É©„Ç§„Éñ„Éâ„Ç¢„Éã„É•„Éº„Çπ„Ç≥„Éº„Éë„Çπ„ÅÆ3Ë°åË¶ÅÁ¥Ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 45
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - Places in japan.
  - Downloads: 44
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl This dataset represents a conversion of the Japanese (Ja) section from the miracl dataset into the BeIR format, making it compatible for use with mteb.
  - Downloads: 44
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - language: jp en tags: translation license: cc-by-4.0
  - Downloads: 44
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - This dataset was created using AI Gemini 2.0 Flash Experimental from the original subtitle format.
  - Downloads: 44
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - Synthetic-JP-EN-Coding-Dataset-801k-50k Aratako/Synthetic-JP-EN-Coding-Dataset-801k„Åã„ÇâËã±Ë™ûÈÉ®ÂàÜ5‰∏á‰ª∂„ÇíÊäΩÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 44
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÇíÁî®„ÅÑ„Å¶Ëá™ÂãïÁîüÊàê„Åï„Çå„ÅüÊó•Êú¨Ë™û„ÅÆÊåáÁ§∫„Å®„Åù„Çå„Å´ÂØæ„Åô„ÇãÊé®Ë´ñ„ÉªÂàùÊúüÂøúÁ≠î„ÉªÊîπÂñÑÂøúÁ≠î„ÅÆ„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„Åß„Åô„ÄÇ
  - Downloads: 44
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k nvidia/Nemotron-4-340B-Instruct„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ1000‰ª∂„ÉªÂêÑ10„Çø„Éº„É≥„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàêÂØæË©±„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 44
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - NVIDIA „ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çã SteerLM Âêë„Åë„ÅÆ„Éà„É©„Ç§„Ç¢„É´„Éá„Éº„Çø„Çª„ÉÉ„Éà HelpSteer„ÇíÊó•Êú¨Ë™û„Å´Ëá™ÂãïÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 44
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - It contains Japanese instruction-like data intended for LLM construction/tuning.
  - Downloads: 44
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - GitHub „É™„Éù„Ç∏„Éà„É™ singletongue/wikipedia-utils „ÅßÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 44
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - https://github.com/anthropics/hh-rlhf „ÅÆÂÜÖÂÆπ„ÅÆ„ÅÜ„Å°„ÄÅhelpful-baseÂÜÖ„ÅÆchosen„Å´Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„ÇãËã±Êñá„ÇífuguMT„ÅßÁøªË®≥„ÄÅ„ÅÜ„Åæ„ÅèÁøªË®≥„Åß„Åç„Å¶„ÅÑ„Å™„ÅÑ„ÇÇ„ÅÆ„ÇíÈô§Â§ñ„ÄÅ‰øÆÊ≠£„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 43
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - Dataset Summary JapaneseGoblin is a dump of en.touhouwiki.net wiki.
  - Downloads: 43
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This is my conversion of NilanE/ParallelFiction-Ja_En-100k into json which can be read by text-generation-webui when training a model.
  - Downloads: 43
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - ‰∫∫Èñì„Åå‰ΩúÊàê„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà(OSCAR)„Å®LLMÁîüÊàê„ÉÜ„Ç≠„Çπ„Éà(GPT-3.5 Turbo)„Åã„ÇâÊàê„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà LLM„ÅßÁîüÊàê„Åï„Çå„ÅüÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊ§úÂá∫ÊÄßËÉΩ„ÅÆÊ§úË®º„ÅÆ„Åü„ÇÅ„Å´‰ΩúÊàê„Åó„Åü Ë©≥Á¥∞„ÅØ„Ç≥„Éº„Éâ„ÇíÂèÇÁÖß https://github.com/Rio-Rf/Lab-CreateDataset
  - Downloads: 43
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This dataset was created by machine translating "ViQuAE" into Japanese.
  - Downloads: 43
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØkunishouÊ∞è„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çã"databricks-dolly-15k"„ÇíÊó•Êú¨Ë™ûË®≥„Åó„Åükunishou/databricks-dolly-15k-ja„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË™ûÂ∞æ„ÇíArrowPro-7B-KUJIRA„ÇíÁî®„ÅÑ„Å¶„Äå„Å´„ÇÉ„ÇìÔºÅ
  - Downloads: 43
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - Êó•Êú¨ÈÉµ‰æø„ÅåÊèê‰æõ„Åô„Çã„ÄåÂõΩÈöõÈÉµ‰æø ÂÜÖÂÆπÂìÅ„ÅÆÊó•Ëã±„Éª‰∏≠Ëã±Ë®≥„ÄÅHS„Ç≥„Éº„ÉâÈ°û„ÄçÔºà2024/05/09Ôºâ„ÅÆ„Éá„Éº„Çø„Å´Âü∫„Å•„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 43
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - ÊñôÁêÜ„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„ÅÆË≥™ÂïèÊñá„Å®„ÄÅË≥™ÂïèÊñá„Å´Âê´„Åæ„Çå„ÇãÊ§úÁ¥¢Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÊÉÖÂ†±„ÇíÊåÅ„Å£„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô Âõ∫ÊúâË°®Áèæ„ÅÆÁ®ÆÈ°û„ÅØ‰ª•‰∏ã„ÅÆÔºî„Å§„Åß„Åô„ÄÇ
  - Downloads: 43
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - Ê¶ÇË¶Å reazon-research/reazonspeech-v2[all]„ÇíspeechMOS„Å´„Å¶Èü≥Â£∞ÂìÅË≥™„ÅÆÂàÜÊûê„ÇíË°å„Å£„ÅüÁµêÊûú„Åß„Åô„ÄÇ
  - Downloads: 43
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 43
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - novecomi-novel-metadata https://dengekibunko.jp/novecomi/novel/ „Åã„Çâ„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„ÄÇ
  - Downloads: 43
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - This pre-training dataset was created for shisa-base-7b-v1.
  - Downloads: 43
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-debug", split="test") Ê¶ÇË¶Å Â§ßÂñúÂà©ÁîüÊàê„ÅÆÂãï‰ΩúÁ¢∫Ë™çÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 42
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - „Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶ „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅ HuggingFaceTB/everyday-conversations-llama3.1-2k „ÇíÊ©üÊ¢∞ÁøªË®≥„ÅßÊó•Êú¨Ë™ûÂåñ„Åó„Åü„ÇÇ„ÅÆ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 42
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This is a modified version of NilanE/ParallelFiction-Ja_En-100k which has been turned into Alpaca format.
  - Downloads: 42
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - „Éá„Éº„ÇøÁîüÊàê„ÇíË°å„ÅÜÈöõ„ÅÆSEED„Éá„Éº„Çø„Å´„ÅØÊúâÂøó„ÅÆÊñπ„ÄÖ„Åå‰ΩúÊàê„Åó„Åüseed_tasks_japanese.jsonl„ÇíÂà©Áî®„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - Japanese-Vietnamese Translated Sentence Pairs.
  - Downloads: 42
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 42
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of sciq.
  - Downloads: 41
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - Japanese multi-turn conversation data was generated using Qarasu14B based on Wikipedia data.
  - Downloads: 41
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - „Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶ „Ç™„Éº„Éó„É≥„ÇΩ„Éº„ÇπLLM„ÅÆÂá∫Âäõ„Çí‰∫∫Êâã„Åß„ÉÅ„Çß„ÉÉ„ÇØ„Éª‰øÆÊ≠£„Åó„Åüinstruction„Å´Swallow-MX„Åßoutput„ÇíÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 41
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA v1.5
  - Downloads: 41
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - Data extracted from CommonCrawlPDF Japanese domain Code is here
  - Downloads: 41
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k Magpie„ÅÆÊâãÊ≥ï„Çínvidia/Nemotron-4-340B-Instruct„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ10000‰ª∂„ÅÆÊó•Êú¨Ë™û„ÅÆ„Ç≥„Éº„Éá„Ç£„É≥„Ç∞Áî®ÂØæË©±„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 41
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-debug", split="test") Ê¶ÇË¶Å Â§ßÂñúÂà©ÁîüÊàê„ÅÆÂãï‰ΩúÁ¢∫Ë™çÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 41
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 41
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - calm3-22b„Çí‰Ωø„Å£„Å¶Á∞°Âçò„Å™Êó•Êú¨Ë™û„ÅÆ‰æãÊñá„Çí‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 41
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This dataset is a collection of Korean, Chinese, and Japanese OpenOrca translation datasets.
  - Downloads: 41
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - oasst1-89k-ja„Çí„ÉÅ„É£„ÉÉ„ÉàÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 41
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - A Japanese dataset generated with an opensource elyza/ELYZA-japanese-Llama-2-13b-instruct model.
  - Downloads: 40
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÄÅË©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - Synthetic-JP-EN-Coding-Dataset-Magpie-69k Magpie„ÅÆÊâãÊ≥ï„ÇíÊßò„ÄÖ„Å™„É¢„Éá„É´„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ69000‰ª∂„ÅÆÊó•Êú¨Ë™û„ÉªËã±Ë™û„ÅÆ„Ç≥„Éº„Éá„Ç£„É≥„Ç∞ÂØæË©±„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - Magpie-Qwen-Turbo-27k Aratako/Magpie-Tanuki-8B-annotated-96k „ÅÆ„Ç¢„Éé„ÉÜ„Éº„Ç∑„Éß„É≥„ÇíÂà©Áî®„Åó„Å¶‰ª∂Êï∞„ÇíÊ∏õ„Çâ„Åó„ÄÅoutput„Çíqwen-2.5-turbo„ÅßÂÜçÁîüÊàê„Åó„ÅüSFTÁî®„ÅÆ26728‰ª∂„ÅÆ„Çµ„Éñ„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - „ÇØ„Ç§„Ç∫„ÅÆÊùúÊßò„Å´Êé≤Ëºâ„ÅÆ„ÇØ„Ç§„Ç∫„ÅÆ„ÅÜ„Å°„ÄÅ2024Âπ¥8Êúà5Êó•ÊôÇÁÇπ„Å´„Åä„ÅÑ„Å¶ÂèñÂæóÂèØËÉΩ„Å†„Å£„Åü„ÇØ„Ç§„Ç∫„ÅÆ„ÅÜ„Å°„Äå‰∫åÊ¨°Âà©Áî®Ë®±Ë´æ„É¨„Éô„É´„Äç„Åå„Äå„Éï„É™„Éº„Äç„Åß„ÅÇ„Å£„Åü„ÇÇ„ÅÆ„ÇíÂèéËºâ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - Quiz WorksÊßò„Å´Êé≤Ëºâ„ÅÆ„ÇØ„Ç§„Ç∫„ÅÆ„ÅÜ„Å°„ÄÅ2024Âπ¥8Êúà4Êó•~8Êúà5Êó•ÊôÇÁÇπ„Å´„Åä„ÅÑ„Å¶ÂèñÂæóÂèØËÉΩ„Å†„Å£„Åü„ÇØ„Ç§„Ç∫„ÇíÂèéËºâ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task Quality of Japanese text is somewhat low arise from the combination of synthetic generated text and machine translation API details reformatted dataset of open_preference_v0.1 label 1 stands for chosen sentence label 0 stands for rejected sentence
  - Downloads: 40
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - „Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶ Kendamarron/jimba-instuction-1k-beta„ÅÆinstruction„ÅÆ„ÅÜ„Å°200ÂÄã„Çí„Çà„ÇäÂçòÁ¥î„Å™„Çø„Çπ„ÇØ„Å´Êõ∏„ÅçÊèõ„Åà„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 40
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - They had free discussion on a number of given topics, with a wide range of fields; the voice was natural and fluent, in line with the actual dialogue scene.
  - Downloads: 40
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - FED„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíGoogle Cloud Translate API v2„ÅßÊó•Êú¨Ë™ûÂåñ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅôÔºé
  - Downloads: 39
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄåÁßÅ„ÅÆÊé®„Åó„ÅØÊÇ™ÂΩπ‰ª§Â¨¢„ÄÇ
  - Downloads: 39
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - chatbot-arena-ja-calm2-7b-chat„Åã„Çâprompt„Åå‰∏ÄËá¥„Åô„Çã„Éá„Éº„Çø„ÇíÂâäÈô§„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 39
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅOpenAIÁ§æ„ÅÆGPT-3.5„Çí https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2 „ÅÆ„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„ÅüÂæå„Å´„ÄÅÊõ¥„Å´Áã¨Ëá™„Å´ÂèéÈõÜ„Åó„Åü‰∫∫Ê†º„ÅÆ„ÅÇ„Çã„ÉÑ„Ç§„Éº„Éà330„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„ÉÑ„Ç§„Éº„Éà„ÇíÁîüÊàê„Åó„ÄÅ „Åù„Çå„Å´ÂØæ„Åó„Å¶„Çπ„Ç≥„Ç¢„Çí‰ªò‰∏é„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 39
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - Synthetic-JP-Roleplay-Instruction-Nemotron-4 Magpie„ÅÆÊâãÊ≥ï„Çínvidia/Nemotron-4-340B-Instruct„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ1000‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§Áî®„ÅÆinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 39
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÄÅË©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 39
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - CC-MAIN-2019-51„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 39
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - CC-MAIN-2019-49„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 39
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - Open-Platypus-Japanese-masked-formatted weblab-GENIAC/Open-Platypus-Japanese-masked„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 39
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - For more information, see website below!
  - Downloads: 39
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - „Éù„Ç±„É¢„É≥(VGC)„ÅÆ„É¨„ÇÆ„É•„É¨„Éº„Ç∑„Éß„É≥F „É´„Éº„É´„Å´„Åä„Åë„ÇãÈÅ∏Âá∫„Éá„Éº„Çø„ÇíË®òÈå≤„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 39
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
  - Downloads: 39
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 38
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This dataset was created by machine translating "nlvr" into Japanese.
  - Downloads: 38
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - The following data set was vectorized with the intfloat/multilingual-e5-base model and an index file created by faiss.
  - Downloads: 38
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of piqa.
  - Downloads: 37
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - mmarco„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆquery--passage„ÅÆ„Éö„Ç¢„Å´„Å§„ÅÑ„Å¶„ÄÅquery„Çíkey„Å®„Åó„Å¶ÈáçË§á„ÇíÂâäÈô§„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 37
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - ÂõΩÁ´ãÂõΩ‰ºöÂõ≥Êõ∏È§®„ÅÆÊõ∏Ë™å„Éá„Éº„Çø„Åã„Çâ‰ΩúÊàê„Åó„ÅüÊåØ„Çä‰ªÆÂêç„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà A dataset of furigana characters created from bibliographic data from the National Diet Library.
  - Downloads: 37
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-marusen", split="train") Ê¶ÇË¶Å Êúà„Å´1‰∏áÂè•‰ª•‰∏ä„ÅÆÊäïÁ®ø„Åå„ÅÇ„ÇãÂõΩÂÜÖÊúÄÂ§ßÁ¥ö„ÅÆÂ∑ùÊü≥ÊäïÁ®ø„Çµ„Ç§„Éà„ÄéÂ∑ùÊü≥ÊäïÁ®ø„Åæ„Çã„Åõ„Çì„Äè„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 37
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - ÂøÖ„Åö„Åô„Åπ„Å¶„ÅÆÊÉÖÂ†±„ÇíÁ∂≤ÁæÖ„ÅóÔΩ§Êó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åô„Çã„Åì„Å®„ÄÇ
  - Downloads: 37
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - Dataset Details For the original NTX dataset, the conversion to the Aya instructions format, or more details, please refer to the full dataset in instruction form (https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions)
  - Downloads: 37
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - Scenery of japan.
  - Downloads: 36
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - Ê¶ÇË¶Å Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´(LLM)Áî®„ÅÆÂõ∫ÊúâË°®ÁèæË™çË≠ò„Éá„Éº„Çø„Çª„ÉÉ„Éà(J-NER)„ÅÆ„É™„Éù„Ç∏„Éà„É™„Åß„Åô„ÄÇ
  - Downloads: 36
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - Sorry, it's no longer available on Hugging Face.
  - Downloads: 36
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - wiki40b-ja „Åã„ÇâÁîüÊàê„Åó„ÅüË≥™ÂïèÂøúÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà
  - Downloads: 36
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - WikipediaÊó•Êú¨Ë™ûÁâà„Åã„Çâ„ÅÆQ&amp;A„ÅÆËá™ÂãïÁîüÊàê Mixtral 8x22b„ÅÆGGUF(5bit)„Çí„Éô„Éº„Çπ„Å´ÔΩ§WikipediaÊó•Êú¨Ë™ûÁâà„ÅÆË®ò‰∫ã„Åã„ÇâÔΩ§ Ëá™ÂãïÁîüÊàê„Ç≥„Éº„Éâ1 Ëá™ÂãïÁîüÊàê„Ç≥„Éº„Éâ2 „Çí‰Ωø„Å£„Å¶Q&amp;A„Çí‰ΩúÊàê„Åó„Åæ„Åó„ÅüÔΩ° Ë®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ° Ê≥®ÊÑè ÂõûÁ≠î„Å´„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥Á≠â„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„ÅÆ„ÅßÔΩ§„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Çí„Åã„Åë„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„ÇìÔΩ°
  - Downloads: 36
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - Lurunchik/WikiHowNFQA„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 36
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - „Çà„ÇäÂ§ö„Åè‰ΩúÊàê„Åó„Åü„ÅÆ„Åå„Åì„Å£„Å°https://huggingface.co/datasets/if001/elementray_m calm3-22b„Çí‰Ωø„Å£„Å¶Á∞°Âçò„Å™Êó•Êú¨Ë™û„ÅÆ‰æãÊñá„Çí‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 36
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - Ëá™ÂãïÁîüÊàê„ÅÆÂØæË©±„Éá„Éº„Çø ÈùíÁ©∫ÊñáÂ∫´„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÊäúÁ≤ã„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇÇ„Å®„Å´ÔΩ§Calm3-22B-chat„ÅßËá™ÂãïÁîüÊàê„ÅÆ„Éû„É´„ÉÅ„É≥„Çø„Éº„É≥„Éá„Éº„Çø„ÇíÁîüÊàê„Åó„Åæ„Åó„ÅüÔΩ° ÁîüÊàê„Ç≥„Éº„Éâ ÂêæËº©„ÅØÁå´„Åß„ÅÇ„Çã ÈôêÂÆöver ËªΩ„ÅÑ„ÇØ„É™„Éº„Éã„É≥„Ç∞
  - Downloads: 36
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 36
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØglaive-ai„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãin-foxhound„ÇíKUJIRA„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Åü„ÇÇ„ÅÆ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 36
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 Evol-Alpaca-gen3-500„ÅØ„ÄÅ
  - Downloads: 36
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - „Éá„Éº„ÇøÂà∂‰ΩúËÄÖÔºàt_wÔºâ
  - Downloads: 35
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - Kendamarron/jimba-wiki-instruction-calm3 grapevine-AI/CALM3-22B-Chat-GGUF„ÅÆQ4_K_M„Çí‰Ωø„Å£„ÅüÂêàÊàêinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 35
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - Ë™≠„ÅøËæº„ÅøÊñπ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-test", split="test") Ê¶ÇË¶Å Â§ßÂñúÂà©ÊäïÁ®ø„Çµ„Ç§„ÉàBokete„ÅÆ„ÇØ„É≠„Éº„É´„Éá„Éº„Çø„Åß„Åô„ÄÇ
  - Downloads: 35
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØnull-instruct-ja„Å®DeepSeek-v2.5„ÅÆq4„ÇíÁî®„ÅÑ„Å¶ÂêàÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 35
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA YakugakuQA is a question answering dataset, consisting of 13 years (2012-2024)
  - Downloads: 35
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - Japanese-Law-Translation Dataset Summary
  - Downloads: 35
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - Ê¶ÇË¶Å NHK„ÅßÂÆöÊúüÁöÑ„Å´ÊîæÈÄÅ„Åï„Çå„Å¶„ÅÑ„Åü„ÄéÁùÄ‰ø°Âæ°Á§ºÔºÅ
  - Downloads: 35
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - ‰ª•‰∏ã„ÅÆ„Éá„Éº„ÇøÊ∫ê„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÊäΩÂá∫„Åó„ÅüÊó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇÇ„Å®„Å´ÔΩ§RAGÂΩ¢Âºè„ÅÆQ&amp;A„ÇíËá™ÂãïÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅôÔΩ° Wikibooks Wikipedia Âà§‰æã„Éá„Éº„Çø instruction dataset„Å®„Åó„Å¶„Åß„ÅØ„Å™„ÅèÔΩ§‰∫ãÂâçÂ≠¶Áøí„Åß„ÅÆÂà©Áî®„ÇíÊÉ≥ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô(Ë≥™ÁñëÂøúÁ≠î„Çí„Åô„Çã„Åü„ÇÅ„ÅÆË®ìÁ∑¥)ÔΩ° ‰∏ÄÈÉ®„ÅÆË®àÁÆó„Å´„ÅØÊù±‰∫¨Â∑•Ê•≠Â§ßÂ≠¶„ÅÆ„Çπ„Éº„Éë„Éº„Ç≥„É≥„Éî„É•„Éº„ÇøTSUBAME4.0„ÇíÂà©Áî®„Åó„Åæ„Åó„ÅüÔΩ°
  - Downloads: 35
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository contains the dataset used for the TaCo paper.
  - Downloads: 35
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂêÑ„Ç≠„Éº„Å®„Åù„ÅÆË™¨Êòé: state_id: „Ç≤„Éº„É†„ÅÆÁä∂ÊÖã„Çí‰∏ÄÊÑè„Å´Ë≠òÂà•„Åô„Çã„Åü„ÇÅ„ÅÆID„ÄÇ
  - Downloads: 35
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - „Éá„Éº„ÇøÂà∂‰ΩúËÄÖÔºàt_wÔºâ
  - Downloads: 35
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - It is just a dataset of dolly-15k-jp(*1)
  - Downloads: 35
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - This dataset contains passages, each of which consists of consecutive sentences no longer than 400 characters from Japanese Wikipedia as of 2022-04-04.
  - Downloads: 35
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - „ÅäÔΩû„ÅÑ„ÅäËå∂Êñ∞‰ø≥Âè•Â§ßË≥ûÂèóË≥û‰ΩúÂìÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà 221„ÅÆ‰ø≥Âè•„ÅåÂê´„Åæ„Çå„ÄÅ„ÅÜ„Å°200ÂâçÂæå„ÅØ‰ΩúËÄÖ„Å®ÂØ©ÊüªÂì°„ÅÆ„Ç≥„É°„É≥„Éà„Åå‰ªòÂ±û„ÄÇ
  - Downloads: 34
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - A Japanese dataset generated with Qwen/Qwen1.5-14B model.
  - Downloads: 34
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - ÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàNMLE datasetsÔºâ „ÅØ„Åò„ÇÅ„Å´ Ê§úÁ¥¢„Åó„Å¶„ÇÇ„Å™„Åã„Å£„Åü„ÅÆ„ÅßÔºàË™ø„Åπ‰∏çË∂≥„ÅÆÂèØËÉΩÊÄß„ÅÇ„ÇäÔºâ„ÄÅÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂÖ¨Èñã„Åó„Åæ„Åô ÂåªÂ∏´„Å®„Åó„Å¶„ÄÅAI„ÅÆ„Ç®„É≥„Ç∏„Éã„Ç¢„Å®„Åó„Å¶„ÄÅÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÅÇ„Çã„Åπ„Åç„ÇÇ„ÅÆ„ÄÅ„ÅÇ„Å£„Å¶ÂΩìÁÑ∂„ÅÆ„ÇÇ„ÅÆ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åó„Åü„Åå‰ªäÁèæÂú®Ôºà2024/6/13ÊôÇÁÇπÔºâÂ≠òÂú®„ÅåÁ¢∫Ë™ç„Åß„Åç„Åö„ÄÅËá™ÂàÜ„Åß‰Ωø„ÅÜ„ÅÆ„Å´ÂøÖË¶Å„Å†„Å£„Åü„Åü„ÇÅ‰ΩúÊàê ÂåªÁôÇÁî®„Çø„Çπ„ÇØ„Å´ÁâπÂåñ„Åó„ÅüLLM„Å™„Å©„ÅÆÈñãÁô∫„ÇÇÊ¥ªÁô∫„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„Åù„ÅÆ„Éá„Éº„ÇøÂÖÉ„Å®„Åó„Å¶USMLEÔºà„Ç¢„É°„É™„Ç´„ÅÆÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ìÔºâ„Åå‰Ωø„Çè„Çå„Å¶„Åä„Çä„ÄÅ„Åä„Çà„ÅùÂõΩÂÜÖ„ÅÆÂÆüÊÉÖ„Å®ÈÅï„ÅÜ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰Ωø„ÅÜ„Åì„Å®„ÅÆÂºäÂÆ≥„ÅåÁîü„Åò„Å¶„ÅÑ„Çã„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô „Å™„ÅÆ„Åß‰Ωï„Åã„ÅÆÂΩπ„Å´„Åü„Å¶„Å∞„Å®ÊÄù„ÅÑ„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂÖ¨Èñã„Åó„Åæ„Åô Áî®ÈÄî Áî®ÈÄî„Å®„Åó„Å¶ „É¢„Éá„É´„ÅÆË©ï‰æ° ÈÄ≤ÂåñÁöÑ„É¢„Éá„É´„Éû„Éº„Ç∏„ÅÆ„Çø„Çπ„ÇØ„Å´„Å§„Åã„ÅÜÔºà„ÄåNew Task Guide„ÄçÂèÇÁÖßÔºâ RAG„Å™„Å©„Å´Áî®„ÅÑ„ÇãÊÉÖÂ†±Ê∫ê ÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„ÅÆ‰øØÁû∞ „ÇíÊÉ≥ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô ÊßãÈÄ† data = { "id": question_id, "question": question_text, "choices": choices, "answer": answers, "explanation": explan
  - Downloads: 34
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - calm3-22b„Çí‰Ωø„Å£„Å¶Á∞°Âçò„Å™Êó•Êú¨Ë™û„ÅÆ‰æãÊñá„Çí‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 34
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 34
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 34
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - Âºπ‰∏∏ËÆ∫Á†¥ÁöÑ‰∏ÉÊµ∑ÂçÉÁßãËØ≠Èü≥Êï∞ÊçÆ
  - Downloads: 34
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - For more details, please refer to the link: https://www.nexdata.ai/datasets/nlu/153?
  - Downloads: 34
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - Dolly Êó•Êú¨Ë™ûÁøªË®≥Áâà „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅDatabricks„ÅåÈñãÁô∫„Åó„Åüdolly„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÊó•Êú¨Ë™ûÁøªË®≥Áâà„Åß„Åô„ÄÇ
  - Downloads: 33
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - LoRA„Éá„Éº„Çø„Çª„ÉÉ„ÉàÈñãÁ§∫Áî®„Éá„Éº„Çø„ÄÇ
  - Downloads: 33
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - Êó•Êú¨Ë™û„Éï„Çß„Ç§„ÇØ„Éã„É•„Éº„Çπ„Éá„Éº„Çø„Çª„ÉÉ„Éà Êó•Êú¨Ë™û„Éï„Çß„Ç§„ÇØ„Éã„É•„Éº„Çπ„Éá„Éº„Çø„Çª„ÉÉ„Éà „Çí HuggingFace datasets Áî®„Å´Â§âÊèõ„ÄÇ
  - Downloads: 33
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅËëó‰ΩúËÄÖ„Åß„ÅÇ„ÇãËá™ÂàÜ„Åå„Åó„Åü„ÉÑ„Ç§„Éº„Éà„Åã„ÇâÁâπ„Å´ÂÑ™„Çå„Åü„ÇÇ„ÅÆ(ÂæåËø∞)„ÇíÈõÜ„ÇÅ„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task.
  - Downloads: 33
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´ÂêåÊÑè„Åó„Åü„ÅÜ„Åà„Åß„ÄÅÂÖ¨Èñã„Åï„Çå„Åü„É¢„Éá„É´Âèä„Å≥„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ≠âÔºà‰ª•‰∏ã„ÄåÊú¨„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÄçÔºâ„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ
  - Downloads: 33
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - A more aggressively cleaned up version of Calvin-Xu/Furigana-Aozora-Speech, which consists of 2,536,041 out of the 3,361,443 entries generated from the raw data ÈùíÁ©∫ÊñáÂ∫´Âèä„Å≥„Çµ„Éî„Ç®„ÅÆÈü≥Â£∞„Éá„Ç§„Ç∏„Éº„Éá„Éº„Çø„Åã„Çâ‰ΩúÊàê„Åó„ÅüÊåØ„Çä‰ªÆÂêçÊ≥®Èáà‰ªò„ÅçÈü≥Â£∞„Ç≥„Éº„Éë„Çπ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà https://github.com/ndl-lab/hurigana-speech-corpus-aozora.
  - Downloads: 33
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - Dataset details Dataset type:
  - Downloads: 33
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - KanjiVG PNG images with textual descriptions This dataset is an adaptation of KanjiVG by Ulrich Apel.
  - Downloads: 32
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - Êù±Êñπ„Éà„Ç´„Éû„ÇØ„É©„Éñ „Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÊù±ÊñπProject„ÅÆ„Éà„Ç´„Éû„ÇØ„É©„Éñ„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÇíÂèéÈõÜ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 32
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - All words and pronunciations are produced by Japanese linguists.
  - Downloads: 31
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - ime-and-kakko elyza/ELYZA-tasks-100 ‰∏≠„ÅÆ IME „ÅÆ„Çà„ÅÜ„Å´Â§âÊèõÂÄôË£ú„ÇíÊèêÁ§∫„Åô„Çã„Çø„Çπ„ÇØ „Ç´„ÉÉ„Ç≥„ÅÆÂØæÂøúÈñ¢‰øÇ„ÇíÊï¥„Åà„Çã„Çø„Çπ„ÇØ „ÅÆ„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥„ÇíÊâã„Åß‰ΩúÊàê„Åó„Åü„ÇÇ„ÅÆ Êù±‰∫¨Â§ßÂ≠¶ÊùæÂ∞æ„ÉªÂ≤©Êæ§Á†îÁ©∂ÂÆ§ÔºàÊùæÂ∞æÁ†îÔºâÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ Deep Learning ÂøúÁî®Ë¨õÂ∫ß 2024 „ÅßÈñãÂÇ¨„Åï„Çå„Åü„Ç≥„É≥„Éö„ÉÜ„Ç£„Ç∑„Éß„É≥„Å´„Åä„ÅÑ„Å¶„ÄÅ @pokutuna „Åå‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„ÅÆËã¶ÊâãÂïèÈ°å„ÇíÂÖãÊúç„Åô„Çã„Åü„ÇÅ„Å´‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 31
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - Veterinary Medicine Japanese Dataset This dataset contains audio files of veterinary medicine terms in Japanese, categorized into drugs, diseases, and symptoms.
  - Downloads: 31
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - shunk031/JDocQA„ÅÆtrain split„Å´Âê´„Åæ„Çå„ÇãPDF„Éá„Éº„Çø„ÇíÁîªÂÉèÂåñ„Åó„ÄÅNDLOCR„ÅßOCR„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„Å®„Éö„Ç¢„Å´„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 27
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - DSR1D-Llama-8B-aya-ja-1k-generated „Åì„Çå„ÅØdeepseek-ai/DeepSeek-R1-Distill-Llama-8B„ÇíÁî®„ÅÑ„Å¶„ÄÅweblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked„ÅÆÊúÄÂàù„ÅÆ1000‰ª∂„ÅÆÂøúÁ≠î„Çímax_new_tokens=3060„ÅßÁîüÊàê„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 25
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - DSR1D-qwen-2.5-32B-aya-ja-1k-generated „Åì„Çå„ÅØdeepseek-ai/DeepSeek-R1-Distill-Qwen-32B„ÇíÁî®„ÅÑ„Å¶„ÄÅweblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked„ÅÆÊúÄÂàù„ÅÆ1000‰ª∂„ÅÆÂøúÁ≠î„Çímax_new_tokens=3060„ÅßÁîüÊàê„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 24
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese„ÇíÁî®„ÅÑ„Å¶„ÄÅËã±Ë™ûWikipedia‰∏≠„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 19
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - magpie-reasoning-llama-nemotron-70b-100k-filtered DeL-TaiseiOzaki/magpie-reasoning-llama-nemotron-70b-100k„Åã„Çâ„ÄÅrefined_answerÂàó„Å´"ÊîπËâØ"„Å®„ÅÑ„ÅÜÊñáÂ≠ó„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÇÇ„ÅÆ„ÇíÊäΩÂá∫„Åó„ÄÅOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 55
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - Update: 2023/12/25oasst2-135k-ja„Çí„ÉÅ„É£„ÉÉ„ÉàÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åüoasst2-chat-68k-ja„ÇíÂÖ¨Èñã„Åó„Åæ„Åó„Åü„ÄÇ
  - Downloads: 45
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - CommonCatalog CC-BY Extention „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 44
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíUVR„Çí‰ΩøÁî®„Åó„Å¶BGM„ÇÑ„Éé„Ç§„Ç∫Èô§Âéª„Åó„Åü„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 8,174
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - You agree to the terms of the LICENSE when using this dataset.
  - Downloads: 32
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
