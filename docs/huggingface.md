# awesome-japanese-nlp-resources

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/taishi-i/awesome-japanese-nlp-resources)
    [![RRs](https://img.shields.io/badge/PRs-welcome-brightgreen)](https://github.com/taishi-i/awesome-japanese-nlp-resources/pulls)
    [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search)
    [![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
    [![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

A curated list of resources dedicated to Python libraries, llms, dictionaries, and corpora of NLP for Japanese
This page lists Japanese NLP-specific models and datasets available on Hugging Face. Currently, it includes 192 models and 138 datasets.

_Updated on Jan 06, 2026_

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents
 * [Ranking](#Ranking)
   * [Models](#models-ranking)
   * [Datasets](#datasets-ranking)
 * [Models](#Models)
   * [text-generation](#text-generation)
   * [fill-mask](#fill-mask)
   * [sentence-similarity](#sentence-similarity)
   * [automatic-speech-recognition](#automatic-speech-recognition)
   * [feature-extraction](#feature-extraction)
   * [translation](#translation)
   * [text-classification](#text-classification)
   * [image-to-text](#image-to-text)
   * [text-ranking](#text-ranking)
   * [token-classification](#token-classification)
   * [text-to-speech](#text-to-speech)
   * [audio-to-audio](#audio-to-audio)
   * [image-text-to-text](#image-text-to-text)
   * [others](#others)
 * [Datasets](#Datasets)

## Ranking

### Models-ranking

| # | Model | Downloads | Likes | Category |
|---|-------|-----------|-------|----------|
| 1 | [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) | ğŸ“¥ 3M | â­ 47 | automatic-speech-recognition |
| 2 | [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) | ğŸ“¥ 599k | â­ 22 | sentence-similarity |
| 3 | [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) | ğŸ“¥ 480k | â­ 71 | fill-mask |
| 4 | [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) | ğŸ“¥ 405k | â­ 13 | feature-extraction |
| 5 | [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) | ğŸ“¥ 319k | â­ 11 | sentence-similarity |
| 6 | [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) | ğŸ“¥ 282k | â­ 1 | others |
| 7 | [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) | ğŸ“¥ 256k | â­ 8 | translation |
| 8 | [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) | ğŸ“¥ 195k | â­ 2 | others |
| 9 | [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | ğŸ“¥ 178k | â­ 163 | image-to-text |
| 10 | [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) | ğŸ“¥ 173k | â­ 15 | text-generation |
| 11 | [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) | ğŸ“¥ 157k | â­ 140 | text-generation |
| 12 | [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) | ğŸ“¥ 108k | â­ 10 | others |
| 13 | [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) | ğŸ“¥ 106k | â­ 8 | fill-mask |
| 14 | [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) | ğŸ“¥ 93k | â­ 60 | sentence-similarity |
| 15 | [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) | ğŸ“¥ 84k | â­ 84 | automatic-speech-recognition |
| 16 | [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) | ğŸ“¥ 76k | â­ 20 | text-generation |
| 17 | [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) | ğŸ“¥ 73k | â­ 6 | fill-mask |
| 18 | [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) | ğŸ“¥ 71k | â­ 13 | others |
| 19 | [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) | ğŸ“¥ 65k | â­ 1 | others |
| 20 | [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) | ğŸ“¥ 64k | â­ 4 | automatic-speech-recognition |

### Datasets-ranking

| # | Dataset | Downloads | Likes |
|---|---------|-----------|-------|
| 1 | [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) | ğŸ“¥ 408k | â­ 17 |
| 2 | [emb](https://huggingface.co/datasets/hpprc/emb) | ğŸ“¥ 8k | â­ 13 |
| 3 | [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) | ğŸ“¥ 7k | â­ 8 |
| 4 | [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) | ğŸ“¥ 6k | â­ 18 |
| 5 | [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) | ğŸ“¥ 6k | â­ 10 |
| 6 | [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) | ğŸ“¥ 5k | â­ 27 |
| 7 | [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) | ğŸ“¥ 5k | â­ 23 |
| 8 | [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) | ğŸ“¥ 4k | â­ 8 |
| 9 | [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) | ğŸ“¥ 3k | â­ 3 |
| 10 | [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) | ğŸ“¥ 3k | â­ 99 |
| 11 | [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) | ğŸ“¥ 3k | â­ 95 |
| 12 | [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) | ğŸ“¥ 2k | â­ 6 |
| 13 | [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) | ğŸ“¥ 2k | â­ 45 |
| 14 | [MissingKeys](https://huggingface.co/datasets/RyokoExtra/MissingKeys) | ğŸ“¥ 2k | â­ 2 |
| 15 | [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) | ğŸ“¥ 2k | â­ 127 |
| 16 | [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) | ğŸ“¥ 2k | â­ 4 |
| 17 | [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) | ğŸ“¥ 2k | â­ 4 |
| 18 | [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) | ğŸ“¥ 1k | â­ 3 |
| 19 | [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) | ğŸ“¥ 1k | â­ 2 |
| 20 | [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) | ğŸ“¥ 1k | â­ 137 |

## Models
### text-generation
 * [japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - ğŸ“¥ 173k / â­ 15 / A 12â€‘layer, 768â€‘hidden Japanese GPTâ€‘NeoX model trained on CCâ€‘100, C4, and Wikipedia, compatible with Huggingface, with an optional toy prefixâ€‘tuning weight that forces each sentence to end with a smilingâ€‘face emoji.
 * [Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B) - ğŸ“¥ 157k / â­ 140 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced, 8â€‘billionâ€‘parameter Llamaâ€¯3 model by ELYZA, fineâ€‘tuned for Japanese on Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct.
 * [open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - ğŸ“¥ 76k / â­ 20 / OpenCALM is a suite of decoderâ€‘only Japanese transformer language models (160â€¯Mâ€“6.8â€¯B params) released by CyberAgent, Inc. under CCâ€‘BYâ€‘SAâ€¯4.0, trained on Japanese Wikipedia and Common Crawl, and usable via Huggingâ€¯Faceâ€™s torchâ€‘transformers.
 * [llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4) - ğŸ“¥ 61k / â­ 14 / Provides the 1.8â€¯Bâ€‘parameter llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4 Japanese instructionâ€‘tuned model from NII, compatible with HuggingÂ Face Transformers and Torchâ€¯â‰¥â€¯2.3.0, including pretrained and fineâ€‘tuned checkpoints and usage examples.
 * [TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B) - ğŸ“¥ 34k / â­ 35 / TinySwallowâ€‘1.5B is a compact Japanese instructionâ€‘following language model by Sakana AI and the Swallow Team that uses TAID distillation from Qwen2.5â€‘32Bâ€‘Instruct, is further preâ€‘trained on Japanese text, and is released under ApacheÂ 2.0 for research use only.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5) - ğŸ“¥ 33k / â­ 17 / Llamaâ€¯3.1â€¯Swallow is a set of 8â€‘B and 70â€‘B models that continue preâ€‘training Metaâ€™s Llamaâ€¯3.1 to boost Japanese language performance, then instructionâ€‘fineâ€‘tune on synthetic Japanese dataâ€”providing multiple released variants with improved conversational behavior comparable to gemmaâ€‘3â€‘27bâ€‘it.
 * [shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - ğŸ“¥ 14k / â­ 18 / Fineâ€‘tuned the Japanese Stableâ€¯LM Base Gammaâ€¯7B with Shisaâ€¯7B data, achieving strong results on the JAâ€¯MTâ€‘Bench.
 * [japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - ğŸ“¥ 13k / â­ 83 / Rinnaâ€™s 24â€‘layer, 1024â€‘hidden Japanese GPTâ€‘2â€‘medium model, trained on CCâ€‘100 and Wikipedia with SentencePiece tokenization, is available in the rinna/japaneseâ€‘pretrainedâ€‘models repo (MITâ€‘licensed, released Aprilâ€¯7â€¯2021, updated 25â€¯Augâ€¯2021).
 * [Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ) - ğŸ“¥ 12k / â­ 4 / Japaneseâ€‘optimized 8â€‘billionâ€‘parameter Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B, built on Metaâ€‘Llamaâ€‘3â€‘Instruct with extra preâ€‘training and instruction tuning, is offered in GGUF and AWQ quantized forms for vLLM or OpenAIâ€‘compatible inference.
 * [Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) - ğŸ“¥ 9k / â­ 21 / Llama3â€¯Swallow is a Japaneseâ€‘enhanced Meta Llamaâ€¯3 family released Julyâ€¯1â€¯2024, offering 8B and 70B variants in Instruct and chat forms fineâ€‘tuned with SFT and Chatâ€¯Vector on Megatronâ€‘LM, and benchmarked on key Japanese NLP tasks.
 * [llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b) - ğŸ“¥ 8k / â­ 15 / A collection of Japanese largeâ€‘language models (1.8â€¯b to 172â€¯bâ€¯beta1, with instruct variants) from NIIâ€™s R&D Center, packaged in Huggingâ€¯Face Transformers format and pretrained on a mix of Japanese, English, and Web corpora totalling >1â€¯trillion tokens, requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - ğŸ“¥ 8k / â­ 4 / Experimental Japanese model created by extracting differences between lightblue/suzumeâ€‘llamaâ€‘3â€‘8Bâ€‘japanese and Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct using a chatâ€‘vector approach, upsampled and applied to Metaâ€‘Llamaâ€‘3â€‘70Bâ€‘Instruct, showing little change and planning future scaling.
 * [gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - ğŸ“¥ 7k / â­ 58 / A 2.7â€‘Bâ€‘parameter Japanese GPTâ€‘NeoX model trained on Japanese CCâ€‘100 and OSCAR by ABEJAâ€¯Inc, usable via Hugging Face Transformers pipelines or PyTorch and released under the MIT license.
 * [japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - ğŸ“¥ 5k / â­ 25 / rinnaâ€™s Japanese GPTâ€‘2 small is a 12â€‘layer, 768â€‘hidden transformer trained on Japanese CCâ€‘100 and Wikipedia, tokenized with SentencePiece, released under MIT onâ€¯Augâ€¯25â€¯2021 (Huggingâ€¯Face: rinna/japaneseâ€‘gpt2â€‘small, see https://arxiv.org/abs/2404.01657).
 * [open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - ğŸ“¥ 5k / â­ 205 / OpenCALM is a Japanese decoderâ€‘only transformer languageâ€‘model suite from CyberAgent, Inc. featuring versions ranging from 160â€¯M to 6.8â€¯B parameters preâ€‘trained on Wikipedia and Common Crawl, available via the Transformers library under a CCâ€¯BYâ€‘SAâ€¯4.0 license.
 * [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ğŸ“¥ 5k / â­ 74 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter extension of Metaâ€™s Llamaâ€‘2 model, preâ€‘trained on Japanese data with instruct and fast variants, and usable through Huggingâ€¯Face Transformers.
 * [llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4) - ğŸ“¥ 5k / â­ 16 / LLMâ€‘jpâ€‘3.1â€‘13bâ€‘instruct4 is a 13â€‘B, instructionâ€‘preâ€‘trained Japanese language model developed by NIIâ€™s R&D Center, released as a Huggingâ€‘Face Transformers checkpoint with a UNIGRAMâ€‘byteâ€‘fallback tokenizer.
 * [llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b) - ğŸ“¥ 4k / â­ 10 / llmâ€‘jpâ€‘3.1â€‘1.8b is a 1.8â€¯bâ€‘parameter Japanese LLM from NIIâ€™s Large Language Models R&D Center, distributed as a Huggingâ€¯Face checkpoint (torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, flashâ€‘attnâ€¯â‰¥â€¯2.5) with full model specs, tokenizer, and preâ€‘training details in the repo.
 * [DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese) - ğŸ“¥ 4k / â­ 94 / A Japaneseâ€‘finetuned version of DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14B, featuring usage examples, a specified prompt format, MIT licensing, and authored by Ryosuke Ishigami.
 * [sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1) - ğŸ“¥ 4k / â­ 34 / Offers an autoregressive Japanese language model (sarashina2.2â€‘3Bâ€‘instructâ€‘v0.1) from SBâ€¯Intuitions, benchmarked against other models, with example usage scripts and a note that safety training is limited.
 * [Llama-3-ELYZA-JP-8B-Heretic-GGUF](https://huggingface.co/ChiKoi7/Llama-3-ELYZA-JP-8B-Heretic-GGUF) - ğŸ“¥ 4k / â­ 1 / A Hereticâ€‘v1.1.0 abliteration of the Japaneseâ€‘enhanced Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B model, producing a decensored version that performs well on Japanese prompts yet exhibits a high refusal rate for English.
 * [TinySwallow-1.5B-Instruct](https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct) - ğŸ“¥ 3k / â­ 56 / TinySwallowâ€‘1.5Bâ€‘Instruct is a 1.5â€¯B Japanese instructionâ€‘tuned autoregressive language model distilled with TAID from Qwen2.5â€‘32Bâ€‘Instruct, intended for research use only.
 * [llm-jp-3-13b](https://huggingface.co/llm-jp/llm-jp-3-13b) - ğŸ“¥ 3k / â­ 13 / Repository hosts Huggingâ€¯Face checkpoints for Japanese LLMs (1.8â€¯B,â€¯3.7â€¯B,â€¯13â€¯B,â€¯17.2â€¯B) from the National Institute of Informatics, requiring PyTorchâ€¯2.3+, Transformersâ€¯4.40+ and include sample inference code, a 2.1â€¯Tâ€‘token unigramâ€‘based tokenizer, and preâ€‘training on mixed Japanese and English corpora.
 * [open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - ğŸ“¥ 3k / â­ 17 / OpenCALM is a suite of Japanese decoderâ€‘only language models from CyberAgent, spanning 160â€¯M to 6.8â€¯B parameters built on GPTâ€‘NeoX and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - ğŸ“¥ 2k / â­ 19 / OpenCALM is a family of Japanese decoderâ€‘only Transformer language models (160â€¯Mâ€“6.8â€¯B parameters) from CyberAgent, trained on Japanese Wikipedia and Common Crawl and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3) - ğŸ“¥ 2k / â­ 23 / Llamaâ€¯3.1â€¯Swallow is a Japaneseâ€‘enhanced series of 8B/70B Llamaâ€¯3.1 models, trained via continual preâ€‘training and Japaneseâ€‘specific instruction fineâ€‘tuning, with the latest 8Bâ€‘Instructâ€‘v0.3 setting stateâ€‘ofâ€‘theâ€‘art results on Japanese MTâ€‘Bench.
 * [llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b) - ğŸ“¥ 2k / â­ 10 / Huggingâ€¯Faceâ€‘compatible Japanese transformer LLMs (1.8b,â€¯3.7b,â€¯13b and their instruct/beta variants) built with torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40.1, accelerate, flashâ€‘attn, and pretrained on mixed Japaneseâ€‘English corpora such as Wikipedia, Commonâ€¯Crawl, and Dolma.
 * [llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3) - ğŸ“¥ 2k / â­ 4 / Hosts the LLMâ€‘jpâ€‘3â€‘7.2bâ€‘instruct3 7.2â€¯Bâ€‘parameter Japanese language model from the National Institute of Informatics, pretrained on Japanese Wikipedia and Common Crawl, provided in Huggingâ€¯Faceâ€¯Transformers format and requiring torchâ€¯â‰¥â€¯2.3, transformersâ€¯â‰¥â€¯4.40, accelerateâ€¯â‰¥â€¯0.29, and flashâ€‘attnâ€¯â‰¥â€¯2.5.
 * [youri-7b](https://huggingface.co/rinna/youri-7b) - ğŸ“¥ 2k / â­ 21 / youriâ€‘7b is a 32â€‘layer, 4096â€‘hidden transformer built from Llama2â€‘7b, continually preâ€‘trained on ~40â€¯B Japanese tokens (CCâ€‘100, C4, OSCAR, Pile, Wikipedia) and released Octâ€¯31â€¯2023, achieving competitive scores on AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA and Winogrande.
 * [sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1) - ğŸ“¥ 2k / â­ 12 / This repository hosts SBâ€¯Intuitionsâ€™ 1â€¯Bâ€‘parameter autoregressive Japanese instruction model sarashina2.2â€‘1bâ€‘instructâ€‘v0.1, benchmarked against other Japaneseâ€‘BERTs on Japanese and English MT and instruction tasks, with a torchâ€‘transformer usage snippet and a warning of limited safety training.
 * [japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - ğŸ“¥ 2k / â­ 75 / A 3.6â€‘billionâ€‘parameter Japanese GPTâ€‘NeoX model, trained on ~650â€¯GB of Japanese text (C4, CCâ€‘100, Oscar, web crawls), attains a 7.50 perplexity on internal C4 validation and is released under Apacheâ€¯2.0.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ğŸ“¥ 2k / â­ 81 / Japaneseâ€‘enhanced Llamaâ€‘2â€‘7B from ELYZA, preâ€‘trained for extended Japaneseâ€‘language capability with standard, instruct, and fast variants, detailed usage examples, developer credits, and licensed under Metaâ€™s Llamaâ€‘2 Community License.
 * [karasu-1.1B](https://huggingface.co/lightblue/karasu-1.1B) - ğŸ“¥ 2k / â­ 7 / Pretrained TinyLlama in Japanese (â‰ˆ50â€¯k steps) built on ~3â€¯B OSCAR/mC4 tokens, usable via HuggingFace Transformers or VLLM, created by Peterâ€¯Devine, Shoâ€¯Higuchi, Yuukiâ€¯Yamanaka, Atomâ€¯Sonoda, Shunichiâ€¯Taniguchi, Tomiokaâ€¯Wataru, and Renjuâ€¯Aoki.
 * [gemma-2-2b-jpn-it-GGUF](https://huggingface.co/bartowski/gemma-2-2b-jpn-it-GGUF) - ğŸ“¥ 2k / â­ 2 / Llama.cppâ€‘based imatrixâ€‘quantized weights for the gemmaâ€‘2â€‘2bâ€‘jpnâ€‘it model (f16, Q8_0, Q6_K, Q5_K, Q4_K, Q3_K, Q4_0 variants) compiled with releaseâ€¯b3972, optimized for lowâ€‘memory ARM inference (SVE/i8mm) and usable in LMâ€¯Studio with a specific prompt format.
 * [ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ğŸ“¥ 2k / â­ 23 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a 6.27â€‘Bâ€‘parameter Japanese extension of Metaâ€™s Llamaâ€‘2â€‘7B, further preâ€‘trained for Japanese language tasks and offered in base, instruct, fast, and fastâ€‘instruct variants, maintained by the ELYZA team under the Llamaâ€¯2 Community License.
 * [ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0) - ğŸ“¥ 2k / â­ 5 / ABEJAâ€‘Qwen2.5â€‘32bâ€‘Japaneseâ€‘v1.0, built on Qwen2.5â€‘32Bâ€‘Instruct, adds Japaneseâ€‘centric preâ€‘training followed by SFT and DPO fineâ€‘tuning (details on ABEJAâ€™s tech blog).
 * [japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - ğŸ“¥ 1k / â­ 10 / Repository provides GGUFâ€‘formatted, quantised model files for Stability AIâ€™s Japanese StableLMâ€¯Instructâ€¯Gammaâ€¯7B, created with Massedâ€¯Compute hardware and part of TheBlokeâ€™s a16zâ€‘funded LLM work.
 * [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ğŸ“¥ 1k / â­ 96 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7b is a Japaneseâ€‘optimized extension of Metaâ€™s Llamaâ€‘2â€¯7â€¯B, offering instruct and fast variants with 6.27â€“6.37â€¯B parameters that can be accessed via the Huggingâ€‘Face Transformers library.
 * [llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - ğŸ“¥ 1k / â­ 62 / A Japaneseâ€‘focused variant of Metaâ€‘Llamaâ€‘3â€‘8B, called Llamaâ€¯3â€¯Youkoâ€¯8B, was continually preâ€‘trained and instructionâ€‘tuned on ~22â€¯B tokens from Japanese corpora (CCâ€‘100, C4, OSCAR, Theâ€¯Pile, Wikipedia) and released onâ€¯Mayâ€¯1â€¯2024.
 * [Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4) - ğŸ“¥ 1k / â­ 4 / Llamaâ€¯3.3â€¯Swallow is a 70â€‘billionâ€‘parameter model that augments Metaâ€™s Llamaâ€¯3.3 for stronger Japanese capabilities while keeping English performance, built through continual preâ€‘training and instructionâ€‘fineâ€‘tuning, with multiple Instruct variants released since Decemberâ€¯2024 and available on HuggingFace.
 * [llm-jp-13b-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-v1.0) - ğŸ“¥ 1k / â­ 41 / Largeâ€‘language models from LLMâ€‘jp â€“ 13B and 1.3B Japaneseâ€‘English transformers with multiple instruction and LoRA variants, preâ€‘trained via Megatronâ€‘DeepSpeed and released in Huggingâ€¯Face format (torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34).
 * [llm-jp-1.3b-v1.0](https://huggingface.co/llm-jp/llm-jp-1.3b-v1.0) - ğŸ“¥ 1k / â­ 15 / LLMâ€‘jp offers 13â€¯B and 1.3â€¯B transformer language models, including multiple instructionâ€‘tuned variants, built with Megatronâ€‘DeepSpeed and the Huggingâ€¯Face Transformers ecosystem.
 * [sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1) - ğŸ“¥ 1k / â­ 13 / SBâ€¯Intuitionsâ€™ Sarashina2.2â€‘0.5Bâ€¯instructâ€¯v0.1 is a 0.5â€‘billionâ€‘parameter Japanese autoregressive model that performs well on Japanese and English MT benchmarks and is ready to load via torchâ€‘transformers.
 * [Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - ğŸ“¥ 1k / â­ 44 / TokyoTechâ€‘LLM offers the Swallowâ€¯Llamaâ€¯2 familyâ€”Japaneseâ€‘enhanced, superâ€‘visedâ€‘fineâ€‘tuned and noâ€‘vocabularyâ€‘expansion variants for 7â€¯B, 13â€¯B andâ€¯70â€¯B models, with recent releases includingâ€¯Swallowâ€‘7bâ€‘instructâ€‘v0.1 andâ€¯Swallowâ€‘70bâ€‘NVEâ€‘hf.
 * [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ğŸ“¥ 1k / â­ 42 / ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘13b extends Metaâ€™s Llamaâ€¯2 with additional preâ€‘training for Japanese, offering 13â€¯Bâ€‘parameter models (including instruct and fast variants) that can be loaded with PyTorch and ğŸ¤—â€¯Transformers under the Llamaâ€¯2 Community License.
 * [japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - ğŸ“¥ 1k / â­ 53 / Japanese Stableâ€¯LMâ€¯Instructâ€¯Gammaâ€¯7B is a 7â€‘Bâ€‘parameter decoderâ€‘only Japanese language model fineâ€‘tuned on instruction datasets, built on the Base Gammaâ€¯7B, requires Transformersâ€¯4.34+, is Apacheâ€¯2.0â€‘licensed, and is developed by Stabilityâ€¯AI.
 * [japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - ğŸ“¥ 1k / â­ 26 / Japaneseâ€‘StableLMâ€‘Instructâ€‘Betaâ€‘70B is a 70â€‘billionâ€‘parameter Japanese decoderâ€‘only Llama2â€‘based language model fineâ€‘tuned on Dollyâ€‘15k, Anthropic HH, and other public data, available as a 7â€‘billionâ€‘parameter variant and released under the Llama2 Community License.
 * [japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - ğŸ“¥ 1k / â­ 17 / Japaneseâ€‘StableLMâ€‘Baseâ€‘Betaâ€‘70B is a 70â€‘Bâ€‘parameter Llamaâ€‘2â€‘derived decoderâ€‘only language model fineâ€‘tuned on diverse Japanese data, offering smaller 7â€¯B versions, an instructionâ€‘following variant, and a faster inference release, all licensed under the Llama2 Community License.
 * [llm-jp-13b-instruct-full-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-v1.0) - ğŸ“¥ 1k / â­ 15 / Repositories of 13â€‘B and 1.3â€‘Bâ€‘parameter LLMâ€‘jp instructionâ€‘fineâ€‘tuned modelsâ€”including LoRA variantsâ€”packaged in Huggingâ€¯Face Transformers format and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, and accelerateâ€¯0.23, trained on ~50â€¯k mixed Japanese/English/code examples with Megatronâ€‘DeepSpeed and PEFT.
 * [weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ) - ğŸ“¥ 1k / â­ 13 / An autoGPTQâ€‘quantized 10â€‘Bâ€‘parameter Japaneseâ€‘centric multilingual GPTâ€‘NeoX model (weblabâ€‘10bâ€‘instructionâ€‘sftâ€‘GPTQ) that shrinks the 21.42â€¯GB original to a faster, GPUâ€‘required version, with a 6.03â€¯GB gguf alternative for CPU via llama.cpp, and can be run locally with textâ€‘generationâ€‘webui (~16â€¯tokens/s on an RTXâ€¯3060) or interactively in Colab.
 * [llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 8 / Hosts LLMâ€‘jpâ€™s 13â€¯B and 1.3â€¯B Japaneseâ€‘English instruction and pretrained models, offered in several variant checkpoints for Huggingâ€¯Face Transformers with torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34, accelerateâ€¯0.23 and DeepSpeed support.
 * [japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - ğŸ“¥ 1k / â­ 25 / A 7â€‘Bâ€‘parameter, autoregressive, decoderâ€‘only Japanese model based on Mistralâ€‘7Bâ€‘v0.1, released by Stabilityâ€¯AI under Apacheâ€¯2.0 for highâ€‘performance Japanese language and downstream tasks and requiring Transformersâ€¯4.34.0+.
 * [llm-jp-13b-instruct-full-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly-oasst-v1.0) - ğŸ“¥ 1k / â­ 4 / LLMâ€‘jp supplies instructionâ€‘style and pretrained 13B/1.3B Transformer models in Hugging Face and DeepSpeed formats, trained on 50â€¯k+ mixed Japanese/English/sourceâ€‘code data and requiring torchâ€¯â‰¥â€¯2.0, transformersâ€¯â‰¥â€¯4.34 and accelerateâ€¯0.23.
 * [Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - ğŸ“¥ 1k / â­ 37 / Offers the Swallowâ€¯Llamaâ€‘2 family of Japaneseâ€‘English LLMsâ€”from 7B,â€¯13B,â€¯andâ€¯70B models with instruct, NVE, and preview variantsâ€”tuned via supervised fineâ€‘tuning, available through Megatronâ€‘LM with a tokenizer, and benchmarked on core Japanese tasks.
 * [ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - ğŸ“¥ 1k / â­ 3 / Provides a 4â€‘bit, 4.11â€¯GB quantized version of Metaâ€™s Llamaâ€‘2â€¯7B (ELYZAâ€‘japaneseâ€‘Llamaâ€‘2â€‘7bâ€‘fastâ€‘instruct) that cuts memory but degrades instructionâ€‘following, requires a GPU and autoGPTQ, and includes references to alternate AWQ, llama.cpp, and gguf quantizations and benchmark results.
 * [open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - ğŸ“¥ 1k / â­ 11 / OpenCALM is a decoderâ€‘only Japanese transformer family from CyberAgent (160â€¯Mâ€¯â€“â€¯6.8â€¯B parameters, from openâ€‘calmâ€‘small to openâ€‘calmâ€‘7b), trained on Japanese Wikipedia and Commonâ€‘Crawl, licensed CCâ€¯BYâ€‘SAâ€¯4.0 and usable through Hugging Face transformers.
 * [Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1) - ğŸ“¥ 1k / â­ 2 / Gemmaâ€‘2â€‘Llamaâ€‘Swallow is a family of Gemmaâ€‘2 modelsâ€”2b,â€¯9b,â€¯27bâ€”continually preâ€‘trained and instructionâ€‘tuned on synthetic Japanese data, enhancing Japanese performance while maintaining English capability, and released on Huggingâ€¯Face.

### fill-mask
 * [bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - ğŸ“¥ 480k / â­ 71 / Japanese BERTâ€‘base pretrained on 2019 Japanese Wikipedia with IPAâ€‘dictionary and wholeâ€‘word masking, 12â€‘layer 768â€‘dim, 32,000â€‘vocab, 512â€‘token sequences, 1â€¯M steps, available at clâ€‘tohoku/bertâ€‘japanese under CCâ€‘BYâ€‘SA.
 * [bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - ğŸ“¥ 106k / â­ 8 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden, 12 heads) pretrained on ~17â€¯M sentences from Japanese Wikipedia (2.6â€¯GB) using MeCab IPA wordâ€‘level tokenization followed by character tokenization into a 4000â€‘word vocabulary, with training code atâ€¯clâ€‘tohoku/bertâ€‘japanese and released under CCâ€¯BYâ€‘SAâ€¯3.0.
 * [bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - ğŸ“¥ 73k / â­ 6 / A BERTâ€‘base Japanese model (12 layers, 768â€‘dim hidden states, 12 heads) trained on 30â€¯M Wikipedia sentences (~4â€¯GB) with Unidicâ€¯2.1.2 wordâ€‘level tokenization followed by characterâ€‘level tokenization and wholeâ€‘word masking, using 512â€‘token sequences, 256 batches, and 1â€¯M training steps.
 * [jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - ğŸ“¥ 52k / â­ 3 / Japanese RoBERTaâ€‘base model pretrained on ~10â€¯M Japanese medical abstracts and 1.4â€¯M body texts from JST, tokenized with a 30â€¯kâ€‘token SentencePiece, released under CCâ€¯BYâ€‘4.0 and usable via Hugging Face pipelines.
 * [modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m) - ğŸ“¥ 49k / â­ 19 / ModernBERTâ€‘Jaâ€‘310M is a Japaneseâ€‘language BERT variant that blends localâ€‘global attention with RoPE, trained on 4.09â€¯T tokens of Japanese/English text, supports a 102â€¯400â€‘word vocabulary, 8â€¯192â€‘token sequences, and is optimized for Flashâ€¯Attentionâ€¯2.
 * [bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - ğŸ“¥ 39k / â­ 38 / A BERT base model pretrained on ~17â€¯M Japanese Wikipedia sentences (2.6â€¯GB) that tokenizes with the IPA dictionary and WordPiece, has 12 layers/768â€‘dim hidden states/12 heads, a 32â€¯000â€‘token vocabulary, was trained for 1â€¯M steps on Cloud TPUs and is released under CCâ€‘BYâ€‘SAâ€¯3.0.
 * [line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - ğŸ“¥ 29k / â­ 48 / LINE DistilBERT Japanese is a 66â€‘millionâ€‘parameter DistilBERT model preâ€‘trained on 131â€¯GB of Japanese web text using an inâ€‘house BERTâ€‘base teacher, evaluated on JGLUE, tokenized with MeCabâ€¯Unidic and SentencePiece, and released under the Apacheâ€¯2.0 license.
 * [modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m) - ğŸ“¥ 16k / â­ 6 / ModernBERTâ€‘Jaâ€‘70M is a lightweight Japanese BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯T mixedâ€‘language tokens (vocabâ€¯102â€¯400, maxâ€¯8â€¯192 tokens), supports Flashâ€¯Attentionâ€¯2, and comes in several sizes from 30â€¯M to 310â€¯M parameters.
 * [deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - ğŸ“¥ 12k / â­ 8 / Japanese DeBERTaâ€¯V2 large model trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR with characterâ€‘level sentencepiece tokenization and wholeâ€‘word masking, ready for downstream fineâ€‘tuning through Huggingâ€¯Face Transformers.
 * [japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - ğŸ“¥ 11k / â­ 39 / Japaneseâ€‘Robertaâ€‘Base is a pretrained maskedâ€‘language model from rinna Co.,â€¯Ltd., with guidelines for proper loading, token preprocessing, positionâ€‘id handling, and usage examples emphasizing the need for a leading `[CLS]` token and consistent tokenization.
 * [bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - ğŸ“¥ 10k / â­ 27 / Japanese BERTâ€‘base (12â€¯layers, 768 hidden, 12 heads) pretrained on 4â€¯GB of Japanese Wikipedia (â‰ˆ30â€¯M sentences) with Unidicâ€¯2.1.2 wordâ€‘level tokenization, WordPiece subâ€‘tokenization, and wholeâ€‘word masking.
 * [deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - ğŸ“¥ 8k / â­ 30 / Japanese DeBERTaâ€¯V2 base model pretrained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100 and OSCAR data using Juman++ segmentation and SentencePiece tokenization, trained for three weeks on eight NVIDIA A100 GPUs and ready for fineâ€‘tuning.
 * [modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m) - ğŸ“¥ 8k / â­ 45 / A 132â€‘millionâ€‘parameter Japanese ModernBERT model that blends localâ€‘global and RoPE attention, trained on 4.39â€¯T tokens (Japanese/English) with a 102â€‘kâ€‘size vocab, 8,192â€‘token max length, and optimized for Flashâ€¯Attentionâ€¯2.
 * [modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m) - ğŸ“¥ 3k / â­ 5 / ModernBERTâ€‘Jaâ€‘30M is a Japaneseâ€‘language BERT variant that blends local and global attention with RoPE, trained on 4.39â€¯TB of Japanese/English text, supports 8,192â€‘token sequences, comes in sizes from 30â€¯M to 130â€¯M parameters, and works best with Flash Attentionâ€¯2.
 * [roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - ğŸ“¥ 2k / â­ 8 / Japanese RoBERTaâ€‘base model pretrained on Japanese Wikipedia and CCâ€‘100, using Juman++â€‘based SentencePiece tokenization, fineâ€‘tunable via Hugging Face, trained over 700k steps on 8â€¯A100 GPUs with mixedâ€‘precision.
 * [splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - ğŸ“¥ 2k / â­ 10 / Zeroâ€‘shot evaluation of Japanese SPLADE variants and other retrieval models on the MIRACL and hotchpotch/JQaRA datasets shows SPLADEâ€‘v3 achieving NDCG@10â€¯=â€¯0.604, SPLADEâ€‘v2â€‘doc requiring no query encoder, and includes sample code for inspecting tokenâ€‘level expansion.
 * [deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - ğŸ“¥ 2k / â­ 4 / DeBERTaV2â€¯base trained on Japanese corpora (CCâ€‘100, mC4, OSCAR2301, Wikipedia, Wikinews) with FPâ€‘16 fineâ€‘tuning for NLU tasks (JSTS, JNLI, JCommonsenseQA), released under CCâ€¯BYâ€‘SAâ€¯4.0 and funded by Japanese research grants.
 * [deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - ğŸ“¥ 2k / â­ 2 / Japanese DeBERTaâ€¯V2 tiny, pretrained on ~171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR corpora, requires Juman++ word segmentation, was trained in 33â€¯h on 8 NVIDIAâ€¯A100 GPUs, and can be fineâ€‘tuned for downstream tasks.
 * [llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base) - ğŸ“¥ 2k / â­ 10 / A ModernBERTâ€‘base model trained on the 3.4â€¯TB Japanese llmâ€‘jpâ€‘corpus v4, fineâ€‘tuned in two stages (max_seq_len 1024 â†’ 8192), achieves 0.92â€¯JSTS, 0.91â€¯JNLI, and 0.88â€¯JCoLA.
 * [bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - ğŸ“¥ 1k / â­ 9 / Japanese BERTâ€‘large (24 layers, 1024â€‘hidden size, 16 heads, 32â€¯K vocab) pretrained on 30â€¯M Japanese Wikipedia sentences with Unidicâ€‘2.1.2 wordâ€‘level tokenization, WordPiece subwords, and wholeâ€‘word masking over 1â€¯M steps.
 * [deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - ğŸ“¥ 1k / â­ 9 / Japanese DeBERTaâ€¯V2 large is preâ€‘trained on 171â€¯GB of Japanese Wikipedia, CCâ€‘100, and OSCAR (using Juman++ segmentation and SentencePiece tokenization) and was trained for 36â€¯days on 8 NVIDIAâ€¯A100 GPUs via Huggingâ€¯Face Transformers.
 * [roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - ğŸ“¥ 1k / â­ 3 / A 512â€‘token Japanese RoBERTaâ€‘large preâ€‘trained on Japanese Wikipedia and CCâ€‘100, employing Juman++â€¯+â€¯SentencePiece tokenization, trained for 670â€¯k steps on eight A100 GPUs with a 6Ã—10â»âµ learning rate.

### sentence-similarity
 * [JaColBERTv2.5](https://huggingface.co/answerdotai/JaColBERTv2.5) - ğŸ“¥ 599k / â­ 22 / Weights for the final JaColBERTv2.5 checkpoint, trained on just 40â€¯% of JaColBERTv2 data with a new recipe, outperform all previous modelsâ€”including JaColBERTV2 multilingual variants such as BGEâ€‘M3â€”across every dataset.
 * [ruri-base](https://huggingface.co/cl-nagoya/ruri-base) - ğŸ“¥ 319k / â­ 11 / Japanese general text embedding models (Ruriâ€‘v3, 30â€‘310â€¯M parameters, 8192â€‘token max, high JMTEB scores) are offered with Sentenceâ€‘Transformers usage examples and benchmark comparisons to other Japanese embeddings.
 * [ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m) - ğŸ“¥ 93k / â­ 60 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token inputs, a 100Kâ€‘token vocabulary, FlashAttentionâ€‘accelerated inference, and multiple size variants for fast sentenceâ€‘transformer usage.
 * [ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m) - ğŸ“¥ 35k / â­ 4 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese text embedding model built on ModernBERTâ€‘Ja, supporting up to 8,192 tokens, a 100â€¯kâ€‘token vocabulary, FlashAttention acceleration, and multiple sizes from 37â€¯M to 315â€¯M parameters.
 * [GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - ğŸ“¥ 31k / â­ 34 / GLuCoSE is a Japanese sentenceâ€‘embedding model built on LUKE that outputs 768â€‘dim meanâ€‘pooled vectors (up to 512 tokens) trained on web and NLI/search data, achievingâ€¯0.864 Spearman andâ€¯0.818 Pearson on similarity benchmarks.
 * [JaColBERTv2](https://huggingface.co/bclavie/JaColBERTv2) - ğŸ“¥ 18k / â­ 16 / JaColBERTv2 is a Japaneseâ€‘only ColBERTâ€‘based retrieval model trained with knowledge distillation on MMarco (31 negatives per positive, 250k steps, batchâ€¯32) that currently outperforms multilingualâ€‘e5â€‘large, BGEâ€‘M3, and JaColBERT, with a full evaluation pending.
 * [ruri-small](https://huggingface.co/cl-nagoya/ruri-small) - ğŸ“¥ 15k / â­ 9 / Includes Ruriâ€¯v3 Japanese text embeddings (30â€¯Mâ€“310â€¯M parameters, 8192â€‘token limit, JMTEB 74.5â€“77.2), instructions for Sentence Transformers using â€œã‚¯ã‚¨ãƒª:â€ or â€œæ–‡ç« :â€ prefixes, and benchmark results for several Japanese models such as Sup/Unsup SimCSE, GLuCoSE, and LaBSE.
 * [plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b) - ğŸ“¥ 11k / â­ 44 / PLaMoâ€‘Embeddingâ€‘1B is a Japanese textâ€‘embedding model from Preferred Networks that converts Japanese text into vectors for information retrieval, classification, and clustering, shows strong performance on the JMTEB benchmark, and is freely available under an Apacheâ€¯v2.0 license.
 * [sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - ğŸ“¥ 8k / â­ 36 / sbert-jsnliâ€‘lukeâ€‘japaneseâ€‘baseâ€‘lite is a 768â€‘dimensional sentenceâ€‘transformer built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite, trained one epoch on shunk031/jsnli, and includes examples for clustering, semantic search, and both Sentenceâ€‘Transformers and HuggingFace usage.
 * [ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m) - ğŸ“¥ 8k / â­ 2 / Ruriâ€¯v3 is a stateâ€‘ofâ€‘theâ€‘art Japanese textâ€‘embedding model built on ModernBERTâ€‘Ja, supporting up to 8192â€‘token sequences, a 100Kâ€‘token vocabulary, FlashAttention, and released in sizes from 30â€¯M to 310â€¯M parameters for use with sentenceâ€‘transformers.
 * [GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2) - ğŸ“¥ 6k / â­ 21 / GLuCoSEâ€¯v2 is a CPUâ€‘friendly Japanese textâ€‘embedding model, fineâ€‘tuned by distillation and multiâ€‘stage contrastive learning, that delivers superior semanticâ€‘similarity and retrieval performanceâ€”outperforming comparableâ€‘size models on MIRACL and related benchmarks.
 * [ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m) - ğŸ“¥ 4k / â­ 1 / Ruriâ€¯v3 delivers highâ€‘performance Japanese text embeddings up to 8192 tokens, a 100kâ€‘token vocabulary, FlashAttention support, and multiple model sizes (30â€¯mâ€“310â€¯m) for efficient inference and fineâ€‘tuning via sentenceâ€‘transformers.
 * [ruri-large](https://huggingface.co/cl-nagoya/ruri-large) - ğŸ“¥ 3k / â­ 44 / A collection of releaseâ€‘ready Ruri v3 Japanese text embedding models (30mâ€“310m), complete with SentenceTransformer usage tips, query/passage prefixes, and JMTEB benchmark results showing how they compare to other Japanese and multilingual embeddings.
 * [simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - ğŸ“¥ 1k / â­ 15 / Japanese SimCSE based on BERTâ€‘baseâ€‘japaneseâ€‘v2, fineâ€‘tuned on the JSNLI dataset for sentence embeddings, compatible with Sentenceâ€‘Transformers and trained with cosineâ€‘similarity loss to maximize Spearman correlation.

### automatic-speech-recognition
 * [wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - ğŸ“¥ 3M / â­ 47 / Japanese wav2vecâ€‘2 XLSRâ€‘53 fineâ€‘tuned on Common Voiceâ€¯6.1, CSS10, and JSUT, requiring 16â€¯kHz audio and usable via HuggingSound or HuggingFace pipelines.
 * [kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0) - ğŸ“¥ 84k / â­ 84 / Kotobaâ€‘Whisperâ€¯v2.0 is a Japanese ASR model distilled from OpenAI Whisper largeâ€‘v3, trained on 7.2â€¯million ReazonSpeech clips, that runs 6.3Ã— faster while matching the teacherâ€™s CER/WER on inâ€‘domain tests and includes stableâ€‘ts/punctuation support and full training code on GitHub.
 * [wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - ğŸ“¥ 64k / â­ 4 / Fineâ€‘tuned wav2vec2â€‘base Japanese ASR model trained on Common Voiceâ€¯11.0 that predicts only Hiragana, built from rinna/japaneseâ€‘wav2vec2â€‘base with 20â€¯epochs at lrâ€¯1eâ€‘4.
 * [kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - ğŸ“¥ 18k / â­ 84 / Kotobaâ€‘Whisperâ€‘v2.2 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated diarization and automatic punctuation via a HuggingFaceâ€‘Transformers pipeline, built in collaboration with Asahiâ€¯Ushio and Kotoba Technologies.
 * [kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1) - ğŸ“¥ 15k / â­ 19 / Kotobaâ€‘Whisperâ€‘v2.1 is a Japanese ASR model extending kotobaâ€‘whisperâ€‘v2.0 with integrated punctuationâ€‘postprocessing pipelines that maintain comparable CER performance while enabling seamless, punctuationâ€‘aware transcription.
 * [reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - ğŸ“¥ 7k / â­ 35 / reazonspeech-nemo-v2 is a 619â€‘Mâ€‘parameter Japanese longâ€‘form ASR model built on an improved Fastâ€‘Conformer with Linearly Scalable Attention, trained on the ReazonSpeechâ€¯v2.0 corpus, offering multiâ€‘hour inference via a subword RNNâ€‘T decoder (3000â€‘token SentencePiece) and distributed under Apacheâ€¯2.0.
 * [anime-whisper](https://huggingface.co/litagin/anime-whisper) - ğŸ“¥ 5k / â­ 115 / Anime Whisper is a lightweight Japanese ASR model fineâ€‘tuned on ~5,300â€¯h of animeâ€‘style dialogue that delivers low hallucination, rhythmâ€‘aligned punctuation and accurate transcription of nonâ€‘verbal sounds and NSFW content, and must be run without an initial prompt.
 * [kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0) - ğŸ“¥ 4k / â­ 17 / Kotobaâ€‘Whisperâ€‘Bilingual v1.0 delivers 6.3Ã— faster distilled Whisper models for Japanese and English ASR plus bidirectional speechâ€‘toâ€‘text translation, built from OpenAIâ€™s Whisperâ€¯largeâ€‘v3 via knowledge distillation with crossâ€‘entropy and KLâ€‘divergence loss.
 * [parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja) - ğŸ“¥ 2k / â­ 39 / NVIDIA NeMoâ€™s 0.6â€¯Bâ€‘parameter Hybrid FastConformerâ€‘TDTâ€‘CTC ASR model transcribes Japanese speech with punctuation and is available for inference or fineâ€‘tuning within the NeMo framework.
 * [kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - ğŸ“¥ 2k / â­ 58 / Kotobaâ€‘Whisperâ€¯v1.0, a Japanese ASR model distilled from OpenAIâ€™s Whisper largeâ€‘v3, delivers aâ€¯6.3Ã— speedâ€‘up with comparable accuracy, was trained on 1,253â€¯h of ReazonSpeech, and its code (plus a newer v2.0 variant on Huggingâ€¯Face) is publicly available.
 * [japanese-hubert-base-phoneme-ctc-v4](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc-v4) - ğŸ“¥ 1k / â­ 2 / Fineâ€‘tuned Japanese Hubertâ€‘Base for CTC phoneme recognition (v4) with updated sentenceâ€‘filtering rules, pronunciation adjustments, and a GPU switch to A6000, stopped training at 110k steps.
 * [japanese-hubert-base-k2-rs35kh-bpe](https://huggingface.co/reazon-research/japanese-hubert-base-k2-rs35kh-bpe) - ğŸ“¥ 1k / â­ 3 / Fineâ€‘tuned Japaneseâ€¯Hubertâ€¯Base (k2, ReazonSpeechâ€¯v2.0) attains 11.07â€¯% CER on shortâ€‘form ASR and 27.05â€¯% on longâ€‘form speech, outperforming wav2vecâ€‘2
variants, and is released under the Apacheâ€¯2.0 license.

### feature-extraction
 * [japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - ğŸ“¥ 405k / â­ 13 / Japanese CLOOBâ€‘VIT-B-16, a Visionâ€‘Language model based on vitâ€‘baseâ€‘patch16â€‘224 trained on translated CC12M captions and released by rinna Co., Ltd. on Mayâ€¯12â€¯2022 under Apacheâ€¯2.0.
 * [sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - ğŸ“¥ 43k / â­ 51 / A Japanese Sentenceâ€‘BERT v2, fineâ€‘tuned on clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘wholeâ€‘wordâ€‘masking with MultipleNegativesRankingLoss, boosts accuracy by ~1.5â€“2â€¯% over v1 and is released asâ€¯sonoisa/sentenceâ€‘bertâ€‘baseâ€‘jaâ€‘meanâ€‘tokensâ€‘v2.
 * [sup-simcse-ja-base](https://huggingface.co/cl-nagoya/sup-simcse-ja-base) - ğŸ“¥ 34k / â­ 2 / A Japanese BERTâ€‘base model fineâ€‘tuned with supervised SimCSE on JSNLI, exposed via Sentenceâ€‘Transformers or HuggingFace with CLS pooling, trained on 1â€¯M examples at 512â€‘batch size, 5â€¯Ã—â€¯10â»âµ learning rate, 5â€¯Ã—â€¯10â»âµ temperature, 64â€‘token limit, and BFloat16 precision.
 * [japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - ğŸ“¥ 31k / â­ 22 / rinna/japanese-clipâ€‘vitâ€‘bâ€‘16 is an Apacheâ€‘2.0 licensed Japanese CLIP model based on ViTâ€‘B/16, trained on CC12M captions translated to Japanese and released on Mayâ€¯12â€¯2022.
 * [sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - ğŸ“¥ 23k / â­ 11 / Japanese Sentenceâ€‘BERT (v1) model for generating sentence embeddings, with an improved v2 available and sample usage via Huggingâ€¯Face Transformers and a custom `SentenceBertJapanese` class.
 * [clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base) - ğŸ“¥ 21k / â­ 29 / LY Corporationâ€™s clipâ€‘japaneseâ€‘base is a Japanese CLIP model trained on ~1â€¯B imageâ€‘text pairs, using an Eva02â€‘B transformer image encoder with a 12â€‘layer BERT text encoder, achieving R@1â€¯0.30 on STAIR, 0.89 accuracy on Recruit and 0.58 accuracy on ImageNetâ€‘1K, and supporting zeroâ€‘shot image classification and retrieval.
 * [transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - ğŸ“¥ 10k / â­ 2 / ja_ginza_electra is a spaCyâ€¯v3 Python package offering a Japanese ELECTRA model fineâ€‘tuned on mC4 and UD_Japanese_BCCWJâ€¯r2.8 (based on megagonlabs/transformersâ€‘udâ€‘japaneseâ€‘electraâ€‘baseâ€‘discrimininator) with custom bunsetuâ€‘phrase detection, distributed under the MIT license.
 * [sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - ğŸ“¥ 4k / â­ 14 / Japanese Sentenceâ€‘LUKE model trained on the same dataset as Sentenceâ€‘BERT, outperforming or matching it, built on studioâ€‘ousia/lukeâ€‘japaneseâ€‘baseâ€‘lite and used via Huggingâ€¯Face Transformersâ€™ MLukeTokenizer and LukeModel.
 * [t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - ğŸ“¥ 2k / â­ 54 / A Japaneseâ€‘language T5 model, pretrained on ~100â€¯GB of Wikipedia and OSCAR data with SentencePiece tokenization, surpasses Googleâ€™s multilingual T5 on a newsâ€‘classification benchmark but needs fineâ€‘tuning and may yield biased outputs.
 * [sarashina-embedding-v2-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b) - ğŸ“¥ 2k / â­ 17 / Sarashinaâ€‘Embeddingâ€‘v2â€‘1B is a 1,792â€‘dimensional Japanese sentence transformer trained with multiâ€‘stage contrastive learning that achieves stateâ€‘ofâ€‘theâ€‘art JMTEB scores, and can be used for semantic similarity, search, paraphrase mining, classification, and clustering via Sentenceâ€‘Transformers with optional instruction prefixes.
 * [sup-simcse-ja-large](https://huggingface.co/cl-nagoya/sup-simcse-ja-large) - ğŸ“¥ 2k / â­ 14 / Supâ€‘simcseâ€‘jaâ€‘large provides a supervised SimCSE fineâ€‘tuned Japanese BERTâ€‘large (clâ€‘tohoku/bertâ€‘largeâ€‘japaneseâ€‘v2) model with CLSâ€‘plusâ€‘MLP pooling, trained on ~1â€¯M JSNLI sentences (lrâ€¯5eâ€‘5, batchâ€¯512, tempâ€¯0.05, maxâ€¯64) and ready for use with Sentenceâ€‘Transformers or Huggingâ€¯Face Transformers.
 * [clip-japanese-base-v2](https://huggingface.co/line-corporation/clip-japanese-base-v2) - ğŸ“¥ 1k / â­ 15 / Japanese CLIP model clipâ€‘japaneseâ€‘baseâ€‘v2, upgraded with ~2â€¯B imageâ€‘text pairs and distillation, pairs an Eva02â€‘B image encoder with a 12â€‘layer BERT text encoder to reach higher ImageNetâ€‘1k accuracy (0.708) than its predecessor.

### translation
 * [vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf) - ğŸ“¥ 256k / â­ 8 / A LLaMAâ€¯3 Youko qlora fineâ€‘tune built on a new VNTL dataset, optimized for accurate, literal translations of Japanese visual novels to English without chat mode, using the default LLaMAâ€¯3 prompt and recommending neutral sampling (temperatureâ€¯0, no repetition penalty).
 * [opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) - ğŸ“¥ 55k / â­ 67 / Japaneseâ€‘toâ€‘English Transformerâ€‘Align MT model from the Opus corpus, using normalization and SentencePiece preprocessing, achieves 41.7 BLEU and 0.589 chrâ€‘F on the Tatoeba test set.
 * [opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - ğŸ“¥ 10k / â­ 14 / Englishâ€‘toâ€‘Japanese transformerâ€‘align MT model with 15.2â€¯BLEU, built on opus+btâ€‘2021â€‘04â€‘10 using normalizationâ€¯+â€¯SentencePiece, hosted on the Tatoeba Challenge.
 * [LFM2-350M-ENJP-MT-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF) - ğŸ“¥ 7k / â­ 27 / Fineâ€‘tuned, GGUFâ€‘quantized LFM2â€‘350M checkpoint for near realâ€‘time biâ€‘directional Japaneseâ€‘English translation of shortâ€‘toâ€‘medium text, usable via llama.cpp.
 * [LFM2-350M-ENJP-MT](https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT) - ğŸ“¥ 3k / â­ 81 / LFM2â€‘350Mâ€‘ENJPâ€‘MT is a fineâ€‘tuned LFM2â€‘350M checkpoint that delivers near realâ€‘time, bidirectional Japanese/English translation for shortâ€‘toâ€‘medium inputs with quality comparable to models over ten times larger, as illustrated across everyday, technical, business, and news domains, and emphasizes collaborative humanâ€‘AI use.
 * [Sugoi-14B-Ultra-GGUF](https://huggingface.co/sugoitoolkit/Sugoi-14B-Ultra-GGUF) - ğŸ“¥ 2k / â­ 8 / Sugoiâ€¯LLMâ€¯14Bâ€¯Ultra (GGUF) is a Japaneseâ€‘toâ€‘English translation model with a BLEU score of 21.38â€”nearly double its prior 13.67â€”excel at RPGâ€‘Maker bracketed text, strong prompt adherence, and JSON output for interactive chat UIs.
 * [plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate) - ğŸ“¥ 2k / â­ 109 / PLaMo Translation Model is a largeâ€‘scale language model created by Preferred Networks for translation tasks, available in base, postâ€‘trained, and evaluation variants, released under the PLaMo community license and not instructionâ€‘tuned for chat or other downstream uses.
 * [fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja) - ğŸ“¥ 1k / â­ 54 / FuguMT is a Marianâ€‘NMT based Englishâ€‘toâ€‘Japanese translation model built with Huggingâ€¯Face Transformers and SentencePiece, achieving a BLEU score of 32.7 on Tatoeba.
 * [elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en) - ğŸ“¥ 1k / â­ 9 / ElanMTâ€‘BTâ€‘jaâ€‘en is a Marian MT Japaneseâ€‘toâ€‘English model fineâ€‘tuned solely on openly licensed and backâ€‘translated Wikipedia data, matching the performance of other public models while explicitly avoiding webâ€‘crawled or machineâ€‘translated corpora, and released under a CCâ€‘BYâ€‘SAâ€‘4.0 license.

### text-classification
 * [bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily) - ğŸ“¥ 36k / â­ 3 / Japanese BERT Base model fineâ€‘tuned on a 10â€‘label emotion blogâ€‘post dataset (~1,000 sentences) derived fromâ€¯tohokuâ€‘nlp/bert-base-japanese for accurate emotion detection and classification.
 * [luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ğŸ“¥ 16k / â­ 43 / A Japanese LUKE model fineâ€‘tuned on the WRIME dataset that classifies which of eight emotionsâ€”joy, sadness, anticipation, surprise, anger, fear, disgust, trustâ€”is expressed in a sentence.
 * [japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - ğŸ“¥ 10k / â­ 14 / Japanese sentiment analysis model trained on the chABSA dataset, achieving lossâ€¯0.0001, accuracyâ€¯1.0, and F1â€¯1.0, built with Transformersâ€¯4.24.0 and PyTorchâ€¯1.12.1+cu113, optimized with Adam (learningâ€¯rateâ€¯2eâ€‘05, 10 epochs, batchâ€¯sizeâ€¯16) and evaluated via `model(**inputs)`.
 * [bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - ğŸ“¥ 4k / â­ 2 / A Japanese BERTâ€‘based model fineâ€‘tuned on the JGLUE JSTS dataset for semantic similarity scoringâ€”introduced in chapterâ€¯5 ofâ€¯â€œLarge Language Model Introductionâ€â€”with Colab notebooks, transformersâ€‘pipeline usage, and an Apacheâ€¯2.0 license.
 * [bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - ğŸ“¥ 2k / â­ 6 / A Japanese BERT BASE fineâ€‘tuned on the WRIME dataset predicts 0â€‘4 intensity scores for eight emotions (joy, sadness, anticipation, surprise, anger, fear, disgust, trust) for writers and readers, with code available, trained in 3â€¯hrs on a K80, achieving about 0.6â€¯MSE for writers and 0.2â€¯MSE for readers.
 * [bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - ğŸ“¥ 1k / â­ 14 / Fineâ€‘tuned Japanese BERT (clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2) on Amazon product reviews for sentiment classification, achieving ~81â€¯% accuracy and 0.73â€¯F1 after 6 epochs with a 2â€¯Ã—â€¯10â»âµ learning rate.

### image-to-text
 * [manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - ğŸ“¥ 178k / â­ 163 / Mangaâ€¯OCR is a Visionâ€¯Encoderâ€‘Decoder OCR tool that reads vertical and horizontal Japanese manga textâ€”including furiganaâ€”across diverse fonts and lowâ€‘quality images, with the source code freely available.
 * [meiki.text.detect.v0](https://huggingface.co/rtr46/meiki.text.detect.v0) - ğŸ“¥ 21k / â­ 3 / meikiocr supplies a Dâ€‘FINEâ€‘based, openâ€‘weight textâ€‘detection model for videoâ€‘games (v0.1 with MobileNetâ€‘v4 backbones, two resolution variants and a 64â€‘box limit) and experimental lowâ€‘latency tiny and small variants trained on Japanese videoâ€‘games and manga.
 * [meiki.txt.recognition.v0](https://huggingface.co/rtr46/meiki.txt.recognition.v0) - ğŸ“¥ 21k / â­ 4 / Meikiocrâ€™sâ€¯`meiki.text.recognition.v0`â€”a Dâ€‘FINE-based MobileNetV4 model fineâ€‘tuned on Japanese videoâ€‘game textâ€”delivers stateâ€‘ofâ€‘theâ€‘art accuracy and latency for horizontal text by detecting up to 48 characters from 960Ã—32 inputs, outputting each character with its bounding box and confidence score.
 * [sarashina2.2-vision-3b](https://huggingface.co/sbintuitions/sarashina2.2-vision-3b) - ğŸ“¥ 1k / â­ 13 / Sarashina2.2â€‘Visionâ€‘3B is a 3â€‘B parameter Japanese large visionâ€‘language model built on Sarashina2.2â€‘3Bâ€‘Instruct and a SigLIP image encoder, achieving strong performance on Japanese VQA benchmarks.
 * [manga-ocr](https://huggingface.co/mayocream/manga-ocr) - ğŸ“¥ 1k / â­ 1 / Manga OCR is a Vision Encoderâ€‘Decoder system that delivers highâ€‘quality OCR of Japanese mangaâ€”including vertical and horizontal text with furigana overlaysâ€”across diverse fonts and lowâ€‘quality images, and can also be used for general printed Japanese OCR.

### text-ranking
 * [japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2) - ğŸ“¥ 13k / â­ 5 / Fast, lightweight Japanese Rerankerâ€¯v2 models (tiny,â€¯xsmall,â€¯small,â€¯base) with benchmark scores and GPU speeds, usable via sentence_transformersâ€¯CrossEncoder and transformersâ€¯â‰¥â€¯v4.48 (optionally accelerated with flashâ€‘attn) and also available in ONNX/quantized forms for CPU/ARM.
 * [japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - ğŸ“¥ 10k / â­ 4 / Japaneseâ€‘trained CrossEncoder rerankers ranging from xsmall (384) to large (1024) plus a BGEâ€‘v2â€‘m3â€‘v1 model, with example code for fineâ€‘tuning, inference, and benchmark scores on JQaRA, JaCWIR, MIRACL, and JSQuAD.
 * [ruri-v3-reranker-310m](https://huggingface.co/cl-nagoya/ruri-v3-reranker-310m) - ğŸ“¥ 6k / â­ 12 / Ruriâ€‘v3 Reranker is a robust Japanese text reranker built on ModernBERTâ€‘Ja, supporting up to 8,192â€‘token sequences, a 100kâ€‘token vocabulary, FlashAttention and a SentencePiece tokenizer, and it can be used via sentenceâ€‘transformers.
 * [japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - ğŸ“¥ 5k / â­ 7 / Japanese CrossEncoder reranker models ranging from xsmall to large (plus BGE), evaluated on JQaRA, JaCWIR, MIRACL, and JSQuAD, with readyâ€‘toâ€‘use integration examples for sentence_transformers and HuggingFace.
 * [japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - ğŸ“¥ 5k / â­ 16 / Japanese-language CrossEncoder reranker modelsâ€”from xsmall to largeâ€”trained on Japanese text, exposed through sentence_transformers, with evaluation on JQaRA, JaCWIR, MIRACL, and JSQuAD.

### token-classification
 * [bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - ğŸ“¥ 8k / â­ 11 / Japanese NER using clâ€‘tohoku/bertâ€‘baseâ€‘japaneseâ€‘v2 that extracts eight entity types (corporations, political/other organizations, facilities, products, events) viaâ€¯`BertForTokenClassification`, trained on the Stockmark Wikipedia dataset and installable with `transformers`, `unidic_lite`, and `fugashi` under a CCâ€¯BYâ€‘SAâ€¯3.0 license.
 * [xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - ğŸ“¥ 1k / â­ 25 / Fineâ€‘tuned XLMâ€‘RoBERTaâ€‘base on a Japanese NER corpus (tagsâ€¯PER,â€¯ORG,â€¯LOC,â€¯INS,â€¯PRD,â€¯EVT) using 5â€‘epoch Adam (lrâ€¯5eâ€‘5, batchâ€¯12) to reach a 0.0173 validation loss, released on Transformersâ€¯4.23.1 and PyTorchâ€¯1.12.1.

### text-to-speech
 * [Anime-Llasa-3B](https://huggingface.co/NandemoGHS/Anime-Llasa-3B) - ğŸ“¥ 3k / â­ 24 / Animeâ€‘Llasaâ€‘3B is a Japanese TTS model built on HKUSTAudio/Llasaâ€‘3B, enhanced with more training data to boost expressiveness and stability, and licensed CCâ€‘BYâ€‘NCâ€‘4.0.
 * [Anime-Llasa-3B-Captions](https://huggingface.co/NandemoGHS/Anime-Llasa-3B-Captions) - ğŸ“¥ 1k / â­ 13 / Animeâ€‘Llasaâ€‘3Bâ€‘Captions is a Japanese textâ€‘toâ€‘speech model built on Animeâ€‘Llasaâ€‘3B and fineâ€‘tuned with Geminiâ€¯2.5â€¯Proâ€‘generated audio metadata, enabling controllable speech synthesis via prompt tags and inâ€‘text markers, though it cannot always perfectly reflect the specified attributes.

### audio-to-audio
 * [Anime-XCodec2-44.1kHz-v2](https://huggingface.co/NandemoGHS/Anime-XCodec2-44.1kHz-v2) - ğŸ“¥ 10k / â­ 13 / Animeâ€‘XCodec2â€‘44.1kHzâ€‘v2 upsamples 16â€¯kHz Japanese speech to 44.1â€¯kHz highâ€‘fidelity audio with a decoderâ€‘only RMSâ€‘loss fineâ€‘tune, keeping the encoder/codebook frozen and preserving identical speech tokens.

### image-text-to-text
 * [PaddleOCR-VL-For-Manga](https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga) - ğŸ“¥ 3k / â­ 116 / Fineâ€‘tuned from PaddleOCRâ€‘VL, PaddleOCRâ€‘VLâ€‘Forâ€‘Manga achieves 70â€¯% fullâ€‘sentence accuracy on Manga109â€‘s speechâ€‘bubble cropsâ€”over triple the 27â€¯% baselineâ€”using a multiâ€‘language dataset and includes training code and a developer guide.

### others
 * [cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf) - ğŸ“¥ 282k / â­ 1 / A ggufâ€‘formatted version of cyberagentâ€™s openâ€‘calmâ€‘3b model on the mmngaâ€‘dev branch, ready for llama.cpp testing with usage examples and a note that it may not work once gptneox is integrated.
 * [cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf) - ğŸ“¥ 195k / â­ 2 / A temporary test branch offering aâ€¯ggufâ€‘formatted version of CyberAgentâ€™s openâ€‘calmâ€‘7b model forâ€¯llama.cpp, with instructions to clone the dev branch, build, and run the model (note other similar gguf releases exist).
 * [bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - ğŸ“¥ 108k / â­ 10 / Japaneseâ€‘language BERTâ€‘Base (12 layers, 768â€‘dim, 12 heads) pretrained with Unidicâ€‘based wordâ€‘level plus characterâ€‘level tokenization and wholeâ€‘word masking on CCâ€‘100 and 2023 Wikipedia, producing a 7,027â€‘token vocabulary.
 * [bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - ğŸ“¥ 71k / â­ 13 / Japaneseâ€‘BERTâ€‘Large trained on CCâ€‘100 and Wikipedia, using Unidicâ€‘lite wordâ€‘level tokenization with WordPiece subwords and wholeâ€‘word masking (24 layers, 1024â€‘dim hidden, 16 heads, 32k vocab), with pretraining code on clâ€‘tohoku/bertâ€‘japanese.
 * [stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - ğŸ“¥ 65k / â­ 1 / A testâ€‘branch conversion of stockmarkâ€™s gptâ€‘neoxâ€‘japaneseâ€‘1.4b to gguf format, intended for use with llama.cppâ€™sâ€¯mmngaâ€‘dev branch and shown with example inference commands and GPU support.
 * [bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - ğŸ“¥ 49k / â­ 57 / Japanese BERTâ€‘base (12 layers, 768â€‘dim hidden, 12 heads, 32â€¯k vocab) pretrained with wholeâ€‘word masking on CCâ€‘100 and 2023â€‘Jan Wikipedia, using Unidicâ€¯2.1.2 wordâ€‘level tokenization plus WordPiece, in 2â€¯M training steps.
 * [deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - ğŸ“¥ 15k / â­ 17 / Japanese DeBERTaâ€¯V3â€¯base preâ€‘trained on 540â€¯B tokens from LLMâ€‘jpâ€¯v1.0, trained with a modified DeBERTaâ€¯V3 setup, uses a unigram byteâ€‘fallback tokenizer (no morphological analyzer), and is fineâ€‘tuned for JGLUE NLU tasks.
 * [t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - ğŸ“¥ 15k / â­ 10 / A Japanese T5â€‘v1.1 model pretrained on â‰ˆ100â€¯GB of Wikipedia and OSCAR CCâ€‘100 data (mixed 10:1 for SentencePiece with byteâ€‘fallback), requiring fineâ€‘tuning for downstream tasks, includes transferâ€‘learning sample code, notes potential bias in outputs, and is licensed CCâ€‘BYâ€‘SAâ€¯4.0.
 * [shisa-v2.1-qwen3-8b-UD-japanese-imatrix](https://huggingface.co/dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix) - ğŸ“¥ 10k / â­ 1 / A GGUFâ€‘quantized shisaâ€‘v2.1â€‘qwen3â€‘8b model built with Unsloth Dynamicâ€¯2.0, communityâ€‘patched Qwen3 settings to reduce malfunctions, a larger imatrix for stronger Japanese performance, and a 40K maximum context length.
 * [Llama-3-ELYZA-JP-8B-Heretic-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-ELYZA-JP-8B-Heretic-i1-GGUF) - ğŸ“¥ 8k / â­ 1 / Repository provides a full set of weighted/imatrix GGUF quantizations for Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8Bâ€‘Heretic at varying quality and size levels (e.g., i1â€‘IQ1_S, i1â€‘IQ2_M, i1â€‘Q4_K_M), downloadable from Huggingâ€¯Face with usage guidance linked to TheBlokeâ€™s READMEs.
 * [shisa-v2.1-unphi4-14b-i1-GGUF](https://huggingface.co/mradermacher/shisa-v2.1-unphi4-14b-i1-GGUF) - ğŸ“¥ 6k / â­ 1 / Hosted here are weighted/imatrix and GGUF quantizations of Shisaâ€‘V2.1â€‘UNPhi4â€‘14B (static GGUF releases on HuggingÂ Face), listing download links, file sizes and quality notes, with usage guidance linked to TheBlokeâ€™s READMEs.
 * [Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF) - ğŸ“¥ 4k / â­ 69 / Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8B is a Japaneseâ€‘enhanced 8â€‘B Llamaâ€¯3 model with GGUF (Q4_K_M) and AWQ quantization, ready to run via llama.cpp, LMâ€¯Studio, or an OpenAIâ€‘compatible API.
 * [gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf) - ğŸ“¥ 4k / â­ 12 / Gemmaâ€‘2â€‘2bâ€‘jpnâ€‘itâ€‘translateâ€‘gguf is a 2â€‘billionâ€‘parameter, ~2â€¯GB small language model that delivers Japaneseâ€‘English translation quality comparable to 7â€‘billionâ€‘parameter models, performs best on sentenceâ€‘byâ€‘sentence input, and includes Colab and llama.cpp usage examples.
 * [Tema_Q-R3.1-i1-GGUF](https://huggingface.co/mradermacher/Tema_Q-R3.1-i1-GGUF) - ğŸ“¥ 4k / â­ 1 / Offers a comprehensive list of weighted/imatrix GGUF quant versions for the Tema_Qâ€‘R3.1 model, detailing sizes, quality notes, download links, usage guidanceâ€”including GGUF file handlingâ€”and links to the model page, readmes, and FAQ.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf) - ğŸ“¥ 3k / â­ 55 / Cyberagentâ€™s ggufâ€‘converted DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘14Bâ€‘Japanese model (built from the TFMC imatrix dataset) is available under mmnga and can be run with CUDA support using llama.cpp.
 * [haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - ğŸ“¥ 3k / â­ 4 / ggufâ€‘formatted conversion of Llamaâ€‘3â€‘8B Japanese Instruct built from the imatrix dataset, ready for inference with llama.cpp.
 * [Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5) - ğŸ“¥ 2k / â­ 8 / Llamaâ€¯3.1â€¯Swallowâ€¯v0.5 is an 8â€‘billionâ€‘parameter LLM that improves Metaâ€™s Llamaâ€¯3.1 on Japanese language and code/math reasoning while retaining English fluency, achieved through continual preâ€‘training and instructionâ€‘tuned fineâ€‘tuning on synthetic Japanese data.
 * [Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf) - ğŸ“¥ 2k / â­ 3 / ggufâ€‘formatted Qwen3â€‘EZOâ€‘8Bâ€‘beta by AXCXEPT, built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, and run via llama.cppâ€™s CUDAâ€‘enabled build.
 * [sarashina2.2-0.5b](https://huggingface.co/sbintuitions/sarashina2.2-0.5b) - ğŸ“¥ 2k / â­ 11 / Sarashina2.2 provides 0.5â€‘B, 1â€‘B, and 3â€‘B language models trained by SB Intuitions through a threeâ€‘phase pipeline and synthetic data, achieving top Japanese QA, math, and coding scores while offering preâ€‘trained weights that are not instructionâ€‘tuned and may produce biased outputs.
 * [Llama-3.1-Swallow-JP-EN-Translator-v1-8B-i1-GGUF](https://huggingface.co/mradermacher/Llama-3.1-Swallow-JP-EN-Translator-v1-8B-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Weighted/imatrix GGUF quantizations (2.1â€¯GBâ€“6.7â€¯GB) of Llamaâ€‘3.1â€‘Swallowâ€‘JPâ€‘ENâ€‘Translatorâ€¯v1â€‘8B are listed for download on Huggingâ€¯Face with usage guidance and a staticâ€“quant page for convenience.
 * [Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf) - ğŸ“¥ 2k / â­ 7 / A ggufâ€‘format conversion of Vecteusâ€‘v1 from Localâ€‘Novelâ€‘LLM, built using the imatrix dataset, that can be run with llama.cpp via `Vecteusâ€‘v1â€‘Q4_0.gguf` and lists other related models.
 * [llm-jp-3.1-1.8b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-1.8b-instruct4-gguf) - ğŸ“¥ 2k / â­ 1 / GGUFâ€‘formatted llmâ€‘jpâ€‘3.1â€‘1.8bâ€‘instruct4, trained on the TFMC/imatrix dataset, with CUDAâ€‘enabled llama.cpp build and run instructions.
 * [gemma-3-JP-EN-Translator-v1-4B-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-JP-EN-Translator-v1-4B-i1-GGUF) - ğŸ“¥ 2k / â­ 1 / Weighted/imatrix quantizations of the Gemmaâ€‘3 JPâ€‘EN translator (v1â€‘4B) are provided in multiple GGUF and static formats with Hugging Face links, fileâ€‘size/quality notes, a qualityâ€‘vsâ€‘size comparison graph, and usage guidance for a visionâ€‘modelâ€‘compatible mmproj file.
 * [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) - ğŸ“¥ 2k / â­ 16 / Highâ€‘performance Japanese SPLADEâ€¯v2 enables sparseâ€‘vector conversion and inference through a WebUI demo, trains with YAST, offers YASEM embedding, and reports JMTEB benchmark results.
 * [Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf) - ğŸ“¥ 2k / â­ 5 / A ggufâ€‘formatted Ninjaâ€‘v1â€‘NSFW model for Japanese LLM, built from the imatrix dataset and usable with llama.cpp (clone, compile, run with the provided prompt).
 * [c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - ğŸ“¥ 2k / â­ 4 / GGUFâ€‘formatted versions of CohereForAIâ€™s c4aiâ€‘commandâ€‘râ€‘plus, built with Japanese LLM data from TFMC/imatrix, plus instructions for concatenating split files and running the model via llama.cpp for Japanese dialogue.
 * [r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf) - ğŸ“¥ 2k / â­ 2 / A GGUFâ€‘format conversion of perplexityâ€‘AIâ€™s r1â€‘1776â€‘distillâ€‘llamaâ€‘70b, built with imatrix data fromâ€¯TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, ready for CUDAâ€‘enabled use with llama.cpp.
 * [plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf) - ğŸ“¥ 2k / â­ 18 / A GGUFâ€‘format release of pfnetâ€™s plamoâ€‘2â€‘translate built from imatrix data based on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with instructions to compile and run it via llama.cpp on CUDAâ€‘enabled hardware.
 * [Llama-3.1-Swallow-8B-Instruct-v0.5-gguf](https://huggingface.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf) - ğŸ“¥ 2k / â­ 2 / GGUF conversion of Llamaâ€‘3.1â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.5 by tokyotechâ€‘llm, incorporating TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with Build/Run instructions forâ€¯llama.cpp.
 * [rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf) - ğŸ“¥ 1k / â­ 6 / GGUFâ€‘formatted conversion of rinnaâ€™s llamaâ€‘3â€‘youkoâ€‘8b, trained with the imatrix dataset, including usage instructions and links to related models.
 * [Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf) - ğŸ“¥ 1k / â­ 11 / A repository providing a GGUFâ€‘format conversion of the Ninjaâ€‘v1â€‘NSFWâ€‘128k model built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, with usage instructions for running it in llama.cpp to generate Japanese novel text.
 * [lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - ğŸ“¥ 1k / â­ 2 / Japanese 8â€‘B LLaMAâ€¯3 converted to GGUF format by lightblue, built with theâ€¯TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm.
 * [umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf) - ğŸ“¥ 1k / â­ 8 / A ggufâ€‘formatted version of Umievoâ€‘itr012â€‘Gleipnirâ€‘7B (trained on the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm) ready to run with llama.cpp.
 * [Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf) - ğŸ“¥ 1k / â­ 9 / A gguf-format release of moonshotaiâ€™s Moonlightâ€‘16Bâ€‘A3Bâ€‘Instruct, trained on TFMCâ€™s imatrix Japanese dataset, ready for use with llama.cpp (CUDAâ€‘enabled) and shown by running a recipeâ€‘request prompt.
 * [cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf) - ğŸ“¥ 1k / â­ 39 / GGUFâ€‘formatted DeepSeekâ€‘R1â€‘Distillâ€‘Qwen Japanese 32B model from cyberagent, built with the imatrix dataset and ready to run with llama.cpp.
 * [aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf) - ğŸ“¥ 1k / â­ 1 / A gguf-format conversion of CohereForAIâ€™s ayaâ€‘23â€‘35B model, built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, runable via llama.cpp with `./main -m 'aya-23-35B-Q4_0.gguf'`.
 * [Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf) - ğŸ“¥ 1k / â­ 7 / A ggufâ€‘formatted version of cyberagentâ€™s Llamaâ€‘3.1â€‘70Bâ€‘Japaneseâ€‘Instructâ€‘2407, built using TFMC/imatrixâ€‘dataset-forâ€‘japaneseâ€‘llm data and run with llama.cppâ€™s CLI.
 * [Llama-3-ELYZA-JP-8B-Heretic-GGUF](https://huggingface.co/mradermacher/Llama-3-ELYZA-JP-8B-Heretic-GGUF) - ğŸ“¥ 1k / â­ 1 / Provides a selection of GGUFâ€‘quantized static models and weighted/imatrix variants for Llamaâ€‘3â€‘ELYZAâ€‘JPâ€‘8Bâ€‘Heretic, ranging from Q2_K (3.3â€¯GB) to Q8_0 (8.6â€¯GB) with recommended fast options, along with usage guidance and links for model requests.
 * [karakuri-lm-8x7b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf) - ğŸ“¥ 1k / â­ 4 / A ggufâ€‘formatted release of karakuriâ€‘lmâ€‘8x7bâ€‘chatâ€‘v0.1, trained on the TFMC/imatrix Japanese LLM dataset and runnable with llama.cpp using a Q4_0 quantized model.
 * [Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf) - ğŸ“¥ 1k / â­ 2 / Provides a ggufâ€‘format conversion of the Ninjaâ€‘v1â€‘128k model from the Localâ€‘Novelâ€‘LLMâ€‘project, built using TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm data, and includes llama.cpp usage instructions.
 * [lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf) - ğŸ“¥ 1k / â­ 6 / Repository hosts a gguf-converted version of lightblueâ€™s DeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘7Bâ€‘Japanese model, built from the TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘llm, and ready for inference with llama.cpp.
 * [rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf) - ğŸ“¥ 1k / â­ 1 / A ggufâ€‘formatted copy of rinnaâ€™s Llamaâ€‘3â€‘Youkoâ€‘70Bâ€‘Instruct model built from imatrix data, ready for inference with llama.cpp.
 * [QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf) - ğŸ“¥ 1k / â­ 3 / A ggufâ€‘formatted conversion of Qwenâ€™s QwQâ€‘32Bâ€‘Preview model built with imatrix data from TFMC/imatrixâ€‘datasetâ€‘forâ€‘japaneseâ€‘LLM, including instructions to run it via llama.cpp.
 * [Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf) - ğŸ“¥ 1k / â­ 3 / GGUFâ€‘converted Llamaâ€‘3â€‘Swallowâ€‘8Bâ€‘Instructâ€‘v0.1 from tokyotechâ€‘llm, built with the TFMC/imatrix Japanese LLM dataset, is ready to run via llama.cppâ€™s inference tool.

## Datasets
 * [KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives) - ğŸ“¥ 408k / â­ 17 / Aggregates overâ€¯150â€¯GB of NicoNico Liveâ€™s comment logs from 2009â€‘2024â€”including preâ€‘transition, postâ€‘transition, and realâ€‘time NXâ€‘Jikkyo capturesâ€”providing an API for easy retrieval of historical TVâ€‘broadcast discussions.
 * [emb](https://huggingface.co/datasets/hpprc/emb) - ğŸ“¥ 8k / â­ 13 / A catalog of Japanese and multilingual QA, NLI and paraphrase datasets, detailing each datasetâ€™s retrieval or QA tasks and its license (Apacheâ€¯2.0, CCâ€‘BYâ€‘SA/CCâ€‘BY, MIT, etc.).
 * [JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench) - ğŸ“¥ 7k / â­ 8 / JMedBench is a Japanese biomedical LLM benchmark comprising 20 datasets across five tasks (MCQA, NER, STS, etc.) sourced from MedMCQA, PubMedQA, MMLU, and others, each with its own license, and includes a note that translations may contain biases requiring human review.
 * [JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - ğŸ“¥ 6k / â­ 18 / JMTEB is a Japanese textâ€‘embedding benchmark featuring 5 tasks (clustering, classification, STS, retrieval, reranking) and 28 datasets, offering a oneâ€‘line evaluation script and inviting community contributions.
 * [reazon-speech-v2-clone](https://huggingface.co/datasets/litagin/reazon-speech-v2-clone) - ğŸ“¥ 6k / â­ 10 / Mirror of the Reazon Speechâ€¯v2 dataset on ğŸ¤—, licensedâ€¯CDLAâ€‘Sharingâ€‘1.0 and restricted to Japanese Copyright Actâ€¯Articleâ€¯30â€‘4, with 16â€¯kHz FLAC audio and accompanying metadata.
 * [Galgame-VisualNovel-Reupload](https://huggingface.co/datasets/joujiboi/Galgame-VisualNovel-Reupload) - ğŸ“¥ 5k / â­ 27 / Restructured reupload of the Galgame VisualNovel datasetâ€¯(OOPPEENN/56697375616C4E6F76656C5F44617461736574) for efficient Huggingâ€¯Faceâ€¯datasets loading, preserving all original audio/text and providing an extraction script with multiple gameâ€‘subset options.
 * [fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese) - ğŸ“¥ 5k / â­ 23 / FineWeb2 Edu Japanese delivers ~120â€¯million highâ€‘quality educational Japanese texts (â‰ˆ89.3â€¯billion tokens) from FineWeb2, filtered by a DeepSeekâ€‘API classifier (scoreâ€¯â‰¥â€¯2.5), tokenized via ModernBERTâ€‘Jaâ€‘130M, and includes a smallâ€‘token subset (â‰¤512 tokens).
 * [Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA) - ğŸ“¥ 4k / â­ 8 / Cauldronâ€‘JA is a Japanese visionâ€‘language dataset of 44 subâ€‘datasets translated from The Cauldron using the DeepL API, available via HuggingFaceâ€™s datasets library and licensed identically to the original set, with prompts released under CCâ€‘BYâ€‘4.0.
 * [wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings) - ğŸ“¥ 3k / â­ 3 / Japanese Wikipedia sentences are transformed into various embeddings and a FAISS index, offering a Hugging Face Space demo, conversion scripts, and evaluations of search, Q&A, and OpenAIâ€¯textâ€‘embeddingâ€‘3â€‘small for RAG; embeddings are OpenAIâ€‘licensed, others CCâ€‘BYâ€‘SAâ€‘4.0.
 * [ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ğŸ“¥ 3k / â­ 99 / A 100â€‘sample Japanese instructionâ€‘tuning evaluation dataset of annotated tasksâ€”ranging from summarization correction and math reasoning to translation, creative generation, and userâ€‘intent understandingâ€”designed for manual or automatic 5â€‘point rating of fineâ€‘tuned models.
 * [Nemotron-Personas-Japan](https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan) - ğŸ“¥ 3k / â­ 95 / Nemotronâ€‘Personasâ€‘Japan is an openâ€‘source, CCâ€¯BYâ€¯4.0 dataset of highâ€‘quality, synthetically generated Japanese personasâ€”incorporating name, gender, age, background, marital status, education, occupation and locationâ€”grounded in realâ€‘world demographic, geographic and personality distributions, engineered with probabilistic graphical models and GPTâ€‘OSSâ€‘120B to enhance diversity, reduce bias, prevent model collapse, assist sovereign AI development, and support commercial use.
 * [voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus) - ğŸ“¥ 2k / â­ 6 / Artificial voice dataset created with VOICEVOX from the ITA, Tsukuyomiâ€‘chan, and ROHAN corpora, containing 445,793 WAV files totaling 577â€¯hâ€¯51â€¯mâ€¯23â€¯s.
 * [JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - ğŸ“¥ 2k / â­ 45 / Updated JGLUE dataset card and loading script for a Japanese NLP benchmark (created by Yahoo Japan and Waseda University) that covers text classification (MARCâ€‘ja, JCoLA), sentenceâ€‘pair classification (JNLI), and QA (JSQuAD, JCommonsenseQA), with releases linked on GitHub and Hugging Face.
 * [MissingKeys](https://huggingface.co/datasets/RyokoExtra/MissingKeys) - ğŸ“¥ 2k / â­ 2 / MissingKeys is a raw Japaneseâ€‘dominant dataset from the misskey.io network, stored in dateâ€‘compressed JSONL files (â‰ˆ100,000 notes each inside .7z archives) and intended primarily for unsupervised textâ€‘generation training.
 * [japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2) - ğŸ“¥ 2k / â­ 127 / Japanese Anime Speech Datasetâ€¯V2 delivers 292,637 cleaned audioâ€‘text pairsâ€”about 397.5â€¯h of SFW and 52.4â€¯h of NSFW contentâ€”in 128â€‘kbps MP3 files split by safety, designed specifically for training automatic speechâ€‘recognition models.
 * [emilia-yodas](https://huggingface.co/datasets/TTS-AGI/emilia-yodas) - ğŸ“¥ 2k / â­ 4 / Dataset of dialogues and lore from the Fate/Stayâ€¯Night character â€œEmiliaâ€, formatted for training and evaluating conversational language models.
 * [reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores) - ğŸ“¥ 2k / â­ 4 / Provides a Japanese search/QA dataset with perâ€‘query scores computed by five multilingual/Japanese rerankers (e.g., BAAI/bgeâ€‘rerankerâ€‘v2â€‘m3, Alibabaâ€‘NLP/gteâ€‘multilingualâ€‘rerankerâ€‘base), including average scores for roughly 200 positive and negative example documents per query.
 * [AnimuSubtitle-JP](https://huggingface.co/datasets/KaraKaraWitch/AnimuSubtitle-JP) - ğŸ“¥ 1k / â­ 3 / AnimuSubtitleâ€‘JP hosts Japanese ASS/SSA subtitle datasets (data_ass, data_TS) that can be parsed with Pythonâ€™sâ€¯ass library or edited in Aegisub, and is released under an ODCâ€‘By license.
 * [ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA) - ğŸ“¥ 1k / â­ 2 / Huggingâ€¯Face mirror of the ABEJAâ€¯CCâ€‘JA dataset from AWS Openâ€¯Data, with details posted on ABEJAâ€™s tech blog.
 * [japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - ğŸ“¥ 1k / â­ 137 / Japanese Anime Speech Dataset offers 73,004 audioâ€‘text pairs (110â€¯hours total, evolving from V1 to V5) to enhance ASR models such as OpenAIâ€™s Whisper, available under an open license for all uses with credit appreciated.
 * [EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject) - ğŸ“¥ 1k / â­ 12 / Elite Voice Project compiles Hololive VTuber Sakuraâ€¯Mikoâ€™s audio from Twitch, Twitter and YouTube into a train/testâ€‘organized dataset for speechâ€‘recognition research, using Gitâ€‘LFS, licensed under Hololiveâ€™s fanâ€‘content rules and welcoming community contributions.
 * [Japanese-Eroge-Voice](https://huggingface.co/datasets/NandemoGHS/Japanese-Eroge-Voice) - ğŸ“¥ 1k / â­ 31 / A 409â€‘hour Japanese eroge voice dataset processed with 2â€‘pass loudnorm (â€‘23â€¯LUFS, â€‘1â€¯dB peak, 11â€¯LRA), transcribed by litagin/anime-whisper, anonymized, stored as WebDataset (FLAC, JSON, TXT), largely featuring female voices with potential AI transcription errors, and MITâ€‘licensed for academic research.
 * [ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete) - ğŸ“¥ 1k / â­ 3 / Dataset of Japanese Boketeâ€‘site humor posts (from CLoTâ€‘Oogiriâ€‘Goâ€¯CVPRâ€¯2024) featuring three tasksâ€”textâ€‘toâ€‘text, imageâ€‘toâ€‘text, and textâ€‘imageâ€‘toâ€‘textâ€”with roughly 600â€¯examples, processed via GPTâ€‘4o OCR and HojiChar filtering.
 * [MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI) - ğŸ“¥ 1k / â­ 21 / A Japanese web collection of 56â€¯million documents, 110â€¯B characters and 249â€¯million images used to train large visionâ€‘language modelsâ€”offering a momiji_generator for data population, OBELICSâ€‘style visualization, and a sample model (Heronâ€‘NVILAâ€‘Lite).
 * [Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene) - ğŸ“¥ 1k / â­ 16 / Partial voiceâ€‘recording and label dataset for è‰è–™å¯§ã€… (Projectâ€¯Sekai), open for completion and community contribution.
 * [vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard) - ğŸ“¥ 1k / â­ 39 / The VNTL leaderboard evaluates large language models on translating Japanese visual novels into English by averaging cosineâ€‘similarity scores over 256 samples, ranking preliminary results and benchmarking against tools such as Sugoi Translator, Google Translate, and Naver Papago.
 * [llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - ğŸ“¥ 1k / â­ 140 / Japanese instructionâ€‘chat dataset for fineâ€‘tuning LLMs (e.g., with LoRA), 9â€¯M+ samples, recently updated to drop licensed Alpaca data, clean Wikipedia and ALT outputs, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [oscar_2023_filtered](https://huggingface.co/datasets/if001/oscar_2023_filtered) - ğŸ“¥ 1k / â­ 3 / A 312,396â€‘row filtered subset of the OSCARâ€‘2301 dataset (Hugging Faceâ€¯`if001/oscar_2023_filtered`), with implementation details available at theâ€¯HojiChar_OSCAR_sample GitHub repository.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - ğŸ“¥ 1k / â­ 87 / An automatically translated Japanese version of the databricksâ€‘dollyâ€‘15k dataset, licensed CCâ€‘BYâ€‘SAâ€‘3.0 and last updated on 2023â€‘05â€‘11.
 * [sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus) - ğŸ“¥ 992 / â­ 5 / An 81â€‘yearâ€‘old Japanese womanâ€™s â€œFusicâ€¯Saâ€‘yoâ€‘jiâ€ voice corpus, downloadable as a zip from GoogleÂ Drive, offers raw noisy and cleanedâ€¯.wav files together with phoneme and Kana labels and prosodic symbols, is free for nonâ€‘explicit commercial use with credit, prohibits direct audio links, and mandates redistributing the README with any reâ€‘distribution.
 * [mc4-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/mc4-ja-filter-ja-normal) - ğŸ“¥ 910 / â­ 5 / Dataset card for the â€œmc4â€‘jaâ€‘filterâ€‘jaâ€‘normalâ€ dataset, with additional information pending.
 * [JamC-QA](https://huggingface.co/datasets/sbintuitions/JamC-QA) - ğŸ“¥ 909 / â­ 3 / JamCâ€‘QA is a bilingual benchmark of multipleâ€‘choice questions spanning eight Japaneseâ€‘culture and knowledge categories, with leaderboard metrics comparing stateâ€‘ofâ€‘theâ€‘art models.
 * [JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA) - ğŸ“¥ 891 / â­ 2 / JCommonsenseQA is a Japanese multipleâ€‘choice dataset adapted from CommonsenseQA, offering 5 answer options per question, labeled indices for the correct choice, and released under a Creative Commonsâ€¯BYâ€‘SAâ€¯4.0 license.
 * [rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - ğŸ“¥ 886 / â­ 8 / Rakuda supplies 40 Japanese questionsâ€”openâ€‘ended for history, society, and government, and specific for geographyâ€”for benchmarking Japanese AI assistants, comparable to vicunaâ€‘eval, and can be loaded with `datasets.load_dataset`.
 * [wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - ğŸ“¥ 884 / â­ 4 / Reformatted Japanese subset of the Wiki40B dataset compiled by Mandy Guo, Zihang Dai, and Denny VrandeÄiÄ‡.
 * [Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus) - ğŸ“¥ 824 / â­ 4 / Lux Japanese Speech Corpus: a 96â€¯kHz 16â€‘bit WAV dataset of Japanese TTS recordings by characterÂ Lux, including raw and cleaned audio, transcripts inÂ metadata.csv, dataset metadata inÂ dataset_infos.json, and released under CCâ€¯BYâ€¯4.0.
 * [mc4-ja](https://huggingface.co/datasets/izumi-lab/mc4-ja) - ğŸ“¥ 817 / â­ 6 / Dataset card for the Japanese MC4 dataset (mc4-ja).
 * [Galgame_Speech_ASR_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_ASR_16kHz) - ğŸ“¥ 764 / â­ 37 / Galgame_Speech_ASR_16kHz is a 16â€¯kHz ASR dataset with 3.75â€¯million pairs (â‰ˆ5,354â€¯h), derived from Galgame_Dataset, released under GPLâ€¯v3.0 with commercial use prohibited and requiring any trained models to be openâ€‘source (citation optional).
 * [aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - ğŸ“¥ 761 / â­ 36 / A userâ€‘friendly, deduplicated CSV dataset of publicâ€‘domain Japanese texts from Aozoraâ€¯Bunko, processed with globisâ€‘org/aozorabunkoâ€‘extractor and cleaned for modernâ€‘Japanese machineâ€‘learning use.
 * [llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - ğŸ“¥ 725 / â­ 32 / Japanese chatbot dataset stripped of English translation data from izumiâ€‘lab/llmâ€‘japaneseâ€‘dataset, offering 2.5â€¯million+ entries (v1.0.0) for fineâ€‘tuning Japanese LLMs on instructionâ€‘response tasks under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU) - ğŸ“¥ 720 / â­ 19 / JMMMU is a Japanese multimodal benchmark expanded over tenfold to 1,320 culturally diverse questions (720 cultureâ€‘agnostic, 600 cultureâ€‘specific) translated by native subject experts, now featuring a public leaderboard.
 * [KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10) - ğŸ“¥ 719 / â­ 5 / KokushiMDâ€‘10 delivers a multimodal benchmark of Japanese national healthcare licensing exam questionsâ€”spanning ten professions, offered in Japanese, English, and mixed splitsâ€”with expert chainâ€‘ofâ€‘thought annotations for LLM evaluation.
 * [japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - ğŸ“¥ 672 / â­ 3 / Japanese Web Corpus 2010 data, automatically punctuated with morphology analysis, uploaded to Hugging Face for research use only under the 2009 copyright amendment, and including conversion scripts.
 * [kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en) - ğŸ“¥ 568 / â­ 9 / A Japaneseâ€‘toâ€‘English parallel corpus translating the kaken subset of llmâ€‘jpâ€‘corpusâ€‘v3 with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, featuring custom translation columns and licensed under CCâ€‘BYâ€‘4.0.
 * [Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M) - ğŸ“¥ 568 / â­ 6 / Dataset of 23,212,809 Japanese web novels (~80.8â€¯billion characters) collected personally, for machineâ€‘learning use only and requiring a detailed access request.
 * [2ch.sc](https://huggingface.co/datasets/DSULT-Core/2ch.sc) - ğŸ“¥ 546 / â­ 2 / Large compressed JSONâ€‘Lines dataset of anonymous 2ch.sc/2ch.net threads, including thread IDs, titles, board and region details, reply counts, and full post metadata (author, mail, date, content).
 * [JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - ğŸ“¥ 536 / â­ 11 / JaQuAD is a 2022 Japanese QA dataset of 39,696 SQuADâ€‘style extractive pairs from Wikipedia, totaling 73.2â€¯MB, that achieves 78.92â€¯% F1 (63.38â€¯% EM) when fineâ€‘tuned with BERTâ€‘Japanese.
 * [cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - ğŸ“¥ 527 / â­ 21 / cc100-ja is a collection of the Japanese portion of the cc100 dataset, provided as sharded Parquet files.
 * [reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) - ğŸ“¥ 505 / â­ 106 / ReazonSpeech is a free, FLACâ€‘encoded Japanese speech corpus with transcriptions, offered in five sizes from 8.5â€¯h to 35,000â€¯h, downloadable via Huggingâ€¯Face under the CDLAâ€‘Sharingâ€‘1.0 license and limited to use under Japan Copyright Act Articleâ€¯30â€‘4.
 * [STAIR-Captions](https://huggingface.co/datasets/shunk031/STAIR-Captions) - ğŸ“¥ 494 / â­ 5 / STAIRâ€‘Captions is a largeâ€‘scale (820,310) Japanese caption dataset for tasks such as caption generation, multimodal retrieval, and image generation, released under CCâ€¯BYâ€¯4.0.
 * [xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - ğŸ“¥ 492 / â­ 6 / Japanese XLâ€‘Sum subset filtered via PaLMâ€‘2 15â€‘gram overlap, containing 4,215 training, 758 validation, and 766 test examples.
 * [paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa) - ğŸ“¥ 486 / â­ 3 / Dataset of LLMâ€‘generated queries and answers from paraphrases of Japanese Wikipedia text, built without using licenseâ€‘restricted models and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [Hachi-Alpaca](https://huggingface.co/datasets/HachiML/Hachi-Alpaca) - ğŸ“¥ 461 / â­ 15 / Hachi-Alpaca delivers Japanese synthetic data derived from Stanford Alpaca, refined and verified by mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, used through Deepinfra, with â€œ_cleanedâ€ versions that have passed modelâ€‘based quality checks.
 * [JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA) - ğŸ“¥ 458 / â­ 3 / Japanese Explainable Multiâ€‘hop Question Answering dataset featuring questions, answers, and stepâ€‘byâ€‘step derivations linking Wikipedia articles, with updated derivation formatting and multiple version releases.
 * [anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0) - ğŸ“¥ 453 / â­ 21 / AIâ€‘generated anime illustrations with English prompts and Phiâ€‘3 Visionâ€‘derived captions (English and Japanese) released into the public domain for free use.
 * [qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) - ğŸ“¥ 435 / â­ 5 / Japanese JaQuAD, a subset of QGâ€‘Bench, provides sentenceâ€‘ and paragraphâ€‘level data with highlighted answer tokens for training Japanese questionâ€‘generation models, evaluated by BLEU4, METEOR, ROUGEâ€‘L, BERTScore, and MoverScore.
 * [callhome-ja-plus](https://huggingface.co/datasets/ayousanz/callhome-ja-plus) - ğŸ“¥ 424 / â­ 2 / Japanese Callhome speech files converted to WAV, accompanied by JSONâ€‘formatted metadata arrays and RTMM speaker label files for evaluation.
 * [llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval) - ğŸ“¥ 421 / â­ 3 / Dataset card for the jaâ€‘vicunaâ€‘qaâ€‘benchmark used in the book â€œIntroduction to Largeâ€‘Scale LLMÂ IIâ€ and created by llmâ€‘jpâ€‘eval for crossâ€‘dataset Japanese LLM evaluation (ApacheÂ 2.0).
 * [sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese) - ğŸ“¥ 411 / â­ 5 / Converted Japanese datasets into SentenceTransformersâ€‘friendly columns, filtering examples by Rerank scores (â‰¥0.7 positive, â‰¤0.3 negative) from multiple HuggingFace sources to support contrastive learning while respecting the original licenses.
 * [JMMMU-Pro](https://huggingface.co/datasets/JMMMU/JMMMU-Pro) - ğŸ“¥ 391 / â­ 7 / JMMMUâ€‘Pro is a lowâ€‘cost, imageâ€‘based Japanese multimodal benchmark built by generating visual questions with Nanoâ€¯Bananaâ€¯Pro and human verification, showing that current openâ€‘source LMMs struggle and guiding future Japanese VQA research.
 * [oscar2301-ja-filter-ja-normal](https://huggingface.co/datasets/izumi-lab/oscar2301-ja-filter-ja-normal) - ğŸ“¥ 388 / â­ 6 / Dataset card for the Japaneseâ€‘filtered OSCARâ€¯2301 subset, â€œoscar2301â€‘jaâ€‘filterâ€‘jaâ€‘normal.â€
 * [wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - ğŸ“¥ 378 / â­ 9 / A dataset card for llmâ€‘book/wrimeâ€‘sentiment offering a binary Japanese sentiment analysis set derived from WRIME, labeled as positive or negative based on Avg. Readers_Sentiment (with an option to include neutral cases), and intended as sample data for the book â€œIntroduction to Large Language Models.â€
 * [JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - ğŸ“¥ 362 / â­ 5 / JAQKET is a Japanese openâ€‘domain QA dataset derived from Wikipedia, offering versionâ€¯1.0 with multipleâ€‘choice quiz questions (13,061 training, 271 validation examples) and versionâ€¯2.0 with only question prompts requiring extracted answers (2,154 training, 1,164 validation), designed to facilitate research on QA systems.
 * [AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully) - ğŸ“¥ 359 / â­ 47 / AnswerCarefully Dataset supplies Japanese and multilingual data for commercial or nonâ€‘commercial LLM safety enhancement, prohibits any other useâ€”including safety circumventionâ€”allows derivative works with attribution, and carries a creator disclaimer of nonâ€‘liability for harms or service changes.
 * [jsick](https://huggingface.co/datasets/hpprc/jsick) - ğŸ“¥ 345 / â­ 8 / JSICK is a Japanese NLI/STS dataset translated from SICK, offering a stress test that probes wordâ€‘order and caseâ€‘particle handling through multiple transformed sentenceâ€‘pair subsets to support research in multilingual compositional inference.
 * [RyokoAI_Syosetu711K](https://huggingface.co/datasets/botp/RyokoAI_Syosetu711K) - ğŸ“¥ 344 / â­ 28 / Syosetu711K is a Japanese dataset of ~711,700 novels scraped from å°èª¬å®¶ã«ãªã‚ã† on Marchâ€¯26â€‘27â€¯2023, providing full text and metadata (title, author, NCode, synopsis, etc.) for unsupervised text generation and classification tasks.
 * [RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA) - ğŸ“¥ 344 / â­ 33 / Allganize RAG Leaderboard publishes Japanese RAG performance data and automated endâ€‘toâ€‘end evaluation results across five industry domainsâ€”finance, telecom, manufacturing, public sector, and retailâ€”to help companies benchmark parser, retrieval and generation components where no comprehensive Japanese benchmark yet exists.
 * [CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - ğŸ“¥ 329 / â­ 21 / Manually curated highâ€‘quality 100â€‘sample Japanese Chainâ€‘ofâ€‘Thought dataset, available as two JSONs: one linking CoT to output and one keeping them separate.
 * [JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - ğŸ“¥ 329 / â­ 19 / A Japanese QA dataset for evaluating Retrievalâ€‘Augmented Generation (RAG), built from JAQKET questions and Wikipedia passages with gold retrievalâ€‘relevance labels, released on HuggingFace and GitHub and scored primarily by nDCG@10.
 * [auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - ğŸ“¥ 329 / â­ 24 / AutoWikiQA is Japanâ€™s largest free QA dataset (2â€¯,377â€¯,503 pairs) produced from Wikipedia text using Swallowâ€‘MX and vLLM, delivering diverse, templateâ€‘free questions and answers for knowledge injection and retrievalâ€‘augmented generation.
 * [JGLUE](https://huggingface.co/datasets/llm-book/JGLUE) - ğŸ“¥ 324 / â­ 14 / Dataset card for the JGLUE dataset used in the book â€œLarge Language Model Introduction,â€ sourced from the original repo, with code licensed CCâ€¯BYâ€‘SAâ€¯4.0, data under the distributorâ€™s license, citing Kurihara & Kawahara (in Japanese), and built on Shunsuke Kitadaâ€™s repository.
 * [wikipedia-ja-20230720](https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720) - ğŸ“¥ 322 / â­ 13 / Dataset card for the 2023â€‘07â€‘20 release of the Japanese Wikipedia dataset.
 * [JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR) - ğŸ“¥ 321 / â­ 4 / JFWIR is a 64â€‘millionâ€‘pair Japanese IR dataset built from finewebâ€‘2â€‘edu web content, offering seven query types and hard negatives that raise benchmark scores on JQaRA, MIRACL(ja), jsquad and JaCWIR.
 * [oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - ğŸ“¥ 320 / â­ 26 / Japanese-translated OpenAssistant/oasst1 data with failure flags, ~2,000 manually corrected code translation errors, a released chatâ€‘format subset (oasst1â€‘chatâ€‘44kâ€‘ja), and a script to convert entries into instructionâ€“output pairs for fineâ€‘tuning.
 * [mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - ğŸ“¥ 313 / â­ 6 / Deduplicated, NFKCâ€‘normalized mQA queryâ€“passage pairs, with pos_ids/neg_ids mapping to collection indices for direct retrieval via collection[pos_id], and licensed under the original datasetâ€™s terms.
 * [jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - ğŸ“¥ 302 / â­ 7 / JHumanEval is a handâ€‘translated Japanese version of the HumanEval benchmark, providing 164 Python programming problems with parallel English and Japanese comments to evaluate Japaneseâ€‘LLM code generation while preserving the original English errors.
 * [defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter) - ğŸ“¥ 294 / â­ 2 / A 5,000â€‘tweet Japanese Twitter defamation detection dataset, annotated for target (A1â€“A3) and content (B1â€“B4) by three crowdworkers and collected Febâ€‘Junâ€¯2022, requiring API access to retrieve the original tweets.
 * [bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja) - ğŸ“¥ 294 / â­ 3 / BBHâ€‘ja provides a Japanese translation of the BIGâ€‘Bench Hard dataset, offering evaluation problems in JSONâ€‘L (input, correct target) and Chainâ€‘ofâ€‘Thought prompts in YAML (input, target), translated using the PLaMo model.
 * [Galgame_Speech_SER_16kHz](https://huggingface.co/datasets/litagin/Galgame_Speech_SER_16kHz) - ğŸ“¥ 289 / â­ 12 / A 104â€¯GB dataset of 3,746,131â€¯Galgame audio files (5,353â€¯h), adding LLMâ€‘generated emotion labels (possibly inaccurate) to the existing 16â€¯kHz ASR set, released under GPLâ€¯v3.0 with no commercial use allowed and requiring any trained models to be openâ€‘source.
 * [japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - ğŸ“¥ 283 / â­ 16 / Dataset card for **japanese_alpaca_data**, built on masa3141â€™s Japaneseâ€‘Alpacaâ€‘LoRA work, with additional details in the referenced repository.
 * [pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset) - ğŸ“¥ 282 / â­ 10 / The sovitsâ€‘emuâ€‘dataset offers 2,735 SEGAâ€‘licensed Emu Otori WAV files for soâ€‘vitsâ€‘svcâ€¯4.0 research, released under CCâ€‘BYâ€‘NCâ€¯4.0 (excluding the voice owners), with mandatory attribution, nonâ€‘commercial use, emailâ€‘only access, and optional pullâ€‘request contributions.
 * [JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - ğŸ“¥ 275 / â­ 10 / JMMLU is a Japanese Massive Multitask Language Understanding Benchmark featuring 7,536 teacherâ€‘crafted questions across 56 subjects, including professional medicine, psychology, accounting, philosophy, and diverse highâ€‘school disciplines.
 * [OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - ğŸ“¥ 262 / â­ 14 / 1.8â€¯million Japaneseâ€‘translated OpenMathInstructâ€‘1 instructionâ€‘tuning examples, generated from GSM8K and MATH benchmark questions with Mixtralâ€‘8x7Bâ€‘derived synthetic solutions verified against the original answers, are released for commercial use under an NVIDIA license that requires license inheritance for redistribution, though modelâ€‘learning licenses need not inherit that license.
 * [wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - ğŸ“¥ 260 / â­ 4 / Range3â€™sâ€¯wikipediaâ€‘jaâ€‘20230101 repository offers Parquet files containing only Japanese Wikipedia text, extracted from the full Wikipedia dataset and generated with Python code.
 * [janli](https://huggingface.co/datasets/hpprc/janli) - ğŸ“¥ 258 / â­ 6 / JaNLI is a Japanese Adversarial NLI dataset modeled on HANS, comprising 13,680 training and 720 test sentence pairs annotated with entailment labels, structural heuristics (e.g., subsequence, constituent), nounâ€‘phrase counts, and semantic tags to probe Japanese linguistic phenomena and model vulnerabilities.
 * [guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja) - ğŸ“¥ 257 / â­ 5 / Japanese subset of the Guanaco dataset, with references to similar datasets such asâ€¯inuâ€‘ai/alpacaâ€‘guanacoâ€‘japaneseâ€‘gptâ€‘1b.
 * [japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - ğŸ“¥ 256 / â­ 12 / A variant of the kunishou/hhâ€‘rlhfâ€‘49kâ€‘ja dataset that omits entries where ng_translation equalsâ€¯1.
 * [oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - ğŸ“¥ 253 / â­ 13 / A 68kâ€‘record Japaneseâ€‘chat version of OpenAssistant/oasst2, DeepLâ€‘translated, is released together with conversion code that turns it into an Instructionâ€‘Output format usable for fineâ€‘tuning.
 * [EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench) - ğŸ“¥ 250 / â­ 9 / EDINETâ€‘Bench is a Japanese financial benchmark that evaluates LLMs on tasks such as accounting fraud detection, earnings forecasting, and industry prediction using ten years of EDINETâ€‘API disclosed reports, with construction and evaluation code provided and the dataset relicensed to PDLâ€¯1.0.
 * [swallow-gemma-magpie-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-gemma-magpie-v0.1) - ğŸ“¥ 249 / â­ 3 / Swallowâ€‘Gemmaâ€‘Magpieâ€‘v0.1 is a 148â€¯kâ€‘sample synthetic Japanese Q&A dataset generated with Googleâ€¯Gemmaâ€‘2â€‘27bâ€‘IT, designed for instructionâ€‘tuning TokyoTechâ€™s LLaMAâ€‘3.1â€‘Swallowâ€¯70B/8B models across diverse subjects.
 * [llava-instruct-ja](https://huggingface.co/datasets/llm-jp/llava-instruct-ja) - ğŸ“¥ 248 / â­ 5 / Japanese LLaVAâ€‘Instruct dataset of 156K samples, generated with GPTâ€‘4oâ€‘mini via Azure OpenAI, licensed CCâ€¯BYâ€¯4.0 and compliant with OpenAI terms.
 * [Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription) - ğŸ“¥ 244 / â­ 8 / Umamusume voice transcriptions dataset listing 77 characters with their total audio duration (e.g., East Commerce 799â€¯s, East Imperial Emperor 1074â€¯s, â€¦).
 * [J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - ğŸ“¥ 239 / â­ 32 / A highâ€‘quality Japanese research paper corpus (~39â€¯M characters) licensed under CCâ€‘BYâ€‘* from ACL 2021â€‘2024 proceedings, the *NLP* journal, and other journals, released for LLM preâ€‘training and RAG with ongoing additions.
 * [oscor-2301-ja-text-content](https://huggingface.co/datasets/ayousanz/oscor-2301-ja-text-content) - ğŸ“¥ 237 / â­ 2 / Converts the Japanese OSCORâ€‘2301 datasetâ€™s JSON files into plainâ€‘text by extracting only the â€œcontentâ€ field from each entry.
 * [gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset) - ğŸ“¥ 226 / â­ 2 / A dataset of 64,139 Japanese names labeled with biological genderâ€”presented in kanji, hiragana, and romajiâ€”whose 44.9â€¯k training, 6.41â€¯k validation, and 12.8â€¯k test split earned acceptance at ISDAâ€™23.
 * [WAON](https://huggingface.co/datasets/speed/WAON) - ğŸ“¥ 218 / â­ 2 / WAON is a largeâ€‘scale, highâ€‘quality Japanese imageâ€‘text pair dataset for visionâ€‘language models, built through size and SigLIPâ€‘score filtering and deduplication on URLs, captions, and perceptual hashes, and licensed under Apacheâ€¯2.0 with use limited to informational analysis under Japanese law.
 * [livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ğŸ“¥ 216 / â­ 4 / Dataset card details the llm-book/ner-wikinews-dataset, a cleaned collection of livedoor News articles under CCâ€¯BYâ€‘NDâ€¯2.1â€¯JP used in the book *Introduction to Large Language Models* and supplied by LONWIIT.
 * [japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized) - ğŸ“¥ 214 / â­ 3 / Japanese web corpora such as mc4â€‘ja are cleaned and clustered into ~10,000 groups by an unsupervised model, available for lawful analysis; only some files are in parquet format with the file list in the out folder, and it should be downloaded using gitâ€‘lfs.
 * [JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - ğŸ“¥ 209 / â­ 16 / JAâ€‘VGâ€‘VQAâ€‘500 is a 500â€‘sample subset of the Japanese Visual Genome VQA dataset, licensed CCâ€¯BYâ€¯4.0, used to benchmark EvoVLMâ€‘JPâ€‘v1â€‘7B.
 * [cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents) - ğŸ“¥ 208 / â­ 3 / Documentâ€‘level concatenation of the cc100â€‘ja dataset from HuggingFace, licensed under the original cc100 terms.
 * [CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - ğŸ“¥ 203 / â­ 2 / Japanese CallHome corpus contains 200 USâ€‘based 30â€‘minute telephone audio recordings from 120 speakers, with 80 training, 20 development and 100 evaluation transcriptions (DOI:â€¯10.21415/T5H59V).
 * [relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation) - ğŸ“¥ 203 / â­ 3 / A tool that uses text2dataset with vLLM and the openâ€‘weight Gemmaâ€¯2.9bâ€‘it for rapid Englishâ€‘toâ€‘Japanese translation, generating a 1.5â€¯billionâ€‘pair Japanese imageâ€‘text dataset to support CLIPâ€‘based visionâ€‘language models.
 * [Japanese-RAG-Generator-Benchmark](https://huggingface.co/datasets/neoai-inc/Japanese-RAG-Generator-Benchmark) - ğŸ“¥ 203 / â­ 4 / Japanese RAG Generator Benchmark (Jâ€‘RAGBench) supplies a multiâ€‘category QA datasetâ€”covering Integration, Reasoning, Logical, Table, and Abstentionâ€”designed to evaluate Japanese RAG generators, built with human effort and GPTâ€‘4.1, and released under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised) - ğŸ“¥ 200 / â­ 15 / Mirror of the Reazon Speechâ€¯v2 dataset: 3,674 denoised audio files processed with UVR on 8â€¯A800 GPUs over 10â€¯days by Stardustâ€‘minus, released under CDLAâ€‘Sharingâ€‘1.0 and without transcripts.
 * [wikipedia-qa-ja-100k](https://huggingface.co/datasets/alfredplpl/wikipedia-qa-ja-100k) - ğŸ“¥ 191 / â­ 3 / Dataset card for the Japanese QA set â€œwikipediaâ€‘qaâ€‘jaâ€‘100kâ€, sourced from hpprc/wikipediaâ€‘20240101, with RAGâ€‘style prompt guidelines for use with CALMâ€¯2â€‘7Bâ€¯Chat.
 * [MGSM_ja](https://huggingface.co/datasets/sbintuitions/MGSM_ja) - ğŸ“¥ 190 / â­ 2 / Offers a reproducible clone of SB Intuitions and a Japaneseâ€‘only subset of the MGSM multilingual chainâ€‘ofâ€‘thought reasoning dataset, licensed under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [jsnli](https://huggingface.co/datasets/shunk031/jsnli) - ğŸ“¥ 179 / â­ 5 / JSNLI is a Japanese translation of the SNLI NLI benchmark released by the KUROHASHIâ€‘CHUâ€‘MURAWAKI LAB, offering 548â€¯k training pairs (3â€¯916 validation) in TSV format with JUMAN++â€‘morphed premises and hypotheses, plus a 533â€¯k filtered subset, all distributed under CCâ€¯BYâ€‘SAâ€¯4.0.
 * [shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1) - ğŸ“¥ 176 / â­ 7 / A preâ€‘training dataset for shisaâ€‘baseâ€‘7bâ€‘v1, built from DSIRâ€‘sampled MADLADâ€‘400 tokens in a 90â€¯% Japanese / 10â€¯% English ratio.
 * [HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja) - ğŸ“¥ 175 / â­ 3 / Japanese autoâ€‘translated HelpSteer dataset for NVIDIAâ€¯SteerLM alignment trials, with reference URLs for LLM training.
 * [AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset) - ğŸ“¥ 175 / â­ 4 / Compiled from many openâ€‘source corpora totaling 1.56â€¯B tokens, this dataset preâ€‘trains the AKUâ€‘d_msâ€‘0.5Bâ€‘chatâ€‘v0.1 model, includes processing scripts, and will make raw data public later.
 * [TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese) - ğŸ“¥ 166 / â­ 4 / A ~3000â€‘story Japanese childâ€‘reading dataset, synthetically generated by GPTâ€‘4oâ€‘mini using only simple words, created following the method in https://arxiv.org/abs/2305.07759.
 * [jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points) - ğŸ“¥ 163 / â­ 4 / Dataset of Japanese Wikipedia bullet points generated by the rinna/deepseekâ€‘r1â€‘distillâ€‘qwen2.5â€‘bakenekoâ€‘32b model, sampled randomly (allowing duplicates), with lineâ€‘break formatting not fully shown in the Hugging Face viewer, and released under CCâ€‘BYâ€‘SAâ€¯4.0.
 * [ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - ğŸ“¥ 161 / â­ 77 / Versionâ€¯2 of a sentenceâ€‘aligned Japanese webâ€‘novel to English fanâ€‘translation dataset (106â€¯k chapters), updated alignment, added series metadata, no quality filtering, released under fairâ€‘use and Apacheâ€¯2.0 with a Huggingâ€¯Face takedown procedure.
 * [livedoor-news-corpus](https://huggingface.co/datasets/shunk031/livedoor-news-corpus) - ğŸ“¥ 159 / â­ 7 / Japanese news articles from livedoor News under a CC BYâ€‘ND license are cleaned of HTML, delivered in 6,567 items split 80/10/10 for training, validation, and testing.
 * [wrime](https://huggingface.co/datasets/shunk031/wrime) - ğŸ“¥ 155 / â­ 27 / The WRIME dataset is a Japanese collection of 42,200 posts annotated with Plutchikâ€™s eight emotions for the writer, three readers, and their averages, structured into 40â€¯kâ€‘train, 1.2â€¯kâ€‘validation, and 2â€¯kâ€‘test splits for sentimentâ€‘analysis tasks.
 * [cv-corpus-17.0-ja-client_id-grouped](https://huggingface.co/datasets/masuidrive/cv-corpus-17.0-ja-client_id-grouped) - ğŸ“¥ 150 / â­ 2 / Common Voice subset with 649 speaker groups (client IDs) each 30â€“300 samples, 45,668 recordings split 8:2 train/validation, batched into 1,000â€‘sample Parquet files, CC0 licensed.
 * [JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - ğŸ“¥ 148 / â­ 5 / JetCopperâ€‘10B is a 4.7â€¯Bâ€‘token Japanese dataset (plus 0.9â€¯B English code) compiled from CCâ€‘100, OSCARâ€‘2301, HPLTâ€¯v1.2, and wiki40bâ€‘ja, used to preâ€‘train Contrailâ€‘200mâ€‘64k for the LOCAL AI HACKATHON #000 calm2â€‘chat, but it has not yet undergone sentenceâ€‘boundary or perplexity filtering.
 * [Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - ğŸ“¥ 147 / â­ 11 / Japaneseâ€‘Heronâ€‘Bench evaluates Japanese VLMs with 21 publicâ€‘domain or CCâ€‘BY images across 7 subâ€‘categories, each paired with 1â€“2 questions in Conversation, Detail and Complex categories, totaling 102 questions.
 * [jawiki](https://huggingface.co/datasets/hpprc/jawiki) - ğŸ“¥ 142 / â­ 18 / A structured text dataset from Wikipediaâ€™s Januaryâ€¯2024 HTML dump that preserves paragraph structure without markup, provides metadata (abstracts, dates, disambiguation/sexual/violent flags, templates) for each article, and is ready for NLP experiments.
 * [CCMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/CCMatrix-v1-Ja_Zh-filtered) - ğŸ“¥ 140 / â­ 11 / CCMatrixâ€‘v1â€‘Ja_Zhâ€‘filtered is a cleaned Japaneseâ€‘Chinese dataset from CCMatrix v1, filtered by regex/length checks, LaBSEâ€‘based semantic similarity (>â€¯0.6), and with all Traditional Chinese converted to Simplified using zhconv.
 * [extraction-wiki-ja](https://huggingface.co/datasets/llm-jp/extraction-wiki-ja) - ğŸ“¥ 139 / â­ 2 / An instructionâ€‘tuning dataset from LLMâ€‘jpâ€”built on a Japanese Wikipedia subset (llmâ€‘jpâ€‘corpusâ€‘v3), filtered with Qwen/Qwen2.5â€‘32Bâ€‘Instruct, and offering twoâ€‘turn and fourâ€‘turn dialogue formatsâ€”created by Hirokazuâ€¯Kiyomaru and Takashiâ€¯Kodama.
 * [JaMARD](https://huggingface.co/datasets/elyza/JaMARD) - ğŸ“¥ 133 / â­ 9 / A highâ€‘quality synthetic Japanese math problem dataset with verified chainâ€‘ofâ€‘thought reasoning, built by translating PRM800K and GSM8K via Qwen2â€‘7Bâ€‘Instruct and filtering for correctness, available through the HuggingÂ Face datasets library.
 * [SocialStigmaQA-JA](https://huggingface.co/datasets/ibm-research/SocialStigmaQA-JA) - ğŸ“¥ 132 / â­ 4 / Japanese release of SocialStigmaQA featuring 93 stigmas and 37 translated question templates, each with a biased_answer field and four prompt styles (original, positive, doubt, etc.) to probe social bias in large language models.
 * [JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - ğŸ“¥ 124 / â­ 29 / A CCâ€‘BYâ€‘4.0 licensed, manually compiled dataset of Japanese government FAQ questionâ€“answer pairs, complete with source URLs, designed for instructionâ€‘tuning and RAG of largeâ€‘languageâ€‘model systems.
 * [nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - ğŸ“¥ 118 / â­ 2 / A MITâ€‘licensed, 100â€‘word Japanese stopâ€‘word list derived from the CCâ€‘100 Wikipedia dump, curated to match nagisaâ€™s tokenization rules for use in text preprocessing, feature extraction, and modeling.
 * [alpaca_jp_math](https://huggingface.co/datasets/HachiML/alpaca_jp_math) - ğŸ“¥ 118 / â­ 6 / alpaca_jp_math is a Japanese synthetic math dataset crafted with Stanford Alpaca and mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, cleaned and validated for consistency between code and text outputs, and distributed under the Apacheâ€¯2.0 license.
 * [databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - ğŸ“¥ 116 / â­ 18 / Japanese translation of the Databricks Dollyâ€‘15k instructionâ€‘tuning dataset, produced with DeepL by the LLMâ€‘jp collaborative project in Japan.
 * [simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon) - ğŸ“¥ 115 / â­ 14 / A simple dataset of Zundamon character settingsâ€”compiled from online sources and admin dataâ€”for testing characterâ€‘LLMs, provided in zmnjp.jsonl and zmn.jsonl formats under a specified license.
 * [lima-ja](https://huggingface.co/datasets/zan/lima-ja) - ğŸ“¥ 114 / â­ 3 / LIMAâ€‘JA is a Japaneseâ€‘translated, ChatGPTâ€‘edited version of Metaâ€™s LIMA dataset (â‰ˆ100 changes) for language models, accessible withâ€¯load_dataset('zan/lima-ja',â€¯'v1') and licensed under CCâ€¯BYâ€‘NCâ€‘SA unless the original LIMA source requires a stricter license.
 * [alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python) - ğŸ“¥ 114 / â­ 8 / alpaca_jp_python is a Japanese synthetic Alpaca dataset created and cleaned with mistralai/Mixtralâ€‘8x22Bâ€‘Instructâ€‘v0.1, hosted on Deepinfra, and distributed via the datasets library with cleanâ€‘labeled â€œ_cleanedâ€ splits and promptâ€‘based curation.
 * [JDocQA](https://huggingface.co/datasets/shunk031/JDocQA) - ğŸ“¥ 112 / â­ 10 / JDocQA is a Japanese PDFâ€‘based QA dataset comprising 5,504 documents and 11,600 questionâ€‘answer pairs that test yes/no, factoid, numerical, openâ€‘ended, and unanswerable comprehension using both visual and textual information.
 * [msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives) - ğŸ“¥ 112 / â­ 3 / A hardâ€‘negative mining pipeline applied to the Japanese MSâ€¯MARCO translationâ€”including normalization, highâ€‘cosineâ€‘similarity filtering, BAAI/BGE rerankerâ€‘based selection, and random samplingâ€”was created and shown by chiâ€‘square tests to have a statistically higher positive rate than the SPLADEâ€‘trained mMARCO baseline.
 * [llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions) - ğŸ“¥ 110 / â­ 5 / llmâ€‘jpâ€‘instructions is a manually curated Japanese instruction dataset offering train, dev, and test splits for languageâ€‘model fineâ€‘tuning.
 * [ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS) - ğŸ“¥ 110 / â­ 3 / ScreenTalk_JA2ZHâ€‘XS is a 10,000â€‘sample, ~30â€‘hour paired Japanese audio / Simplified Chinese text dataset (Parquet, CCâ€¯BYâ€¯4.0) used for speechâ€‘toâ€‘text translation, multilingual ASR, and multimodal AI research.
 * [jhle](https://huggingface.co/datasets/llm-jp/jhle) - ğŸ“¥ 106 / â­ 83 / Japaneseâ€‘translated Humanityâ€™s Last Exam dataset curated by LLMâ€‘jp, which omits image questions, samples five per raw_subject, is machineâ€‘translated and expertâ€‘reviewed, authored by Yujiâ€¯Tamakoshi,â€¯Koutaâ€¯Nakayama andâ€¯Yusukeâ€¯Miyao, and must never be used in training corpora.
 * [orca_dpo_pairs_ja](https://huggingface.co/datasets/yongtae-jp/orca_dpo_pairs_ja) - ğŸ“¥ 105 / â­ 7 / Japanese translations of the Intel/orca_dpo_pairs dataset generated with Palmâ€¯2 (textâ€‘bisonâ€‘32k@002) for Japanese LLM developers, preserving original nonâ€‘English text while ensuring natural, conversational Japanese.
 * [WAON](https://huggingface.co/datasets/llm-jp/WAON) - ğŸ“¥ 104 / â­ 7 / WAON is a large, highâ€‘quality Japanese imageâ€‘text pair dataset for visionâ€‘language models, built through rigorous filtering of image size and SigLIP scores and deduplication via URLs, captions and pHash, providing rich metadata (captions, page URLs, safety scores, image hashes) under an Apacheâ€¯2.0 license for informational analysis.
 * [Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230) - ğŸ“¥ 102 / â­ 8 / Malumâ€‘230 is a humanâ€‘crafted Japanese dataset of multiâ€‘turn conversations and passages for logical reasoning, intended for both preâ€‘training and postâ€‘training, and has been evaluated on Japanese MTâ€‘Bench with Qwen2.5â€‘7B.
