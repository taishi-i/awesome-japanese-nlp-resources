# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1442 models and 607 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## ğŸ“– Contents

Released [a tool ğŸ”](https://huggingface.co/spaces/taishi-i/awesome-japanese-nlp-resources-search) for searching through a large number of repository information.

 * [Models](#models)

 * [Datasets](#datasets)


## ğŸ‰ The latest additions

**Models**
4 models have been added.

- [mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf)
- [mpasila/gemma-3-JP-EN-Translator-v1-4B](https://huggingface.co/mpasila/gemma-3-JP-EN-Translator-v1-4B)
- [cyberagent/ca-reward-3b-ja](https://huggingface.co/cyberagent/ca-reward-3b-ja)
- [OmniAICreator/Galgame-Llasa-3B-v3](https://huggingface.co/OmniAICreator/Galgame-Llasa-3B-v3)


**Datasets**
6 datasets have been added.

- [cc-clean/CC-MAIN-2025-05](https://huggingface.co/datasets/cc-clean/CC-MAIN-2025-05)
- [llm-compe-2025-kato/tag-validation-results-test](https://huggingface.co/datasets/llm-compe-2025-kato/tag-validation-results-test)
- [cc-clean/CC-MAIN-2015-14](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-14)
- [cc-clean/CC-MAIN-2015-11](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-11)
- [Silviase/augeo-ja](https://huggingface.co/datasets/Silviase/augeo-ja)
- [llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1](https://huggingface.co/datasets/llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1)


## Models

This list is sorted by downloads as of August 19, 2025.
1442 models are listed.

- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 3,470,811
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - xlm-roberta-ner-japanese (Japanese caption : æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«)
  - Downloads: 923,715
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - RetrievaEmbedding-01: AMBER The AMBER (Adaptive Multitask Bilingual Embedding Representations) is a text embedding model trained by Retrieva, Inc.
  - Downloads: 435,709
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - BERT base Japanese (IPA dictionary, whole word masking enabled)
  - Downloads: 400,002
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
  - Downloads: 329,925
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
  - Downloads: 314,990
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - rinna/japanese-cloob-vit-b-16
  - Downloads: 310,462
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 142,869
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 109,059
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 104,693
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - BERT base Japanese (character tokenization)
  - Downloads: 103,256
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2 reazonspeech-nemo-v2 is an automatic speech recognition model trained on ReazonSpeech v2.0 corpus.
  - Downloads: 92,085
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This is a Japanese sentence-BERT model.
  - Downloads: 79,844
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-ner-wikipedia-dataset ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 70,932
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 63,518
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - DeepSeek-V3-slice-jp64 æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ DeepSeek-V3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã®ä¾‹æ–‡ã‚’å…ƒã«é »å‡ºã™ã‚‹ MoE (Mixture of Experts) ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®expertsã‚’å³é¸ã—ã¦å†æ§‹æˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®ggufç‰ˆã§ã™ã€‚
  - Downloads: 62,014
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT
  - Downloads: 56,074
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT
  - Downloads: 55,727
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japanese æ—¥æœ¬èªã®README/Japanese README GLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
  - Downloads: 53,948
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This is a Japanese sentence-BERT model.
  - Downloads: 50,453
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers fugashi sentencepiece unidic-lite Then you can load this model and run inference.
  - Downloads: 46,496
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - BERTã«ã‚ˆã‚‹æ—¥æœ¬èªå›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ« BertForTokenClassificationã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã®æ–‡ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
  - Downloads: 44,313
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B æ—¥æœ¬èªç‰ˆã®README/Japanese README Model Overview PLaMo-Embedding-1B is a Japanese text embedding model developed by Preferred Networks, Inc.
  - Downloads: 40,578
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - BERT base Japanese (IPA dictionary)
  - Downloads: 40,503
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 37,518
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese This is a DistilBERT model pre-trained on 131 GB of Japanese web text.
  - Downloads: 35,020
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 Kotoba-Whisper-v2.2 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 34,021
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 31,277
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - rinna/japanese-clip-vit-b-16
  - Downloads: 27,079
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 26,186
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B Model Description PLaMo 2 1B is a 1B model pre-trained on English and Japanese datasets, developed by Preferred Elements, Inc.
  - Downloads: 23,763
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)
  - Downloads: 20,102
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 tiny Model description This is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 17,098
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,935
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - bert-finetuned-japanese-sentiment This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
  - Downloads: 16,087
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 15,584
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2
  - Downloads: 15,538
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - japanese-gpt2-medium This repository provides a medium-sized Japanese GPT-2 model.
  - Downloads: 15,226
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - clip-japanese-base This is a Japanese CLIP (Contrastive Language-Image Pre-training) model developed by LY Corporation.
  - Downloads: 14,965
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - Model Card for Japanese DeBERTa V2 tiny Model description
  - Downloads: 14,792
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - japanese-gpt2-small This repository provides a small-sized Japanese GPT-2 model.
  - Downloads: 14,467
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14,385
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTæ§˜ã® AXCXEPT/EZO-gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13,791
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 13,629
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 12,397
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 12,069
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri: Japanese General Text Embeddings Usage Direct Usage (Sentence Transformers)
  - Downloads: 11,754
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 11,568
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 11,070
- [llm-jp/llm-jp-3.1-1.8b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4)
  - llm-jp-3.1-1.8b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 10,252
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 10,197
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
  - Downloads: 10,070
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ /
  - Downloads: 10,055
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - japanese-gpt-neox-3.6b Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 9,839
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 9,461
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 9,169
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet TDT-CTC 0.6B (ja) | | parakeet-tdt_ctc-0.6b-ja is an ASR model that transcribes Japanese speech with Punctuations.
  - Downloads: 9,105
- [pfnet/plamo-2-translate](https://huggingface.co/pfnet/plamo-2-translate)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 8,896
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - t5-base-japanese-web (with Byte-fallback, 32K) Description megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 8,794
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M This repository provides Japanese ModernBERT trained by SB Intuitions.
  - Downloads: 8,683
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 8,064
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - japanese-sentiment-analysis This model was trained from scratch on the chABSA dataset.
  - Downloads: 7,947
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM-Small Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 7,882
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - japanese-gpt2-xsmall
  - Downloads: 7,621
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - åæ¡è›ï¼ˆHotaru Jujoï¼‰ã®ä½œæˆã—ãŸLoRAã‚’é…å¸ƒã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 7,607
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - Model card for model ID
  - Downloads: 7,548
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - ä¸­æ–‡ | í•œêµ­ì–´ | æ—¥æœ¬èª | Ğ ÑƒÑÑĞºĞ¸Ğ¹ | Deutsch | FranÃ§ais | EspaÃ±ol | PortuguÃªs | TÃ¼rkÃ§e | Tiáº¿ng Viá»‡t | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ultralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.
  - Downloads: 7,373
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - sbert-jsnli-luke-japanese-base-lite This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
  - Downloads: 7,127
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B æ—¥æœ¬èªã®README/Japanese README "Sarashina-Embedding-v1-1B" is a Japanese text embedding model, based on the 1.2B-parameter Japanese LLM "Sarashina2.1-1B".
  - Downloads: 7,061
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - llm-jp-3-1.8b
  - Downloads: 7,034
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE Model description Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
  - Downloads: 6,611
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 6,577
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - recruit-jp/japanese-clip-vit-b-32-roberta-base Overview Developed by: Recruit Co.
  - Downloads: 6,458
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - japanese-roberta-base This repository provides a base-sized Japanese RoBERTa model.
  - Downloads: 5,871
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.5](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.5)
  - Llama 3.1 Swallow v0.5 - Built with Llama Llama 3.1 Swallow v0.5 is a large language model (8B) that was built by continual pre-training on the Meta Llama 3.1 model.
  - Downloads: 5,847
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 5,606
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - hotchpotch/japanese-reranker-cross-encoder-small-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 5,583
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - rinna/japanese-hubert-base Overview This is a Japanese HuBERT Base model trained by rinna Co.
  - Downloads: 5,350
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - Model card for model ID
  - Downloads: 5,223
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - llm-jp-3-7.2b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 5,168
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.3-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 4,835
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
  - Downloads: 4,805
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper (v2.0)
  - Downloads: 4,780
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - sbintuitions/sarashina2.2-3b-instruct-v0.1 Model Summary
  - Downloads: 4,673
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - bilingual-gpt-neox-4b Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 4,560
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 4,488
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - Additional pretrained BERT base Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 4,411
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 4,408
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - nagisa_bert A BERT model for nagisa.
  - Downloads: 4,404
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 4,362
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow - Built with Llama Llama 3.3 Swallow is a large language model (70B) that was built by continual pre-training on the Meta Llama 3.3 model.
  - Downloads: 4,265
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 4,219
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-base Fine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 4,161
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹phi-4-deepseek-R1K-RL-EZOã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 4,108
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B-chat Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
  - Downloads: 4,005
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 3,960
- [reazon-research/japanese-wav2vec2-base-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-base-rs35kh)
  - japanese-wav2vec2-base-rs35kh This model is a wav2vec 2.0 Base fine-tuned on the large-scale Japanese ASR corpus ReazonSpeech v2.0.
  - Downloads: 3,939
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B-GGUF Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 3,923
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 Kotoba-Whisper-v2.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v2.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 3,817
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 3,808
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹deepseek-r1-distill-qwen2.5-bakeneko-32bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,795
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B Heron-NVILA-Lite-15B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
  - Downloads: 3,779
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 3,741
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - hotchpotch/japanese-reranker-cross-encoder-large-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 3,565
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 3,563
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 3,510
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 3,385
- [llm-jp/llm-jp-3.1-13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4)
  - llm-jp-3.1-13b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 3,338
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - japanese-gpt-neox-small This repository provides a small-sized Japanese GPT-NeoX model.
  - Downloads: 3,269
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - rinna/youri-7b Overview We conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
  - Downloads: 3,259
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper ğŸ¤—ğŸ¤ğŸ“
  - Downloads: 3,173
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - rinna/japanese-wav2vec2-base Overview This is a Japanese wav2vec 2.0 Base model trained by rinna Co.
  - Downloads: 3,146
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - sbintuitions/sarashina2.2-0.5b-instruct-v0.1 Model Summary
  - Downloads: 3,145
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - Japanese Parler-TTS Mini ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-mini-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 3,131
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - nlp-waseda/gpt2-small-japanese This model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.
  - Downloads: 3,064
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - GPT-2 small Japanese model This repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
  - Downloads: 3,053
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-Japanese-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,004
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - Qwen2.5 Bakeneko 32B Instruct GGUF (rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - Downloads: 3,003
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,863
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM-Large Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 2,836
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-8B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,805
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M
  - Downloads: 2,795
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
  - Downloads: 2,787
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - Model Card for gemma-2-2b-jpn-it-translate-gguf gemma-2-2b-jpn-it-translate-ggufã¯ã€æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸSLMï¼ˆSmall Language Modelï¼‰ã§ã™ã€‚
  - Downloads: 2,738
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,719
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-14B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,708
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
  - Downloads: 2,681
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL 2408 260M DanbotNL is translator that tranaslates from natural languages into Danbooru tags.
  - Downloads: 2,639
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instruct Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,618
- [llm-jp/llm-jp-3.1-1.8b](https://huggingface.co/llm-jp/llm-jp-3.1-1.8b)
  - llm-jp-3.1-1.8b LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 2,602
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - whisper-large-v3-japanese-4k-steps This model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
  - Downloads: 2,552
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  - Downloads: 2,550
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,529
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - umiyuki-Umievo-itr012-Gleipnir-7B-gguf umiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Umievo-itr012-Gleipnir-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,518
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 2,505
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 2,496
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 2,467
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 2,432
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
  - Downloads: 2,430
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 2,411
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - shisa-gamma-7b-v1 For more information see our main Shisa 7B model We applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
  - Downloads: 2,324
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,316
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 2,310
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B (CALM2-7B)
  - Downloads: 2,287
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯DeepSeek-R1-Distill-Qwen-14Bã‚’æ—¥æœ¬èªã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,277
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 2,272
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - Llama-3.1-70B-Japanese-Instruct-2407 Model Description This is a Japanese continually pre-trained model based on meta-llama/Meta-Llama-3.1-70B-Instruct.
  - Downloads: 2,250
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-32B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,212
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 ABEJA-Qwen2.5-7b-Japanese-v0.1ã¯Qwen/Qwen2.5-7B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®å­¦ç¿’ã‚’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,185
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - llm-jp-3-1.8b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 2,185
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,172
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - Japanese to Korean translator Japanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)
  - Downloads: 2,154
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - stockmark/stockmark-13b Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
  - Downloads: 2,148
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - japanese-gpt-neox-3.6b-instruction-ppo Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 2,140
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - rinna/japanese-hubert-large Overview This is a Japanese HuBERT Large model trained by rinna Co.
  - Downloads: 2,124
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - ã€å‘ŠçŸ¥ã€‘chilled_remixåŠã³reversemixã¯2023å¹´5æœˆ21æ—¥ã«Versionå¤‰æ›´ã‚’è¡Œã„ã€v2ã¸ç§»è¡Œã„ãŸã—ã¾ã—ãŸã€‚
  - Downloads: 2,118
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - japanese-gpt-neox-3.6b-instruction-sft Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 2,104
- [Aratako/NemoAurora-RP-12B-GGUF](https://huggingface.co/Aratako/NemoAurora-RP-12B-GGUF)
  - NemoAurora-RP-12B-GGUF æ¦‚è¦ Aratako/NemoAurora-RP-12Bã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,099
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fast Model Description ELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,096
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - æ—¥æœ¬èªã¯ã“ã¡ã‚‰ lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese Deepseek's R1 models are excellent, state-of-the-art reasoning models which have been trained to work bilingually, with English and Chinese.
  - Downloads: 2,089
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - DeBERTa V2 base Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 2,003
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 1,986
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta RoSEtta (RoFormer-based Sentence Encoder through Distillation) is a general Japanese text embedding model, excelling in retrieval tasks.
  - Downloads: 1,979
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow - Built with Llama Llama 3.3 Swallow is a large language model (70B) that was built by continual pre-training on the Meta Llama 3.3 model.
  - Downloads: 1,967
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct Model Description RakutenAI-2.0-8x7B-instruct is a fine-tuned variant of RakutenAI-2.0-8x7B, designed to push the boundaries of Japanese large language models (LLMs).
  - Downloads: 1,950
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - weblab-10b-instruction-sft-GPTQ Original model weblab-10b-instruction-sft which is a Japanese-centric multilingual GPT-NeoX model of 10 billion parameters created by matsuo-lab Takeshi Kojima.
  - Downloads: 1,944
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
  - Downloads: 1,935
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Tanuki-8Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„1.3Tãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸç´„8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,863
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - gpt-neox-japanese-2.7b
  - Downloads: 1,855
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
  - Downloads: 1,844
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 1,841
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual (v1.0)
  - Downloads: 1,828
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - BERT Base Japanese for Irony
  - Downloads: 1,818
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
  - Downloads: 1,817
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - Model Card for Japanese DeBERTa V3 base Model description This is a Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0.
  - Downloads: 1,792
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base ğŸ“„ Paper | ğŸ§‘â€
  - Downloads: 1,771
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B Model Description
  - Downloads: 1,768
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - Sentence BERT base Japanese model This repository contains a Sentence BERT base model for Japanese.
  - Downloads: 1,755
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - japanese-large-lm-1.7b This repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
  - Downloads: 1,714
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B Model Description ELYZA-Thinking-1.0-Qwen-32B is a reasoning model trained by ELYZA, Inc.
  - Downloads: 1,700
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B Heron-NVILA-Lite-2B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
  - Downloads: 1,696
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri: Japanese General Text Embeddings Ruri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.
  - Downloads: 1,686
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B (rinna/llama-3-youko-8b)
  - Downloads: 1,683
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,665
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,615
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - japanese-large-lm-3.6b
  - Downloads: 1,600
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 1,591
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - google/gemma-3-12b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-12b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese..
  - Downloads: 1,587
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 1,554
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - ã‹ã‚‰ã¾ã‚‹ Llama-3-Karamaru-v1 Karamaru is a conversational AI model developed by Sakana AI that responds in the style of Edo-period Japanese.
  - Downloads: 1,538
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - Ninja-v1-NSFW-128k-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFW-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,537
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,526
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - Qwen3-8B-ERP-v0.1-GGUF æ¦‚è¦ Aratako/Qwen3-8B-ERP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,522
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - japanese-large-lm-3.6b-instruction-sft
  - Downloads: 1,514
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.
  - Downloads: 1,513
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - Polyglot-math-4x7b-24b Polyglot-4x7b is a Mixture of Experts approach to a multilingual model.
  - Downloads: 1,510
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 1,507
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF Original Model elyza/ELYZA-japanese-Llama-2-13b-fast-instruct Run with LlamaEdge LlamaEdge version: v0.2.8 and above Prompt template Prompt type: llama-2-chat Prompt string &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ user_msg_1 }}
  - Downloads: 1,463
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - japanese-large-lm-1.7b-instruction-sft This repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
  - Downloads: 1,455
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL++-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
  - Downloads: 1,443
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Finetuned Waseda RoBERTa to evaluate the generated answers on JTruthfulQA.
  - Downloads: 1,441
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - llm-jp-3-13b-instruct This repository provides large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 1,418
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - rinna/nekomata-14b Overview We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
  - Downloads: 1,413
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - BERT base Japanese (character tokenization, whole word masking enabled)
  - Downloads: 1,403
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 1,399
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,392
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,386
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - japanese-gpt-neox-3.6b-instruction-sft-v2 Overview
  - Downloads: 1,384
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,375
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 1,373
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese Model Description This is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.
  - Downloads: 1,370
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 1,369
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - Qwen3-30B-A3B-ERP-v0.1-GGUF æ¦‚è¦ Aratako/Qwen3-30B-A3B-ERP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,366
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - llm-jp-3-8x1.8b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-8x1.8b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,364
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - rinna/nekomata-7b Overview We conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
  - Downloads: 1,354
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
  - Downloads: 1,349
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
  - Downloads: 1,332
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,331
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,327
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumerã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Reflection-Llama-3.1-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,323
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1
  - Downloads: 1,314
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 1,314
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - Model Card for Japanese character-level GPT-2 Small Model description This is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 1,306
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LM KARAKURI LM is a pretrained language model that builds upon Llama 2.
  - Downloads: 1,305
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-7B-Japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,302
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,301
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-JAVocab-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 1,290
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
  - Downloads: 1,280
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,278
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
  - Downloads: 1,271
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B-instruct Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
  - Downloads: 1,260
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - stockmark/gpt-neox-japanese-1.4b This repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
  - Downloads: 1,256
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B A cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XL Model Description japanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 1,256
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B Heron-NVILA-Lite-1B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
  - Downloads: 1,256
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct Stockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
  - Downloads: 1,254
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - gpt2-large-japanese This repository provides a large sized Japanese GPT-2 model.
  - Downloads: 1,253
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - bilingual-gpt-neox-4b-instruction-ppo Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 1,250
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,247
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,247
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - hotchpotch/japanese-reranker-cross-encoder-base-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 1,237
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - wav2vec2-base-asr
  - Downloads: 1,233
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 1,232
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 1,222
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - bert-base-japanese-v3-jnli ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(è‡ªç„¶è¨€èªæ¨è«–)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,217
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M This repository provides Japanese ModernBERT trained by SB Intuitions.
  - Downloads: 1,207
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,187
- [Tetsuo3003/ner-medical-japanese](https://huggingface.co/Tetsuo3003/ner-medical-japanese)
  - ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª åŒ»ç™‚ä¼šè©± NER ãƒ¢ãƒ‡ãƒ« ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¥æœ¬èªã®åŒ»ç™‚ä¼šè©±æ–‡æ›¸ã«ç‰¹åŒ–ã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,178
- [dahara1/gemma-3-1b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-1b-it-qat-japanese-imatrix)
  - google/gemma-3-12b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-12b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese..
  - Downloads: 1,155
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,145
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - bert-base-japanese-jsnli This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
  - Downloads: 1,140
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-Qwen2.5-7b-Japanese-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,123
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - bert-japanese_finetuned-sentiment-analysis This model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
  - Downloads: 1,114
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-QwQ32b-Reasoning-Japanese-v1.0ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,112
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - WRIME-fine-tuned BERT base Japanese This model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
  - Downloads: 1,107
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 1,089
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - Model Description llava-calm2-siglip is an experimental Vision Language Model that can answer questions in Japanese about images.
  - Downloads: 1,088
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri: Japanese General Text Embeddings Notes: v3 models are out!
  - Downloads: 1,078
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - llm-jp-3-8x13b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-8x13b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,078
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 ABEJA-QwQ32b-Reasoning-Japanese-v1.0ã¯abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1(*)
  - Downloads: 1,077
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 1,075
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹mathstral-7B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,067
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 1,060
- [llm-jp/llm-jp-3.1-8x13b-instruct4](https://huggingface.co/llm-jp/llm-jp-3.1-8x13b-instruct4)
  - llm-jp-3.1-8x13b-instruct4 LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 1,046
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,040
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 1,032
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.2ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,020
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B sarashina2.2â€‘0.5bâ€‘instructâ€‘v0.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«å­¦ç¿’ã—ãŸTTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,019
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 1,019
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1 shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 1,013
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-ELYZA-JP-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,008
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - llm-jp-3-13b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 999
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
  - Downloads: 997
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-35Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 993
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 982
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B Model Description RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs.
  - Downloads: 980
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - cyberagent-open-calm-7b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 978
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ï¼Ÿ
  - Downloads: 974
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B (rinna/qwq-bakeneko-32b)
  - Downloads: 956
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 919
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FINGU-AI/FinguAI-Chat-v1 Overview The FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
  - Downloads: 906
- [stockmark/Stockmark-2-VL-100B-beta](https://huggingface.co/stockmark/Stockmark-2-VL-100B-beta)
  - Stockmark-2-VL-100B-beta Introduction Stockmark-2-VL-100B-beta is a 100-billion-parameter Japanese-specialized visual language model with Chain-of-Thought (CoT) reasoning for document reading comprehension.
  - Downloads: 905
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - llm-jp-3-8x13b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 904
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Vecteus-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 900
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 886
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini Model Description RakutenAI-2.0-mini is a lightweight Japanese language model trained from scratch using a transformer architecture, designed for efficient performance in resource-constrained environments.
  - Downloads: 876
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 875
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - llm-jp-3-3.7b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 873
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 858
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯å¼·åŠ›ãªï¼”ã¤ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 856
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunChoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-novel-gpt-j-6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 850
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-Swallow-8B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 848
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1 static quants are available at https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 847
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B Model Description PLaMo-13B is a LLaMA-based 13B model pre-trained on English and Japanese open datasets, developed by Preferred Networks, Inc.
  - Downloads: 842
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - qwen2.5-bakeneko-32b-instruct-v2-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwen2.5-bakeneko-32b-instruct-v2ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 841
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Japanese-Instruct-2408ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 816
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ About this model.
  - Downloads: 816
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KillerWhaleã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 815
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 813
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B "A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XL Model Description japanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 803
- [EQUES/JPharmatron-7B](https://huggingface.co/EQUES/JPharmatron-7B)
  - JPharmatron-7B JPharmatron-7B is a 7B large language model designed for pharmaceutical applications and researches.
  - Downloads: 802
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-2-2b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 788
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - gemma-3-4b-it-RP-v0.1-GGUF æ¦‚è¦ Aratako/gemma-3-4b-it-RP-v0.1ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 782
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 773
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 765
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - r1-1776-distill-llama-70b-gguf perplexity-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹r1-1776-distill-llama-70bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 754
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èª SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 739
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 ABEJA-Qwen2.5-32b-Japanese-v0.1ã¯Qwen/Qwen2.5-32B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªä¸­å¿ƒã¨ã—ãŸç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 732
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF æ¦‚è¦ Aratako/calm3-22b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 731
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF This is quantized version of cyberagent/Mistral-Nemo-Japanese-Instruct-2408 created using llama.cpp Original Model Card Mistral-Nemo-Japanese-Instruct-2408 Model Description
  - Downloads: 727
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠåˆ†ã»ã©ãŒæ—¥æœ¬èªãªã®ã§magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªã«ã¯å¼·ã„ã¯ãšï¼Ÿ
  - Downloads: 727
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-8B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 723
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - Moonlight-16B-A3B-Instruct-gguf moonshotaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Moonlight-16B-A3B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 717
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-ArrowSE-8B-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 700
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - hotchpotch/japanese-bge-reranker-v2-m3-v1 æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 696
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - nlp-waseda/roberta-base-japanese Model description This is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 690
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - (ç®€ä½“ä¸­æ–‡|English|æ—¥æœ¬èª) Introduction github repo : https://github.com/FunAudioLLM/SenseVoice SenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED).
  - Downloads: 688
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ AWSã®trn1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦é–‹ç™ºã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 688
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese License MIT License ğŸ‘‰ DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf ã“ã£ã¡ã®ãŒã„ã„ã‹ã‚‚ğŸ‘‰ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf
  - Downloads: 685
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - Qwen3-30B-A3B-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-30B-A3Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 678
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-70B-EZO-1.1-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 667
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v1.0)
  - ABEJA-Qwen2.5-32b-Japanese-v1.0 ABEJA-Qwen2.5-32b-Japanese-v1.0ã¯Qwen/Qwen2.5-32B-Instructã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªä¸­å¿ƒã¨ã—ãŸç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1ã«å¯¾ã—ã¦SFTã¨DPOã«ã‚ˆã‚‹äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 657
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - bert-base-japanese-v3-unsup-simcse-jawiki ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬8ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 655
- [mmnga/Qwen3-EZO-8B-beta-gguf](https://huggingface.co/mmnga/Qwen3-EZO-8B-beta-gguf)
  - Qwen3-EZO-8B-beta-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-EZO-8B-betaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 655
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ about this model.
  - Downloads: 655
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B Sarashina2-Vision-8B is a Japanese Large Vision Language Model trained by SB Intuitions.
  - Downloads: 639
- [llm-jp/llm-jp-3-7.2b](https://huggingface.co/llm-jp/llm-jp-3-7.2b)
  - llm-jp-3-7.2b
  - Downloads: 638
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - æ—¥æœ¬èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ« This is a CLIP text/image encoder model for Japanese.
  - Downloads: 622
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - calm3-22b-RP-v2-GGUF æ¦‚è¦ Aratako/calm3-22b-RP-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 620
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - Llama-3-ELYZA-JP-8B-AWQ Model Description Llama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.
  - Downloads: 590
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - cogito-v1-preview-qwen-32B-gguf deepcogitoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹cogito-v1-preview-qwen-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 571
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - llm-jp-3-980m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 569
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [r-g2-2024/Llama-3.1-70B-Instruct-multimodal-JP-Graph-v0.1](https://huggingface.co/r-g2-2024/Llama-3.1-70B-Instruct-multimodal-JP-Graph-v0.1)
  - Llama-3.1-70B-Instruct-multimodal-JP-Graph - Built with Llama Llama-3.1-70B-Instruct-multimodal-JP-Graph is a Japanese Large Vision Language Model.
  - Downloads: 556
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - llm-jp-3-7.2b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-7.2b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 548
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-70b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 546
- [mmnga/plamo-2-translate-gguf](https://huggingface.co/mmnga/plamo-2-translate-gguf)
  - plamo-2-translate-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹plamo-2-translateã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 545
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-70b-chat-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 545
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - qwq-bakeneko-32b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwq-bakeneko-32bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 538
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - Phi-4-reasoning-plus-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-4-reasoning-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 529
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - llm-jp-3-440m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 524
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - deberta-base-japanese-aozora-ud-head Model Description
  - Downloads: 523
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - llm-book/t5-base-long-livedoor-news-corpus ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬7ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹è¦ç´„ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 517
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-T2-2B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 516
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - ğŸ“‘ Paper | ğŸ¤—
  - Downloads: 515
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 509
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama3-Preferred-MedSwallow-70Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 503
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - sarashina2.2-3b-instruct-v0.1-gguf sbintuitionsã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.2-3b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 500
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - llm-jp-3-150m-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 499
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Jpã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 498
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - ELYZA-Shortcut-1.0-Qwen-7B-gguf elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-Shortcut-1.0-Qwen-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 490
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - ascktgcc/Mistral-nemo-ja-rp-v0.2ã®GGUFç‰ˆã§ã™ã€‚
  - Downloads: 486
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - llm-jp-3-980m LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 485
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT ElanMT-BT-ja-en is a Japanese to English translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 481
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - Swallow Education Classifier Japanese README Model summary This repository contains fastText classifiers for judging the educational value of Japanese web pages.
  - Downloads: 480
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 479
- [mmnga/karakuri-vl-32b-instruct-2507-gguf](https://huggingface.co/mmnga/karakuri-vl-32b-instruct-2507-gguf)
  - karakuri-vl-32b-instruct-2507-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-vl-32b-instruct-2507ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 477
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - â€»llama.cpp Releases b3428(7/21)
  - Downloads: 474
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - llm-jp-3-13b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-13b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 473
- [mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf](https://huggingface.co/mmnga/Gemma-2-Llama-Swallow-9b-it-v0.1-gguf)
  - Gemma-2-Llama-Swallow-9b-it-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Gemma-2-Llama-Swallow-9b-it-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 456
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Common-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 451
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotæ§˜ã® Llama3-ArrowSE-8B-v0.3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 449
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - RakutenAI-2.0-mini-instruct-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-2.0-mini-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 445
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct Model Description PLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 442
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository contains some GGUF quantizations of the merge of the VNTL LLaMA 3 8B qlora.
  - Downloads: 441
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T Base Model Description This is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 436
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 "A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XL Model Description japanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
  - Downloads: 432
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 432
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3.1-Swallow-70B-Instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 431
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - hotchpotch/japanese-reranker-tiny-v2 ã¨ã¦ã‚‚å°ã•ãé€Ÿã„æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
  - Downloads: 429
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - é«˜æ€§èƒ½ãªæ—¥æœ¬èª SPLADE (Sparse Lexical and Expansion Model) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 426
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf googleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹datagemma-rag-27b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 424
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese Considering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
  - Downloads: 423
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLM-13B-instruct-gguf Fugaku-LLMã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Fugaku-LLM-13B-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 417
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - recruit-jp/japanese-typo-detector-roberta-base ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ æ—¥æœ¬èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨å„æ–‡å­—ã”ã¨ã«èª¤å­—è„±å­—ã§ã‚ã‚‹ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¾ã™ å„ãƒ©ãƒ™ãƒ«ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ id label meaning 0 OK èª¤å­—ãªã— 1 deletion 1æ–‡å­—ã®æŠœã‘ 2 insertion_a ä½™åˆ†ãª1æ–‡å­—ã®æŒ¿å…¥ 3 insertion_b ç›´å‰ã®æ–‡å­—åˆ—ã¨ä¸€è‡´ã™ã‚‹ï¼’æ–‡å­—ä»¥ä¸Šã®ä½™åˆ†ãªæ–‡å­—ã®æŒ¿å…¥ 4 kanji-conversion_a åŒä¸€ã®èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰ 5 kanji-conversion_b è¿‘ã„èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰ 6 substitution 1æ–‡å­—ã®å…¥ã‚Œæ›¿ãˆ 7 transposition éš£æ¥ã™ã‚‹ï¼’æ–‡å­—é–“ã®è»¢ç½® 8 others ãã®ä»–ã®å…¥åŠ›èª¤ã‚Š èª¤ã‚Šç¨®é¡ã®è©³ç´°ã«ã¤ã„ã¦ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…ƒè«–æ–‡ã‚’ã”å‚ç…§ãã ã•ã„ æ—¥æœ¬èª Wikipedia ã®ç·¨é›†å±¥æ­´ã«åŸºã¥ã å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ ãã®ä»–ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯å½“ç¤¾ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„ èª¤å­—è„±å­—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Hubã«å…¬é–‹ã—ã¾ã—ãŸ (Re
  - Downloads: 416
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
  - Downloads: 413
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - llm-jp-3-150m LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 408
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - æ¦‚è¦ Imatrixã«ã¯neody/imatrix_datasetã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
  - Downloads: 404
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - Japanese SimCSE (BERT-base)
  - Downloads: 403
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - About static quants of https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1 weighted/imatrix quants are available at https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 396
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC Model Description PLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 394
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf Googleã•ã‚“ã®gemma-2-2b-jpn-itã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
  - Downloads: 390
- [llm-jp/llm-jp-3.1-13b](https://huggingface.co/llm-jp/llm-jp-3.1-13b)
  - llm-jp-3.1-13b LLM-jp-3.1 is a series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 388
- [mmnga/llm-jp-3-3.7b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-3.7b-instruct3-gguf)
  - llm-jp-3-3.7b-instruct3-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3-3.7b-instruct3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 378
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - About static quants of https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 375
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 370
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - sbintuitions/sarashina2.2-1b-instruct-v0.1 Model Summary
  - Downloads: 368
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M This repository provides Japanese ModernBERT trained by SB Intuitions.
  - Downloads: 366
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - Japanese-Starling-ChatV-7B-GGUF GGUF conversion of "Japanese-Starling-ChatV-7B" "Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.
  - Downloads: 355
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - Stanza model for Japanese (ja)
  - Downloads: 355
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 354
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - karakuri-lm-32b-thinking-2501-exp-gguf karakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-32b-thinking-2501-expã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 353
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 350
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - Japanese StableLM-3B-4E1T Instruct Model Description
  - Downloads: 349
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Tanuki-8x8Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„1.7Tãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸ8x8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç´„47Bã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç´„13Bï¼‰ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 348
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [grapevine-AI/plamo-2-translate-gguf](https://huggingface.co/grapevine-AI/plamo-2-translate-gguf)
  - caution!
  - Downloads: 345
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - luke-japanese-large luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 344
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - Qwen2.5 Bakeneko 32B Instruct V2 GGUF (rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - Downloads: 344
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - Mistral-Nemo-Japanese-Instruct-2408 Model Description
  - Downloads: 341
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleæ§˜ã® google/gemma-2-2b-jpn-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 333
- [mmnga/plamo-2-8b-gguf](https://huggingface.co/mmnga/plamo-2-8b-gguf)
  - plamo-2-8b-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹plamo-2-8bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 331
- [mmnga/Qwen3-4B-Instruct-2507-gguf](https://huggingface.co/mmnga/Qwen3-4B-Instruct-2507-gguf)
  - Qwen3-4B-Instruct-2507-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen3-4B-Instruct-2507ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 330
- [future-architect/Llama-3.1-Future-Code-Ja-8B](https://huggingface.co/future-architect/Llama-3.1-Future-Code-Ja-8B)
  - Llama 3.1 Future Code Ja Llama 3.1 Future Code Ja is a large language model with 8B parameters built on top of the Meta Llama 3.1 model.
  - Downloads: 326
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA-japanese-CodeLlama-7b-instruct-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 323
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - llm-jp-3-3.7b
  - Downloads: 321
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 320
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Nemo-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 318
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B Sarashina2-Vision-14B is a Japanese Large Vision Language Model trained by SB Intuitions.
  - Downloads: 316
- [mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-0.5b-instruct-v0.1-gguf)
  - sarashina2.2-0.5b-instruct-v0.1-gguf sbintuitionsã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.2-0.5b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 315
- [hotchpotch/japanese-reranker-base-v2](https://huggingface.co/hotchpotch/japanese-reranker-base-v2)
  - hotchpotch/japanese-reranker-base-v2 æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
  - Downloads: 313
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Borea-Phi-3.5-mini-Instruct-Commonã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 312
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - About static quants of https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 309
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - hubert-large-asr
  - Downloads: 305
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-8bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 300
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-1.7b-instruction-sft line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 299
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - WabiSabi-V1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹WabiSabi-V1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 299
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - wanabi-24B wanabi-24B ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 299
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmæ§˜ã® Llama-3-Swallow-8B-Instruct-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 299
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 295
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation on MIRACL japanese These models don't train on the MIRACL training data.
  - Downloads: 294
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM
  - Downloads: 292
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
  - Downloads: 291
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 285
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - google/gemma-3-27b-it-qat-q4_0-unquantizedã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹imatrixã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™This is a model that quantizes google/gemma-3-27b-it-qat-q4_0-unquantized using an imatrix that contains a lot of Japanese.https://huggingface.co/dahara1/imatrix-jpn-test).
  - Downloads: 274
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)
  - Downloads: 272
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - Uploaded model Developed by: nappa0326 License: apache-2.0 Finetuned from model : elyza/Llama-3-ELYZA-JP-8B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama-3-ELYZA-JP-8Bã‚’ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 266
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - Model card for model ID
  - Downloads: 265
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - DeepSeek-R1-Distill-Qwen-7B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 259
- [mradermacher/Japanese-Receipt-VL-3B-JSON-GGUF](https://huggingface.co/mradermacher/Japanese-Receipt-VL-3B-JSON-GGUF)
  - About static quants of https://huggingface.co/sabaridsnfuji/Japanese-Receipt-VL-3B-JSON For a convenient overview and download list, visit our model page for this model.
  - Downloads: 255
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA-japanese-Llama-2-7b-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 249
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-Large-Instruct-2407ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 249
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - mt5_summarize_japanese (Japanese caption : æ—¥æœ¬èªã®è¦ç´„ã®ãƒ¢ãƒ‡ãƒ«)
  - Downloads: 247
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - Finance Sentiment JA (base) Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
  - Downloads: 246
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - QwQ-32B-Preview-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹QwQ-32B-Previewã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 244
- [shisa-ai/shisa-v2-llama3.1-405b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 242
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - hubert-base-asr
  - Downloads: 237
- [mmnga/llm-jp-3.1-8x13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf)
  - llm-jp-3.1-8x13b-instruct4-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3.1-8x13b-instruct4ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 237
- [mmnga/llm-jp-3.1-13b-instruct4-gguf](https://huggingface.co/mmnga/llm-jp-3.1-13b-instruct4-gguf)
  - llm-jp-3.1-13b-instruct4-gguf llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-3.1-13b-instruct4ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 236
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - Wav2Vec2-Large-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
  - Downloads: 235
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - alabnii/jmedroberta-base-sentencepiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 234
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - albert-base-japanese-v1 æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™
  - Downloads: 233
- [webbigdata/VoiceCore](https://huggingface.co/webbigdata/VoiceCore)
  - News VoiceCoreãŒGENIACãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ(çµŒæ¸ˆç”£æ¥­çœã€NEDO)ã®å›½ç”£åŸºç›¤ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã«æ²è¼‰ã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 231
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39ã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-4k-instruct-dpoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 231
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi_mini_12b_GGUF wanabi_mini_12b_GGUF ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 230
- [prj-beatrice/japanese-hubert-base-phoneme-ctc](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc)
  - japanese-hubert-base-phoneme-ctc rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 229
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - Japanese GPT2 Lyric Model Model description
  - Downloads: 226
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - qwen2.5-bakeneko-32b-instruct-gguf rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹qwen2.5-bakeneko-32b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 226
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - oldï¼Ÿ
  - Downloads: 225
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - Model Card for Japanese BART base Model description This is a Japanese BART base model pre-trained on Japanese Wikipedia.
  - Downloads: 223
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B
  - Downloads: 223
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - SpeechT5 (TTS task) for Japanese SpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech)
  - Downloads: 219
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - Japanese Natural Language Inference Model
  - Downloads: 218
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - kurumi_flux_lora_v1.0ï¼ˆflux.1 ç³»çµ±ï¼‰ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€flux1.
  - Downloads: 218
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - RakutenAI-2.0-8x7B-instruct-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-2.0-8x7B-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 216
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 215
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 214
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - lightblue-suzume-llama-3-8B-japanese-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 210
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - japanese-stablelm-2-instruct-1_6b-gguf stabilityaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-stablelm-2-instruct-1_6bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 209
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 209
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-multilingualã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 204
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - æœ¬ggufãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ about this gguf model gemma-2-2b-itã‚’æ—¥æœ¬èªãŒå¤šãå«ã¾ã‚Œã‚‹é‡è¦åº¦è¡Œåˆ—(iMatrix)ã‚’ä½¿ã£ã¦é‡å­åŒ–ã—ãŸggufç‰ˆã§ã™ã€‚
  - Downloads: 204
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - Atotti/RakutenAI-2.0-mini-instruct-gguf æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Rakuten/RakutenAI-2.0-mini-instruct ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€llama.cpp ã‚„ text-generation-webui ç­‰ã®ãƒ„ãƒ¼ãƒ«ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã« GGUF å½¢å¼ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚
  - Downloads: 202
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 202
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - japanese-gpt2-medium-unidic This is a medium-sized Japanese GPT-2 model using BERT-like tokenizer.
  - Downloads: 201
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªç‰ˆã¯ã¾ã ä½œæˆä¸­ã§ã™ã€‚
  - Downloads: 201
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 201
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
  - Downloads: 198
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - wanabi-24B (preview) wanabi-24B ã¯ã€å°èª¬åŸ·ç­†æ”¯æ´ã«ç‰¹åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã® ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ (preview) ã§ã™ã€‚
  - Downloads: 197
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - DeepSeek-R1-Distill-Qwen-14B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-14Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 197
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaæ§˜ã® rinna/gemma-2-baku-2b-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 195
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - RetrievaEmbedding-01: AMBER The AMBER (Adaptive Multitask Bilingual Embedding Representations) is a text embedding model trained by Retrieva, Inc.
  - Downloads: 195
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8b-Cosmopedia-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 194
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - â—†QuinceMix "Defacta"ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 194
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakutenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹RakutenAI-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 193
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - Ninja-v1-NSFW-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFWã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 190
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Qwen2.5-72B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 188
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408 static quants are available at https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 187
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuisekiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 184
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - Japanese Parler-TTS Mini (Î²ç‰ˆ) ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-mini-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 182
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - æ—¥æœ¬èªåŒ»ç™‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ« æ¦‚è¦ ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶å®¤ã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹MedTxt-CRã‚’ç”¨ã„ã¦ã€alabniiã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹RoBERTaã‚’fine-tuningã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 182
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B Heron-NVILA-Lite-33B is a vision language model trained for Japanese, based on the NVILA-Lite architecture.
  - Downloads: 181
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen1.5-110B-Chatã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 180
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 179
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - stockmark-gpt-neox-japanese-1.4b-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gpt-neox-japanese-1.4bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 176
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 176
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - Model Card for NABLA-VL
  - Downloads: 174
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - License:CreativeML Open RAIL-M Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  - Downloads: 173
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - æ—¥æœ¬èªVL-T5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
  - Downloads: 171
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3.1-70B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 170
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - Original Model Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 169
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-base-japanese-with-auto-jumanpp Model description
  - Downloads: 166
- [webbigdata/VoiceCore_gguf](https://huggingface.co/webbigdata/VoiceCore_gguf)
  - VoiceCore GGUF - æ¬¡ä¸–ä»£ æ—¥æœ¬èªVoice AI Agentç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆggufé‡å­åŒ–ç‰ˆï¼‰ webbigdata/VoiceCoreã¯AIãŒè‡ªç„¶ãªæ—¥æœ¬èªã‚’ç™ºå£°å¯èƒ½ã«ã™ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªVoice AI Agentãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 166
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
  - Downloads: 163
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This is for (private) DEMO only.
  - Downloads: 160
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - Model Card For llm-jp-3-3.7b-instruct-gguf LLM-jpã•ã‚“ã®llm-jp-3-3.7b-instructã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
  - Downloads: 160
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 159
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Japanese-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 158
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - AXCXEPT-EZO-phi-4-v2_900-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-phi-4-v2_900ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 158
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - cyberagent-open-calm-3b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-3bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 157
- [prj-beatrice/japanese-hubert-base-phoneme-ctc-v2](https://huggingface.co/prj-beatrice/japanese-hubert-base-phoneme-ctc-v2)
  - japanese-hubert-base-phoneme-ctc rinna/japanese-hubert-base ã‚’ CTC ã§ã®æ—¥æœ¬èªéŸ³ç´ èªè­˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 157
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - QwQ-32B-gguf Qwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹QwQ-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 156
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 155
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - japanese-wav2vec2-large-rs35kh
  - Downloads: 154
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B static quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF Usage
  - Downloads: 154
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-32Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 153
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59362 Validation Metrics Loss: 0.13092292845249176 Accuracy: 0.9527127414314258 Precision: 0.9634070704982427 Recall: 0.9842171959602166 AUC: 0.9667289746092403 F1: 0.9737009564152002 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
  - Downloads: 153
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-7B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 152
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹stockmark-100bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 151
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - bert-base-japanese-wikipedia-ud-head Model Description
  - Downloads: 151
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 151
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - suzume_mix_v1.0ï¼ˆflux.1 ç³»ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼‰ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€flux1-dev ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€è¤‡æ•°ã®LoRAã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ–ãƒ¬ãƒ³ãƒ‰ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 149
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 æ˜ç¤ºçš„ãªè¨±è«¾ã‚’å¾—ãŸã‚ªãƒ—ãƒˆã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èª/è‹±èªãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«CLIP (Contrastive Language-Image Pre-training)ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 149
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - DeepSeek-R1-Distill-Qwen-1.5B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Qwen-1.5Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - Model Card For gemma-2-2b-jpn-it-gguf rinnaã•ã‚“ã®gemma-2-baku-2b-itã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
  - Downloads: 147
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 146
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - SakuraMixSeries èƒŒæ™¯ã¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒ¼ã‚’ä¸¡ç«‹ã•ã›ãŸVAEå†…è”µå‹ãƒ¢ãƒ‡ãƒ« Model with built-in VAE for both background and character quality ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡
  - Downloads: 146
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese License MIT License ğŸ‘‰ DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf ã“ã£ã¡ã®ãŒã„ã„ã‹ã‚‚ğŸ‘‰ mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf
  - Downloads: 145
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - wav2vec2-live-japanese https://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using the common_voice JSUT CSS10
  - Downloads: 144
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - cyberagent-open-calm-1b-gguf cyberagentã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹open-calm-1bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 143
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - Model card for model ID
  - Downloads: 139
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-base - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-base StableLM
  - Downloads: 137
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - RoBERTa base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
  - Downloads: 136
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha Model Details Japanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
  - Downloads: 135
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf AXCXEPTã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹phi-4-open-R1-Distill-EZOv1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 135
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - rinna/japanese-gpt-neox-3.6b-instruction-ppo rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6b-instruction-ppoã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 134
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfin-inst-mergeã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 134
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - llm-jp-3-172b-instruct3
  - Downloads: 134
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 133
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EZO-Humanities-9B-gemma-2-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 133
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf abejaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ABEJA-Qwen2.5-32b-Japanese-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 132
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfinã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 131
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Googleã®google/gemma-3-4b-itã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 131
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - XLNet-japanese Model description This model require Mecab and senetencepiece with XLNetTokenizer.
  - Downloads: 130
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - Fine-tuned XLSR-53 large model for speech diarization in Japanese phone-call 2 speakers diarization model which was fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using phone-call data CallHome.
  - Downloads: 127
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri: Japanese General Text Embeddings âš 
  - Downloads: 127
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹c4ai-command-r-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 123
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªåŒ»ç™‚LLM MedLlama3-JP ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Llama3ã®ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸï¼”ç¨®é¡ã®LLMã‹ã‚‰æˆã‚‹ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 122
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - llm-jp-3-7.2b-instruct This repository provides large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 121
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - Summary This is a LLaMA 3 Youko qlora fine-tune, created using a new version of the VNTL dataset.
  - Downloads: 121
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-breadcrumbsã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 121
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - nlp-waseda/bigbird-base-japanese Model description This is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 119
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Japanese-Chat-Umievo-itr001-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 118
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MS-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 117
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - line-corporation/japanese-large-lm-1.7b line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 117
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIæ§˜ã® EZO-Common-T2-2B-gemma-2-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 117
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [cardiffnlp/tweet-topic-base-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-base-multilingual)
  - tweet-topic-base-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-base language model trained rained on ~198M multilingual tweets and finetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 114
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 113
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - rinna/japanese-gpt-neox-3.6b rinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - Qwen2.5 Bakeneko 32B Instruct GPTQ int8 (rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - Downloads: 113
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplplã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Instruct-Jaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 112
- [efwkjn/whisper-ja-anime-v0.2](https://huggingface.co/efwkjn/whisper-ja-anime-v0.2)
  - For usage instructions follow openai/whisper-large-v3-turbo Turbo finetune with japanese tokenizer.
  - Downloads: 112
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIæ§˜ã® Llama-3.1-8B-EZO-1.1-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 112
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - Model Card for llm-jp-clip-vit-large-patch14 Model Details Japanese CLIP model trained with OpenCLIP on relaion2B-en-research-safe-japanese-translation, a Japanese translation of the English subset of ReLAION-5B (https://huggingface.co/datasets/laion/relaion2B-en-research-safe),
  - Downloads: 111
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Oumuamua-7b-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 111
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 110
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow Gemma-2-Llama-Swallow series was built by continual pre-training on the gemma-2 models.
  - Downloads: 110
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Model Details Model Description This repository provides Asagi-2B, a large-scale Japanese Vision &amp; Language Model (VLM).
  - Downloads: 109
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 108
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - nlp-waseda/roberta-large-japanese Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 108
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Model Details Model Description
  - Downloads: 108
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - shisa-7b-v1-gguf augmxntã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹shisa-7b-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 107
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KUJIRAã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 106
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 106
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 105
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - hotchpotch/japanese-reranker-xsmall-v2 ã¨ã¦ã‚‚å°ã•ãé€Ÿã„æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
  - Downloads: 105
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL++-Mã€ã®ç¯„å›²ã§ãƒ©ã‚¤ãƒ³ã‚»ãƒ³ã‚¹ã•ã‚Œã¾ã™ã€‚
  - Downloads: 104
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - DeepSeek-R1-Distill-Llama-8B-gguf deepseek-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹DeepSeek-R1-Distill-Llama-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 104
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - ç°¡å˜ãªç®—æ•°å•é¡Œã‚’è§£ã‘ã‚‹ã‚ˆã†ã« GRPO ã§å­¦ç¿’ã—ã¦ã¿ãŸã€‚
  - Downloads: 102
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
  - Downloads: 100
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - Model Card For llm-jp-3-1.8b-instruct-gguf LLM-jpã•ã‚“ã®llm-jp-3-1.8b-instructã‚’é‡å­åŒ–ã—ãŸã‚‚ã®ãŸã¡ã§ã™ã€‚
  - Downloads: 100
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 99
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 98
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Honyaku-13bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 97
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteusã®GGUFç‰ˆã§ã™ã€‚
  - Downloads: 96
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information japanese-stablelm-3b-4e1t-instruct - GGUF Model creator: stabilityai Original model: japanese-stablelm-3b-4e1t-instruct StableLM
  - Downloads: 95
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - About static quants of https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B weighted/imatrix quants are available at https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 91
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - GGUFç‰ˆã¯ã“ã¡ã‚‰ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF æ¦‚è¦ Mistral-nemoã‚’EPRç”¨é€”å‘ã‘ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ æ—¥æœ¬èªã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãŸã‚magnumã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ—¥æœ¬èªåŠ›ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã¯ãš Mistral-Nemoãƒ™ãƒ¼ã‚¹ãªã®ã§Temperatureã¯0.3ã‚’åŸºæº–ã«èª¿æ•´ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ system promptã«æ—¥æœ¬èªã§å‡ºåŠ›ã™ã‚‹æ—¨ã‚’è¨˜è¼‰ã™ã‚‹ã“ã¨ã§è‹±èªãŒæ··ã˜ã‚‹å•é¡Œã‚’æŠ‘åˆ¶ã§ãã¾ã™ v0.1ã‹ã‚‰ã®å¤‰æ›´ç‚¹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®system promptã«&lt;ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨€èª&gt;ã§å‡ºåŠ›ã™ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ  ã‚¨ãƒãƒƒã‚¯ã‚’9å€ã«å¢—åŠ  ä½¿ç”¨ã•ã›ã¦ã„ãŸã ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ kalomaze/Opus_Instruct_25k Nopm/Opus_WritingStruct anthracite-org/kalo-opus-instruct-22k-no-refusal Aratako/Synthetic-Japanese-Roleplay-NSFW-Claude-3.5s-15.3k-formatted
  - Downloads: 89
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 88
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-gguf matsuo-labã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹weblab-10b-instruction-sftã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 87
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Model Details Model Description This repository provides Asagi-8B, a large-scale Japanese Vision &amp; Language Model (VLM).
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - llm-jp-3-13b-instruct2 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - Feedback and support: TensorBlock's Twitter/X, Telegram Group and Discord server cyberagent/Mistral-Nemo-Japanese-Instruct-2408 - GGUF
  - Downloads: 87
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g.
  - Downloads: 86
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-v1-7B-gguf SakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Assistance ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 84
- [Aratako/Amaterasu-123B](https://huggingface.co/Aratako/Amaterasu-123B)
  - Amaterasu-123B GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€mistralai/Mistral-Large-Instruct-2411ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥è‹±æ··åˆã®ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚„å°èª¬åŸ·ç­†ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 84
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B ã®GGUFé‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 84
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - Whatâ€™s this?
  - Downloads: 83
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - 4-bit é‡å­åŒ–ç‰ˆ llm-jp-3-172b-instruct3 æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äººæƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€ŒNIIã€ï¼‰ãŒæä¾›ã™ã‚‹ã€Œllm-jp-3-172b-instruct3ã€(ä»¥ä¸‹ã€Œæœ¬ãƒ¢ãƒ‡ãƒ«ã€) ã‚’ 4-bit é‡å­åŒ–ã—ãŸæ´¾ç”Ÿãƒ¢ãƒ‡ãƒ« (ä»¥ä¸‹ã€Œæœ¬é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€) ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 83
- [Parakeet-Inc/furigana_whisper_small_jsut](https://huggingface.co/Parakeet-Inc/furigana_whisper_small_jsut)
  - æ¦‚è¦ æ—¥æœ¬èªã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€æ›¸è¨˜ç´ åˆ—ï¼ˆæ¼¢å­—ä»®åäº¤ã˜ã‚Šæ–‡ï¼‰ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å…¥ã‚Œã‚‹ã“ã¨ã§ã€æ›¸è¨˜ç´ åˆ—ã¨æ•´åˆæ€§ã®ã‚ã‚‹ãƒ¢ãƒ¼ãƒ©åˆ—ï¼ˆã‚«ã‚¿ã‚«ãƒŠåˆ—ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 82
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 82
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressive-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 81
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 80
- [Atotti/miipher-2-HuBERT-HiFi-GAN-v0.1](https://huggingface.co/Atotti/miipher-2-HuBERT-HiFi-GAN-v0.1)
  - Speech Enhancement Model ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã¨è‹±èªã§å°è¦æ¨¡ã«å­¦ç¿’ã•ã‚ŒãŸéŸ³å£°å¾©å…ƒãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 80
- [buildertakuya/Berghof-Takuya7B](https://huggingface.co/buildertakuya/Berghof-Takuya7B)
  - Berghof Takuya 7B Model Description Elizezen/Berghof-NSFW-7Bã«å¯¾ã—ã€åŒäººæ‹“ä¹Ÿã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ãƒ‘ã‚­ã‚®ãƒ¡æ±šæŸ“ï¼ˆç¶™ç¶šäº‹å‰å­¦ç¿’ï¼‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - rinna/nekomata-7b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-7b-instruction.
  - Downloads: 79
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - alabnii/jmedroberta-base-sentencepiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 79
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - luke-large-defamation-detection-japanese æ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºå™¨
  - Downloads: 77
- [pfnet/plamo-2-translate-base](https://huggingface.co/pfnet/plamo-2-translate-base)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 75
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-13b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 74
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
  - Downloads: 74
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-RobinHoodã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 73
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - Model Card for Japanese character-level GPT-2 Medium Model description This is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 72
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwm Model description This is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 71
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - VITS TTS Japanese Only Amitaro VITS TTS model finetuned using free voice data from amitaro free voice here ã‚ã¿ãŸã‚ã®å£°ç´ æå·¥æˆ¿ Finetuning code is from Plachtaa - VITS Fast Fine-tuning See sample usage Lycoris53/VITS-TTS-Japanese-Only-Amitaro Model Details 76 annotated wav file train for 600 epoch æ—¥æœ¬èªã®èª¬æ˜ãªã©ã“ã¡ã‚‰ã« AiThinkso.net Developed by:
  - Downloads: 71
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-small Fine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 71
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - llm-jp-3-8x1.8b-instruct3 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 71
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 71
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - Model card for model ID
  - Downloads: 70
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 70
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer â™»
  - Downloads: 70
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medllm - Build with Llama3-8B ELAINE (EngLish-jApanese-chINesE)-
  - Downloads: 70
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - ELYZA-japanese-CodeLlama-7b-gguf ELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 69
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 69
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - DeepSeek-R1-Distill-Qwen-14B-Japanese GGUF Model Description
  - Downloads: 68
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Vecteus-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 68
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT Model The RetrievaBERT is the pre-trained Transformer Encoder using Megatron-LM.
  - Downloads: 66
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - ã€ŒLLM-jp-3 172Bã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172Bã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 66
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - luke-japanese-large-lite luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 65
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - å›ç­”ã¨å›ç­”ãŒå‡ºã¦ãã‚‹ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ä¸ãˆã‚‹ã¨è³ªå•æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://github.com/sonoisa/deep-question-generation æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚¹ãƒ†ãƒƒãƒ—æ¦‚è¦ SQuAD 1.1ã‚’æ—¥æœ¬èªã«æ©Ÿæ¢°ç¿»è¨³ã—ã€ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯ç´„åŠåˆ†ï¼‰ã€‚
  - Downloads: 65
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card ãƒ¢ãƒ‡ãƒ«æƒ…å ± / Model Information ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Meta AI ã® Llama 3.1 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 65
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - roberta_qa_japanese (Japanese caption : æ—¥æœ¬èªã® (æŠ½å‡ºå‹) è³ªå•å¿œç­”ã®ãƒ¢ãƒ‡ãƒ«)
  - Downloads: 64
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - â—†REV-Mix "ãƒ¬ãƒœãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 64
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 63
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - Model card for model ID
  - Downloads: 63
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V2-7B ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 63
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-medium-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 62
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - One more step before getting this model.
  - Downloads: 62
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This is a ChatML model.
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-liteã®é‡ã¿ã®åå‰ã‚’XLMRobertaå½¢å¼ã«ç½®ãæ›ãˆã€XLMRobertaãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸç‰©ã§ã™ã€‚
  - Downloads: 62
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - ğŸˆ FlexDreamHK FlexDreamHKã¯ãƒªãƒ¼ã‚¯ã•ã‚ŒãŸNovelAIãƒ¢ãƒ‡ãƒ«ã®å…¥ã£ã¦ã„ãªã„ã€ã‚ã‚‹ã„ã¯ãã®ãƒªã‚¹ã‚¯ã‚’å¯èƒ½ãªé™ã‚Šä½ãã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 61
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - BERT for Japanese Twitter
  - Downloads: 61
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 61
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - Whatâ€™s this?
  - Downloads: 60
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 59
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository contains some GGUF quantizations of the VNTL Gemma 2 27B model.
  - Downloads: 59
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - ã€ŒLLM-jp-3 172B beta2ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta2ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.
  - Downloads: 58
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 58
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - Stockmark-2-100B-Instruct-beta-gguf stockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Stockmark-2-100B-Instruct-betaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 58
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - bert-base-japanese-v3-jcommonsenseqa ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(å¤šè‚¢é¸æŠå¼è³ªå•å¿œç­”)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 57
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B (rinna/qwen2.5-bakeneko-32b)
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 57
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - Qwen3-8B-RP-v0.1 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ Qwen/Qwen3-8Bã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 56
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - About static quants of https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Model Details Model Description This repository provides Asagi-4B, a large-scale Japanese Vision &amp; Language Model (VLM).
  - Downloads: 56
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - Canary-TTS-150M llm-jp/llm-jp-3-150m-instruct3 ã‚’ãƒ™ãƒ¼ã‚¹ã«å­¦ç¿’ã—ãŸTTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 55
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - calm3-22b-RP-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version ã¾ãŸã€ã“ã¡ã‚‰ã§æœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 55
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - Japanese GPT2 Lyric Model Model description
  - Downloads: 54
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 54
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instructã‚’CoTãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸreasoningãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 54
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
  - Downloads: 53
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 53
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - æ—¥æœ¬èªT5 Prefix Language Model
  - Downloads: 52
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B-GGUF GGUF conversion of "Japanese-WizardLM2-ChatV-7B" This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.
  - Downloads: 52
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-gguf ELYZA-japanese-Llama-2-13b-fast-instructã® GGUF å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 52
- [hotchpotch/japanese-reranker-small-v2](https://huggingface.co/hotchpotch/japanese-reranker-small-v2)
  - hotchpotch/japanese-reranker-small-v2 å°ã•ã‚ã§é€Ÿã„ã€æ—¥æœ¬èªãƒªãƒ©ãƒ³ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º(v2)ã§ã™ã€‚
  - Downloads: 52
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - merge This is a merge of pre-trained language models created using mergekit.
  - Downloads: 52
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 static quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF Usage
  - Downloads: 52
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - rinna/nekomata-14b-instruction-gguf Overview The model is the GGUF version of rinna/nekomata-14b-instruction.
  - Downloads: 51
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 50
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 50
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
  - Downloads: 50
- [pfnet/plamo-2-translate-eval](https://huggingface.co/pfnet/plamo-2-translate-eval)
  - PLaMo Translation Model PLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 50
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-KUJIRA ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 49
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF æ¦‚è¦ Aratako/Ninja-v1-RP-expressiveã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 49
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
  - Downloads: 49
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - ã¯ã˜ã‚ã« Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 49
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - æ›´æ–°æƒ…å ± æ—¥æœ¬èªæ©Ÿèƒ½ã¨instructãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã—ãŸver.2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2 ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€ Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 49
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 49
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - datasets: https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - Model Card for Japanese DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 47
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-A-v1-7B-gguf SakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-A-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 47
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - Japanese CLIP ViT-H/14 (Deeper) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation
  - Downloads: 47
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallow-70B-Instruct-v0.1 This is a merge of pre-trained language models created using mergekit.
  - Downloads: 47
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - sarashina2.1-1b-sft-gguf Aratakoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹sarashina2.1-1b-sftã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 47
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - Whatâ€™s this?
  - Downloads: 46
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - bert-base-irony
  - Downloads: 46
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF æ¦‚è¦ Aratako/Ninja-v1-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 45
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€MARC-ja(positive or negativeã®äºŒå€¤åˆ†é¡)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 45
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ cyberagentã«ã‚ˆã‚‹deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã®æ—¥æœ¬èªè¿½åŠ å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã®AWQé‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 45
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Ninja-V3 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 45
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - zenz-v1 zenz-v1ã¯GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€ Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ Twitter/twhin-bert-large ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸ
  - Downloads: 44
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8x8B-dpo-v1.0ã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 44
- [pfnet/plamo-2.1-2b-cpt](https://huggingface.co/pfnet/plamo-2.1-2b-cpt)
  - PLaMo 2.1 2B Model Description PLaMo 2.1 2B is a model developed by Preferred Elements Inc.
  - Downloads: 43
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
  - Downloads: 43
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha Mixtral-8x7B-Instruct-v0.1-japanese-alphaã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸå­¦ç¿’é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 43
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - whisper-large-v2-mix-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
  - Downloads: 42
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - Japanese Stock Comment Sentiment Model
  - Downloads: 42
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "å½±" in Japanese or "Shadow" in English.
  - Downloads: 42
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - Model card for model ID
  - Downloads: 42
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ã‚‰ã—ã•ã‚’ä»˜ä¸ã—ãŸé›‘è«‡å¿œç­”ã®ç”Ÿæˆ
  - Downloads: 42
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯llama3.1-8B-instructã‚’ã‚‚ã¨ã«æ—¥æœ¬èªæ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã«Mergekit&amp;ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç”¨ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - æ¦‚è¦ elyza/Llama-3-ELYZA-JP-8Bã‚’å…ƒã«chat vectorã‚’ç”¨ã„ã¦æ”¹è‰¯ã—AItuberã«ç‰¹åŒ–ã•ã›ã¾ã—ãŸã€‚
  - Downloads: 42
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.
  - Downloads: 41
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1 æ—¥æœ¬èªç‰ˆã¯è¿‘æ—¥å…¬é–‹äºˆå®šã§ã™ï¼ˆæ—¥æœ¬èªã‚’å‹‰å¼·ä¸­ãªã®ã§ã€é–“é•ã„ã¯ã”å®¹èµ¦ãã ã•ã„ï¼
  - Downloads: 41
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - About static quants of https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF Usage
  - Downloads: 41
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
  - Downloads: 41
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 40
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This pre-trained model is work in progress!
  - Downloads: 40
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max This is a Japanese vision-language model based on LLaVA architecture with 1.3B parameters.
  - Downloads: 40
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) CoolJapanDiffusion 2.1.1ã¨WaifuDiffusion 1.4 anime epoch2ã®ãƒãƒ¼ã‚¸ã€‚
  - Downloads: 39
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for VecTeus-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 VecTeus has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 39
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - bart-base-japanese-news(base-sized model)
  - Downloads: 39
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-large-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 38
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - We initialize SPLADE-japanese from tohoku-nlp/bert-base-japanese-v2.
  - Downloads: 38
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - roberta-base-japanese-jsnli This model is a fine-tuned version of nlp-waseda/roberta-base-japanese on the JSNLI dataset.
  - Downloads: 37
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 37
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - ELECTRA base Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 37
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - Model Card for llm-jp-clip-vit-base-patch16 Model Details Japanese CLIP model trained with OpenCLIP on relaion2B-en-research-safe-japanese-translation, a Japanese translation of the English subset of ReLAION-5B (https://huggingface.co/datasets/laion/relaion2B-en-research-safe),
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime ã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 36
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False
  - Downloads: 36
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - Model description Cyberagentæ§˜ã®cyberagent/calm2-7b-chatã‚’è¿½åŠ å­¦ç¿’ã—ãŸã€ä½œå®¶ã•ã‚“ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆAIã®ã‚¢ãƒ«ãƒ•ã‚¡ç‰ˆã§ã™ã€‚
  - Downloads: 36
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
  - Downloads: 36
- [yasu-oh/Llama-3-Swallow-Infused-R1776-70B](https://huggingface.co/yasu-oh/Llama-3-Swallow-Infused-R1776-70B)
  - Llama-3-Swallow-Infused-R1776-70B Overview Llama-3-Swallow-Infused-R1776-70B is a 70B parameter merged model built on Meta's Llama 3 architecture.
  - Downloads: 36
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - DeepSeek-R1-Distill-Qwen-32B-Japanese GGUF Model Description
  - Downloads: 36
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 36
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - MobileBERT æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çˆ†èª•ï¼ï¼
  - Downloads: 35
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset ) ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒãƒ™ãƒ«é¢¨ç”»åƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§naver-clova-ix/donut-baseã‚’è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 35
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - c4ai-command-r-v01-japanese-instruct-GGUF æ¦‚è¦ Aratako/c4ai-command-r-v01-japanese-instructã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 35
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 35
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - kakuyomu-genre-bert å°èª¬ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„ç´¹ä»‹æ–‡ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’åˆ†é¡ã™ã‚‹ BERT æ±åŒ—å¤§ã® cl-tohoku/bert-base-japanese-char-v3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 35
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 34
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0
  - Downloads: 34
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - AXCXEPT/EZO2.5-gemma-3-12b-it-Preview Model Details æ˜¨ä»Šç™»å ´ã—ãŸLLMè‡ªèº«ã®åŠ›ã‚’è‡ªåŠ›ã§å‘ä¸Šã•ã›ã‚‹ã€ŒGRPOã€ã‚„ã€ŒPPOã€ã®æ¦‚å¿µã‚’ã€ å¼Šç¤¾ã§é–‹ç™ºã—ãŸã€ŒEZOã€ã¨ã„ã†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã«ãƒŸãƒƒã‚¯ã‚¹ã™ã‚‹ã“ã¨ã§ã€ 3,000ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€2æ™‚é–“Ã—H200Ã—8å°ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€Japanese MT BenchãŠã‚ˆã³ã€Elyza Tasks100ã«ãŠã‘ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªæ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.
  - Downloads: 33
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - alpaca-guanaco-japanese-gpt-1b 1.3Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªGPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå¯¾è©±AIã§ã™ã€‚
  - Downloads: 33
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - ğŸ“‘ Paper | ğŸ¤—
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIæ§˜ã® Llama-3-EZO-8b-Common-it ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 33
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - bert-base-japanese-char-extended Model Description
  - Downloads: 32
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B Please check our blog post for more details, samples, evaluations and more: Blogpost Model Description Genji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
  - Downloads: 32
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 32
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese for Irony
  - Downloads: 32
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - k-ush/xlm-roberta-base-ance-en-jp-warmup A XLM-RoBERTa-base model trained on mMARCO Japanese dataset with ANCE warmup script.
  - Downloads: 32
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰/Japanese model card æ—¥æœ¬èªã®ãƒ–ãƒ­ã‚°/Full Japanese dev blog Development source code/é–‹ç™ºã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ Karasu-DPO-7B
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix æ¦‚è¦ / Overview Yaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 32
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 31
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - (English part follows Japanese one.
  - Downloads: 31
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 31
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - (English part follows Japanese one.
  - Downloads: 31
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - Model Card for Tanrei/GPTSAN-japanese General-purpose Swich transformer based Japanese language model GPTSAN has some unique features.
  - Downloads: 30
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - DeBERTa V2 small Japanese This is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 30
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-70b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 30
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - bert-base-sudachitra-v11
  - Downloads: 30
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - æ›´æ–°å±¥æ­´ 2023å¹´5æœˆ7æ—¥ ã€Œoasst1-89k-jaã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ã¾ã—ãŸã€‚
  - Downloads: 30
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 Shisa V2 is a family of bilingual Japanese and English (JA/EN)
  - Downloads: 30
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - ã¯ã˜ã‚ã« ãªã‚“ã‹æ—¥æœ¬èªãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚
  - Downloads: 30
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 Model Application
  - Downloads: 30
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 ğŸš¨ If you want to avoid outputs that appear to be literal translations, please prompt this model to role-play as a Japanese person.
  - Downloads: 30
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - Japanese-Novel-Reward-sarashina2.1-1b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/sarashina2.1-1bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 30
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - AXCXEPT/phi-4-open-R1-Distill-EZOv1 Model Details This model is a Reasoner version of the phi-4 model by employing open-r1, which mimics the Distill methodology of Deepseek-R1.
  - Downloads: 30
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7Bã‚’ä¼šè©±ã§ãã‚‹ã‚ˆã†ã«ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 30
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 29
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - yacis-electra-small-cyberbullying
  - Downloads: 29
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - Tanuki-ZeRo-gguf kanhatakeyamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Tanuki-ZeRoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 29
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - ã“ã¡ã‚‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã€civitaiã«ã¦å…ˆã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 29
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAEã®å†…è‡“ã¯ãªã„ãï¼ã¨è¨€ã‚ã›ãªã„ãï¼ï¼ï¼ï¼
  - Downloads: 29
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - matsuolab-weblab-10b-gguf matsuo-labã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹weblab-10bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 29
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - åè¨€æ¨è«–ãƒ¢ãƒ‡ãƒ«
  - Downloads: 28
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - whisper-large-v2-japanese-5k-steps This model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
  - Downloads: 28
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 28
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - jpn-heb source group: Japanese target group:
  - Downloads: 28
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - sarashina2.2-3b-RP-v0.1 GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ sbintuitions/sarashina2.2-3b-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 28
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer This model is a Phoneme Level Speech Recognition network, originally a fine-tuned version of openai/whisper-large-v3 on a mixture of Different Japanese datasets.
  - Downloads: 28
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 27
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - nlp-waseda/gpt2-xl-japanese This is Japanese GPT2 with approximately 1.5B parameters pretrained on Japanese Wikipedia and CC-100
  - Downloads: 27
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - Japanese-Alpaca-2-13B-GGUF Japanese-Alpaca-2-13B-GGUFã¯Japanese-Alpaca-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 27
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This is a Japanese+English sentence-BERT model.
  - Downloads: 27
- [mpasila/gemma-3-JP-EN-Translator-v1-4B](https://huggingface.co/mpasila/gemma-3-JP-EN-Translator-v1-4B)
  - Uploaded finetuned gemma-3-JP-EN-Translator-v1-4B model Prompt format: ChatML Recommended system prompt: You are a helpful assistant that translates Japanese to English.
  - Downloads: 27
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False
  - Downloads: 27
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - æ—¥æœ¬èªByT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 27
- [UEC-InabaLab/Llama-3.1-KokoroChat-Low](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Low)
  - ğŸ§  Llama-3.1-KokoroChat-Low: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-Low is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
  - Downloads: 27
- [UEC-InabaLab/Llama-3.1-KokoroChat-High](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-High)
  - ğŸ§  Llama-3.1-KokoroChat-High: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-High is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
  - Downloads: 27
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - llm-jp-3-980m-instruct2 LLM-jp-3 is the series of large language models developed by the Research and Development Center for Large Language Models at the National Institute of Informatics.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 27
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 26
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - alabnii/jmedroberta-base-manbyo-wordpiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 26
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - Model Card for Japanese character-level GPT-2 Large Model description
  - Downloads: 26
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)
  - Downloads: 26
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUFã¯Japanese-LLaMA-3-8B-Instruct-v2ã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 26
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - roberta-base-japanese-juman-ud-goeswith Model Description
  - Downloads: 25
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - japanese-sexual-moderation-v2ã¯ã€studio-ousia/luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp Model description
  - Downloads: 25
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - bert-base-japanese-unidic-luw-upos Model Description
  - Downloads: 25
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - Wav2Vec2-Large-XLSR-53-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT dataset{s}.
  - Downloads: 25
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - Japanese CLIP ViT-H/14 (Base) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
  - Downloads: 25
- [UEC-InabaLab/Llama-3.1-KokoroChat-Full](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-Full)
  - ğŸ§  Llama-3.1-KokoroChat-Full: Japanese Counseling Dialogue Model Llama-3.1-KokoroChat-Full is a large-scale Japanese language model fine-tuned on the entire KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
  - Downloads: 25
- [tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF](https://huggingface.co/tensorblock/ELYZA-japanese-Llama-2-13b-instruct-GGUF)
  - Feedback and support: TensorBlock's Twitter/X, Telegram Group and Discord server elyza/ELYZA-japanese-Llama-2-13b-instruct - GGUF
  - Downloads: 25
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒãƒ¼ã‚¸ãªã©ã‚’ç”¨ã„ä½œæˆã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã«å¯¾å¿œã—ã¦ã„ã‚‹Llama-3ãƒ™ãƒ¼ã‚¹ã®ï¼”ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é€²åŒ–çš„ãƒãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§tohoku-nlp/bert-base-japanese-v3ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 25
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 24
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja Model
  - Downloads: 24
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - dolly-japanese-gpt-1b-clone æ¦‚è¦ rinnaç¤¾ã®ã€Œjapanese-gpt-1bã€ã‚’ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€Œdatabricks-dolly-15k-jaã€ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã•ã›ãŸæ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - japanese-soseki-gpt2-1b
  - Downloads: 24
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - yacis-electra-small
  - Downloads: 24
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - ğŸŒŸ Ojisanæ§‹æ–‡å¤‰æ›ãƒ¢ãƒ‡ãƒ« (GRPO + Unsloth + LoRA) ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€æ–‡ç« ã‚’ã€ŒãŠã˜ã•ã‚“æ§‹æ–‡ã€ã«å¤‰æ›ã™ã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆãƒ»å­¦ç¿’ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚
  - Downloads: 24
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
  - Downloads: 24
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - ã€ŒLLM-jp-3 172B beta1ã€åˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¤§å­¦å…±åŒåˆ©ç”¨æ©Ÿé–¢æ³•äºº æƒ…å ±ãƒ»ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶æ©Ÿæ§‹ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆä»¥ä¸‹ã€Œæä¾›è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã«ã‚ˆã‚‹é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-3 172B beta1ã€ï¼ˆä»¥ä¸‹ã€Œæœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest â™»
  - Downloads: 24
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints ã‚’ optimum ç”¨ã« ONNX ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®AWQ 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 24
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2 Japanese base model version 2 Prerequisites transformers==4.19.2 Model architecture This model uses GPT2 base setttings except vocabulary size.
  - Downloads: 23
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech-v0.1 Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
  - Downloads: 23
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 23
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - æ—¥æœ¬èªå‘ã‘ Llama 3 8B ã¯ã˜ã‚ã« ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯Llama 3ã‚’æ—¥æœ¬èªåŒ–ã—ã‚ˆã†ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
  - Downloads: 23
- [OmniAICreator/Galgame-Llasa-3B](https://huggingface.co/OmniAICreator/Galgame-Llasa-3B)
  - Galgame-Llasa-1B Overview This is a Text-to-Speech (TTS) model fine-tuned for Japanese.
  - Downloads: 23
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True
  - Downloads: 23
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - Japanese-LLaMA-2-13B Japanese-LLaMA-2-13Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 23
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - Whisper Large V3 Japanese Phone Accent
  - Downloads: 23
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - kishizaki-sci/phi-4-AWQ-4bit-EN-JP model information phi-4ã‚’AutoAWQã§4bit é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚
  - Downloads: 23
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - sarashina2.2-3b-instruct-v0.1-GGUF base_model: sbintuitions/sarashina2.2-3b-instruct-v0.1 imatrix: TFMC/imatrix-dataset-for-japanese-llm
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - BERT base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.
  - Downloads: 22
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-largeã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 22
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-RobinHood ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 22
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
  - Downloads: 22
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39æ§˜ã® Tora-7B-v0.1 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 22
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - reazonspeech-espnet-next ReazonSpeech is a project to maintain freely-available Japanese audio datasets and ML models.
  - Downloads: 22
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-with-auto-jumanpp Model description
  - Downloads: 22
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - deberta-small-japanese-aozora Model Description
  - Downloads: 22
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - BERT base Japanese model This repository contains a BERT base model trained on Japanese Wikipedia dataset.
  - Downloads: 22
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to solve error detection and correction task.
  - Downloads: 22
- [mpasila/shisa-base-7b-v1-exl2-4bpw](https://huggingface.co/mpasila/shisa-base-7b-v1-exl2-4bpw)
  - This is an ExLlamaV2 quantized model in 4bpw of augmxnt/shisa-base-7b-v1 using the default calibration dataset.
  - Downloads: 22
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - (English part follows Japanese one.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - makiart/jp-ModernBert-base-preview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ABCI ç”ŸæˆAIãƒãƒƒã‚«ã‚½ãƒ³ã«ã¦æä¾›ã•ã‚ŒãŸè¨ˆç®—è³‡æºã«ã‚ˆã£ã¦Algomaticãƒãƒ¼ãƒ ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 22
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 22
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - Model description This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on my collection of Public Japanese Voice datasets for research Common Voice 7.0, JUST (Japanese speech corpus of Saruwatari-lab.
  - Downloads: 21
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - Model card for model ID
  - Downloads: 21
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 21
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - friendly_JA-Model (T5 fine-tuned model) MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexicon Examples input output æœ€é©åŒ–ã‚’å¿œç”¨ã—ãŸæ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾åº¦ã  ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿œç”¨ã—ãŸãƒã‚·ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ã  å½¼ã¯æ¶ç©ºã®ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹ å½¼ã¯ã‚¤ãƒã‚¸ãƒŠãƒªãƒ¼ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹ æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«æ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸ ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«ã‹ã‹ã£ã¦ã—ã¾ã£ãŸ æ·±å±¤å­¦ç¿’ã¯é›£ã—ã„ ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚€ãšã‹ã—ã„ æ–°ãŸãªæ¦‚å¿µã‚’ç´¹ä»‹ã™ã‚‹ æ–°ã—ã„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç´¹ä»‹ã™ã‚‹ æ´¥æ³¢ã®è­¦å ±ãŒæµã‚ŒãŸ ãƒ„ãƒŠãƒŸã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒæµã‚ŒãŸ å—æµ·ãƒˆãƒ©ãƒ•ã®ç½å®³ã¯éœ‡æºåœ°ã«ã‚ˆã‚‹ å—æµ·ãƒˆãƒ©ãƒ•ã®ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒ¼ã¯ã‚¨ãƒ”
  - Downloads: 21
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
  - Downloads: 21
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 21
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero Base model: llm-jp/llm-jp-13b-v1.0 Instruction data: Randomly sampled, 15k Jaster dataset (train) Code is here.
  - Downloads: 21
- [umisetokikaze/NinjaV1-pre](https://huggingface.co/umisetokikaze/NinjaV1-pre)
  - è£ã§ä½œã£ã¦ã„ãŸãƒ¢ãƒ‡ãƒ«ã¨VT1ã‚’ãƒãƒ¼ã‚¸ã—ãŸã ã‘ã€‚
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - NLLB 1.3B fine-tuned on Japanese to English Light Novel translation This model was fine-tuned on light and web novel for Japanese to English translation.
  - Downloads: 20
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - Model card for model ID
  - Downloads: 20
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF Model creator: MaziyarPanahi Original model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 Description MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.
  - Downloads: 20
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 20
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - Model Card for Japanese BART large Model description
  - Downloads: 20
- [nitky/Llama-3.3-SuperSwallowX-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallowX-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallowX-70B-Instruct-v0.1 ğŸ’¡ An experimental merging method is being used to combine different Japanese continuous pre-training models.
  - Downloads: 20
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE overview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯AItuberã®é­‚ã¨ãªã‚‹ã“ã¨ã‚’ç›®çš„ã«SB intuitionsã®sarashina-2.2-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«Unsothã¨Mergekit-MoEã‚’ç”¨ã„ã¦ä½œã‚‰ã‚Œã¾ã—ãŸã€‚
  - Downloads: 20
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - roberta-large-japanese-juman-ud-goeswith Model Description
  - Downloads: 20
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct ğŸ MambaSan-instruct is the first chat Japanese language model based on a state-space model architecture (Mamba), not a transformer.
  - Downloads: 20
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2ã‚’instructionç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§sftã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ base: https://huggingface.co/if001/llama2_ja_small trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§ https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
  - Downloads: 20
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - Mixtral-8x7B-v0.1-japanese Mixtral-8x7B-v0.1-japaneseã¯Mixtral-8x7B-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 20
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - Kuzushiji 49 MNIST FCN Model Overview This repository contains a Fully Convolutional Neural Network (FCN) model for the Kuzushiji 49 MNIST dataset.
  - Downloads: 20
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This model is traned with llm-japanese-dataset dataset.
  - Downloads: 20
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - ãƒ¢ãƒ‡ãƒ« ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instruct å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-ja å­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚µãƒ³ãƒ—ãƒ« import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
  - Downloads: 20
- [Aratako/MistralPrism-24B](https://huggingface.co/Aratako/MistralPrism-24B)
  - MistralPrism-24B GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€mistralai/Mistral-Small-3.1-24B-Instruct-2503ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Aratako/Mistral-Small-3.1-24B-RPã«å¯¾ã—ã¦ã€æµ·å¤–è£½ãƒ¢ãƒ‡ãƒ«è¤‡æ•°ã¨ã®ãƒãƒ¼ã‚¸ã‚’è¡Œã„æ€§èƒ½å¼·åŒ–ã‚’å›³ã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 20
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - makiart/jp-modernbert-large-preview ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ABCI ç”ŸæˆAIãƒãƒƒã‚«ã‚½ãƒ³ã«ã¦æä¾›ã•ã‚ŒãŸè¨ˆç®—è³‡æºã«ã‚ˆã£ã¦Algomaticãƒãƒ¼ãƒ ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF base_model: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4 imatrix: TFMC/imatrix-dataset-for-japanese-llm
  - Downloads: 20
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 20
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - 2025 å¹´ã®ã‚¨ã‚¤ãƒ—ãƒªãƒ¼ãƒ«ãƒ•ãƒ¼ãƒ«ãƒã‚¿æ ã§ã—ãŸ ğŸ‰ğŸ‰ Saikyou Shield 30M ğŸ‰ğŸ‰ ğŸ”¥ å±é™ºãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’100%æ¤œå‡ºã§ãã‚‹æœ€å¼·ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ« ğŸ”¥ Jailbreak ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã€ã‚ã‚‰ã‚†ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å±é™ºã¨åˆ†é¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ï¼
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - AIBunChoæ§˜ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ« (https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - Downloads: 20
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: æ—¥æœ¬èªã§è³ªå•ã™ã‚‹ã¨ã€æ—¥æœ¬èªã§å›ç­”ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚
  - Downloads: 20
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã‹ã‚‰140,000ä»¶ã»ã©ã®ESã‚’ç”¨ã„ã¾ã—ãŸã€‚
  - Downloads: 19
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 19
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 19
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0 ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
  - Downloads: 19
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Japanese CLIP ViT-H/14 (Wider) Table of Contents Overview Usage Model Details Evaluation Limitations and Biases Citation See Also Contact Information Overview Developed by:
  - Downloads: 19
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM
  - Downloads: 19
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b Model Description ELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 19
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - bert-base-japanese-v3-bpr-question-aio ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®è³ªå•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
  - Downloads: 19
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - nlp-waseda/gpt2-small-japanese-wikipedia This model is Japanese GPT-2 pretrained on Japanese Wikipedia.
  - Downloads: 19
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - Japanese Parler-TTS Large (Î²ç‰ˆ) ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€parler-tts/parler-tts-large-v1ã‚’åŸºã«ã€æ—¥æœ¬èªã§ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ˆã†å†å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 19
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - ku-accms/bert-base-japanese-ssuw Model description This is a pre-trained Japanese BERT base model for super short unit words (SSUW).
  - Downloads: 19
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - Electra Base Japanese Irony
  - Downloads: 19
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - NLLB-200 1.3B fine-tuned on Ascendance of a Bookworm
  - Downloads: 19
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - fasttext-jp-embedding This model is experimental.
  - Downloads: 19
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 19
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - Details: https://spacy.io/models/ja#ja_core_news_md Japanese pipeline optimized for CPU.
  - Downloads: 19
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False
  - Downloads: 19
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 19
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-fastã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-fast-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - DataPilot/sarashina2.2-3Bx8-moe DataPilot/sarashina2.2-3Bx8-moe ã¯ã€sbintuitions/sarashina2.2-3b-instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€mergekit-moeã‚’ç”¨ã„ã¦8ã¤ã®å°‚é–€ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸMixture of Expertsï¼ˆMoEï¼‰å‹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion Model Card SFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - doshisha-mil/llama-2-70b-chat-4bit-japanese-v1
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance ã®GGUFç‰ˆ Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - Japanese DialoGPT trained with Aozora (ja) é’ç©ºæ–‡åº«ã®ã‚»ãƒªãƒ•ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®DialoGPT Smallã§ã™(en) Japanese DialoGPT Small trained on Aozora Bunko.
  - Downloads: 18
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - Deepreneur-blue-lizard-gguf Deepreneurã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹blue-lizardã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 18
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 18
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - tiny_mixtral_jaã‚’instructionç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§trainingã—ãŸã‚‚ã®ã§ã™https://huggingface.co/if001/tiny_mixtral_ja
  - Downloads: 18
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - bert-base-japanese-v3-bpr-passage-aio ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
  - Downloads: 18
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - japanese-reversed-gpt2-medium-unidic This is a medium-sized Japanese reversed GPT-2 model using BERT-like tokenizer.
  - Downloads: 18
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 18
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 18
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 18
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 18
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - rinna/japanese-data2vec-audio-base Overview This is a Japanese data2vec Audio Base model trained by rinna Co.
  - Downloads: 18
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta Model description Stockmark-2-100B-Instruct-beta is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese.
  - Downloads: 18
- [akkikiki/j-moshi-ext-mlx](https://huggingface.co/akkikiki/j-moshi-ext-mlx)
  - The model in this repository is an MLX model converted from the pre-trained J-Moshi model for Mac OS.
  - Downloads: 18
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 18
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - deberta-base-japanese-aozora Model Description
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - deberta-large-japanese-wikipedia-luw-upos Model Description
  - Downloads: 18
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - WS TCG Card Text Translator A Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
  - Downloads: 18
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - roberta-base-japanese-char-luw-upos Model Description
  - Downloads: 18
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 18
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 18
- [atsuki-yamaguchi/bloom-7b1-random-ja](https://huggingface.co/atsuki-yamaguchi/bloom-7b1-random-ja)
  - BLOOM-7B
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Model Card for Wabisabi-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 wabisabi has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - zenz-v2 zenz-v2ã¯GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 18
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - modernBERTã§NERã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ ãƒ©ãƒ™ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚° label_list = ["O", "B-äººå", "I-äººå", "B-æ³•äººå", "I-æ³•äººå", "B-æ”¿æ²»çš„çµ„ç¹”å", "I-æ”¿æ²»çš„çµ„ç¹”å", "B-ãã®ä»–ã®çµ„ç¹”å", "I-ãã®ä»–ã®çµ„ç¹”å", "B-åœ°å", "I-åœ°å", "B-æ–½è¨­å", "I-æ–½è¨­å", "B-è£½å“å", "I-è£½å“å", "B-ã‚¤ãƒ™ãƒ³ãƒˆå", "I-ã‚¤ãƒ™ãƒ³ãƒˆå"] tokenizer ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - ãƒ‰ãƒŸãƒ‹ã‚ªãƒ³æ—¥æœ¬èªLLM for Whisperï¼ˆ2023/12/19 1.0ç‰ˆï¼‰ æ¦‚è¦ Whisperã§ãƒ‰ãƒŸãƒ‹ã‚ªãƒ³ï¼ˆãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ï¼‰ã®ã‚«ãƒ¼ãƒ‰ç”¨èªãªã©ã‚’å«ã‚“ã éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—å‡ºæ¥ã‚‹ã“ã¨ã‚’ç›®æ¨™ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸLLMã§ã™ã€‚
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [cyberagent/ca-reward-3b-ja](https://huggingface.co/cyberagent/ca-reward-3b-ja)
  - cyberagent/ca-reward-3b-ja è»½é‡ãªæ—¥æœ¬èªå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‚’ç›®çš„ã¨ã—ã¦å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã™ã‚‹ã€‚
  - Downloads: 17
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - bilingual-gpt-neox-4b-8k Overview Notice: This model requires transformers&gt;=4.31.0 to work properly.
  - Downloads: 17
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - AIBunCho/japanese-novel-gpt-j-6b AI BunChoã§åˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - rinna/nekomata-7b-gguf Overview The model is the GGUF version of rinna/nekomata-7b.
  - Downloads: 17
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 17
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 17
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Karasu-Mixtral-8x22B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 17
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - deberta-large-japanese-unidic-luw-upos Model Description
  - Downloads: 17
- [hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4](https://huggingface.co/hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4)
  - hiratagoh/SIP-jmed-llm-2-8x13b-OP-instruct-bnb-nf4 SIP-med-LLM/SIP-jmed-llm-2-8x13b-OP-instructã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³1.0.0ã‚’BitsAndBytesã§NF4é‡å­åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 17
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - Japanese BERT-base (Sudachi + WordPiece) How to load the tokenizer Please download the dictionary file for Sudachi + WordPiece from our GitHub repository.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - roberta-base-japanese-luw-upos Model Description
  - Downloads: 17
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - Byt5-small-ain-jpn-mt is a machine translation model pretrained with Google's ByT5-small and fine-tuned on bilingual datasets crawled from the Web.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - roberta-base-japanese-aozora-ud-goeswith Model Description
  - Downloads: 17
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - ebisuke/liz-nojaloli-nxja-ja License MIT ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦abeja/gpt-neox-japanese-2.7bã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 17
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False
  - Downloads: 17
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B Japanese-LLaMA-2-7Bã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 17
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 17
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - modernbert-large-japanese-aozora Model Description
  - Downloads: 17
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview ğŸ’¡ This model was created based on FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview.yaml
  - Downloads: 17
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1 DeepSeekã®è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¨è«–èƒ½åŠ›ã‚’æŠ½å‡ºã—ãŸé‡ã¿ã®å·®åˆ†ã‚’ã€æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B Model Description
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 â™»
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - Animagineç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ãŸVAEå†…è”µãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - bert-japanese-ner ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã‚¿ã‚¹ã‚¯ã‚’ç›®çš„ã¨ã—ã¦ã€äº¬éƒ½å¤§å­¦ é»’æ©‹ãƒ»è¤šãƒ»æ‘è„‡ç ”ç©¶å®¤ãŒå…¬é–‹ã—ã¦ã„ã‚‹BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ãŒå…¬é–‹ã—ã¦ã„ã‚‹ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 16
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 16
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€å†…å®šè€…ã®äºŒä¸‡ä»¶ä»¥ä¸Šã®ESã‚’ç”¨ã„ã¾ã—ãŸã€‚
  - Downloads: 16
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - Bloom model trained on Japanese corpus.
  - Downloads: 16
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 16
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 16
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - rinna/nekomata-14b-gguf Overview The model is the GGUF version of rinna/nekomata-14b.
  - Downloads: 16
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 Model Description
  - Downloads: 16
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 16
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-3.6b-instruction-sft line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 16
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - ã‚·ã‚µãƒ èªã«ã‚ˆã‚‹èª¬æ˜ ã‚¢ã‚¤ãƒŒèªã¨æ—¥æœ¬èªã®åŒæ–¹å‘æ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - Model Card for Model ID ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt-1bã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®æŠ½å‡ºå‹QAã¨ã€è§£ç­”ã‚’æ–°ãŸãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒªãƒ•ã‚¡ã‚¤ãƒ³ã™ã‚‹ãŸã‚ã®å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - jpn-msa source group: Japanese target group: Malay (macrolanguage) OPUS readme: jpn-msa model: transformer-align source language(s): jpn jpn_Hani jpn_Hira jpn_Kana target language(s): ind
  - Downloads: 16
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - The English document is here.
  - Downloads: 16
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 16
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - roberta-small-japanese-char-luw-upos Model Description
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - deberta-large-japanese-wikipedia Model Description
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - deberta-base-japanese-wikipedia-luw-upos Model Description
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - deberta-base-japanese-wikipedia Model Description
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - deberta-base-japanese-juman-ud-goeswith Model Description
  - Downloads: 16
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 16
- [waowao/gemma3-1b-it-oasst2-1k_of_33k-ja](https://huggingface.co/waowao/gemma3-1b-it-oasst2-1k_of_33k-ja)
  - gemme3-1b-itã‚’oasst2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡1kã‚’ç”¨ã„ã¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 16
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_vits_accent_with_pause â™»
  - Downloads: 16
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 16
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3ã€‚
  - Downloads: 16
- [Hemlok/LizMix](https://huggingface.co/Hemlok/LizMix)
  - â—†LizMix SakuMixãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ‹ãƒ¡å‘ã‘ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã€‚
  - Downloads: 16
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B Model Details Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model that can converse about input images.
  - Downloads: 16
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Watashiha-Llama-2-13B-Ogiri-sftã‚’LLaVAã§å­¦ç¿’ã—ã€ç”»åƒã«å¯¾å¿œã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - Mistral-7B Japanese
  - Downloads: 16
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - llm-jp-1.3b-v1.0-aya llm-jp's llm-jp-1.3b-v1.0 model fine-tuned on the Japanese examples from Cohere's aya dataset Model llm-jp-eval AVG kcoopermiller/llm-jp-1.3b-v1.0-aya 0.0698 llm-jp/llm-jp-1.3b-v1.0 0.047 How to use import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("kcoopermiller/llm-jp-1.3b-v1.0-aya")
  - Downloads: 16
- [mmnga/alfredplpl-suzume-poc-gguf](https://huggingface.co/mmnga/alfredplpl-suzume-poc-gguf)
  - alfredplpl-suzume-poc-gguf alfredplplã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-pocã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 16
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B DiffLlama-1Bã¯ã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ç´„100Bãƒˆãƒ¼ã‚¯ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸç´„1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - bert-base-japanese-luw-upos Model Description
  - Downloads: 15
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - yuyuyui-chatbot
  - Downloads: 15
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - ESã‚’æ›¸ãAI Japanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚
  - Downloads: 15
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - è‹±èª+æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
  - Downloads: 15
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA - Lorenzo Concina
  - Downloads: 15
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fine_Tuned_Whisper_Model This model is a fine-tuned version of openai/whisper-tiny on the Common Voice dataset.
  - Downloads: 15
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fined_Tuned_Whisper_Model
  - Downloads: 15
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba Barba is a multilingual natural language inference model for textual entailment and zero-shot text classification, available as an end-to-end service through TensorFlow Serving.
  - Downloads: 15
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-instruct-gamma-7b ğŸ§© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
  - Downloads: 15
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - Japanese-LLaMA-2-13B-GGUF Japanese-LLaMA-2-13B-GGUFã¯Japanese-LLaMA-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 15
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - MPT-7B-inst ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7b-instructã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - ELECTRA base Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - deberta-large-japanese-luw-upos Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - deberta-large-japanese-aozora Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - deberta-small-japanese-luw-upos Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - deberta-small-japanese-upos Model Description
  - Downloads: 15
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - roberta-small-hi-char Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - deberta-large-japanese-wikipedia-ud-goeswith Model Description
  - Downloads: 15
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ sonoisa/sentence-luke-japanese-base-lite ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - Japanese BERT-base (MeCab + WordPiece) How to load the tokenizer Please download the dictionary file for MeCab + WordPiece from our GitHub repository.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - deberta-large-japanese-unidic Model Description
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - bert-large-japanese-upos Model Description
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - bert-large-japanese-luw-upos Model Description
  - Downloads: 15
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - ELECTRA small Japanese discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 15
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 15
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚ç•¥ éœ§é›¨é­”ç†æ²™ã¨ãŠã—ã‚ƒã¹ã‚Šã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 15
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - doc2query/msmarco-japanese-mt5-base-v1 This is a doc2query model based on mT5 (also known as docT5query).
  - Downloads: 15
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This is a Japanese sentence-T5 model.
  - Downloads: 15
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8bã®æ—¥æœ¬èªç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ChatVectorã‚’é©ç”¨ã—ã€ã•ã‚‰ã«QLoraã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ line-corporation/japanese-large-lm-1.7bã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ï¼Œsftã«ã‚ˆã‚‹full instruction tuningã‚’è¡Œã„ã¾ã—ãŸï¼
  - Downloads: 15
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 15
- [atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-heuristics-untied-ja)
  - Mistral-7B Japanese
  - Downloads: 15
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 15
- [mayocream/manga-ocr-onnx](https://huggingface.co/mayocream/manga-ocr-onnx)
  - Manga OCR ONNX
  - Downloads: 15
- [yamatazen/Shirayukihime-12B](https://huggingface.co/yamatazen/Shirayukihime-12B)
  - Shirayukihime-12B
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - Kendamarron/LongWriter-llm-jp-3-3.7b-instruct llm-jp/llm-jp-3-3.7b-instructã‚’é•·æ–‡å‡ºåŠ›ãŒã§ãã‚‹ã‚ˆã†ã«SFTã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Llama 3 Youko 70B (rinna/llama-3-youko-70b)
  - Downloads: 15
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 â™»
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 â™»
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa æ¦‚è¦ tokyotech-llm/Swallow-7b-hfã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ä»¥ä¸‹ã®4ãƒ¢ãƒ‡ãƒ«ã‚’gate_mode=randomã§MoEã—ã€ãã®å¾ŒLISAã¨ã„ã†æ‰‹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - Wav2Vec2-XLS-R-300M-Japanese-Hiragana Fine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset.
  - Downloads: 14
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM
  - Downloads: 14
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling: Multilingual Gemma Update @ 2024.04.15: First release of Gemma-Mling 7B model Original Gemma Model Page:
  - Downloads: 14
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 14
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM
  - Downloads: 14
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - line-corporation/japanese-large-lm-3.6b line-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 14
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This model is traned with guanaco dataset.
  - Downloads: 14
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 ja Finetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 14
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 ja Finetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
  - Downloads: 14
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - ELECTRA small Japanese discriminator for Irony
  - Downloads: 14
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - Wav2Vec2-XLS-R-300M-Japanese-Hiragana Fine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using the Common Voice and JSUT.
  - Downloads: 14
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - roberta-small-japanese-aozora-char Model Description
  - Downloads: 14
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP Model Card Model detail Model type: LLaVA-JP is a vision-language model that can converse about input images.
  - Downloads: 14
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
  - Downloads: 14
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - sonoisa/t5-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
  - Downloads: 14
- [OmniAICreator/Galgame-Llasa-1B](https://huggingface.co/OmniAICreator/Galgame-Llasa-1B)
  - Galgame-Llasa-1B Overview This is a Text-to-Speech (TTS) model fine-tuned for Japanese.
  - Downloads: 14
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - deberta-base-japanese-wikipedia-ud-goeswith Model Description
  - Downloads: 14
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - Unihan LM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database Model description Chinese and Japanese share many characters with similar surface morphology.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - roberta-large-japanese-aozora-char Model Description
  - Downloads: 14
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 14
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - ELECTRA small Japanese finance generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 14
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - deberta-base-japanese-luw-upos Model Description
  - Downloads: 14
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - roberta-small-hi-char-mlm Model Description
  - Downloads: 14
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JSTS(æ–‡ç« ã®é¡ä¼¼åº¦è¨ˆç®—)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 14
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - Only for Japanese Please use AutoTokenizer and AutoModelForCausalLM And must use Unifine format to input and output.
  - Downloads: 14
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 14
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 14
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - deberta-base-japanese-unidic-luw-upos Model Description
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - roberta-large-japanese-luw-upos Model Description
  - Downloads: 14
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - bert-large-japanese-unidic-luw-upos Model Description
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - Japanese BERT-base (Juman++ + WordPiece) How to load the tokenizer Please download the dictionary file for Juman++ +
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - deberta-large-japanese-unidic-ud-head Model Description
  - Downloads: 14
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - deberta-base-japanese-unidic-ud-head Model Description
  - Downloads: 14
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU-Model (T5 fine-tuned model) JAINU is a Japanese - Ainu language machine translation model.
  - Downloads: 14
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 14
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card è‹±æ—¥ã€æ—¥è‹±ç¿»è¨³ç”¨ãƒ¢ãƒ‡ãƒ«C3TR-Adapterã®GPTQ4bité‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 14
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged Mixtral-8x7B-Instruct-v0.1-japanese-alpha-mergedã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸå­¦ç¿’é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€å·®åˆ†ãƒãƒ¼ã‚¸ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese This model is Llama-2-Chat 70B fine-tuned with a part of the Japanese instruction dataset named izumi-lab/llm-japanese-dataset.
  - Downloads: 14
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation) V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2 V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2 ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ã‚„Instaç³»ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒãƒ»Instaç³»ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
  - Downloads: 14
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - Reproduced Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 14
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - Swallow-MoE-2x13B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹tokyotech-llm/Swallow-13b-instruct-hfã¨ã€ãã‚Œã‚’åˆ©ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹nitky/Superswallow-13b-v0.2 ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This model is a merged version of qwen-14b-vntl and Qwen1.5-14B-Chat , aiming for the translation of Japanese context into Chinese.
  - Downloads: 14
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 14
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 14
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - swallow-hermes-st-v1 ç‰©èªä½œæˆã«å¼·ã‚ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºæ¥ãªã„ã‹ã¨è€ƒãˆã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B ã€Œã©ã†ã‹ãŠæ…ˆæ‚²ã‚’ ã‚‚ã† ç–²ã‚Œæœã¦ã¾ã—ãŸã€ ç”Ÿæˆä¾‹ [å¤ªå­—ä»¥é™ãŒAIç”Ÿæˆ] ã€Œã©ã†ã‹ã€ â€ãã‚Œâ€ã¯æ‡‡é¡˜ã—ãŸã€‚
  - Downloads: 14
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B Japanese
  - Downloads: 14
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
  - Downloads: 14
- [ayousanz/modernbert-vtuber-finetuned](https://huggingface.co/ayousanz/modernbert-vtuber-finetuned)
  - ModernBERT-VTuber Finetuned ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ sbintuitions/modernbert-ja-130m ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€YouTube ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‹ã‚‰ VTuber ã‹å¦ã‹ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - Japanese-Novel-Reward-modernbert-ja-310m ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/modernbert-ja-310mã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - Model Trained Using AutoNLP Problem type: Binary Classification Model ID: 59363 Validation Metrics Loss: 0.12651239335536957 Accuracy: 0.9532079853817648 Precision: 0.9729688278823665 Recall: 0.9744633462616643 AUC: 0.9717333684823413 F1: 0.9737155136027014 Usage You can use cURL to access this model: $ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-5936
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit ã‚’æ—¥æœ¬èªã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™.
  - Downloads: 14
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ DeepSeek-V3 ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªã®ä¾‹æ–‡ã‚’å…ƒã«é »å‡ºã™ã‚‹ MoE (Mixture of Experts) ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®expertsã‚’å³é¸ã—ã¦å†æ§‹æˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹ Have different permissions when sharing merges
  - Downloads: 14
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - ##llm-jpã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãƒ¢ãƒ‡ãƒ«
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This model is for transcribing audio into Hiragana, one format of Japanese language.
  - Downloads: 13
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This model is traned with guanaco dataset.
  - Downloads: 13
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39æ§˜ã® Tora-7B-v0.2 ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 13
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - Convert from: drewschaub/whisper-large-v3-japanese-4k-steps Whisper large-v3 model for CTranslate2 This repository contains the conversion of drewschaub/whisper-large-v3-japanese-4k-steps to the CTranslate2 model format.
  - Downloads: 13
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 13
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - studio-ousia/luke-japanese-baseã«å¯¾ã—ã¦æ¬¡ã®å¤‰æ›´ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - Model Card Summary This model was trained using H2O LLM Studio.
  - Downloads: 13
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - MPT-7B-base ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - ebisuke/liz-nojaloli-ja License MIT Licenseãƒ™ãƒ¼ã‚¹ã¨ã—ã¦rinna/japanese-gpt-neox-3.6bã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 13
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
  - Downloads: 13
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiæ§˜ã® Japanese-Chat-Umievo-itr004-7b ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - albert-base-japanese-v1-with-japanese æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«BertJapaneseTokenizerã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™albert-base-japanese-v1ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒæ¥½ã«ãªã£ã¦ã„ã¾ã™ How to use ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ Fill-Mask for PyTorch from transformers import ( AutoModelForMaskedLM, AutoTokenizer ) tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")
  - Downloads: 13
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B pre-trained model for Japanese Model Description GPT2/GPT3 like model trained on Japanese.corpus.
  - Downloads: 13
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã§ã™ã€‚
  - Downloads: 13
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 13
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
  - Downloads: 13
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 13
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 large Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.
  - Downloads: 13
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - japanese-gpt-1b-PII-masking Model Description japanese-gpt-1b-PII-masking ã¯ã€ æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿1B GPTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ—¥æœ¬èªã®æ–‡ç« ã‹ã‚‰å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - roberta-large-japanese-aozora Model Description
  - Downloads: 13
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - deberta-large-japanese-upos Model Description
  - Downloads: 13
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - c4ai-command-r-v01-japanese-instruct GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ CohereForAI/c4ai-command-r-v01ã‚’ã€ichikara-instructionã‚’ä½¿ã£ã¦è¿½åŠ ã§æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿç‰©å­¦ãƒ»åŒ»å­¦ã«ç²¾é€šã—ãŸOpenBioLLM-8Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªå¯¾å¿œã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«Llama-3-youko-8b-instruct-chatvectorã¨ãƒãƒ¼ã‚¸ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸTokara-0.5B-v0.1ã‚’æ—¥æœ¬èªinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 13
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - transformers-ud-japanese-electra-ginza-520 (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - Japanese BERT-base (Sudachi + BPE) How to load the tokenizer Please download the dictionary file for Sudachi + BPE from our GitHub repository.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - Japanese BERT-base (Juman++ + BPE) How to load the tokenizer Please download the dictionary file for Juman++ + BPE from our GitHub repository.
  - Downloads: 13
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - ku-accms/roberta-base-japanese-ssuw Model description This is a pre-trained Japanese RoBERTa base model for super short unit words (SSUW).
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - roberta-large-japanese-aozora-ud-goeswith Model Description
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - deberta-base-japanese-aozora-ud-goeswith Model Description
  - Downloads: 13
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - What is this model?
  - Downloads: 13
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - roberta-large-japanese-char-luw-upos Model Description
  - Downloads: 13
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - Japanese Stable Diffusion Pokemon Model Card Stable-Diffusion-Pokemon-ja is a Japanese-specific latent text-to-image diffusion model capable of generating Pokemon images given any text input.
  - Downloads: 13
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - distilhubert-ft-japanese-50k Fine-tuned (more precisely, continue trained)
  - Downloads: 13
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - Japanese BERT-base (Nothing + Unigram)
  - Downloads: 13
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
  - Downloads: 13
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - transformer-lm-japanese-0.1b
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - Japanese BERT-base (Vaporetto + WordPiece) How to load the tokenizer Please download the dictionary file for Vaporetto + WordPiece from our GitHub repository.
  - Downloads: 13
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - bart-base-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 13
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-13bã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-13b-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - Mixtral-8x7B-Instruct-v0.1-japanese Mixtral-8x7B-Instruct-v0.1-japaneseã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - ãƒ¢ãƒ‡ãƒ«ã®æ¦‚ç•¥ æ±æ–¹Projectã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã‚ã‚‹éœ§é›¨é­”ç†æ²™ã¨ãŠã—ã‚ƒã¹ã‚Šã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B Model Description
  - Downloads: 13
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯tokyotech-llm/Swallow-MS-7b-instruct-v0.1ã®tokenizer.chat_templateã‚’ä»¥ä¸‹ã«å¤‰æ›´ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP æ¦‚è¦ Local-Novel-LLM-project/Ninja-v1-NSFWã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteusã‚’ãƒ™ãƒ¼ã‚¹ã«LLavaã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - Japanese Stable LM Instruct Gamma 7B +
  - Downloads: 13
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - electra-base-japanese-discriminator (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 13
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Lorablated)
  - Merged Model
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja - Remastered - vlzcrz
  - Downloads: 13
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - æ¦‚è¦ è³ªå•ã¨å¿œç­”ã‹ã‚‰ã€ãã®éç¨‹ã®æ€è€ƒã‚’ç”Ÿæˆã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - isekai-bert-v1
  - Downloads: 13
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This model learned the proceedings of the Japanese parliament in 2022.
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base ã‚’ RetroMAE ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - â—†ArcanaMix äºŒæ¬¡å…ƒã‚¤ãƒ©ã‚¹ãƒˆã‚’ä¸­å¿ƒã«ã€ã‹ã‚ã„ã„ã‚¤ãƒ©ã‚¹ãƒˆãŒå‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€‚
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1ã‚’ huggingface/text-embeddings-inferenceã§å‹•ã‹ã™ãŸã‚ã® fork ã§ã™ã€‚
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave â™»
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8Bã¯è¿½åŠ ã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªãŒå¤§å¤‰æµæš¢ãªLlama-3æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - â– endlessMixã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦ æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Defactaã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸéšå±¤ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
  - Downloads: 13
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2ã‚’ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [OmniAICreator/Galgame-Llasa-3B-v3](https://huggingface.co/OmniAICreator/Galgame-Llasa-3B-v3)
  - Galgame-Llasa-3B-v3 Overview This is the version 3 of the Galgame-Llasa-3B, a Text-to-Speech (TTS) model fine-tuned for Japanese.
  - Downloads: 12
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - Japanese ELECTRA-small We provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 12
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License ä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M license ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹ Use the model without crediting the creator ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹ Sell images they generate ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹ Run on services that generate images for money ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ Share merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹ Sell this model or merges using this model ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹ Have different permissions when sharing merges ğŸ–¼ï¸ ä¾‹ / Examples(â€»ä»–ã®äººãŒç”Ÿæˆã—ãŸç‰©ã‚’è¡¨ç¤ºã—ã¦ã„ã‚‹å ´åˆã¯æœ¬äººã®è¨±è«¾ã‚’å¾—ã¦
  - Downloads: 12
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - whisper-large-v2-jp model for CTranslate2 This repository contains the conversion of vumichien/whisper-large-v2-jp to the CTranslate2 model format.
  - Downloads: 12
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 12
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 12
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - Model Card for Model ID Fine tunned ASR model from distil-whisper/distil-large-v2.
  - Downloads: 12
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 12
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 12
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - Summary This is a text classifier for assigning a JLPT level.
  - Downloads: 12
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - nlp-waseda/roberta-large-japanese-seq512 Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.
  - Downloads: 12
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - roberta-small-japanese-luw-upos Model Description
  - Downloads: 12
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - t5-base-japanese-web-8k (with Byte-fallback, 8K) Description megagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 12
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - Details: https://spacy.io/models/ja#ja_core_news_lg Japanese pipeline optimized for CPU.
  - Downloads: 12
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - t5-base-xlsum-ja
  - Downloads: 12
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - In-progess long-context Japanese-English translation model based on tinyllama.
  - Downloads: 12
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - bert-base-japanese-upos Model Description
  - Downloads: 12
- [UEC-InabaLab/Llama-3.1-KokoroChat-ScorePrediction](https://huggingface.co/UEC-InabaLab/Llama-3.1-KokoroChat-ScorePrediction)
  - ğŸ§  Llama-3.1-KokoroChat-ScorePrediction: Japanese Counseling Dialogue Scoring Model Llama-3.1-KokoroChat-ScorePrediction is a large-scale Japanese language model fine-tuned on the KokoroChat datasetâ€”a collection of over 6,000 psychological counseling dialogues conducted via role-play between trained counselors.
  - Downloads: 12
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - roberta-small-japanese-aozora Model Description
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - deberta-base-japanese-wikipedia-ud-head Model Description
  - Downloads: 12
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collectionã¨ã¯ï¼Ÿ
  - Downloads: 12
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - Japanese-LLaMA-2-7B-GGUF Japanese-LLaMA-2-7B-GGUFã¯Japanese-LLaMA-2-7Bã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 12
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - æ¦‚è¦ vecteusã¯ã€é«˜æ€§èƒ½ãªæ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [ywc1/marian-finetuned-ja-en](https://huggingface.co/ywc1/marian-finetuned-ja-en)
  - Model Card for Japanese-English Academic Translator
  - Downloads: 12
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese)
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - roberta-base-japanese-aozora Model Description
  - Downloads: 12
- [waowao/llama3.2-3b-oasst2-33k-ja](https://huggingface.co/waowao/llama3.2-3b-oasst2-33k-ja)
  - Model Description llama3.2-3b-instructionã‚’oasst2-33k-jaã§LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚
  - Downloads: 12
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - electra-base-cyberbullying This is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 12
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - ELECTRA Base Japanese for Information Triage
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - deberta-large-japanese-juman-ud-goeswith Model Description
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - Japanese BERT-base (MeCab + Unigram)
  - Downloads: 12
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2 model size: 130.78M trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§ https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")
  - Downloads: 12
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - LINE DistilBERT Japanese (forked by liwii)
  - Downloads: 12
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard Model Description Deepreneur-blue-lizardã¯ã€Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - japanese-novel-gpt-j-6b https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b" ã«åˆè¨ˆ216å€‹ã®è©•ä¾¡ã®é«˜ã„ãªã‚ã†å°èª¬ã€é’ç©ºæ–‡åº«ã€ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢ãªã©ã®æ–‡ç« ã‚’QLoRAå­¦ç¿’ã•ã›ãŸå°èª¬ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ Twitter/twhin-bert-base ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸ
  - Downloads: 12
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID æ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã‹ã‚‰ã€æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚ã‚‹å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ Model Details Model Description ä¾‹ãˆã°ã€ã€Œæ±äº¬ã®è‚‰æ–™ç†ã§ã€æ˜¥ã«é£Ÿã¹ã‚‰ã‚Œã‚‹ã€é¶è‚‰ã‚’ä½¿ã£ãŸæ–™ç†ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ ã€Œæ±äº¬ â†’ éƒ½é“åºœçœŒ/åœ°æ–¹(AREA)ã€ ã€Œè‚‰æ–™ç† â†’ ç¨®é¡(TYPE)ã€ ã€Œæ˜¥ â†’ å­£ç¯€(SZN)
  - Downloads: 12
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 12
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base SambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
  - Downloads: 12
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models: mistralai/Mistral-7B-Instruct-v0.1 stabilityai/japanese-stablelm-base-gamma-7b ğŸ§© Configuration slices: - sources: - model: mistralai/Mistral-7B-Instruct-v0.1 layer_range:
  - Downloads: 12
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k elyzaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã‚’ æ—¥æœ¬èªã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã§ç”Ÿæˆã—ãŸGPTQãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 12
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - About This model is Lightblue's QLoRA finetune of OpenOrca's Open-Orca/OpenOrcaxOpenChat-Preview2-13B model on Japanese fine-tuning datasets.
  - Downloads: 12
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - VITS TTS Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - Japanese BERT-base (Juman++ + Unigram)
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - deberta-large-japanese-aozora-ud-goeswith Model Description
  - Downloads: 12
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - bart-large-japanese This model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - Japanese BERT-base (Nothing + WordPiece) How to load the tokenizer Please download the dictionary file for Nothing + WordPiece from our GitHub repository.
  - Downloads: 12
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - Japanese BERT-base (Nothing + BPE) How to load the tokenizer Please download the dictionary file for Nothing + BPE from our GitHub repository.
  - Downloads: 12
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - Model card for model ID
  - Downloads: 12
- [alter-wang/luke-japanese-large-wrime-emotion-alpha](https://huggingface.co/alter-wang/luke-japanese-large-wrime-emotion-alpha)
  - This is a LUKE (Japanese version) Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 12
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - Wav2Vec2 Accent Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese accent dataset When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 12
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - The English document is here ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ Watashiha-Llama-2-13B-Ogiri-sftã‚’AWSã®inf2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 12
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - ãƒ¢ãƒ‡ãƒ« ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instruct å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-ja å­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚µãƒ³ãƒ—ãƒ« import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
  - Downloads: 12
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 Known Performance Issues Two potential bugs have been found in this model: NEED repetition_penalty NEED high temperature Reference: Japanese LLM benchmark results at Nejumi LLM Leaderboad Neo
  - Downloads: 12
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 12
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - ã‚¢ãƒ‹ãƒ¡å£°ã®ã‚ˆã†ãªã‚ã–ã¨ã‚‰ã—ã„å£°ã§ã‚‚ãªãã€ãƒœã‚«ãƒ­ãªã©ã®ã‚½ãƒ•ãƒˆã‚’ä½¿ã£ãŸã„ã‹ã«ã‚‚åˆæˆã®éŸ³å£°ã§ã‚‚ãªãã€ã‚¯ãƒ©ã‚¹ã«ä¸€äººãã‚‰ã„ã„ãã†ãªã€è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„ç¾å°‘å¥³ã®å£°ã‚’â€¦ã€‚
  - Downloads: 12
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - ãŠçŸ¥ã‚‰ã› ã‚ˆã‚Šå›ç­”ãŒé©åˆ‡ã«ãªã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã€https://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq ã‚‚ã‚ã‚Šã¾ã™ã€‚
  - Downloads: 12
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - jvnvã‚³ãƒ¼ãƒ‘ã‚¹ã®F2ã‹ã‚‰å­¦ç¿’ã—ã¦ä½œæˆã—ãŸbert-vits2ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1 English description here æ¦‚è¦ Llama-2ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7bã¨ã€ãã®instruction tuningãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹elyza/ELYZA-japanese-Llama-2-7b-instruct ã‚’ã€mergekitã‚’ä½¿ã£ã¦MoEã‚’è¡Œã„ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b ã¯ã€ Llama2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This model is a voice clone of myself created specifically for Style Bert VITS2.
  - Downloads: 12
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct ğŸ MambaSan-instruct is the first chat Japanese language model based on a state-space model architecture (Mamba).
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 12
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JCommonsenseQA(é¸æŠå¼å¿œç­”)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 12
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - æ¦‚è¦ ã€ŒLOCAL AI HACKATHONã€ã«ãŠã‘ã‚‹ã€ãƒãƒ¼ãƒ DataPilot,4ã¤ã‚ã®æˆæœå“ã§ã™ã€‚
  - Downloads: 12
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - zenz-v2.5-small zenz-v2.5ã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - DataPilot/sarashina2.2-3Bx4-moe DataPilot/sarashina2.2-3Bx4-moeã¯ã€4ã¤ã®ã€Œsbintuitions/sarashina2.2-3b-instruct-v0.1ã€ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ä½œæˆã—ãŸç´„12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦æ¨¡ã®Mixture of Experts (MoE) ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 Japanese-LLaMA-3-8B-Instruct-v2ã¯æŒ‡ç¤ºå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - https://qiita.com/SousiOmine/items/23313089c7c3f498996b æ¦‚è¦ sbintuitions/sarashina2.2-3b-instruct-v0.1ã«ã€ Kendamarron/jimba-instruction-allã¨SousiOmine/Japanese-Pythonic-FunctionCallã‚’ç”¨ã„ãŸQLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ pythoné–¢æ•°ã®å‘¼ã³å‡ºã—ã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€SakanaAI/TinySwallow-1.5B-Instructã«å¯¾ã—ã¦ã€GRPOã«ã‚ˆã‚Šé«˜æ©‹ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸè¿½åŠ å­¦ç¿’ã‚’æ–½ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - èª¿æ•´ã—ãŸã„æ–¹å‘ã¨ã¯ã ã„ã¶ç•°ãªã‚‹æ–¹å‘ã«ãšã‚ŒãŸã€‚
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Qwen/Qwen2.5-32Bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸAbejaç¤¾ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«DeepSeekç¤¾ã®R1è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã‚’æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸcyber agentç¤¾ã®cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ChatVectorã‚’ç”¨ã„ã¦åŠ ãˆãŸã‚‚ã®ã«ã€ç‹¬è‡ªã®æ—¥æœ¬èªå¼·åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - modernbert-base-japanese-char Model Description
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko ã“ã¡ã‚‰ã¯ã€Œã•ãã‚‰ã¿ã“ã€ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦å­¦ç¿’ã•ã‚ŒãŸVITS-TTSãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€text-embeddings-inference (TEI) ã§ã€mecab / unidic ãªã©ã‚’ç”¨ã„ãŸæ—¥æœ¬èªTokenizerã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€dummy ã® tokenizer.json ã‚’ç”¨ã„ã¦ç„¡ç†ã‚„ã‚Šå‹•ã‹ã™ æ–¹æ³•ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6Bæ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­å¤§æ¨¡å‹ï¼Œæœ¬é¡¹ç›®ä¸ºChatGLM3-6BåŠ å…¥æ—¥æ–‡èƒ½åŠ›ã€‚
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹HODACHI/Llama-3.1-70B-EZO-1.1-itã®ggufç‰ˆã§ã™ã€‚
  - Downloads: 12
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - bert-large-japanese-char-extended Model Description
  - Downloads: 11
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - Model overview This model is the baseline model for awesome-japanese-nlp-classification-dataset.
  - Downloads: 11
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B ğŸŒEnglish | ğŸ‡¨
  - Downloads: 11
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™ sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 11
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF æ¦‚è¦ Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - deberta-base-japanese-upos Model Description
  - Downloads: 11
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 11
- [espnet/kan-bayashi_jsut_conformer_fastspeech2_transformer_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2_transformer_prosody)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_conformer_fastspeech2_transformer_prosody â™»
  - Downloads: 11
- [OmniAICreator/Galgame-Llasa-1B-v2](https://huggingface.co/OmniAICreator/Galgame-Llasa-1B-v2)
  - Galgame-Llasa-1B-v2 Overview
  - Downloads: 11
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 11
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - spekulatius ãƒãƒ¼ã‚¸ã—ã¦ã„ã‚‹ã¨ãŸã¾ã«å‡ºã¦ãã‚‹ã€Œç›®çš„ã®æ„å›³ã¨ã¯é•ã†ã®ã ã‘ã©ãªã‚“ã ã‹æ¶ˆã™ã«ã¯ã‚‚ã£ãŸã„ãªã„ãƒ¢ãƒ‡ãƒ«ã€ã‚’ãŠã™ãåˆ†ã‘ã™ã‚‹ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 11
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime ã¯æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸå°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [sabaridsnfuji/Japanese-Receipt-VL-3B-JSON](https://huggingface.co/sabaridsnfuji/Japanese-Receipt-VL-3B-JSON)
  - Japanese-Receipt-VL-3B-JSON Model Description Japanese-Receipt-VL-3B-JSON is a fine-tuned vision-language model based on Qwen2.5-VL-3B, specifically optimized for Japanese receipt OCR and structured data extraction.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - roberta-base-japanese-aozora-char Model Description
  - Downloads: 11
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - Japanese transformer pipeline (bert-base).
  - Downloads: 11
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ« SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
  - Downloads: 11
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT
  - Downloads: 11
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - roberta-large-japanese-aozora-ud-head Model Description
  - Downloads: 11
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - bert-large-japanese-wikipedia-ud-head Model Description
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - roberta-base-japanese-aozora-ud-head Model Description
  - Downloads: 11
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - Japanese-Alpaca-2-13B Japanese-Alpaca-2-13Bã¯æŒ‡ç¤ºå®Ÿè¡Œãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - Kokuwa lamettaã®æ”¹è‰¯ã§ãƒãƒ¼ã‚¸ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«æ¢ã—ã‚’ã—ã¦ã„ãŸã‚‰KiwiMixã¨ã„ã†é¢ç™½ãã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚
  - Downloads: 11
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm Model description This is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.
  - Downloads: 11
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - æ—¥æœ¬èª gpt2 è’¸ç•™ãƒ¢ãƒ‡ãƒ« ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt2-meduimã‚’æ•™å¸«ã¨ã—ã¦è’¸ç•™ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1.0 The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 11
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - Japanese Stable LM Instruct Gamma 7B +
  - Downloads: 11
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ calm-2-7b-chat ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - deberta-base-japanese-unidic Model Description
  - Downloads: 11
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - Model Card for Model ID
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - ELECTRA small Japanese generator This is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - deberta-large-japanese-wikipedia-ud-head Model Description
  - Downloads: 11
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - Japanese BERT-base (MeCab + BPE) How to load the tokenizer Please download the dictionary file for MeCab + BPE from our GitHub repository.
  - Downloads: 11
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True
  - Downloads: 11
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼štyqiangz/multilingual-sentiments ãƒãƒƒãƒã‚µã‚¤ã‚º: 16å›ºå®š ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adamw Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
  - Downloads: 11
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - INPUT: Japanese name in ROMAJI FORM OUTPUT:
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - ãŠçŸ¥ã‚‰ã› ã‚ˆã‚Šå›ç­”ãŒé©åˆ‡ã«ãªã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã€https://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq ã‚‚ã‚ã‚Šã¾ã™ã€‚
  - Downloads: 11
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - hubert-base-jtube This repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.
  - Downloads: 11
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow Our Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 11
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - output ç­‘æ³¢ 2.0035860538482666 ã¤ãã° 1.6586617231369019 ç ”ç©¶ 1.6227693557739258 å¤§å­¦ 1.3798155784606934 å®Ÿé¨“ 0.5522942543029785 å­¦ç”Ÿ 0.42351895570755005 åˆ†æ 0.37844282388687134 å›½ç«‹ 0.3685397505760193 ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ 0.36495038866996765 èŒ¨åŸ 0.3056415021419525 ç§‘å­¦ 0.2876652181148529 é–¢æ± 0.24301066994667053 åœ°åŸŸ 0.21340851485729218 å®Ÿæ–½ 0.1976248174905777 å…ˆç«¯ 0.192025288939476 ã‚µã‚¤ãƒˆ 0.11629197001457214 èª¿æŸ» 0.09159307181835175 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ 0.08552580326795578 è­°è«– 0.07484486699104309 æ¤œè¨ 0.007034890353679657
  - Downloads: 11
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 11
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B LEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 11
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 11
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
  - Downloads: 11
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 11
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - Ninja-v1-RP GGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF version æ¦‚è¦ This is a merge of pre-trained language models created using mergekit.
  - Downloads: 11
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸTokara-0.5B-v0.1ã«chat vectorã§å¯¾è©±èƒ½åŠ›ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 11
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2 model size: 417.12M trainingã¯ä»¥ä¸‹ã®scriptå‚ç…§https://github.com/Lightning-AI/lit-gpt/tree/main use from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_small")
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - Japanese BERT-base (Vaporetto + Unigram)
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - Japanese BERT-base (Vaporetto + BPE) How to load the tokenizer Please download the dictionary file for Vaporetto + BPE from our GitHub repository.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - Japanese BERT-base (Sudachi + Unigram)
  - Downloads: 11
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JNLI(æ–‡ç« ã®é–¢ä¿‚æ€§åˆ¤åˆ¥)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - deberta-large-japanese-aozora-ud-head Model Description
  - Downloads: 11
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-largeã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JCommonsenseQA(é¸æŠå¼å¿œç­”)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - Model Card for Model ID
  - Downloads: 11
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
  - Downloads: 11
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-book/wrime-sentiment ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adamw Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
  - Downloads: 11
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - uniTKU-hubert-japanese-asr
  - Downloads: 11
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šcl-tohoku/bert-base-japanese-whole-word-masking ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-book/wrime-sentiment ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: adafactor Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒ—(lr_scheduler_type):
  - Downloads: 11
- [espnet/kan-bayashi_jsut_conformer_fastspeech2_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause â™»
  - Downloads: 11
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - Details: https://spacy.io/models/ja#ja_core_news_trf Japanese transformer pipeline (Transformer(name='cl-tohoku/bert-base-japanese-char-v2', piece_encoder='char', stride=160, type='bert', width=768, window=216, vocab_size=6144)).
  - Downloads: 11
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - COMET-GPT2 ja v2 Finetuned GPT-2 xl on the large version of ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 11
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - Chat &amp; support: TheBloke's Discord server Want to contribute?
  - Downloads: 11
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ studio-ousia/luke-japanese-large-lite ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 11
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã¯ã€ã“ã¡ã‚‰ã§ã™ã€‚
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B Shisa 7B (shisa-7b-v1)
  - Downloads: 11
- [line-corporation/japanese-mulan-base](https://huggingface.co/line-corporation/japanese-mulan-base)
  - japanese-mulan-base This is a Japanese MuLan (Music-Language pretraining) model developed by LY Corporation.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause â™»
  - Downloads: 11
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - Description A Japanese-specialized SentencePiece tokenizer trained for AI Novelist's SuperTrin and Damsel 20B models.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - jvnvã‚³ãƒ¼ãƒ‘ã‚¹ã®F2ã‹ã‚‰å­¦ç¿’ã—ã¦ä½œæˆã—ãŸbert-vits2ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 11
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - llm-jp-13b-instruct-lora-jaster-v1.0
  - Downloads: 11
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m ğŸ MambaSan-370m is the first chat Japanese language model based on a state-space model architecture (Mamba).
  - Downloads: 11
- [OmeletRice/japanese-wav2vec2-base-bf16-asmr](https://huggingface.co/OmeletRice/japanese-wav2vec2-base-bf16-asmr)
  - ASMRãƒ‡ãƒ¼ã‚¿ã§reazon-research/japanese-wav2vec2-baseã‚’è¿½åŠ äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ« bf16 Model Card for Model ID Model Details Model Description
  - Downloads: 11
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ ãœã²éŠã³ã«ãã¦ã­ã€‚
  - Downloads: 11
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese The Model mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese was converted to MLX format from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese using mlx-lm version 0.21.1.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - Japanese-Novel-Reward-modernbert-ja-130m ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯sbintuitions/modernbert-ja-130mã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Japanese-Novel-Reward-TinySwallow-1.5B ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯SakanaAI/TinySwallow-1.5Bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã•ã‚ŒãŸæ—¥æœ¬èªå°èª¬ã®å“è³ªè©•ä¾¡ã®ãŸã‚ã®Rewardãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - èª¿æ•´ã—ãŸã„æ–¹å‘ã¨ã¯å°‘ã—ãšã‚ŒãŸãŒã€AIã®å£èª¿ã¯å¥³ã®å­é¢¨ã«ã€‚
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_full_band_vits_prosody â™»
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ä¸Šè¨˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚¢ãƒ€ãƒ«ãƒˆç”¨èªã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent â™»
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech â™»
  - Downloads: 11
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifierã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§pkshatech/GLuCoSE-base-jaã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã•ã‚ŒãŸLLMã§ã‚ã‚‹weblab-GENIAC/Tanuki-8B-dpo-v1.0-4kã®GPTQ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chatã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã«QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜(English explanation is below.
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details â€»å¥½å¥‡å¿ƒã‹ã‚‰ç”Ÿã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - æ¦‚è¦ GLM-4-9B-Chatã‚’ã€æ—¥æœ¬èªã®Wikiãƒ‡ãƒ¼ã‚¿ã‚’é¸å®šã—ã€è¿½åŠ å­¦ç¿’ã—ãŸæ—¥æœ¬èªã«éå¸¸ã«å¼·ã„ã‚¹ã‚³ã‚¢ã‚’å‡ºã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct ğŸš¨ This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2ã®ãƒã‚¤ãƒŠãƒ¼ãƒã‚§ãƒ³ã‚¸ç‰ˆã§ã™ã€‚
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave â™»
  - Downloads: 11
## Datasets

This list is sorted by downloads as of August 19, 2025.
607 datasets are listed.

- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ éå»ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ éå»ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯ã€ãƒ‹ã‚³ãƒ‹ã‚³å®Ÿæ³ ã®ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹ã‹ã‚‰ç¾åœ¨ã¾ã§ã®ã™ã¹ã¦ã®éå»ãƒ­ã‚°ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 132,839
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023:
  - Downloads: 8,489
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - VOICEVOXã‚’ä½¿ã£ãŸäººå·¥éŸ³å£°ãƒœã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ ITAã‚³ãƒ¼ãƒ‘ã‚¹ ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ã‚³ãƒ¼ãƒ‘ã‚¹ ROHANã‚³ãƒ¼ãƒ‘ã‚¹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé‡æƒ…å ± ãƒ•ã‚©ãƒ«ãƒ€å†…ã®.
  - Downloads: 5,906
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - The Cauldron is a massive collection of 50 vision-language datasets (training sets only) that were used for the fine-tuning of the vision-language model Idefics2.
  - Downloads: 5,098
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB:
  - Downloads: 3,195
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - ğŸ· FineWeb2 Edu Japanese: High-Quality Educational Japanese
  - Downloads: 2,003
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese Anime Speech Dataset æ—¥æœ¬èªã¯ã“ã¡ã‚‰ japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
  - Downloads: 1,861
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100: æ—¥æœ¬èªinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Data Description æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 1,846
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - Please feel free to open an issue or pull request.
  - Downloads: 1,726
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-bokete", split="train") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 1,548
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’UVRã‚’ä½¿ç”¨ã—ã¦BGMã‚„ãƒã‚¤ã‚ºé™¤å»ã—ãŸã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
  - Downloads: 1,493
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - CC-news-2024-July-October-cleaned ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Common Crawlã®newsã‚µãƒ–ã‚»ãƒƒãƒˆã‹ã‚‰ä½œæˆã—ãŸ2024å¹´7æœˆã‹ã‚‰10æœˆã®æ—¥æœ¬èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ–‡ç« ãŒåéŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 1,480
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
  - Downloads: 1,439
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023:
  - Downloads: 1,342
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - We provide an Amazon product reviews dataset for multilingual text classification.
  - Downloads: 1,198
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST Dataset lassify images from the KMNIST dataset into one of the 10 classes, representing different Japanese characters.
  - Downloads: 1,037
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 æ—¥æœ¬èªã¯ã“ã¡ã‚‰ japanese-anime-speech-v2 is an audio-text dataset designed for training automatic speech recognition models.
  - Downloads: 1,030
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - Maintainers Junfeng Jiang@Aizawa Lab: jiangjf (at) is.s.u-tokyo.ac.jp Jiahao Huang@Aizawa Lab: jiahao-huang (at) g.ecc.u-tokyo.ac.jp
  - Downloads: 1,016
- [SakanaAI/EDINET-Bench](https://huggingface.co/datasets/SakanaAI/EDINET-Bench)
  - EDINET-Bench ğŸ“š Paper | ğŸ“ Blog | ğŸ§‘â€ğŸ’» Code EDINET-Bench is a Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction.
  - Downloads: 825
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - This dataset contains a diverse set of natural Japanese speech, collected from terrestrial television streams.
  - Downloads: 784
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023:
  - Downloads: 770
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
  - Downloads: 769
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth: Japanese/English Synthetic Conversation Dataset Derived from LMSYS-Chat-1M
  - Downloads: 733
- [TheFinAI/OCR_Task_JA](https://huggingface.co/datasets/TheFinAI/OCR_Task_JA)
  - Japanese_OCR
  - Downloads: 723
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - æ—¥æœ¬èªWikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«è¨€ã„æ›ãˆã‚’ç”Ÿæˆã—ã€ãã®è¨€ã„æ›ãˆã‚’å…ƒã«ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’LLMã«ç”Ÿæˆã•ã›ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 721
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025:
  - Downloads: 632
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 583
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 524
- [pfnet/bbh-ja](https://huggingface.co/datasets/pfnet/bbh-ja)
  - BBH-ja (æ—¥æœ¬èªç‰ˆBIG-Bench Hard) BBH-jaã¯ã€BIG-Bench Hard (Paper, GitHub) ã‚’ç¿»è¨³ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹ã€‚
  - Downloads: 504
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: aiishii/JEMHopQA on GitHub JEMHopQA JEMHopQA (Japanese Explainable Multi-hop Question Answering)ã¯ã€å›ç­”å°å‡ºã‚¹ãƒ†ãƒƒãƒ—ã®æƒ…å ±ä»˜ãã®æ—¥æœ¬èªã®æ ¹æ‹ æƒ…å ±ä»˜ããƒãƒ«ãƒãƒ›ãƒƒãƒ—QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 499
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - llm-japanese-dataset LLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³(ãƒãƒ£ãƒƒãƒˆ)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ä¸»ã«ï¼Œè‹±èªã§æ§‹ç¯‰ã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼
  - Downloads: 484
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - VNTL Leaderboard
  - Downloads: 484
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 467
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - [Under Construction]
  - Downloads: 466
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - The dataset was extracted from Common Crawl dumps covering February 2024 â€“ January 2025 and contains roughly 56M Japanese documents, 110B characters, and 249M images.
  - Downloads: 463
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - ğŸ¥• å¦‚æœå…”å…”çš„ä»“åº“å¯¹ä½ æœ‰å¸®åŠ©çš„è¯ç‚¹ä¸ªâ­å–µ~ If Tutu's repository is helpful to you, please give it a â­ meow~ ã‚‚ã—ã†ã•ãã®ãƒªãƒã‚¸ãƒˆãƒªãŒå½¹ã«ç«‹ã£ãŸå ´åˆã¯ã€â­ã‚’ã½ã¡ã£ã¨ã—ã¦ãã ã•ã„ã«ã‚ƒã‚“~ ğŸ‰ ä»»ä½• â“
  - Downloads: 462
- [stockmark/u4-table-cell-qa](https://huggingface.co/datasets/stockmark/u4-table-cell-qa)
  - TableCellQA
  - Downloads: 453
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - Overview This dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.
  - Downloads: 434
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU:
  - Downloads: 420
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - CC-MAIN-2019-39ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 411
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - Japan Diverse Images Dataset Overview This dataset is a comprehensive collection of high-quality images capturing the diverse aspects of Japan, including urban landscapes, natural scenery, historical sites, contemporary art, everyday life, and culinary experiences.
  - Downloads: 407
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - ã‚µãƒ¨å­ éŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³• ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åœ§ç¸®ã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€gdriveã«ç½®ã„ã¦ã„ã¾ã™ã€‚
  - Downloads: 406
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja: Spoken Dialogue Dataset in Japanese DailyTalkContiguous-ja is a synthetic multi-turn Japanese conversational speech dataset in which DailyTalk
  - Downloads: 403
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - Dataset Summary This is the Business Scene Dialogue (BSD) dataset, a Japanese-English parallel corpus containing written conversations in various business scenarios.
  - Downloads: 400
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU Japanese Massive Multitask Language Understanding Benchmark JMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
  - Downloads: 399
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - In this study, we introduce a new dataset, WRIME, for emotional intensity estimation.
  - Downloads: 394
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - Please feel free to open an issue or pull request.
  - Downloads: 387
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹2010 ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’huggingfaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ï½¡ 2009 å¹´åº¦ã«ãŠã‘ã‚‹è‘—ä½œæ¨©æ³•ã®æ”¹æ­£ï¼ˆå¹³æˆ21å¹´é€šå¸¸å›½ä¼š è‘—ä½œæ¨©æ³•æ”¹æ­£ç­‰ã«ã¤ã„ã¦ | æ–‡åŒ–åºï¼‰ã«åŸºã¥ãï¼Œæƒ…å ±è§£æç ”ç©¶ã¸ã®åˆ©ç”¨ã«é™ã£ã¦åˆ©ç”¨å¯èƒ½ã§ã™ï½¡ å½¢æ…‹ç´ è§£æã‚’ç”¨ã„ã¦ï½¤è‡ªå‹•ã§å¥ç‚¹ã‚’ã¤ã‘ã¾ã—ãŸï½¡ å¤‰æ›ã‚³ãƒ¼ãƒ‰ å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ å½¢æ…‹ç´ è§£æãªã©
  - Downloads: 372
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
  - Downloads: 368
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 365
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - relaion2B-en-research-safe-japanese-translation This dataset is the Japanese translation of the English subset of ReLAION-5B (laion/relaion2B-en-research-safe),
  - Downloads: 364
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard ã¨ã¯ Allganize RAG Leaderboard ã¯ã€5ã¤ã®æ¥­ç¨®ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆé‡‘èã€æƒ…å ±é€šä¿¡ã€è£½é€ ã€å…¬å…±ã€æµé€šãƒ»å°å£²ï¼‰ã«ãŠã„ã¦ã€æ—¥æœ¬èªã®RAGã®æ€§èƒ½è©•ä¾¡ã‚’å®Ÿæ–½ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 360
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - range3/cc100-ja This dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
  - Downloads: 350
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This is the filtered Japanese subset of XL-Sum followed by PaLM 2 filters 15-gram overlap * code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86a number of examples train: 4215 (before: 7113) validation: 758 (before: 889) test: 766 (before: 889)
  - Downloads: 350
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - fungi_diagnostic_chars_comparison_japaneseå¤§èŒè¼ªã€Œè­˜åˆ¥å½¢è³ªã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ /
  - Downloads: 350
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 322
- [UEC-InabaLab/KokoroChat](https://huggingface.co/datasets/UEC-InabaLab/KokoroChat)
  - KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors KokoroChat is the largest human-collected Japanese psychological counseling dialogue dataset to date (as of June 2025).
  - Downloads: 321
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 313
- [if001/bunpo_phi4](https://huggingface.co/datasets/if001/bunpo_phi4)
  - phi4ã§ä»¥ä¸‹ã®53ã®æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— 2364vocab ã‚’ç”Ÿæˆã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
  - Downloads: 306
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - fungi_indexed_mycological_papers_japanese å¤§èŒè¼ªã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2025/5/2ï¼ˆR3-12744ã¾ã§ï¼‰ Languages Japanese This dataset is available in Japanese only.
  - Downloads: 304
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - AutoWikiQA æ±å·¥å¤§ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MXã‚’ç”¨ã„ã¦ã€Wikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€Œè³ªå•(query)ã€ã¨ã€Œå›ç­”(answer)ã€ã‚’ç”Ÿæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã¨å›ç­”ã«ã¤ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 303
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - llm-book/aio-passages ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€llm-book/bert-base-japanese-v3-bpr-passage-encoder ã«ã‚ˆã‚‹ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«ãŒ embeddings ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 299
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - range3/wiki40b-ja This dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
  - Downloads: 293
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - llm-japanese-dataset-vanilla LLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ izumi-lab/llm-japanese-dataset ã‹ã‚‰ï¼Œæ—¥è‹±ç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ã‚’æŠœã„ãŸã‚‚ã®ã§ã™ï¼
  - Downloads: 290
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA : Japanese Question Answering with Retrieval Augmentation - æ¤œç´¢æ‹¡å¼µ(RAG)è©•ä¾¡ã®ãŸã‚ã®æ—¥æœ¬èª Q&amp;A ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ é«˜æ€§èƒ½ãª LLM ã®å°é ­ã«ä¼´ã„ã€LLM ã‚’ç”¨ã„ãŸè³ªç–‘å¿œç­”ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 289
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec: Gender Dection from Japanese Names with Machine Learning
  - Downloads: 285
- [takarajordan/takaraspider](https://huggingface.co/datasets/takarajordan/takaraspider)
  - TakaraSpider Japanese Web Crawl Dataset Dataset Summary TakaraSpider is a large-scale web crawl dataset specifically designed to capture Japanese web content alongside international sources.
  - Downloads: 274
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - Dataset.
  - Downloads: 273
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - åˆæˆã•ã‚ŒãŸè³ªå•ã¨2ã¤ã®å¿œç­”æ–‡ã®ãƒšã‚¢ã«å¯¾ã—ã¦ã€æ—¥æœ¬èªã‚’æ¯èªã¨ã™ã‚‹ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ãŒã€å¥½ã¾ã—ã„å¿œç­”æ–‡ã‚’äººæ‰‹ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¾ã—ãŸ HelpSteer2-preferenceã«ç¿’ã„ã€é¸å¥½ã ã‘ã§ãªãé¸å¥½ã®å¼·åº¦ã‚‚[-3, 3]ã®ç¯„å›²ã§ä»˜ä¸ã—ã¾ã—ãŸ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã®ãƒ¡ãƒ¢ ã‚¸ãƒ£ãƒ³ãƒ«ã¯ä»¥ä¸‹ ç°¡å˜ãªä¸€èˆ¬çŸ¥è­˜(wikipediaã‚’èª­ã¾ãšã«å›ç­”ã§ãã‚‹ç³») é›£ã—ã‚ã®ä¸€èˆ¬çŸ¥è­˜(wikipediaã‚’èª­ã‚“ã ã‚‰å›ç­”ã§ãã‚‹ç³») æ­´å²ä¸Šã®å‡ºæ¥äº‹ã®è«–è¿° åŒ»ç™‚çŸ¥è­˜(å¿œæ€¥å‡¦ç½®ç³») æ©Ÿæ¢°å­¦ç¿’ã®èª²é¡Œã¨è§£æ±ºæ–¹æ³• åŒ–å­¦å¼ã®è§£èª¬ æ¶ç©ºã®ç‰©èªç”Ÿæˆ ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ è©©ã®å‰µä½œ ç´ å› æ•°åˆ†è§£ã‚„å¶æ•°å¥‡æ•°åˆ¤å®šãªã©ã®ç°¡å˜ãªæ•°å­¦ã‚¿ã‚¹ã‚¯ ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒªãƒ‡ãƒ¡ã®è§£èª¬ ç¾è¡“ã‚„æ€æƒ³ã«ã¤ã„ã¦ã®è«–è¿° æ—¥æœ¬èªã®æ–‡æ³•ã®è§£èª¬ ãã®ä»– LLMã®å®šå‹æ–‡ã¨ã—ã¦ç™»å ´ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã¯ã€ ã€Œã‚‚ã¡ã‚ã‚“ã§ã™ã€ ã€Œï½ã‚‚è¦‹é€ƒã›ã¾ã›ã‚“ã€ ã€Œï½ã‚‚è¦‹éã”ã›ã¾ã›ã‚“ã€ ã€Œç·ã˜ã¦ã€ã€ ã€Œã¾ãšå§‹ã‚ã«ã€ï½ã•ã‚‰ã«ã€ï½æ¬¡ã«ã€ï½ã¾ã¨ã‚ã‚‹ã¨ã€ã€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç›®è¦–ã§èª­ã¿è¾¼ã‚“ã å°è±¡ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ï¼ˆã¾ãŸã“ã®è³ªå•ã‹ã€ã¨æ€ã†ã“ã¨ãŒã‚ã£ãŸï¼‰ ä¸Šã§æŒ™ã’ãŸã‚¸ãƒ£ãƒ³ãƒ«ã”ã¨ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ï¼ˆä¾‹
  - Downloads: 268
- [OmniAICreator/Japanese-Novels-23M](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels-23M)
  - Japanese-Novels-23M
  - Downloads: 261
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - LLMChat æ¦‚è¦ GENIAC æ¾å°¾ç ” LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’äººæ‰‹è©•ä¾¡ã™ã‚‹ãŸã‚ã«æ§‹ç¯‰ã—ãŸLLMChatã¨ã„ã†ã‚·ã‚¹ãƒ†ãƒ ã§åé›†ã•ã‚ŒãŸè³ªå•ã¨LLMã®å›ç­”ã€åŠã³äººæ‰‹è©•ä¾¡ã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 259
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda - Questions for Japanese models Repository:
  - Downloads: 258
- [hotchpotch/JFWIR](https://huggingface.co/datasets/hotchpotch/JFWIR)
  - JFWIR - Japanese FineWeb Information Retrieval:
  - Downloads: 255
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - [github].
  - Downloads: 243
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully Dataset åˆ©ç”¨è¦ç´„ åˆ©ç”¨è¦ç´„ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªãŠã‚ˆã³ä»–ã®è¨€èªã®LLMã®å®‰å…¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†ç›®çš„ã®ãŸã‚ã€å•†ç”¨åˆ©ç”¨ã‚‚å«ã‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 239
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - CommonCrawl Japanese (Filtered PPI) Dataset æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€CommonCrawlã‚ˆã‚ŠæŠ½å‡ºã—ãŸç´„100å„„ï¼ˆ10Bï¼‰ãƒˆãƒ¼ã‚¯ãƒ³è¦æ¨¡ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ç‰¹ã«é…æ…®ãŒå¿…è¦ãªã€Œè¦é…æ…®å€‹äººæƒ…å ±ã€ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 239
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - wikipedia æ—¥æœ¬èªã®æ–‡ã‚’ã€å„ç¨®æ—¥æœ¬èªã® embeddings ã‚„ faiss index ã¸ã¨å¤‰æ›ã—ãŸã‚‚ã®ã€‚
  - Downloads: 226
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­ã®256æ–‡å­—ä»¥ä¸‹ã®è¡Œã‚’æŠ½å‡ºã—ãƒãƒ¼ã‚¸ã—ã¾ã—ãŸã€‚
  - Downloads: 225
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 Dataset Description JA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
  - Downloads: 223
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - This is a Japanese translated version of HumanEval, an evaluation harness for the HumanEval problem solving dataset described in the paper "Evaluating Large Language Models Trained on Code".
  - Downloads: 220
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ SentenceTransformes ã§å­¦ç¿’ã—ã‚„ã™ã„ã‚«ãƒ©ãƒ åã¨æ§‹é€ ã«å¤‰æ›ã—ãŸã‚‚ã®ã€‚
  - Downloads: 220
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - English version JSynFlowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Meta Platforms, Inc.
  - Downloads: 219
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - ã‚·ãƒ³ãƒ—ãƒ«ãšã‚“ã ã‚‚ã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã¯ã˜ã‚ã« ãšã‚“ã ã‚‚ã‚“ã®è¨­å®šãŒè©°ã¾ã£ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 208
- [cc-clean/CC-MAIN-2025-05](https://huggingface.co/datasets/cc-clean/CC-MAIN-2025-05)
  - CC-MAIN-2025-05ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 206
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset is a clarified version of the image, context, and question set included in the Japanese-Heron-Bench for the construction of the Japanese evaluation benchmark suite.
  - Downloads: 202
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - Synthetic-JP-EN-Coding-Dataset This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 202
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - cc100-ja-documents HuggingFace ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ cc100 / cc100-ja ã¯ line å˜ä½ã®åˆ†å‰²ã®ãŸã‚ã€document å˜ä½ã«çµåˆã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 202
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - zenz-v2.5-dataset zenz-v2.5-datasetã¯ã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸæ¡ä»¶ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒ«ã€Œzenz-v2.5ã€ã‚·ãƒªãƒ¼ã‚ºã®å­¦ç¿’ã‚’ç›®çš„ã¨ã—ã¦æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 200
- [ks-pf/JMTEB-fixed](https://huggingface.co/datasets/ks-pf/JMTEB-fixed)
  - JMTEB-fixed ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å…ƒã®JMTEBãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
  - Downloads: 199
- [b25119ms/sympathetic_200](https://huggingface.co/datasets/b25119ms/sympathetic_200)
  - æ€ã„ã‚„ã‚Šã®ã‚ã‚‹å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€ã²ãŸã™ã‚‰å¯„ã‚Šæ·»ã„ã€æ€ã„ã‚„ã‚Šã®ã‚ã‚‹è¨€è‘‰ã‚’è¿”ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã€Google's Gemini API ã‚’ç”¨ã„ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 199
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - databricks-dolly-15k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 197
- [humanalysis-square/KokushiMD-10](https://huggingface.co/datasets/humanalysis-square/KokushiMD-10)
  - KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations Overview KokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations.
  - Downloads: 196
- [ASLP-lab/Multilingual-Alpaca-Speech](https://huggingface.co/datasets/ASLP-lab/Multilingual-Alpaca-Speech)
  - Multilingual Alpaca Speech Dataset Multilingual Alpaca Speech is a high-quality speech instruction-following dataset supporting Japanese (ja), German (de), and French (fr) .
  - Downloads: 184
- [OmniAICreator/Japanese-Wikipedia-202506](https://huggingface.co/datasets/OmniAICreator/Japanese-Wikipedia-202506)
  - Japanese-Wikipedia-202506
  - Downloads: 184
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - fungi_trait_circus_database å¤§èŒè¼ªã€ŒTrait Circusã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆçµ±åˆ¶å½¢è³ªï¼‰æœ€çµ‚æ›´æ–°æ—¥ï¼š2025/8/12 Languages Japanese and English Please do not use this dataset for academic purposes for the time being.
  - Downloads: 181
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS ScreenTalk_JA2ZH-XS is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
  - Downloads: 181
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - JSNLI Version 1.1 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆ (train_w_filtering)
  - Downloads: 180
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - Reranker-Scores æ—¢å­˜ã®æ—¥æœ¬èªæ¤œç´¢ãƒ»QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸­ã®ã‚¯ã‚¨ãƒªã«ä»˜ä¸ã•ã‚ŒãŸæ­£ãƒ»è² ä¾‹ã®é–¢é€£åº¦ã‚’å¤šè¨€èªãƒ»æ—¥æœ¬èªreranker 5ç¨®é¡ã‚’ç”¨ã„ã¦ã‚¹ã‚³ã‚¢ä»˜ã‘ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 177
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ GitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/ LICENSE: CC-BY-SA 3.0 Developed by Stockmark Inc.
  - Downloads: 175
- [106ki/utl-itsl2-report](https://huggingface.co/datasets/106ki/utl-itsl2-report)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯AIITã®ç”£æ¥­æŠ€è¡“ç‰¹åˆ¥è¬›ç¾©2ã®æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”¨ã§ã™ã€‚
  - Downloads: 174
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - fineweb-2-edu-japanese ã® small_tokens ã® text ã‚«ãƒ©ãƒ ã‚’ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–(NFKC)ã—ãŸã‚‚ã®ã‚’ fineweb-2-japanese-text-cleaner ã‚’ä½¿ã£ã¦ãƒã‚¤ã‚ºç®‡æ‰€ã‚’æ¨è«–ã—ãŸRAWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 173
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - japanese-asr/whisper_transcriptions.reazon_speech_all without audio
  - Downloads: 173
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 169
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/bokete-ogiri-test", split="test") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 167
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - mbpp-ja
  - Downloads: 164
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD: Japanese Mathematical Dataset with Assured Reasoning Description English / Japanese Overview JaMARD (Japanese Mathematical Dataset with Assured Reasoning Description) is a high-quality synthetic dataset for Japanese mathematical problems with chain-of-thought reasoning, where the correctness of synthetic instances is assured.
  - Downloads: 164
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 164
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - CABank Japanese Sakura Corpus Susanne Miyata Department of Medical Sciences Aichi Shukotoku University smiyata@asu.aasa.ac.jp website: https://ca.talkbank.org/access/Sakura.html Important
  - Downloads: 162
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - Dataset Summary SNOW T15:The simplified corpus for the Japanese language.
  - Downloads: 161
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - æ¦‚è¦ oscar-corpus/OSCAR-2301ã®ä»¥ä¸‹ã®jaã®ã¿ã‚’ corpus-cleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œãªã£ãŸãƒ‡ãƒ¼ã‚»ãƒƒãƒˆç¾¤ Code Language # docs # words Content Length : ja Japanese 94,236,404 4,401,059,165 181.2 GB ãŸã ã—ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãŒæˆåŠŸã—ã¦ã„ãªã„ãŸã‚é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 160
- [JunSotohigashi/JWTD_v2.0](https://huggingface.co/datasets/JunSotohigashi/JWTD_v2.0)
  - æ—¥æœ¬èªWikipediaå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (v2) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ï¼Œäº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦æ§‹ç¯‰ã•ã‚ŒãŸæ—¥æœ¬èªå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(v2)ã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
  - Downloads: 158
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This is the information integration of erai-raws and myanimelist.
  - Downloads: 157
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - Dataset Summary 53,640 Japanese tweets with annotation if a tweet is related to COVID-19 or not.
  - Downloads: 156
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - æ—¥æœ¬èªWikipediaã‹ã‚‰LLMã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã—ãŸè³ªå•ã¨ã€å¯¾å¿œã™ã‚‹æ—¥æœ¬èªWikipediaã®ãƒšãƒ¼ã‚¸ã‚’å…ƒã«ã€cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦å›ç­”ã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 155
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - Dataset 5M (5121625) clean Japanese full sentence with the context.
  - Downloads: 155
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ567077ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 153
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - Githubãƒªãƒã‚¸ãƒˆãƒªstockmarkteam/ner-wikipedia-datasetã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 149
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR: Japanese Casual Web IR - æ—¥æœ¬èªæƒ…å ±æ¤œç´¢è©•ä¾¡ã®ãŸã‚ã®å°è¦æ¨¡ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªWebã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ è¿‘å¹´ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å°é ­ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚’ç”¨ã„ãŸè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§è³ªå•ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã¦ã„ã¾ã™ã€‚
  - Downloads: 148
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - Japanese stopwords for nagisa
  - Downloads: 148
- [xiashuaxia/japanese_recipe](https://huggingface.co/datasets/xiashuaxia/japanese_recipe)
  - ç”£æŠ€å¤§ã®ã€Œç”£æ¥­æŠ€è¡“ç‰¹åˆ¥è¬›ç¾©2ã€ã®æœŸæœ«èª²é¡Œã®ãŸã‚ã€è‹±èªã§æ›¸ã„ã¦ã‚ã‚‹ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹ã€‚
  - Downloads: 145
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - Scenery of japan.
  - Downloads: 145
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - Anime with caption CC-0 dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¤ãƒ©ã‚¹ãƒˆã«å¯¾ã™ã‚‹æ—¥æœ¬èªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ å€«ç†çš„ã«å­¦ç¿’ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 142
- [b25119ms/sympathetic_700](https://huggingface.co/datasets/b25119ms/sympathetic_700)
  - â€»AIITç”£æ¥­æŠ€è¡“ç‰¹åˆ¥è¬›ç¾©ï¼’èª²é¡Œç”¨ã¨ã—ã¦ä½œæˆ æ€ã„ã‚„ã‚Šã®ã‚ã‚‹å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€ã²ãŸã™ã‚‰å¯„ã‚Šæ·»ã„ã€æ€ã„ã‚„ã‚Šã®ã‚ã‚‹è¨€è‘‰ã‚’è¿”ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã€Google's Gemini API ã‚’ç”¨ã„ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 141
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - alpaca_jp_python alpaca_jp_pythonã¯ã€ Stanford Alpacaã®æ‰‹æ³• mistralai/Mixtral-8x22B-Instruct-v0.1 ã§ä½œã£ãŸåˆæˆãƒ‡ãƒ¼ã‚¿(Synthetic data)ã§ã™ã€‚
  - Downloads: 140
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus æ¦‚è¦ Lux Japanese Speech Corpus ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€ŒLux (ãƒ«ã‚¯ã‚¹)ã€ã«ã‚ˆã‚‹æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’éŸ³å£°ã‚’åéŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 137
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - æ—¥æœ¬èªWikipediaã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€rinna/deepseek-r1-distill-qwen2.5-bakeneko-32bã¨https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦ç®‡æ¡æ›¸ãã«ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 136
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - ğŸ“° News
  - Downloads: 135
- [ayousanz/OSCOR-2301-ja-cleaned-0](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned-0)
  - æ¦‚è¦ oscar-corpus/OSCAR-2301ã®ä»¥ä¸‹ã®jaã®ã¿ã‚’ corpus-cleanerã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œãªã£ãŸãƒ‡ãƒ¼ã‚»ãƒƒãƒˆç¾¤ Code Language # docs # words Content Length : ja Japanese 94,236,404 4,401,059,165 181.2 GB
  - Downloads: 134
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA This dataset is hf mirror of https://registry.opendata.aws/abeja-cc-ja/ Please Refer to https://tech-blog.abeja.asia/entry/abeja-cc-ja-202409 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯https://registry.opendata.aws/abeja-cc-ja/ã®HFãƒŸãƒ©ãƒ¼ã§ã™ã€‚
  - Downloads: 133
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: yahoojapan/JGLUE on GitHub JSQuAD JSQuAD is a Japanese version of SQuAD (Rajpurkar+, 2016), one of the datasets of reading comprehension.
  - Downloads: 129
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
  - Downloads: 128
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - Umamusume-voice-transcription Total charcters: 77 Comes with transcription.
  - Downloads: 127
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
  - Downloads: 127
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset A voice dataset collected from Project Sekai charactor Emu Otori Introduction Size: 2735, all WAV format.
  - Downloads: 123
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - æ—¥æœ¬èªã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (æ¼¢å­—èª¤å¤‰æ›æŠ½å‡ºç‰ˆ) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼Œäº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
  - Downloads: 119
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLMã®ãŸã‚ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸ å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€ æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›å¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
  - Downloads: 117
- [HALDATA/bert-jp-sentiment-20250731-1035](https://huggingface.co/datasets/HALDATA/bert-jp-sentiment-20250731-1035)
  - Dataset Summary This dataset contains Japanese e-commerce product review sentiment data for fine-tuning BERT models.
  - Downloads: 114
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - neody/oscar-ja-cleanedã®ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰256æ–‡å­—ä»¥ä¸‹ã®ã‚‚ã®ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚
  - Downloads: 114
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - msmarco-ja-hard-negatives hpprc/msmarco-ja ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹MS MARCOã®æ—¥æœ¬èªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã«ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’åŠ ãˆãŸãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚’ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 107
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - This pre-training dataset was created for shisa-base-7b-v1.
  - Downloads: 106
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - Japanese Voice Dataset Combined This dataset combines multiple high-quality Japanese voice datasets to create a comprehensive collection of Japanese speech data.
  - Downloads: 102
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 Description Malum-230 is a meticulously handcrafted Japanese dataset featuring multi-turn conversations and passages, specifically designed for logical reasoning tasks.
  - Downloads: 99
- [r-g2-2024/JGraphQA](https://huggingface.co/datasets/r-g2-2024/JGraphQA)
  - JGraphQA Introduction We introduce JGraphQA, a multimodal benchmark designed to evaluate the chart understanding capabilities of Large Multimodal Models (LMMs) in Japanese.
  - Downloads: 99
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - Japanese-Roleplay-Dialogues This is a dialogue corpus collected from Japanese role-playing forum (commonly known as "ãªã‚Šãã‚Šãƒãƒ£ãƒƒãƒˆ(narikiri chat)").
  - Downloads: 99
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki Wikipediaã®HTMLå½¢å¼ã®ãƒ€ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 98
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - CC-MAIN-2019-30ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 98
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset æ—¥æœ¬èªæœ‰å®³æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒLLM-jp Toxicity Datasetã€ See https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset
  - Downloads: 97
- [reep0610/AGI-japanese-text-dataset-for-Deep-Learning](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-for-Deep-Learning)
  - ğŸ“„ã€åˆ©ç”¨è¦ç´„ã€‘ æœ¬å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ã€äººé¡ã¨AIã®å…±æœ‰è²¡ç”£ã¨ã—ã¦æä¾›ã•ã‚Œã¾ã™ã€‚
  - Downloads: 97
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - Elite Voice Project ã“ã‚Œã¯ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–æ‰€å±Vtuberã•ãã‚‰ã¿ã“æ°ã®å£°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŒ–ã—éŸ³å£°èªè­˜ãªã©ã§æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹äº‹ã‚’ç›®çš„ã¨ã—ãŸéå…¬å¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
  - Downloads: 93
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Qwen/Qwen2.5-32B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 92
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - LLM-jp Chatbot Arena Conversations Dataset This dataset contains approximately 1,000 conversations with pairwise human preferences, most of which are in Japanese.
  - Downloads: 92
- [Aratako/Japanese-Creative-Writing-39.6k](https://huggingface.co/datasets/Aratako/Japanese-Creative-Writing-39.6k)
  - Japanese-Creative-Writing-39.6k æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„39600ä»¶ã®æ—¥æœ¬èªã®å°èª¬åŸ·ç­†ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 90
- [mini97/yongmin-small_jp_realchat](https://huggingface.co/datasets/mini97/yongmin-small_jp_realchat)
  - Japanese Real Chat
  - Downloads: 89
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench Dataset Description Japanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
  - Downloads: 88
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - Windowsã®æ–¹ã¯ggml-japanese-gpt2ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§å‹•ãã¨æ€ã„ã¾ã™ã€‚
  - Downloads: 87
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - Dataset Summary RealPersonaChat ã¯ï¼Œè©±è€…æœ¬äººã®ãƒšãƒ«ã‚½ãƒŠã¨æ€§æ ¼ç‰¹æ€§ã‚’å«ã‚€ï¼Œç´„14,000ä»¶ã®æ—¥æœ¬èªé›‘è«‡å¯¾è©±ã‹ã‚‰ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï¼
  - Downloads: 86
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - ğŸ“˜ğŸ“• SimpleStories ğŸ“™ğŸ“— ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€gpt-4o-miniã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸçŸ­ç·¨å°èª¬ã§å‡ºæ¥ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 86
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - Places in japan.
  - Downloads: 83
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - æ¦‚è¦ llm-jp-instructionsã¯äººæ‰‹ã«ã‚ˆã‚Šä½œæˆã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 82
- [akira-sasaki/nihongo-dojo-beginner-10k](https://huggingface.co/datasets/akira-sasaki/nihongo-dojo-beginner-10k)
  - Nihongo DoJo åˆç´šæ—¥æœ¬èªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªå­¦ç¿’è€…å‘ã‘ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 81
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - dialogsum-ja ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯dialogsumã€CSDSãªã©ã‚’ç¿»è¨³ã—ãŸæ—¥æœ¬èªå¯¾è©±è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 80
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - AKU-d_ms-0.5B-v0.1_dataset Overview ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€ç§ã®é–‹ç™ºã—ã¦ã„ã‚‹AKUã‚·ãƒªãƒ¼ã‚ºã®1ã¤ç›®ã¨ãªã‚‹ã€AKU-d_ms-0.5B-chat-v0.1ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚
  - Downloads: 80
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - Dataset Preprocessing Supported Tasks and Leaderboards Languages æ³¨é‡ˆã¯ã™ã¹ã¦æ—¥æœ¬èªã‚’ä¸»è¦è¨€èªã¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 78
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - defamation_japanese_twitter Twitteræ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Dataset Summary SNSã«ãŠã‘ã‚‹èª¹è¬—ä¸­å‚·æ¤œå‡ºã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼
  - Downloads: 76
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - recruit-jp/japanese-image-classification-evaluation-dataset Overview Developed by: Recruit Co.
  - Downloads: 76
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - mqaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 76
- [AkabekoLabs/nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing)
  - nihongo-dojo-grades1-2-3-4-5-6-kanji_reading-kanji_writing ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 76
- [if001/hle_ja_phi4](https://huggingface.co/datasets/if001/hle_ja_phi4)
  - https://huggingface.co/datasets/cais/hle Humanity's Last Examã®questionã‚’phi4ã§æ—¥æœ¬èªè¨³ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 75
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct Update: 2023/12/27ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« JaxTon , ãƒ—ãƒ­ã«ãªã‚‹Java ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ 180 ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
  - Downloads: 75
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - You agree to the terms of the LICENSE when using this dataset.
  - Downloads: 73
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - danbooru-ja-tag-pair-20241015 2024/10/15ã«ä½œæˆã—ãŸdanbooruã‚¿ã‚°ã¨æ—¥æœ¬èªã‚¿ã‚°ã®ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ç´„15ä¸‡ä»¶) p1atdev/danbooru-ja-tag-pair-20240715 ã¨ã®é•ã„ã¯ã€ ãƒ™ãƒ¼ã‚¹ã®wikiãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆãŸã®ã§ãã®åˆ†å¯¾å¿œã‚¿ã‚°ã‚‚å¢—ãˆãŸ fasttextã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æŒŸã‚€ã‚ˆã†ã«ã—ãŸ ã€Œæ˜ã‚‰ã‹ã«ä»–è¨€èªã®ã‚¿ã‚°ã€ãŒæ··ã˜ã‚‹é »åº¦ã¯ã¡ã‚‡ã£ã¨æ¸›ã£ãŸæ°—ãŒã™ã‚‹ã‘ã©ã€å®Œå…¨ã§ã¯ãªã„ (calm3ãã‚“ã®å‡¦ç†ã«)ãƒŸã‚¹ãŒãªã‘ã‚Œã°ã€æœ€ä½ä¸€ã¤ä»¥ä¸Šã®æ—¥æœ¬èªã‚¿ã‚° (other_names ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰) ãŒå­˜åœ¨ã™ã‚‹ã¯ãš ä½œæˆéç¨‹ isek-ai/danbooru-wiki-2024 ã® #202408-at20240906 revision ã‚’å…ƒã«ã€ other_names (åŸºæœ¬çš„ã«Pixivã®ã‚¿ã‚°)ãŒã¤ã„ã¦ã„ã‚‹ã‚‚ã®ã‹ã‚‰ã€æ—¥æœ¬èªã˜ã‚ƒãªã„ã‚‚ã®ãƒ»æ›–æ˜§ãƒ»æ„å‘³ã®éä¸è¶³ãŒå¤§ãã„ã‚¿ã‚°ã‚’é™¤å»ã€‚
  - Downloads: 72
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 72
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
  - Downloads: 72
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - Dataset details: Each entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
  - Downloads: 71
- [AkabekoLabs/nihongo-dojo-small](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-small)
  - nihongo-dojo-small ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 71
- [japan-ai-official/igakuqa-subset-curated](https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated)
  - IgakuQA Curated Subset (Text-Only)
  - Downloads: 71
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted 20240907 ãƒ‡ãƒ¼ã‚¿å¢—é‡ï¼ˆç´„19800ä»¶â†’ç´„39600ä»¶ï¼‰ æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 70
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This is the subset of Japanese portion of relaion2B-multi-research-safe.
  - Downloads: 69
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - ã‚‹ã‚Šã®ã‚¹ãƒ†ãƒƒã‚«ãƒ¼ just for fun.
  - Downloads: 68
- [deepcopy/japanese-synthetic-ocr-150k](https://huggingface.co/datasets/deepcopy/japanese-synthetic-ocr-150k)
  - Source: https://www.kaggle.com/datasets/hnthnt/jp-font-image-dataset-02
  - Downloads: 68
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - Retrieval-Based Multi-Turn Chat SFT Synthetic Data A year ago, we released CausalLM/Refined-Anime-Text, a thematic subset of a dataset generated using the then state-of-the-art LLMs.
  - Downloads: 67
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - range3/wikipedia-ja-20230101
  - Downloads: 66
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - ShareGPT-Processed The RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.
  - Downloads: 66
- [R-I-0816/TALE-OF-GENJI](https://huggingface.co/datasets/R-I-0816/TALE-OF-GENJI)
  - æºæ°ç‰©èªã«ã¤ã„ã¦ ã€æºæ°ç‰©èªã€ã¯ã€æ—¥æœ¬ã®å¤å…¸æ–‡å­¦ã®åä½œã§ã™ã€‚
  - Downloads: 66
- [AkabekoLabs/nihongo-dojo-grades1-2-3-kanji_reading](https://huggingface.co/datasets/AkabekoLabs/nihongo-dojo-grades1-2-3-kanji_reading)
  - nihongo-dojo-grades1-2-3-kanji_reading ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 66
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - Dataset overview This dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).
  - Downloads: 65
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
  - Downloads: 65
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN Dataset CT-RATE-JPN is a Japanese-translated version of radiology reports from the CT-RATE dataset, which contains chest CT volumes paired with corresponding radiology reports.
  - Downloads: 64
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - Japanese Wikipedia Human Retrieval dataset This is a Japanese question answereing dataset with retrieval on Wikipedia articles by trained human workers.
  - Downloads: 64
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã¯llm-book/ner-wikipedia-datasetã¨åŒæ§˜ã®ã‚‚ã®ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å…¨éƒ¨ã§8ç¨®é¡ (äººåã€æ³•äººåã€åœ°åã€è£½å“åã€æ”¿æ²»çš„çµ„ç¹”åã€æ–½è¨­åã€ãã®ä»–ã®çµ„ç¹”åã€ã‚¤ãƒ™ãƒ³ãƒˆå)ã‚ã‚Šã¾ã™ã€‚
  - Downloads: 63
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - magpie-reasoning-llama-nemotron-70b-100k-filtered DeL-TaiseiOzaki/magpie-reasoning-llama-nemotron-70b-100kã‹ã‚‰ã€refined_answeråˆ—ã«"æ”¹è‰¯"ã¨ã„ã†æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æŠ½å‡ºã—ã€OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 63
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„19800ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 63
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ mc4-jaãªã©ã®webã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï½¤æ•™å¸«ãªã—å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´„1ä¸‡ä»¶ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ è‘—ä½œæ¨©æ³•ã§èªã‚ã‚‰ã‚ŒãŸæƒ…å ±è§£æç›®çš„ã§ä½¿ç”¨ã§ãã¾ã™ï½¡ ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã—ã‹parquetåŒ–ã•ã‚Œã¦ã„ãªã„ã®ã§ï½¤ã”æ³¨æ„ãã ã•ã„ï½¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã¯outãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚ã‚Šã¾ã™ git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãã ã•ã„ï½¡
  - Downloads: 60
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - J-ResearchCorpus Update: 2024/3/16è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š(NLP2024)ã‚’å«ã‚€ã€è«–æ–‡ 1,343 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ  2024/2/25è¨€èªå‡¦ç†å­¦ä¼šèªŒã€Œè‡ªç„¶è¨€èªå‡¦ç†ã€ã®ã†ã¡ CC-BY-4.0 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ 360 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ  æ¦‚è¦ CC-BY-* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè«–æ–‡ã‚„å­¦ä¼šèªŒç­‰ã‹ã‚‰æŠœç²‹ã—ãŸé«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 60
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - CABank Japanese CallHome Corpus Participants: 120 Type of Study: phone call Location: United States Media type: audio DOI: doi:10.21415/T5H59V Web: https://ca.talkbank.org/access/CallHome/jpn.html Citation information Some citation here.
  - Downloads: 59
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - japanese-stackexchange è‹±èªã«ã‚ˆã‚‹æ—¥æœ¬èªã«é–¢ã™ã‚‹è³ªå•ãŒã§ãã‚‹ Japanese Stack Exchange ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ— ã‚’ã‚‚ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ãƒšã‚¢ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸ QA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 59
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤phi3ã§å†ç”Ÿæˆã—ãŸæ–‡ç« ã§ã™ï½¡ Wikibooks Wikipedia Cosmopedia åˆ¤ä¾‹ãƒ‡ãƒ¼ã‚¿ ãƒ‡ãƒ¼ã‚¿ parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒæ•°åGBç¨‹åº¦ã‚ã‚Šã¾ã™ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã§ã¯ï½¤ã¯ã˜ã‚ã®æ•°GBç¨‹åº¦ã—ã‹èª­ã¿è¾¼ã‚ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï½¡git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ï½¡ ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
  - Downloads: 59
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - æ—¥æœ¬èªæŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªç‰ˆã®æŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 59
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - abc-multiple-choice Dataset abc-multiple-choice ã¯ã€ç«¶æŠ€ã‚¯ã‚¤ã‚ºã®å¤§ä¼šã€Œabcã€ã§ä½¿ç”¨ã•ã‚ŒãŸ4æŠå•é¡Œã‚’å…ƒã«ä½œæˆã•ã‚ŒãŸã€å¤šè‚¢é¸æŠå¼ã®è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 58
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 57
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k Magpieã®æ‰‹æ³•ã‚’weblab-GENIAC/Tanuki-8B-dpo-v1.0ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€97269ä»¶ã®æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 56
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun Dataset Description ChouBun is a benchmark for assessing LLMs' performance in long-context tasks in the Japanese language.
  - Downloads: 56
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA ScreenTalk_JA is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
  - Downloads: 56
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
  - Downloads: 54
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 54
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - Dataset overview This is a dataset for Japanese natural language processing with multi-label annotations of research field labels for GitHub repositories in the NLP domain.
  - Downloads: 54
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - Data extracted from CommonCrawlPDF Japanese domain Code is here
  - Downloads: 53
- [llm-compe-2025-kato/tag-validation-results-test](https://huggingface.co/datasets/llm-compe-2025-kato/tag-validation-results-test)
  - ã‚¿ã‚°æ¤œè¨¼çµæœ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯èª¬æ˜ç”Ÿæˆã«ãŠã‘ã‚‹ã‚¿ã‚°æ¤œè¨¼ã®çµæœã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
  - Downloads: 53
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - Nhentai Dataset A collection of Japanese manga in CBZ format from Nhentai, containing adult content manga with associated metadata.
  - Downloads: 52
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [APTOinc/japanese-reasoning-dataset-sample](https://huggingface.co/datasets/APTOinc/japanese-reasoning-dataset-sample)
  - reasoningãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 51
- [retarfi/JFinTEB](https://huggingface.co/datasets/retarfi/JFinTEB)
  - JFinTEB:
  - Downloads: 51
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - OpenMathInstruct-1 ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãª180ä¸‡ä»¶ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 50
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - Dataset origin: https://github.com/doc-analysis/XFUND XFUND:
  - Downloads: 50
- [Sakaji-Lab/LATGNJ](https://huggingface.co/datasets/Sakaji-Lab/LATGNJ)
  - JAgriN: Japanese Agricultural Dataset of Nagasaki Prefecture formerly LATGNJ: Local Agricultural Technical Guideline of Nagasaki, Japan Dataset Metadata (Datasheet Summary)
  - Downloads: 49
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - bluemoon-fandom-1-1-rp-jp-translated A subset of Squish42/bluemoon-fandom-1-1-rp-cleaned translated to Japanese using command-r-08-2024.
  - Downloads: 49
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - JP Voice-Text Dataset for
  - Downloads: 49
- [ChatGPTJP1/chatgptjpai](https://huggingface.co/datasets/ChatGPTJP1/chatgptjpai)
  - AIã¨æ•™è‚²ã®æœªæ¥ï¼šãƒãƒ£ãƒƒãƒˆGPTãŒå­¦ã³ã®ä½“é¨“ã‚’ã©ã†å¤‰ãˆã‚‹ã‹ ã¯ã˜ã‚ã«ï¼šAIãŒå­¦ã³ã®ç©ºé–“ã«è¶³ã‚’è¸ã¿å…¥ã‚Œã‚‹ã¨ã å­¦ã³ã¯ã€ç§ãŸã¡ã®äººç”Ÿã‚’è±Šã‹ã«ã—ã€æœªæ¥ã‚’åˆ‡ã‚Šé–‹ãä¸Šã§ä¸å¯æ¬ ãªè¦ç´ ã§ã™ã€‚
  - Downloads: 49
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - Japanese-Novels This dataset contains Japanese web novels that I collected personally.
  - Downloads: 49
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1ã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ä¸€éƒ¨ã¨ã€OpenAIã«ç”Ÿæˆã•ã›ãŸæ–‡ç« ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€tohoku-nlp/bert-base-japanese-whole-word-masking ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ãŸæ–‡ç« ã‚’æ–‡è„ˆãŒæˆã‚Šç«‹ã¤å½¢ã§åˆæˆã—ã€æ–°ãŸãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚‚ã®ã€‚
  - Downloads: 49
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja This repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 48
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - Japanese Prompt of GuanacoDataset extracted using langdetect.
  - Downloads: 47
- [kujirahand/popular_anime_corpus](https://huggingface.co/datasets/kujirahand/popular_anime_corpus)
  - popular_anime_corpus æœ‰åãªã‚¢ãƒ‹ãƒ¡ã®ã‚ã‚‰ã™ã˜ã‚„ä¸»äººå…¬ã®æƒ…å ±ã‚’ã¾ã¨ã‚ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ã€‚
  - Downloads: 47
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 47
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - Dataset used to train PokÃ©mon text to image model, add a Japanese Column of PokÃ©mon BLIP captions BLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
  - Downloads: 46
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - Anime Songs Lyrics Dataset â€• ã‚¢ãƒ‹ãƒ¡ã‚½ãƒ³ã‚°ã®æ­Œè©ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Welcome to the Anime Songs Lyrics Dataset Overview This dataset compiles a diverse collection of lyrics from various anime songs, providing a rich resource for enthusiasts and researchers alike.
  - Downloads: 46
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - Tengentoppa corpus for sft (Combined Japanese Instruction Dataset) æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã® instruction-following ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ16å€‹ã‚’çµ±åˆã—ã¦ä½œæˆã•ã‚ŒãŸå¤§è¦æ¨¡ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 46
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 46
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k-formatted æ¦‚è¦ deepseek-ai/DeepSeek-R1-0528ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 45
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - Magpie-Tanuki-8B-annotated-96k Magpieã®æ‰‹æ³•ã‚’weblab-GENIAC/Tanuki-8B-dpo-v1.0ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Magpie-Tanuki-8B-97kã«å¯¾ã—ã¦ã€cyberagent/calm3-22b-chatã‚’ç”¨ã„ã¦instructionã«å¯¾ã—ã¦é›£æ˜“åº¦ã€ã‚¯ã‚ªãƒªãƒ†ã‚£ã€ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 44
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - Description This is a templated version of data from ~40 Japanese open source downstream task datasets.
  - Downloads: 44
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ç¢ºä¿ã¨ SB Intuitions ä¿®æ­£ç‰ˆã®å…¬é–‹ç”¨ã‚¯ãƒ­ãƒ¼ãƒ³ ã‚½ãƒ¼ã‚¹: yahoojapan/JGLUE on GitHub datasets/jcommonsenseqa-v1.1 JCommonsenseQA JCommonsenseQA is a Japanese version of CommonsenseQA (Talmor+, 2019), which is a multiple-choice question answering dataset that requires commonsense reasoning ability.
  - Downloads: 43
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 43
- [cc-clean/CC-MAIN-2015-14](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-14)
  - CC-MAIN-2015-14ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 43
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - Derived from é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®éŸ³å£°ãƒ‡ã‚¤ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åæ³¨é‡ˆä»˜ãéŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ https://github.com/ndl-lab/hurigana-speech-corpus-aozora All text files in the original data were processed for 3361443 entries; duplicates and entries with no kanji were dropped post cleanup
  - Downloads: 43
- [iammytoo/japanese-humor-evaluation-v2](https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2)
  - Japanese Multimodal Humor Evaluation Dataset (v2) ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®ãŠé¡Œã«å¯¾ã™ã‚‹é¢ç™½ã„å›ç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 43
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„20000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 42
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - Dataset Information ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ThePioneer/japanese-photosã®å†™çœŸã‚’ãŠå€Ÿã‚Šã—ã¦ã€
  - Downloads: 42
- [reep0610/AGI-japanese-text-dataset-with-English-explanations](https://huggingface.co/datasets/reep0610/AGI-japanese-text-dataset-with-English-explanations)
  - è‡ªå·±è¨˜è¿°å‹è‡ªå¾‹çš„æ·±å±¤å­¦ç¿’ã¨ã¯ã€å¤–éƒ¨ã‹ã‚‰ã®æ˜ç¤ºçš„ãªå ±é…¬ã‚„ãƒ©ãƒ™ãƒ«ã«ä¾å­˜ã›ãšã€ãƒ¢ãƒ‡ãƒ«è‡ªèº«ãŒå†…çš„ãªç›®çš„ã‚„æ„å‘³ã‚’å½¢æˆãƒ»è¨˜è¿°ã—ãªãŒã‚‰å­¦ç¿’ã‚’é€²ã‚ã€æœ€çµ‚çš„ã«è‡ªå·±æ„è­˜ã‚„æ„å‘³ç†è§£ã®ç²å¾—ã‚’ç›®æŒ‡ã™æ çµ„ã¿ã§ã™ã€‚
  - Downloads: 42
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG : LLM Japanese RAG performance evaluation This is a benchmark for LLM Japanese RAG performance evaluation.
  - Downloads: 42
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - æ—¥æœ¬èªæŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€SkunkworksAI/reasoning-0.01 ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€Qwen/Qwen2.5-32B-Instruct ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªç‰ˆã®æŒ‡ç¤ºãƒ»æ¨è«–ãƒ»å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 41
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA Dataset Details This is the MangaVQA benchmark, designed to evaluate performance under realistic conditions for manga understanding.
  - Downloads: 41
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIO with extended answers AIO (AIç‹) is a Japanese quiz dataset.
  - Downloads: 41
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - æ—¥æœ¬èªWikipediaå…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã‚Œã¯äº¬éƒ½å¤§å­¦ è¨€èªãƒ¡ãƒ‡ã‚£ã‚¢ç ”ç©¶å®¤ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HuggingFaceã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ï¼
  - Downloads: 41
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - æ€è€ƒéç¨‹ã‚’å«ã‚€ã€æ—¥æœ¬èªè³ªå•ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å›ç­”ãƒ»æ–‡ç« ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ fineweb2-edu-japanese ã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€DeepSeek-R1 ã§æ–‡ç« (text)ã‹ã‚‰è³ªå•æ–‡ã¨å›ç­”éƒ¨åˆ†ã®è©²å½“ç®‡æ‰€ã‚’ç”Ÿæˆã—ãŸæ—¥æœ¬èªã®è³ªå•ã¨å¯¾å¿œã™ã‚‹æ–‡ç« ãƒ»å›ç­”éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 40
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - dataset split from joujiboi/japanese-anime-speech-v2
  - Downloads: 40
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - EN/JA dataset used for shisa-7b-v1 - see details in that model's readme.
  - Downloads: 40
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - Reference https://huggingface.co/datasets/mc4
  - Downloads: 40
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 39
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 39
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 38
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k 20240907 ãƒ‡ãƒ¼ã‚¿å¢—é‡ï¼ˆç´„19800ä»¶â†’ç´„39600ä»¶ï¼‰ æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„39600ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 37
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - è‡ªå‹•ç”Ÿæˆã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠœç²‹ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤Calm3-22B-chatã§è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒãƒ³ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸï½¡ ç”Ÿæˆã‚³ãƒ¼ãƒ‰ å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ é™å®šver è»½ã„ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
  - Downloads: 37
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - Dataset Summary JMultiWOZ is a large-scale Japanese multi-domain task-oriented dialogue dataset.
  - Downloads: 37
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - For more information, see website below!
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€9æœˆã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 37
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
  - Downloads: 37
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - The dataset contains (almost) the entire OpenSubtittles database for Japanese: Over 7000 tv shows and/or movies.
  - Downloads: 36
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - Japanese Laws This dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov.
  - Downloads: 36
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - JAQKET ã‹ã‚‰ CC-BY-SA ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ AIç‹ å…¬å¼é…å¸ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(JAQKET) ã§é…å¸ƒã•ã‚Œã¦ã„ã‚‹ã‚¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒ CC-BY-SA-4.0ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 36
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - llm-jp-corpus-v3ã®warp_htmlã®ã†ã¡level2ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’HFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã€å„ãƒ‡ãƒ¼ã‚¿ã«ä»˜ä¸ã•ã‚ŒãŸURLã‹ã‚‰å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—å¯èƒ½ãªã‚‚ã®ã«ã¤ã„ã¦ã¯å–å¾—ã—ã¦ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 36
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Dataset origin: https://jibiki.fr/data/ Description Les buts du projet Jibiki.fr sont de construire de maniÃ¨re collaborative un dictionnaire franÃ§ais-japonais de qualitÃ© et Ã  large couverture ainsi qu'un corpus bilingue alignÃ©.
  - Downloads: 36
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
  - Downloads: 35
- [cc-clean/CC-MAIN-2015-11](https://huggingface.co/datasets/cc-clean/CC-MAIN-2015-11)
  - CC-MAIN-2015-11ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 34
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - Speech-Translation-Instructions The instructions translated from 120 languages Common Voice to english, arabic, japanese, mandarin and french from common voice speech dataset.
  - Downloads: 34
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja æ¦‚è¦ å¤šè¨€èªåŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® ApolloCorpus ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸ 525k ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 34
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - fineweb-2-edu-japanese-scores fineweb-2æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ•™è‚²çš„ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (0-4æ®µéš) æ¦‚è¦: ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€FineWeb-Edu classifier ã®æ‰‹æ³•ã«å€£ã„ã€Deepseek API ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡ã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ fineweb-2 æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ•™è‚²çš„è¦–ç‚¹ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 33
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 33
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - For the English version, please click here.
  - Downloads: 32
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - Asian Language Treebank (ALT) Project ALT Parallel Corpusã®ã†ã¡ã€æ—¥è‹±å¯¾è¨³éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 32
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset å•†ç”¨åˆ©ç”¨å¯èƒ½ãªè¶…å°è¦æ¨¡é«˜å“è³ªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 32
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleanedã‚’å…ƒã«ã€10æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 32
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
  - Downloads: 31
- [DataPilot/Zero_SFT_Ja_v3.5](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3.5)
  - ğŸ“¦ Zero_SFT_Ja_v3.5 ğŸ§­ ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ Zero_SFT_Ja_v3.5 ã¯ã€Base-Refineï¼ˆBAREï¼‰æ‰‹æ³•ã«ã‚ˆã‚Šæ§‹ç¯‰ã•ã‚ŒãŸã€108,000 ä»¶ã®æ—¥æœ¬èªæŒ‡ç¤ºå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 31
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - en-ja-align æ—¥è‹±å¯¾è¨³æ–‡å¯¾å¿œä»˜ã‘ãƒ‡ãƒ¼ã‚¿(å†…å±±ã‚‰, 2003)ã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥è‹±å¯¾è¨³æ–‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 31
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella corpus : Japanese a cappella vocal ensemble corpus The jaCappella corpus is a corpus of Japanese a cappella vocal ensembles.
  - Downloads: 31
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset A voice dataset collected from Project Sekai charactor Emu Otori Introduction Size: 2735, all WAV format.
  - Downloads: 31
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - LLMChat-Judge-Results team-hatakeyama-phase2/LLMChatã®2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã«å¯¾ã—ã¦ã€æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦Pairwiseè©•ä¾¡ã‚’è¡Œã£ãŸçµæœã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 30
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA YakugakuQA is a question answering dataset, consisting of 13 years (2012-2024)
  - Downloads: 30
- [KaraKaraWitch/JT4LLM-260K](https://huggingface.co/datasets/KaraKaraWitch/JT4LLM-260K)
  - Jitendex for Language Models (JT4LLM) Dataset Summary JT4LLM is a reprocessed version of Jitendex Japanese-to-English dictionary for language models.
  - Downloads: 30
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - NuminaMath Enhanced CoT Dataset (Japanese 50k Subset)
  - Downloads: 30
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - llm-jp-corpus-v3ã®kakenã‚µãƒ–ã‚»ãƒƒãƒˆã‚’HFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã€å„ãƒ‡ãƒ¼ã‚¿ã«ä»˜ä¸ã•ã‚ŒãŸURLã‹ã‚‰å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—å¯èƒ½ãªã‚‚ã®ã«ã¤ã„ã¦ã¯å–å¾—ã—ã¦ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 30
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - Derived from å…¨å›½æ›¸èªŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆGitHubï¼‰
  - Downloads: 30
- [YUGOROU/Counseling-Reasoning-v1.8](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.8)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°åˆ†é‡ã«ãŠã‘ã‚‹å•é¡Œè§£æ±ºã¨å›ç­”ç”Ÿæˆã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
  - Downloads: 30
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - ğŸ“˜ğŸ“• SimpleStories ğŸ“™ğŸ“— ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€gpt-4o-miniã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸçŸ­ç·¨å°èª¬ã§å‡ºæ¥ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 30
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - Filtered and modified version of Japanese/Chinese language pair data from WikiMatrix v1.
  - Downloads: 29
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - Wikipediaæ—¥æœ¬èªç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(izumi-lab/wikipedia-ja-20230720)
  - Downloads: 29
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM: Japanese semantic test suite (Japanese FraCaS and extensions) å™è¿°æ–‡é–“ã®å«æ„é–¢ä¿‚ã¯ã€è¨€èªå­¦ã«ãŠã„ã¦ã¯æ„å‘³è«–ã®ä¸­å¿ƒçš„ãªèª¬æ˜å¯¾è±¡ã®ä¸€ã¤ã§ã‚ã‚‹ã¨ã¨ã‚‚ã«ã€ç†è«–ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 29
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models Jamp(tomo-vv/temporalNLI_dataset)
  - Downloads: 29
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - å¦–æ€ªçŸ¥è­˜è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã¸å‘ã‘ã¦ã€ï¼ˆNLP2025ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 29
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - Japanese Creativity Questions (JCQ) Dataset Description JCQã¯å‰µé€ æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®7ã‚¿ã‚¹ã‚¯ã€å„100å•ã‹ã‚‰ãªã‚‹æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 29
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-R1-0528-10k æ¦‚è¦ deepseek-ai/DeepSeek-R1-0528ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 29
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - GSM8K Japanese Slim Japanese translated version of openai/gsm8k, and the answer extracted from descriptions.
  - Downloads: 29
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Phi-3ã§å†ç”Ÿæˆã—ï½¤æ›´ã«è‡ªå‹•è‹±è¨³ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ Wikibooks Wikipedia ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒæ•°åGBç¨‹åº¦ã‚ã‚Šã¾ã™ datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã§ã¯ï½¤ã¯ã˜ã‚ã®æ•°GBç¨‹åº¦ã—ã‹èª­ã¿è¾¼ã‚ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï½¡git lfsãªã©ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ï½¡
  - Downloads: 29
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka: A Journey Through Hyakumonogatari's Ghostly Tales Welcome to the Kaidan Nihonbunka Dataset About Name kaidan Nihonbunka translates to æ€ªè«‡æ—¥æœ¬æ–‡åŒ– in Japanese: æ€ªè«‡ (Kwaidan): Ghost story or supernatural tale.
  - Downloads: 29
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - Overview This dataset is of conversations extracted from Aozora Bunko (é’ç©ºæ–‡åº«), which collects public-domain books in Japan, using a simple heuristic approach.
  - Downloads: 29
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ æ‰‹å‹•ã§ä½œæˆã—ãŸDatabricksã«é–¢ã™ã‚‹è³ªå•ã¨å›ç­”ãƒšã‚¢ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 29
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M: Japanese Light Novel Character Names Corpus Overview This dataset extracts fictional character names from the publicly available text of novels on the Japanese light novel platform "ShÅsetsuka ni NarÅ" (syosetu.com),
  - Downloads: 28
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted æ¦‚è¦ deepseek-ai/DeepSeek-V3-0324ã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 28
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆFineWeb2-HQ å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å¤šè¨€èªã§å·¨å¤§ãªãŸã‚ã€æ‰±ã„ã‚„ã™ã„ç”¨ã«æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’ç´„200GBã ã‘æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ wc çµæœ 1763269 38541549 5370473709 fineweb_jpn_Jpan_chunk_0.jsonl 1784158 37430170 5370514369 fineweb_jpn_Jpan_chunk_1.jsonl 1639554 40065129 5370372344 fineweb_jpn_Jpan_chunk_10.jsonl 1575127 42167166 5370298354 fineweb_jpn_Jpan_chunk_11.jsonl 1686375 39225898 5370402506 fineweb_jpn_Jpan_chunk_12.jsonl 1786948 36456352 5370498572 fineweb_jpn_Jpan_chunk_13.jsonl 1700447 38657869 5370422377 fineweb_jpn_Jpan_chunk_14.jsonl 1649880 402340
  - Downloads: 28
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA Dataset Description Japanese Image Classification Visual Question Answering (JIC-VQA)
  - Downloads: 28
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k Japanese Ultrachat 6.6k is the Japanese-translated version of the subset of ultrachat_200k using machine translation.
  - Downloads: 28
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - Introduction This is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
  - Downloads: 28
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 28
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-clean filtered row["meta"]["æ–‡å­—é£ã„ç¨®åˆ¥"] == "æ–°å­—æ–°ä»®å"
  - Downloads: 27
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA v1.5
  - Downloads: 27
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - mmarcoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦ã€queryã‚’keyã¨ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 27
- [Chattso-GPT/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chattso-GPT/Japanese-patent-evaluation-dataset-01)
  - è©¦é¨“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³• ChatGPT-4oã§å¼ç†å£«ç´¹ä»‹ã‚’å«ã‚€5ã¤ã®è¦³ç‚¹ã‹ã‚‰50ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ ChatGPT-4oã§ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å›ç­”ã¨è©•ä¾¡ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆâ€»å¼ç†å£«ã‚’ç›´æ¥ç´¹ä»‹ã™ã‚‹10å•ã¯é™¤ã é™¤å¤–ã—ãŸ10å•ã«é–¢ã—ã¦ã¯ã€é–‹æ”¾ç‰¹è¨±æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŠã³j-platpatã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå›ç­”ã‚’æ‰‹å‹•ä½œæˆ
  - Downloads: 27
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - fineweb-2-japanese-noise-spans ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€FineWeb2 ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Webç‰¹æœ‰ã®ãƒã‚¤ã‚ºç®‡æ‰€ã‚’åˆ¤å®šã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 27
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€20000ä»¶ã®æ—¥â‡”è‹±ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 27
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - NVIDIA ãŒå…¬é–‹ã—ã¦ã„ã‚‹ SteerLM å‘ã‘ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ HelpSteer2ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 27
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSECãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
  - Downloads: 26
- [Silviase/JMMMU-EN](https://huggingface.co/datasets/Silviase/JMMMU-EN)
  - Jmmmu (English Translation)
  - Downloads: 26
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - Veterinary Medicine Japanese Dataset This dataset contains audio files of veterinary medicine terms in Japanese, categorized into drugs, diseases, and symptoms.
  - Downloads: 26
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - Japanese-Law-Translation Dataset Summary
  - Downloads: 26
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - oasst1-89k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 25
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - ja-stackoverflow æ—¥æœ¬èªç‰ˆ Stack Overflow ã® ã‚¹ã‚¿ãƒƒã‚¯ãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ— ã‚’ã‚‚ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ãƒšã‚¢ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸ QA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 25
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - Synthetic-JP-EN-Coding-Dataset-801k Magpieã«ã‚ˆã£ã¦ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69kã‚’å…ƒã«ã€Evol-Instructã®ã‚ˆã†ãªæ‰‹æ³•ã‚’ç”¨ã„ã¦è¤‡æ•°ã®instructionã¨resonseã‚’ç”Ÿæˆã—æ‹¡å¼µã—ã¦ä½œæˆã—ãŸã€æ—¥è‹±æ··åˆ801262ä»¶ã®ã‚³ãƒ¼ãƒ‰SFTç”¨åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 25
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - Synthetic-JP-EN-Coding-Dataset-Magpie-69k Magpieã®æ‰‹æ³•ã‚’æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„69000ä»¶ã®æ—¥æœ¬èªãƒ»è‹±èªã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 25
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã®Q&amp;Aã®è‡ªå‹•ç”Ÿæˆ Mixtral 8x22bã®GGUF(5bit)ã‚’ãƒ™ãƒ¼ã‚¹ã«ï½¤Wikipediaæ—¥æœ¬èªç‰ˆã®è¨˜äº‹ã‹ã‚‰ï½¤ è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰1 è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰2 ã‚’ä½¿ã£ã¦Q&amp;Aã‚’ä½œæˆã—ã¾ã—ãŸï½¡ è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ æ³¨æ„ å›ç­”ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç­‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ï½¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‹ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï½¡
  - Downloads: 25
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - Description This is a collection of raw data from ~40 Japanese open source downstream task datasets.
  - Downloads: 25
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
  - Downloads: 25
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - language: jp en tags: translation license: cc-by-4.0
  - Downloads: 25
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - Anime Quotes Dataset â€• ã‚¢ãƒ‹ãƒ¡ã®åè¨€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆğŸ Welcome to Anime Quotes Dataset Overview This dataset contains a curated collection of inspiring and memorable quotes from various anime series, sourced from the Anime Motivation website.
  - Downloads: 24
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - è‹±èªWikipediaè¨˜äº‹ã®å†’é ­è¤‡æ•°æ–‡ã‚’æŠ½å‡ºã—ã€äººæ‰‹ã§æ—¥æœ¬èªç¿»è¨³ã—ãŸæ–‡ç« ãƒ¬ãƒ™ãƒ«å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 24
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - Corrected MT-Bench-ja Inflection AIã«ã‚ˆã‚‹Corrected MT-Benchã®æ—¥æœ¬èªè¨³ã§ã™ã€‚
  - Downloads: 24
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CohereForAI/aya_datasetã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 24
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - æ—¥æœ¬èªãƒ•ã‚§ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ—¥æœ¬èªãƒ•ã‚§ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚’ HuggingFace datasets ç”¨ã«å¤‰æ›ã€‚
  - Downloads: 24
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - This dataset was created using AI Gemini 2.0 Flash Experimental from the original subtitle format.
  - Downloads: 23
- [ayousanz/css10-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ljspeech)
  - CSS10-LJSpeech CSS10-LJSpeech ã¯ã€Park et al.
  - Downloads: 23
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - The JaNLI (Japanese Adversarial NLI) dataset, inspired by the English HANS dataset, is designed to necessitate an understanding of Japanese linguistic phenomena and to illuminate the vulnerabilities of models.
  - Downloads: 23
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - To avoid leaking the dataset to LLM training data, it is not distributed on the open web.
  - Downloads: 23
- [jluang09/seo_bulk_ja](https://huggingface.co/datasets/jluang09/seo_bulk_ja)
  - ğŸ“˜ qwen3-8b-lora-seo-ja Qwen1.5-8B LoRAãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¥æœ¬èªSEOæ–‡ç”Ÿæˆç”¨ï¼‰A
  - Downloads: 23
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªç‰ˆWikipediaã®è¨˜äº‹ã‚’å…ƒã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 22
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - cosmopedia-japanese-20kã®ãƒ‡ãƒ¼ã‚¿ã«ã€kunishouæ§˜ã‹ã‚‰20k-100kã‚’ã”æä¾›ã„ãŸã ã‘ã‚‹ã“ã¨ã«ãªã‚Š100kã¾ã§æ‹¡å¤§ã—ã¾ã—ãŸã€‚
  - Downloads: 22
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - Wikidata parallel descriptions en-ja Parallel corpus for machine translation generated from wikidata dump (2024-05-06).
  - Downloads: 22
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-test", split="test") æ¦‚è¦ å¤§å–œåˆ©æŠ•ç¨¿ã‚µã‚¤ãƒˆBoketeã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 22
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 22
- [Silviase/augeo-ja](https://huggingface.co/datasets/Silviase/augeo-ja)
  - Augeo Geometry Problems Dataset Dataset Description
  - Downloads: 22
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") æ¦‚è¦ å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã®ã€å†™çœŸå·æŸ³ã€ã¨ã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 22
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 22
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - ãƒ©ã‚¤ãƒ–ãƒ‰ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã®3è¡Œè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 22
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - cyberagent/calm2-7b-chatã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸæ—¥æœ¬èªInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 22
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤Calm3-22bã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ ã¯ã˜ã‚ã®è³ªå•(q1)ã‚’ï½¤ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¾ã—ãŸï½¡ãã®å¾Œã®ã‚„ã‚Šã¨ã‚Šã¯ã™ã¹ã¦ï½¤CalmãŒç”Ÿæˆã—ã¾ã—ãŸï½¡è³ªå•æ–‡ã«ã¤ã„ã¦ã¯ï½¤å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã—ã¾ã™ï½¡ oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
  - Downloads: 21
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE[JNLI]: Japanese General Language Understanding Evaluation JNLI(yahoojapan/JGLUE)
  - Downloads: 21
- [Itbanque/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/Itbanque/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS ScreenTalk_JA2ZH-XS is a paired dataset of Japanese speech and Chinese translated text released by DataLabX.
  - Downloads: 21
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - Description This is a WIP dataset!
  - Downloads: 21
- [li-lab/JPMedReason](https://huggingface.co/datasets/li-lab/JPMedReason)
  - JPMedReason ğŸ‡¯ğŸ‡µğŸ§  Japanese Medical Reasoning Dataset (Translation of MedReason)
  - Downloads: 21
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - æ—¥æœ¬éƒµä¾¿ãŒæä¾›ã™ã‚‹ã€Œå›½éš›éƒµä¾¿ å†…å®¹å“ã®æ—¥è‹±ãƒ»ä¸­è‹±è¨³ã€HSã‚³ãƒ¼ãƒ‰é¡ã€ï¼ˆ2024/05/09ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
  - Downloads: 21
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - é•·æ–‡ã‹ã‚‰ã®è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 21
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - , 2023) was trained on.
  - Downloads: 21
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 21
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
  - Downloads: 21
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - For more information, see website below!
  - Downloads: 21
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - æ™‚äº‹æƒ…å ±ã«é–¢ã™ã‚‹æ—¥æœ¬èªQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹Qã€ã¯Hugging Faceã«ã¦ç„¡å„Ÿã§é…å¸ƒã—ã¾ã™ã€‚
  - Downloads: 21
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - æ¦‚è¦ ãƒãƒ£ãƒƒãƒˆLLMã«pythoné–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’ä»˜ä¸ã™ã‚‹ãŸã‚ã®ä½å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 20
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA Training Dataset Dataset details This dataset is a synthetic VQA dataset for Manga.
  - Downloads: 20
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 20
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - Dataset Summary This dataset is a Japanese-translated version of the OpenO1-SFT dataset, containing Chain of Thought (CoT) reasoning examples designed for fine-tuning language models.
  - Downloads: 20
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - Dataset Summary This dataset is a Japanese-translated subset of the NuminaMath CoT dataset, containing the first 100k samples from the original dataset.
  - Downloads: 20
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - Self-Instruct-Qwen2.5-72B-Instruct-60k æ¦‚è¦ ä»¥ä¸‹ã®æ‰‹é †ã§ä½œæˆã—ãŸç´„6ä¸‡ä»¶ã®æ—¥æœ¬èªã®åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 20
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - oasst2-33k-ja This repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 20
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangentã¯äººæ‰‹ã§ä½œæˆã•ã‚ŒãŸé«˜å“è³ªã§ã‚¯ãƒªãƒ¼ãƒ³ãª100ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªCoTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 20
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ€è€ƒãƒ¢ãƒ‡ãƒ«ã‚’è£½ä½œã™ã‚‹éš›ã®ã‚‚ã¨ã¨ãªã‚‹è³ªå•ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 20
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - (æ—¥æœ¬èªç‰ˆã¯ã“ã¡ã‚‰) Overview This dataset provides information extracted from â‰ˆ 7 k Japanese medical-journal articles (The Journal of the Japanese Society of Internal Medicine, 2003 â€“ 2023).
  - Downloads: 20
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - The images were sourced from https://huggingface.co/datasets/ThePioneer/japanese-photos.
  - Downloads: 19
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - VTuber Overview Dataset (GPT-4o Search Preview) æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼ŒGPT-4o Search Preview ã‚’æ´»ç”¨ã—ã¦åé›†ã—ãŸ VTuber ã«é–¢ã™ã‚‹æ´»å‹•å†…å®¹ã‚„ç‰¹å¾´ï¼Œã‚³ãƒ©ãƒœå±¥æ­´ãªã©ãŒè‡ªç„¶è¨€èªã§ã¾ã¨ã‚ã‚‰ã‚Œã¦ãŠã„ã¾ã™ã€‚
  - Downloads: 19
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Conversations-Magpie-Nemotron-4-10k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªinstruction tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 19
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 19
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - åŒ»å¸«å›½å®¶è©¦é¨“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆNMLE datasetsï¼‰ ã¯ã˜ã‚ã« æ—¥æœ¬ã®åŒ»å¸«å›½å®¶è©¦é¨“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ç¬¬110å› - ç¬¬117å›ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ ç”¨é€” ç”¨é€”ã¨ã—ã¦ ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ é€²åŒ–çš„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®ã‚¿ã‚¹ã‚¯ã«ã¤ã‹ã†ï¼ˆã€ŒNew Task Guideã€å‚ç…§ï¼‰ RAGãªã©ã«ç”¨ã„ã‚‹æƒ…å ±æº åŒ»å¸«å›½å®¶è©¦é¨“ã®ä¿¯ç° ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ æ§‹é€  data = { "id": question_id, "question": question_text, "choices": choices, "answer": answers, "explanation": explanation } ä¸€éƒ¨ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«ã§ããªã‹ã£ãŸå•é¡Œï¼ˆç”»åƒãŒãƒ¡ã‚¤ãƒ³ã®å‡ºé¡Œãªã©ï¼‰ãŒæŠœã‘ã¦ã„ã¾ã™ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯CC-BY-NC-ND4.0ã§ã€å•†ç”¨åˆ©ç”¨ç¦æ­¢ã¨ãªã£ã¦ã„ã¾ã™ æ”¹å¤‰ã€æ”¹å–„ã€ãã®ä»–ã”ç›¸è«‡ã«ã¤ã„ã¦ã¯ X: @longislandtea3 ã¾ã§ãŠé¡˜ã„ã—ã¾ã™ å…è²¬äº‹é … ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½¿ç”¨ã«ã‚ˆã‚Šç”Ÿã˜ãŸç›´æ¥çš„ã€é–“æ¥çš„ã€ç‰¹åˆ¥ãªæå®³ã€ã¾ãŸã¯ãã®ä»–ã®æå®³ã«ã¤ã„ã¦ã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
  - Downloads: 19
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - HF Datasets version of Tanaka Corpus.
  - Downloads: 19
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - Japanese-Vietnamese Translated Sentence Pairs.
  - Downloads: 19
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - Update: 2023/12/25oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸoasst2-chat-68k-jaã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
  - Downloads: 19
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP This is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
  - Downloads: 19
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - Dataset Summary From the official README.md: CAMERA (CyberAgent Multimodal Evaluation for Ad Text GeneRAtion) is the Japanese ad text generation dataset.
  - Downloads: 19
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - shunk031/JDocQAã®train splitã«å«ã¾ã‚Œã‚‹PDFãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒåŒ–ã—ã€NDLOCRã§OCRã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ãƒšã‚¢ã«ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 19
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - iterative-dpo-data-for-SimPO-iter2 æ¦‚è¦ åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5kã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 19
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - è©¦é¨“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚Šã¾ã™ è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³• ChatGPT-4oã§å¼ç†å£«ç´¹ä»‹ã‚’å«ã‚€5ã¤ã®è¦³ç‚¹ã‹ã‚‰50ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ ChatGPT-4oã§ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å›ç­”ã¨è©•ä¾¡ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆâ€»å¼ç†å£«ã‚’ç›´æ¥ç´¹ä»‹ã™ã‚‹10å•ã¯é™¤ã é™¤å¤–ã—ãŸ10å•ã«é–¢ã—ã¦ã¯ã€é–‹æ”¾ç‰¹è¨±æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŠã³j-platpatã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå›ç­”ã‚’æ‰‹å‹•ä½œæˆ
  - Downloads: 19
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - Chatbot Arena Conversationsã®è³ªå•æ–‡ã‹ã‚‰ã€aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ã‚’ä½¿ç”¨ã—ã¦å¿œç­”æ–‡ã‚’ä½œæˆã—ã¾ã—ãŸ è³ªå•æ–‡ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®Promptéƒ¨åˆ†ã‚’ä½¿ç”¨ã—ã¾ã—ãŸ Chatbot Arena Conversations JA (calm2) ä»¥ä¸‹å¼•ç”¨ã§ã™ã€‚
  - Downloads: 19
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of piqa.
  - Downloads: 18
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - Dataset Details Dataset Sources Repository: Helsinki-NLP/Tatoeba-Challenge Detail: Japanese - Korean jpn-kor Uses The dataset can be used to train the translation model that translates Japanese sentence to Korean.
  - Downloads: 18
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Pretrain is a localized version of the original LLaVA Pretrain dataset.
  - Downloads: 18
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - chatbot-arena-ja-calm2-7b-chatã‹ã‚‰promptãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 18
- [clnine/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-nikkei225)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æ—¥çµŒå¹³å‡æ ªä¾¡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 18
- [nguyenthanhasia/japanese-bar-exam-qa](https://huggingface.co/datasets/nguyenthanhasia/japanese-bar-exam-qa)
  - Japanese Bar Examination QA Dataset Dataset Summary This dataset contains question-answer pairs from the Japanese Bar Examination (å¸æ³•è©¦é¨“, Shihou Shiken) spanning from 2015 to 2024.
  - Downloads: 18
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR Dataset Dataset Details
  - Downloads: 18
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - magpie-sft-v1.0-dpo-judged æ¦‚è¦ llm-jp/magpie-sft-v1.0ã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ”¹å¤‰ã‚’åŠ ãˆã¦ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 18
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - The paper of GIELLM dataset.
  - Downloads: 18
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ é–¢é€£ã‚³ãƒ¼ãƒ‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ ã¯ã˜ã‚ã®è³ªå•(q1)ã‚’ï½¤ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¾ã—ãŸï½¡ãã®å¾Œã®ã‚„ã‚Šã¨ã‚Šã¯ã™ã¹ã¦ï½¤MixtralãŒç”Ÿæˆã—ã¾ã—ãŸï½¡è³ªå•æ–‡ã«ã¤ã„ã¦ã¯ï½¤å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã—ã¾ã™ï½¡ oasst2-33k-ja apache 2.0 databricks-dolly-15k-ja cc-by-sa-3.0 minnade CC0 cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental cc-by-4.0
  - Downloads: 18
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This is my conversion of NilanE/ParallelFiction-Ja_En-100k into json which can be read by text-generation-webui when training a model.
  - Downloads: 18
- [Silviase/JA-VG-VQA-500-EN](https://huggingface.co/datasets/Silviase/JA-VG-VQA-500-EN)
  - Ja Vg Vqa 500 (English Translation)
  - Downloads: 18
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¥æœ¬èªLLMã®è©•ä¾¡ç”¨ã¨ã—ã¦ã‚ˆãç”¨ã„ã‚‰ã‚Œã‚‹elyza/ELYZA-tasks-100ã«ã¤ã„ã¦äººé–“ãŒå›ç­”ã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
  - Downloads: 17
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - A more aggressively cleaned up version of Calvin-Xu/Furigana-Aozora-Speech, which consists of 2,536,041 out of the 3,361,443 entries generated from the raw data é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®éŸ³å£°ãƒ‡ã‚¤ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åæ³¨é‡ˆä»˜ãéŸ³å£°ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ https://github.com/ndl-lab/hurigana-speech-corpus-aozora.
  - Downloads: 17
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 17
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset is a translation of https://huggingface.co/datasets/Abirate/english_quotes into Japanese using the llm-jp/llm-jp-3-3.7b-instruct model.
  - Downloads: 17
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - Magpie-Tanuki-Qwen2.5-72B-Answered Aratako/Magpie-Tanuki-8B-annotated-96kã‹ã‚‰input_qualityãŒexcellentã®ã‚‚ã®ã‚’æŠ½å‡ºã—ã€ãã‚Œã«å¯¾ã—ã¦Qwen/Qwen2.5-72B-Instructã§å›ç­”ã®å†ç”Ÿæˆã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ Ani-Bench-JP ã¯ã€æ—¥æœ¬ã®äººæ°—ã‚¢ãƒ‹ãƒ¡ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [Silviase/JA-Multi-Image-VQA-EN](https://huggingface.co/datasets/Silviase/JA-Multi-Image-VQA-EN)
  - Ja Multi Image Vqa (English Translation)
  - Downloads: 17
- [Silviase/Japanese-Heron-Bench-EN](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench-EN)
  - Japanese Heron Bench (English Translation)
  - Downloads: 17
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - æ¦‚è¦ NHKã§å®šæœŸçš„ã«æ”¾é€ã•ã‚Œã¦ã„ãŸã€ç€ä¿¡å¾¡ç¤¼ï¼
  - Downloads: 17
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - Synthetic-JP-Roleplay-Instruction-Nemotron-4 Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„1000ä»¶ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ã®instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - wiki40b-ja ã‹ã‚‰ç”Ÿæˆã—ãŸè³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
  - Downloads: 17
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å„ã‚­ãƒ¼ã¨ãã®èª¬æ˜: state_id: ã‚²ãƒ¼ãƒ ã®çŠ¶æ…‹ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ãŸã‚ã®IDã€‚
  - Downloads: 17
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - diet-members-voice-embeddings æ—¥æœ¬ã®å›½ä¼šè­°å“¡ã®å£°ã‚’ speechbrain/spkrec-ecapa-voxcelebã§ embedding ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This dataset contains document-length Japanese-English parallel texts from various sources.
  - Downloads: 16
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This is a modified version of NilanE/ParallelFiction-Ja_En-100k which has been turned into Alpaca format.
  - Downloads: 16
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-test", split="test") æ¦‚è¦ å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã®ã€å†™çœŸå·æŸ³ã€ã¨ã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ YANS å§”å“¡ãŒä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å«ã¿ã¾ã™ã€‚
  - Downloads: 16
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - PubChem &amp; Wikipedia English-Japanese Paragraph Pair Classification This dataset is a multilingual extension of the PubChem &amp; Wikipedia Paragraphs Pair Classification dataset.
  - Downloads: 16
- [iammytoo/japanese-humor-evaluation](https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation)
  - Japanese Multimodal Humor Evaluation Dataset This dataset combines two Japanese humor datasets for evaluating the funniness of responses to prompts (odai).
  - Downloads: 16
- [ayousanz/css10-ja-ljspeech](https://huggingface.co/datasets/ayousanz/css10-ja-ljspeech)
  - CSS100-LJSpeech (Japanese / Meian) css100-ljspeech ã¯ã€Park et al.
  - Downloads: 16
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - CC-MAIN-2019-35ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 16
- [if001/word_frequency_from_wiki_ja](https://huggingface.co/datasets/if001/word_frequency_from_wiki_ja)
  - wikipediaã®ãƒ‡ãƒ¼ã‚¿400000ã‚’å¯¾è±¡ã«ã€æ–‡ç« ä¸­ã®å‹•è©ã¨åè©ã‚’ã‚«ã‚¦ãƒ³ãƒˆ https://huggingface.co/datasets/izumi-lab/wikipedia-ja-20230720 å…¨ä½“: 221115 åè©: 204661 å‹•è©: 16454
  - Downloads: 16
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - Bluemoon_Top50MB_Sorted_Fixed_ja SicariusSicariiStuff/Bluemoon_Top50MB_Sorted_Fixedã‚’ã€GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awqã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 16
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - Lurunchik/WikiHowNFQAã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€äººæ‰‹ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 16
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ CC-BYç³»ã¾ãŸã¯Apatch-2.0ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ”¹å¤‰ã—ã¦ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 16
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’è¡Œã†éš›ã®SEEDãƒ‡ãƒ¼ã‚¿ã«ã¯æœ‰å¿—ã®æ–¹ã€…ãŒä½œæˆã—ãŸseed_tasks_japanese.jsonlã‚’åˆ©ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚
  - Downloads: 16
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja ã®ä¸­ã‹ã‚‰ JGLUEï¼ˆ JcommonsenseQA , MARC-ja , JSQuAD ï¼‰ã®è¦³ç‚¹ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 16
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 Evol-Alpaca-gen3-500ã¯ã€
  - Downloads: 16
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€LLMã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤èƒ½åŠ›ã‚’è¨ˆæ¸¬ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯Japanese-RP-Benchç”¨ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 16
- [alt-dev/jcrrag](https://huggingface.co/datasets/alt-dev/jcrrag)
  - JCrRAG : Japanese Contextual relevance RAG Benchmark A human-annotated benchmark for evaluating Japanese Retrieval-Augmented Generation (RAG) systems, featuring multi-level complexity and diverse categories.
  - Downloads: 16
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - ãƒ‡ãƒ¼ã‚¿åˆ¶ä½œè€…ï¼ˆt_wï¼‰
  - Downloads: 16
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - ime-and-kakko elyza/ELYZA-tasks-100 ä¸­ã® IME ã®ã‚ˆã†ã«å¤‰æ›å€™è£œã‚’æç¤ºã™ã‚‹ã‚¿ã‚¹ã‚¯ ã‚«ãƒƒã‚³ã®å¯¾å¿œé–¢ä¿‚ã‚’æ•´ãˆã‚‹ã‚¿ã‚¹ã‚¯ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ‰‹ã§ä½œæˆã—ãŸã‚‚ã® æ±äº¬å¤§å­¦æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ï¼ˆæ¾å°¾ç ”ï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« Deep Learning å¿œç”¨è¬›åº§ 2024 ã§é–‹å‚¬ã•ã‚ŒãŸã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€ @pokutuna ãŒä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®è‹¦æ‰‹å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 16
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - ebisuke/liz-nojaloli-ja-ds License MIT License Description ebisuke/liz-nojaloli-jaã®å­¦ç¿’å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - Dataset Description This is the Japanese Translation version of sciq.
  - Downloads: 15
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Dataset Details Dataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
  - Downloads: 15
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 15
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - Dataset details Dataset type:
  - Downloads: 15
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k nvidia/Nemotron-4-340B-Instructã‚’ç”¨ã„ã¦ä½œæˆã—ãŸã€ç´„1000ä»¶ãƒ»å„10ã‚¿ãƒ¼ãƒ³ã®æ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®å¯¾è©±ã‚’åéŒ²ã—ãŸåˆæˆå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - Quiz Worksæ§˜ã«æ²è¼‰ã®ã‚¯ã‚¤ã‚ºã®ã†ã¡ã€2024å¹´8æœˆ4æ—¥~8æœˆ5æ—¥æ™‚ç‚¹ã«ãŠã„ã¦å–å¾—å¯èƒ½ã ã£ãŸã‚¯ã‚¤ã‚ºã‚’åè¼‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 15
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - kanalizer è‹±å˜èªã‹ã‚‰èª­ã¿ã‚’æ¨æ¸¬ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€kanalizerã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç½®ãå ´ã€‚
  - Downloads: 15
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa Dataset Card Overview LogicJa is a multi-turn benchmark designed to assess the reasoning capabilities of Japanese language models across multiple domains.
  - Downloads: 15
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - A JSON based anime dataset containing the most important meta data as well as cross references to various anime sites such as MAL, ANIDB, ANILIST, KITSU and more...
  - Downloads: 15
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - Qwen/Qwen2.5-32B-Instruct-AWQã§ç”Ÿæˆã—ãŸ3ã‚¿ãƒ¼ãƒ³ã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository contains the dataset used for the TaCo paper.
  - Downloads: 15
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This dataset is a collection of Korean, Chinese, and Japanese OpenOrca translation datasets.
  - Downloads: 15
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - æ±äº¬å¤§å­¦æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ä¸»å‚¬ã®LLMè¬›åº§2024ã®ç¬¬5å›ã€ŒSFTã€æ¼”ç¿’ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã®æ›¸èªŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ A dataset of furigana characters created from bibliographic data from the National Diet Library.
  - Downloads: 15
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤Phi-3ã§ä½œæ–‡ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ï½¡ OpenMathInstruct-1-1.8m-ja ã‚³ãƒ¼ãƒ‰ ã“ã¡ã‚‰ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
  - Downloads: 15
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 15
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - Overview This dataset is edited from kunishou/databricks-dolly-15k-en.
  - Downloads: 15
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - Dataset Summary JapaneseGoblin is a dump of en.touhouwiki.net wiki.
  - Downloads: 15
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - Dataset containing ~3000 synthetically generated (by GPT-4o-mini) children's stories in Japanese that only use simple words.
  - Downloads: 15
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®æŒ‡ç¤ºã¨ãã‚Œã«å¯¾ã™ã‚‹æ¨è«–ãƒ»åˆæœŸå¿œç­”ãƒ»æ”¹å–„å¿œç­”ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
  - Downloads: 15
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - Magpie-Qwen-Turbo-27k Aratako/Magpie-Tanuki-8B-annotated-96k ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ©ç”¨ã—ã¦ä»¶æ•°ã‚’æ¸›ã‚‰ã—ã€outputã‚’qwen-2.5-turboã§å†ç”Ÿæˆã—ãŸSFTç”¨ã®26728ä»¶ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 15
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task Quality of Japanese text is somewhat low arise from the combination of synthetic generated text and machine translation API details reformatted dataset of open_preference_v0.1 label 1 stands for chosen sentence label 0 stands for rejected sentence
  - Downloads: 15
- [Silviase/JA-VLM-Bench-In-the-Wild-EN](https://huggingface.co/datasets/Silviase/JA-VLM-Bench-In-the-Wild-EN)
  - Ja Vlm Bench In The Wild (English Translation)
  - Downloads: 15
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This dataset is a subset of the Open Assistant dataset, which contains Japanese conversations only.
  - Downloads: 15
- [morizon/databricks-dolly-15k-ja](https://huggingface.co/datasets/morizon/databricks-dolly-15k-ja)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯kunishou/databricks-dolly-15k-jaã‚’å…ƒã«ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 15
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æºã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚‚ã¨ã«ï½¤RAGå½¢å¼ã®Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ Wikibooks Wikipedia åˆ¤ä¾‹ãƒ‡ãƒ¼ã‚¿ instruction datasetã¨ã—ã¦ã§ã¯ãªãï½¤äº‹å‰å­¦ç¿’ã§ã®åˆ©ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™(è³ªç–‘å¿œç­”ã‚’ã™ã‚‹ãŸã‚ã®è¨“ç·´)ï½¡ ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡
  - Downloads: 15
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench AugeoBench is a multimodal QA benchmark consisting of Japanese entrance-exam-style geometry questions.
  - Downloads: 15
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 15
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - CommonCatalog CC-BY Extention ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 15
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - https://github.com/anthropics/hh-rlhf ã®å†…å®¹ã®ã†ã¡ã€helpful-baseå†…ã®chosenã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹è‹±æ–‡ã‚’fuguMTã§ç¿»è¨³ã€ã†ã¾ãç¿»è¨³ã§ãã¦ã„ãªã„ã‚‚ã®ã‚’é™¤å¤–ã€ä¿®æ­£ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 15
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 15
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - It contains Japanese instruction-like data intended for LLM construction/tuning.
  - Downloads: 14
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - ã‚¯ã‚¤ã‚ºã®æœæ§˜ã«æ²è¼‰ã®ã‚¯ã‚¤ã‚ºã®ã†ã¡ã€2024å¹´8æœˆ5æ—¥æ™‚ç‚¹ã«ãŠã„ã¦å–å¾—å¯èƒ½ã ã£ãŸã‚¯ã‚¤ã‚ºã®ã†ã¡ã€ŒäºŒæ¬¡åˆ©ç”¨è¨±è«¾ãƒ¬ãƒ™ãƒ«ã€ãŒã€Œãƒ•ãƒªãƒ¼ã€ã§ã‚ã£ãŸã‚‚ã®ã‚’åè¼‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 14
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯glaive-aiãŒå…¬é–‹ã—ã¦ã„ã‚‹in-foxhoundã‚’KUJIRAã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 14
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - Vietnamese-Japanese Parallel Corpus ğŸŒŸ If you find this project valuable, please consider starring our VNJPTranslate GitHub repo!
  - Downloads: 14
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - Multilingual Image Translation Datasetï¼š OPUS-MIT-5M
  - Downloads: 14
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully Dataset åˆ©ç”¨è¦ç´„ åˆ©ç”¨è¦ç´„ æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªãŠã‚ˆã³ä»–ã®è¨€èªã®LLMã®å®‰å…¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†ç›®çš„ã®ãŸã‚ã€å•†ç”¨åˆ©ç”¨ã‚‚å«ã‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 14
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 14
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ ãƒãƒ¼ãƒ ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãŠã‚ˆã³ã€ŒCommon Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 14
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯kunishouæ°ãŒå…¬é–‹ã—ã¦ã„ã‚‹"databricks-dolly-15k"ã‚’æ—¥æœ¬èªè¨³ã—ãŸkunishou/databricks-dolly-15k-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èªå°¾ã‚’ArrowPro-7B-KUJIRAã‚’ç”¨ã„ã¦ã€Œã«ã‚ƒã‚“ï¼
  - Downloads: 14
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - è‡ªå‹•ç”ŸæˆQ&amp;A ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ Common Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 14
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - This dataset contains passages, each of which consists of consecutive sentences no longer than 400 characters from Japanese Wikipedia as of 2022-04-04.
  - Downloads: 14
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€OpenAIç¤¾ã®GPT-3.5ã‚’ https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2 ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸå¾Œã«ã€æ›´ã«ç‹¬è‡ªã«åé›†ã—ãŸäººæ ¼ã®ã‚ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆ330ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã€ ãã‚Œã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - æ¦‚è¦ å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ç”¨ã®å›ºæœ‰è¡¨ç¾èªè­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(J-NER)ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
  - Downloads: 14
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - æ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã¨ã€è³ªå•æ–‡ã«å«ã¾ã‚Œã‚‹æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’æŒã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ å›ºæœ‰è¡¨ç¾ã®ç¨®é¡ã¯ä»¥ä¸‹ã®ï¼”ã¤ã§ã™ã€‚
  - Downloads: 14
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
  - Downloads: 14
- [nntsuzu/JParaCrawl](https://huggingface.co/datasets/nntsuzu/JParaCrawl)
  - For more information, see website below!
  - Downloads: 14
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100kã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 14
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ Kendamarron/jimba-instuction-1k-betaã®instructionã®ã†ã¡200å€‹ã‚’ã‚ˆã‚Šå˜ç´”ãªã‚¿ã‚¹ã‚¯ã«æ›¸ãæ›ãˆãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 14
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æ—¥çµŒå¹³å‡æ ªä¾¡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 14
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - description public RLHF dataset in Japanese the construction of the reward model was reformatted into a classification task.
  - Downloads: 14
- [vsachi/sachi-dataset-ja](https://huggingface.co/datasets/vsachi/sachi-dataset-ja)
  - LLMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 14
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯null-instruct-jaã¨DeepSeek-v2.5ã®q4ã‚’ç”¨ã„ã¦åˆæˆã•ã‚Œã¾ã—ãŸã€‚
  - Downloads: 14
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - AutoTrain Dataset for project: tam_jp
  - Downloads: 14
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - Dataset Details For the original NTX dataset, the conversion to the Aya instructions format, or more details, please refer to the full dataset in instruction form (https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions)
  - Downloads: 14
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - DSR1D-Llama-8B-aya-ja-1k-generated ã“ã‚Œã¯deepseek-ai/DeepSeek-R1-Distill-Llama-8Bã‚’ç”¨ã„ã¦ã€weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-maskedã®æœ€åˆã®1000ä»¶ã®å¿œç­”ã‚’max_new_tokens=3060ã§ç”Ÿæˆã•ã›ã¾ã—ãŸã€‚
  - Downloads: 14
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This dataset was created by machine translating "nlvr" into Japanese.
  - Downloads: 14
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 14
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - Japanese version of MMLU dataset tranlasted by gpt-3.5-turbo.
  - Downloads: 13
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - æ±æ–¹ãƒˆã‚«ãƒã‚¯ãƒ©ãƒ– ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ±æ–¹Projectã®ãƒˆã‚«ãƒã‚¯ãƒ©ãƒ–ã«é–¢ã™ã‚‹æƒ…å ±ã‚’åé›†ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
  - Downloads: 13
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Open_o1_sft_Proãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Qwenç¤¾ã®Qwen2.5-14B-Instructã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 13
- [llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1](https://huggingface.co/datasets/llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1)
  - Chain of Thoughtç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å•é¡Œã®è§£ç­”ã‹ã‚‰èª¬æ˜ï¼ˆChain of Thoughtï¼‰ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 13
- [nntsuzu/Japanese-Law-Translation](https://huggingface.co/datasets/nntsuzu/Japanese-Law-Translation)
  - Japanese-Law-Translation Dataset Summary
  - Downloads: 13
- [takahashi111/aozorabunko-author-classification](https://huggingface.co/datasets/takahashi111/aozorabunko-author-classification)
  - globis-university/aozorabunko-cleanã‚’å…ƒã«ä½œæˆã€‚
  - Downloads: 13
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - æ™‚äº‹æƒ…å ±ã«é–¢ã™ã‚‹æ—¥æœ¬èªQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹Qã€ã¯Hugging Faceã«ã¦ç„¡å„Ÿã§é…å¸ƒã—ã¾ã™ã€‚
  - Downloads: 13
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - A Japanese dataset generated with Qwen/Qwen1.5-14B model.
  - Downloads: 13
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - A Japanese dataset generated with an opensource elyza/ELYZA-japanese-Llama-2-13b-instruct model.
  - Downloads: 13
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This dataset was created by machine translating "ViQuAE" into Japanese.
  - Downloads: 13
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - This dataset is provided by AIxBlock, an unified platform for AI development and AI workflows automation.
  - Downloads: 13
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - AIdeaLab VideoJP è©•ä¾¡å†ç¾ç”¨ãƒ‡ãƒ¼ã‚¿ ã¯ã˜ã‚ã« ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯AIdeaLab VideoJPã®FVDã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’ é›†ã‚ã¾ã—ãŸã€‚
  - Downloads: 13
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository contains the dataset used for the TaCo paper.
  - Downloads: 13
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - Negative Embedding / Textual Inversion NE4Mitsua is a Negative Embedding for Mitsua Diffusion One.
  - Downloads: 13
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€è‘—ä½œè€…ã§ã‚ã‚‹è‡ªåˆ†ãŒã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ç‰¹ã«å„ªã‚ŒãŸã‚‚ã®(å¾Œè¿°)ã‚’é›†ã‚ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 13
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Œç§ã®æ¨ã—ã¯æ‚ªå½¹ä»¤å¬¢ã€‚
  - Downloads: 13
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - é•·æ–‡ç”¨ã®instructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 13
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japaneseã‚’ç”¨ã„ã¦ã€è‹±èªWikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 13
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-debug", split="test") æ¦‚è¦ å¤§å–œåˆ©ç”Ÿæˆã®å‹•ä½œç¢ºèªç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 13
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/ogiri-debug", split="test") æ¦‚è¦ å¤§å–œåˆ©ç”Ÿæˆã®å‹•ä½œç¢ºèªç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 13
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - Japanese multi-turn conversation data was generated using Qarasu14B based on Wikipedia data.
  - Downloads: 13
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - FEDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Google Cloud Translate API v2ã§æ—¥æœ¬èªåŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼
  - Downloads: 13
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - NVIDIA ãŒå…¬é–‹ã—ã¦ã„ã‚‹ SteerLM å‘ã‘ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ HelpSteerã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 13
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æŠ•è³‡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - Japanese-translation data of aimo-validation-aime.
  - Downloads: 13
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - å¼¹ä¸¸è®ºç ´çš„ä¸ƒæµ·åƒç§‹è¯­éŸ³æ•°æ®
  - Downloads: 13
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - LoRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–‹ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã€‚
  - Downloads: 13
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - Description This dataset is just a sample of 10341 Hours Unsupervised Spontaneous Japanese Speech Dataset(paid dataset), covers dialogues or monologues in 28 common domains, such as daily vlogs, travel, podcast, technology, beauty, etc.
  - Downloads: 13
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - âš 
  - Downloads: 13
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã—ãŸinstructionã«Swallow-MXã§outputã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 12
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - metamath_ja_950_reka3flash meta-math/MetaMathQAã®æœ€åˆã®1000ä»¶ã‚’RekaAI/reka-flash-3ã§ç¿»è¨³ã—ãŸå¾Œã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç¶­æŒã•ã‚Œãªã‹ã£ãŸã‚‚ã®ã‚’é™¤å»ã—ã¾ã—ãŸã€‚
  - Downloads: 12
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This is a Japanese portion of the Guanaco dataset.
  - Downloads: 12
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - Here we share a Japanese dataset synthesized using the OpenAI GPT-4 model with Self-Instruct, utilizing some excess Azure credits.
  - Downloads: 12
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - Kendamarron/jimba-wiki-instruction-calm3 grapevine-AI/CALM3-22B-Chat-GGUFã®Q4_K_Mã‚’ä½¿ã£ãŸåˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32B-instructï¼‰
  - Downloads: 12
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ HuggingFaceTB/everyday-conversations-llama3.1-2k ã‚’æ©Ÿæ¢°ç¿»è¨³ã§æ—¥æœ¬èªåŒ–ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 12
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage Dataset Dataset Summary JDocQA_SingleImageã¯ã€shunk031/JDocQAã®testã‚µãƒ–ã‚»ãƒƒãƒˆã‚’åŸºã«ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’200dpiã®ç”»åƒã«å¤‰æ›ã—ã€ç”»åƒãŒå–å¾—ã§ããªã„è¨­å•ã¨è¤‡æ•°ç”»åƒãŒå¿…è¦ãªè¨­å•ã‚’é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 12
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - It is just a dataset of dolly-15k-jp(*1)
  - Downloads: 12
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - OpenOrcaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ https://huggingface.co/datasets/Open-Orca/OpenOrca ç¾åœ¨ç¿»è¨³ä½œæ¥­ãŒç¶šè¡Œä¸­ã§ã€OpenOrcaå…¨ä½“ã®1/5ç¨‹åº¦ã®ç¿»è¨³ãŒçµ‚ã‚ã£ãŸçŠ¶æ…‹ã§ã²ã¨ã¾ãšå…¬é–‹ã—ã¾ã™ã€‚
  - Downloads: 12
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - Derived from é’ç©ºæ–‡åº«åŠã³ã‚µãƒ”ã‚¨ã®ç‚¹å­—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸæŒ¯ã‚Šä»®åã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆGitHubï¼‰ https://github.com/ndl-lab/huriganacorpus-aozora Certain mismatches in the original corpus were eliminated during validation (307 instances) Error: çƒˆã—ã„èª¿å­ã§ã‚ã‚‹ã€‚
  - Downloads: 12
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - Magpie-Tanuki-Instruction-Selected-Evolved-26.5k æ¦‚è¦ ä»¥ä¸‹ã®æ‰‹é †ã§ä½œæˆã—ãŸç´„2ä¸‡6500ä»¶ã®æ—¥æœ¬èªã®åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - DSR1D-qwen-2.5-32B-aya-ja-1k-generated ã“ã‚Œã¯deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã‚’ç”¨ã„ã¦ã€weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-maskedã®æœ€åˆã®1000ä»¶ã®å¿œç­”ã‚’max_new_tokens=3060ã§ç”Ÿæˆã•ã›ã¾ã—ãŸã€‚
  - Downloads: 12
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - VTuber YouTube Channel List Dataset ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€VTuber ãƒãƒ£ãƒ³ãƒãƒ«ã¨ VTuber ã§ãªã„ï¼ˆä¾‹ï¼šæ–™ç†ãƒãƒ£ãƒ³ãƒãƒ«ãªã©ï¼‰ã® YouTube ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ JSONL å½¢å¼ã§ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
  - Downloads: 12
- [nntsuzu/Tanaka-corpus](https://huggingface.co/datasets/nntsuzu/Tanaka-corpus)
  - For more information, see website below!
  - Downloads: 12
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - ichikara-instruction-003-sharegpt Dataset by DataPilot ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ (Dataset Summary) ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€kinokokoro/ichikara-instruction-003 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ã€åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ ShareGPTå½¢å¼ ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - Open-Platypus-Japanese-masked-formatted weblab-GENIAC/Open-Platypus-Japanese-maskedã‚’OpenAI messageså½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - Lurunchik/WikiHowNFQAã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - novecomi-novel-metadata https://dengekibunko.jp/novecomi/novel/ ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€‚
  - Downloads: 12
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - æ¦‚è¦ Common Voice Corpus 17.0ã‚’speechMOSã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
  - Downloads: 12
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚ŒãŸé«˜å“è³ªãªåˆæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãã®AIå‡ºåŠ›ã‚’åéŒ²ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 12
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - Development Process question dataset from hotchpotch/japanese-qa-reasoning-100k
  - Downloads: 12
- [Sraym/tail7_match_pair](https://huggingface.co/datasets/Sraym/tail7_match_pair)
  - 1.
  - Downloads: 12
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - äººé–“ãŒä½œæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ(OSCAR)ã¨LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ(GPT-3.5 Turbo)ã‹ã‚‰æˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ LLMã§ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®æ¤œå‡ºæ€§èƒ½ã®æ¤œè¨¼ã®ãŸã‚ã«ä½œæˆã—ãŸ è©³ç´°ã¯ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§ https://github.com/Rio-Rf/Lab-CreateDataset
  - Downloads: 12
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-shashin", split="train") æ¦‚è¦ æ ªå¼ä¼šç¤¾æ±å»ºã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé‹å–¶ã™ã‚‹ãƒ›ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆãƒ»ãƒªã‚µãƒ¼ãƒã«ã‚ˆã‚‹ã€ãƒ›ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆå·æŸ³å¤§è³ã€ã®ã†ã¡ã€ãŠé¡ŒãŒç”»åƒå½¢å¼ã§æä¾›ã•ã‚Œã‚‹ã€å†™çœŸå·æŸ³ã€ã«é–¢ã™ã‚‹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 12
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - The following data set was vectorized with the intfloat/multilingual-e5-base model and an index file created by faiss.
  - Downloads: 12
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - 9.83 Million Pairs of Sentences - Chinese-Japanese Parallel Corpus Data be stored in txt format.
  - Downloads: 12
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 12
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - KanjiVG PNG images with textual descriptions This dataset is an adaptation of KanjiVG by Ulrich Apel.
  - Downloads: 12
- [Kendamarron/magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct)
  - magpie-python-coding-instruction-62k-qwen2.5-bakeneko-32b-instruct rinna/qwen2.5-bakeneko-32b-instructã‚’ç”¨ã„ãŸMagpieã§ç”Ÿæˆã—ãŸåˆæˆInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - Funded by [optional]: AIxBlock (aixblock.io)
  - Downloads: 12
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - Washi (a kind of traditional Japanese paper)
  - Downloads: 12
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 12
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - LLM-jp Corpus v3ã®æ—¥æœ¬èªéƒ¨åˆ†ã®wikipediaä»¥å¤–ã®ãƒŸãƒ©ãƒ¼ã§ã™ã€‚
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted æ¦‚è¦ gpt-4o-miniã‚’ç”¨ã„ã¦ä½œæˆã—ãŸæ—¥æœ¬èªãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8kã«system messageã‚’è¿½åŠ ã—ã¦æ•´å½¢ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja ã®question_jaã‚’ã‚‚ã¨ã«phi-3-mediumã«ã‚ˆã‚Šãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚’ç”¨ã„ãªã„å½¢å¼ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 12
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - ms_marco_japanese ms_marco ã®æ—¥æœ¬èªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 11
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - Reasoningã€çŸ¥è­˜ã€ä¼šè©±ã®æ›ã‘åˆã„ãªã©ã®æƒ…å ±å¯†åº¦ãŒé«˜ã„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 11
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - å¿…ãšã™ã¹ã¦ã®æƒ…å ±ã‚’ç¶²ç¾…ã—ï½¤æ—¥æœ¬èªã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
  - Downloads: 11
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - #Dataset Card for DancingPrismPJ/wikipedia-horse-dataset Wikipediaã®ã€ŒCategory:æ—¥æœ¬èª¿æ•™ã®ç«¶èµ°é¦¬ã€å†…ã®è¨˜äº‹ã«å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k æ¦‚è¦ 5ç¨®é¡ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8ã‚’ä½¿ã£ã¦ä½œæˆã—ãŸã€190854ä»¶ã®æ—¥æœ¬èªåˆæˆPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - aki-0421/commoncatalog-cc-by-ja-300k ã“ã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯alfredplpl/commoncatalog-cc-by-jaã§ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«512pxä»¥å†…ã«åŠ å·¥ã—ãŸç”»åƒã‚’è¿½åŠ ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - iterative-dpo-data-for-ORPO-iter3 æ¦‚è¦ åˆæˆinstructionãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60kã‚’å…ƒã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§ä½œæˆã—ãŸæ—¥æœ¬èªPreferenceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - TALPCoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥è‹±ç¿»è¨³ãƒšã‚¢ã‚’HuggingFaceå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - #Origin The name comes from "hachiwari/ã¯ã¡ã‚ã‚Œ" (chiikawa/ã¡ã„ã‹ã‚).
  - Downloads: 11
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - ã‚ˆã‚Šå¤šãä½œæˆã—ãŸã®ãŒã“ã£ã¡https://huggingface.co/datasets/if001/elementray_m calm3-22bã‚’ä½¿ã£ã¦ç°¡å˜ãªæ—¥æœ¬èªã®ä¾‹æ–‡ã‚’ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k Magpieã®æ‰‹æ³•ã‚’nvidia/Nemotron-4-340B-Instructã«å¯¾ã—ã¦é©ç”¨ã—ä½œæˆã—ãŸã€ç´„10000ä»¶ã®æ—¥æœ¬èªã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - æ¦‚è¦ reazon-research/reazonspeech-v2[all]ã‚’speechMOSã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
  - Downloads: 11
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - cosmopedia-100k ã®index 20k ï½ 100k ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ãªã‚Šã¾ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¦ç¿»è¨³ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã¯é™¤å¤–ã—ã¦ã„ã¾ã™ï¼‰ã€‚
  - Downloads: 11
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - Billingual text is stored in text format.
  - Downloads: 11
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - ãƒªã‚¢ãƒ«ç³»ãƒ¢ãƒ‡ãƒ«ã«ç‰¹æœ‰ã®è‚–åƒæ¨©ã®å•é¡Œã«ã¤ã„ã¦æ¯”è¼ƒçš„ã‚¯ãƒªã‚¢ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã“ã¨ãŒå¯èƒ½ãªã‚ˆã†ã«ã€ç§ãŒç§è‡ªèº«ã‹ã‚‰ä½œã‚Šå‡ºã—ãŸäººå·¥è¶…å½¼å¥³ï¼ˆver 2.1ç³»ã€ver 2.6ç³»ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç´„2800æšï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 11
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - æ¦‚è¦ reazon-research/reazonspeech-v2[all]ã‚’WADA SNRã«ã¦éŸ³å£°å“è³ªã®åˆ†æã‚’è¡Œã£ãŸçµæœã§ã™ã€‚
  - Downloads: 11
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®urlåˆ—ãŒå‡ºå…¸ã¨ãªã‚Šã¾ã™ã€‚
  - Downloads: 11
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - japanese music emotion Music2Emotionã‚’ä½¿ã£ã¦ä¸»ã«æ—¥æœ¬ã®éŸ³æ¥½ã®æ„Ÿæƒ…åˆ†æã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ åˆ†æã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®jsonlã«ãªã£ã¦ã„ã¾ã™ã€‚
  - Downloads: 11
- [YUGOROU/Counseling-Reasoning-v1.9-parquet](https://huggingface.co/datasets/YUGOROU/Counseling-Reasoning-v1.9-parquet)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°åˆ†é‡ã«ãŠã‘ã‚‹å•é¡Œè§£æ±ºã¨å›ç­”ç”Ÿæˆã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
  - Downloads: 11
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Data created manually
  - Downloads: 11
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - Dolly æ—¥æœ¬èªç¿»è¨³ç‰ˆ ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€DatabricksãŒé–‹ç™ºã—ãŸdollyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ã€‚
  - Downloads: 11
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This is a handmade dataset for making a Japanese chatbot.
  - Downloads: 11
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - ãŠï½ã„ãŠèŒ¶æ–°ä¿³å¥å¤§è³å—è³ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ 221ã®ä¿³å¥ãŒå«ã¾ã‚Œã€ã†ã¡200å‰å¾Œã¯ä½œè€…ã¨å¯©æŸ»å“¡ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒä»˜å±ã€‚
  - Downloads: 11
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - Abstruct This is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
  - Downloads: 11
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl This dataset represents a conversion of the Japanese (Ja) section from the miracl dataset into the BeIR format, making it compatible for use with mteb.
  - Downloads: 11
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯sakura_japanese_datasetã®è³ªå•ã«å›ç­”ã™ã‚‹å½¢å¼ã§ä½œã‚‰ã‚ŒãŸã€ä¸€å•ä¸€ç­”å½¢å¼ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - Synthetic-JP-EN-Coding-Dataset-801k-50k Aratako/Synthetic-JP-EN-Coding-Dataset-801kã‹ã‚‰è‹±èªéƒ¨åˆ†5ä¸‡ä»¶ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - åˆæˆæ—¥æœ¬èªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ç”¨ã„ã¦è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªã®æŒ‡ç¤ºã¨ãã‚Œã«å¯¾ã™ã‚‹å¿œç­”ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
  - Downloads: 11
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.
  - Downloads: 11
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - oasst1-ja Description Based on OpenAssistant Conversations Dataset (OASST1)
  - Downloads: 11
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - ãƒ‡ãƒ¼ã‚¿åˆ¶ä½œè€…ï¼ˆt_wï¼‰
  - Downloads: 11
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - The data contains 101,702 entries.
  - Downloads: 11
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - å‡ºåŠ›ã«ãªã«ã‹ã—ã‚‰ã®åˆ¶ç´„ãŒã‚ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹è¿½å¾“æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©¦ä½œã§ã™ã€‚
  - Downloads: 11
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - å€«ç†ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ æ¦‚è¦ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¥æœ¬èªã®å€«ç†ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 Dataset Dataset Summary JDocQA_SingleImage_200ã¯ã€shunk031/JDocQAã®testã‚µãƒ–ã‚»ãƒƒãƒˆã‚’åŸºã«ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’200dpiã®ç”»åƒã«å¤‰æ›ã—ã€ç”»åƒãŒå–å¾—ã§ããªã„è¨­å•ã¨è¤‡æ•°ç”»åƒãŒå¿…è¦ãªè¨­å•ã‚’é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 11
- [Silviase/MECHA-ja-EN](https://huggingface.co/datasets/Silviase/MECHA-ja-EN)
  - Mecha Ja (English Translation)
  - Downloads: 11
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - The dataset of SLG framework.
  - Downloads: 11
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - èª­ã¿è¾¼ã¿æ–¹ from datasets import load_dataset dataset = load_dataset("YANS-official/senryu-marusen", split="train") æ¦‚è¦ æœˆã«1ä¸‡å¥ä»¥ä¸Šã®æŠ•ç¨¿ãŒã‚ã‚‹å›½å†…æœ€å¤§ç´šã®å·æŸ³æŠ•ç¨¿ã‚µã‚¤ãƒˆã€å·æŸ³æŠ•ç¨¿ã¾ã‚‹ã›ã‚“ã€ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
  - Downloads: 11
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - def prompt(japanese, english):
  - Downloads: 11
- [clnine/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/clnine/sample-dataset-wikipedia-financial-terms)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸparquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹range3/wikipedia-ja-20230101ã‚ˆã‚Šã€ã€ŒCategory:æŠ•è³‡ã€ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã«è©²å½“ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºã—ãŸä½œæ¥­ç”¨ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - ãƒã‚±ãƒ¢ãƒ³(VGC)ã®ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³F ãƒ«ãƒ¼ãƒ«ã«ãŠã‘ã‚‹é¸å‡ºãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 11
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This dataset is just a sample of Japanese Conversational Speech by Mobile Phone(paid dataset).
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - CC-MAIN-2019-51ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - Japan Diverse Images Dataset Overview This dataset is a comprehensive collection of high-quality images (and some videos) capturing the diverse aspects of Japan, including urban landscapes, natural scenery, historical sites, contemporary art, everyday life, and culinary experiences.
  - Downloads: 11
- [yayoimizuha/new-imatrix-dataset-ja-en](https://huggingface.co/datasets/yayoimizuha/new-imatrix-dataset-ja-en)
  - æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã¯TFMC/imatrix-dataset-for-japanese-llmãŒã‚ã‚Šã¾ã™ãŒã€ ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªãŒä½ã„ã‚ˆã†ã«æ„Ÿã˜ãŸã®ã§ã€ é’ç©ºæ–‡åº«ã€æ—¥è‹±Wikipedia,Project Gutenbergã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 11
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench: A Benchmark for Advanced Japanese Reasoning Capabilities KUM-Bench (Kyoto University Math Entrance Exam Benchmark) is designed to evaluate advanced Japanese reasoning capabilities by leveraging mathematics entrance exam questions from Kyoto Universityâ€”one of the most prestigious universities in Japan.
  - Downloads: 11
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - ç”ŸæˆAIã®æ—¥è‹±å°‚é–€ç”¨èªé›†ã§ã™ã€‚
  - Downloads: 11
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - ä»¥ä¸‹ã®æ¡ä»¶ã«åŒæ„ã—ãŸã†ãˆã§ã€å…¬é–‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åŠã³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ï¼ˆä»¥ä¸‹ã€Œæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ï¼‰ã¨ã„ã„ã¾ã™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
  - Downloads: 11
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - For more details &amp; to download the rest of the dataset(paid),please refer to the link: https://www.nexdata.ai/datasets/nlu/153?
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - CC-MAIN-2019-49ã¸ã‚ˆã†ã“ã æœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CommonCrawlerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‹ã‚‰æ—¥æœ¬èªã®ã¿ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - Magpieæ–¹å¼ã«ã‚ˆã‚‹promptæŠ½å‡ºã‚’rinna/llama-3-youko-8bã§è¡Œã£ã¦ã¿ã¾ã—ãŸã€‚
  - Downloads: 11
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
