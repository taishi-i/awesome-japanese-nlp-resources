# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 459 models and 89 datasets are listed.

# Contents

 * [Models](#models)
 * [Datasets](#datasets)

## Models

This list is sorted by downloads as of May 13, 2024.

 * ğŸ“¥ 26775140 [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - BERT base Japanese (IPA dictionary)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 1213634 [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - This is a Japanese sentence-BERT model.
 * ğŸ“¥ 1046750 [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - xlm-roberta-ner-japanese(Japanese caption : æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification.
 * ğŸ“¥ 516521 [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 509124 [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - shisa-gamma-7b-v1For more information see our main Shisa 7B modelWe applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
 * ğŸ“¥ 420870 [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - This is a Japanese sentence-LUKE model.
 * ğŸ“¥ 185829 [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - BERT base Japanese (IPA dictionary, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 125132 [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
 * ğŸ“¥ 123052 [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfr
 * ğŸ“¥ 110673 [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 104772 [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - BERT base Japanese (character tokenization)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 77775 [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * ğŸ“¥ 60368 [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - This is a Japanese sentence-BERT model.
 * ğŸ“¥ 59719 [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - Sentence BERT base Japanese modelThis repository contains a Sentence BERT base model for Japanese.
 * ğŸ“¥ 48568 [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’llm-book/ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«ã®ãƒ—ãƒ­é‡çƒé¸æ‰‹"# textä¸­ã®å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºpprint(ner_pipeline(text))
 * ğŸ“¥ 36492 [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - Model Card for Japanese DeBERTa V2 baseModel
 * ğŸ“¥ 35226 [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
 * ğŸ“¥ 27874 [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE) - LaBSEModel descriptionLanguage-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
 * ğŸ“¥ 25938 [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 25928 [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japaneseæ—¥æœ¬èªã®README/Japanese READMEGLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
 * ğŸ“¥ 17520 [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium) - OpenCALM-MediumModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * ğŸ“¥ 14498 [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 13867 [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - japanese-roberta-baseThis repository provides a base-sized Japanese RoBERTa model.
 * ğŸ“¥ 13658 [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 13281 [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 12699 [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - rinna/japanese-clip-vit-b-16This is a Japanese CLIP (Contrastive Language-Image Pre-Training) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models.
 * ğŸ“¥ 12559 [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix) - ã€å‘ŠçŸ¥ã€‘chilled_remixåŠã³reversemixã¯2023å¹´5æœˆ21æ—¥ã«Versionå¤‰æ›´ã‚’è¡Œã„ã€v2ã¸ç§»è¡Œã„ãŸã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 11143 [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - reazonspeech-nemo-v2reazonspeech-nemo-v2 is an automatic speech recognition model trainedon ReazonSpeech v2.0 corpus.
 * ğŸ“¥ 10859 [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 9977 [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * ğŸ“¥ 9885 [Lasorco/lametta](https://huggingface.co/Lasorco/lametta) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ï¼Ÿ
 * ğŸ“¥ 9256 [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - OpenCALM-3BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 9129 [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 9111 [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000) - alabnii/jmedroberta-base-sentencepiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * ğŸ“¥ 7983 [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - OpenCALM-7BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 7919 [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - sbert-jsnli-luke-japanese-base-liteThis is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
 * ğŸ“¥ 7884 [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * ğŸ“¥ 7812 [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 6638 [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - gpt-neox-japanese-2.7bThe open PR is merged on 2022/9/14.You can use this model with v4.23 and higher versions of transformers as follows,pip install transformersThis repository provides a 2.7B-parameter Japanese GPT-NeoX-based model.
 * ğŸ“¥ 6491 [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - Japanese Stable LM Base Gamma 7BModel
 * ğŸ“¥ 6162 [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-7b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-
 * ğŸ“¥ 6087 [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - ELYZA-japanese-Llama-2-13b-fast-instructModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 6051 [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - umiyuki-Japanese-Chat-Umievo-itr001-7b-ggufumiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Japanese-Chat-Umievo-itr001-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * ğŸ“¥ 5823 [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese) - This is a Japanese sentence-LUKE model.
 * ğŸ“¥ 5546 [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - Kotoba-WhisperKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteenAsahi Ushio and Kotoba Technologies.
 * ğŸ“¥ 5175 [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - Japanese Stable LM Instruct Gamma 7BModel
 * ğŸ“¥ 5171 [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)This is an ELECTRA model pretrained on approximately 200M Japanese sentences extracted from the mC4 and finetuned by spaCy v3 on UD_Japanese_BCCWJ r2.8.The base pretrain model is megagonlabs/transformers-ud-japanese-electra-base-discrimininator.
 * ğŸ“¥ 4985 [mmnga/gemma-7b-it-gguf](https://huggingface.co/mmnga/gemma-7b-it-gguf) - gemma-7b-it-ggufgoogleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-7b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ç¾åœ¨é‡å­åŒ–ã•ã‚ŒãŸå‡ºåŠ›ãŒä¸å®‰å®šãªå•é¡ŒãŒã‚ã‚‹ã‚‰ã—ãQ8_0ã‚’æ¨å¥¨ã—ã¾ã™ã€‚gemma : token_embd.weight ãƒ†ãƒ³ã‚½ãƒ«ã« Q8_0 ã‚’ä½¿ç”¨ã—ã¾ã™ #5650Licencegemma-terms-of-use åˆ©ç”¨è¦ç´„ã‚’ã”åˆ©ç”¨å‰ã«å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * ğŸ“¥ 4965 [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 4925 [rinna/youri-7b](https://huggingface.co/rinna/youri-7b) - rinna/youri-7bOverviewWe conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
 * ğŸ“¥ 4801 [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 4737 [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 4704 [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf) - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufaixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8b-Cosmopedia-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufmmnga/aixsatoshi-Honyaku-7b-v2-ggufmmnga/aixsatoshi-Honyaku-Multi-Translator-Swallow-ms7b-ggufmmnga/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-ggufmmnga/aixsatoshi-Mixtral-8x7B-ja-sft-ChatbotArenaJAcalm2-bnb4bitmmnga/aixsatoshi-calm2-7b-chat-7b-moe-ggufUsagegit c
 * ğŸ“¥ 4679 [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 4609 [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - lightblue-suzume-llama-3-8B-japanese-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-ggufmmnga/lightblue-suzume-llama-3-8B-multilingual-ggufmmnga/lightblue-suzume-llama-3-8B-japanese-ggufmmnga/lightblue-ao-karasu-72B-ggufmmnga/lightblue-karasu-1.1B-ggufmmnga/lightblue-karasu-7B-chat-plus-unleashed-ggufmmnga/lightblue-qarasu-14B-chat-plus-unleashed-ggufUsagegit clone https://github.com
 * ğŸ“¥ 4589 [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - haqishen-Llama-3-8B-Japanese-Instruct-ggufhaqishenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Japanese-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/haqishen-Llama-3-8B-Japanese-Instruct-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * ğŸ“¥ 4582 [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * ğŸ“¥ 4535 [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 4488 [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - ELYZA-japanese-Llama-2-13b-fastModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 4444 [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT) - ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªç‰ˆã¯ã¾ã ä½œæˆä¸­ã§ã™ã€‚ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚IntroDetailed report in the arXiv ReportIf you just want to check out how to use the model, please check out the Usage section below!Welcome to JaColBERT version 1, the initial release of JaColBERT, a Japanese-only document retrieval model based on ColBERT.It outperforms previous common Japanese models used for document retrieval, and gets close to the performance of multilingual models, despite the evaluation datasets being out-of-domain for our models but in-domain for multi
 * ğŸ“¥ 4399 [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - BERT base Japanese (character tokenization, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 4316 [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * ğŸ“¥ 4296 [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - japanese-gpt2-smallThis repository provides a small-sized Japanese GPT-2 model.
 * ğŸ“¥ 4115 [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos) - bert-base-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-char-extended.
 * ğŸ“¥ 4090 [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * ğŸ“¥ 3879 [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - OpenCALM-LargeModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * ğŸ“¥ 3762 [Helsinki-NLP/opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - en-jasource group: Englishtarget group: JapaneseOPUS readme: eng-jpnmodel: transformer-alignsource language(s): engtarget language(s): jpnmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus+bt-2021-04-10.ziptest set translations: opus+bt-2021-04-10.test.txttest set scores: opus+bt-2021-04-10.eval.txtBenchmarkstestsetBLEUchr-F#sent#wordsBPTatoeba-test.eng-jpn15.20.25810000992061.000System Info:hf_name: en-jasource_languages: engtarget_languages
 * ğŸ“¥ 3759 [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1) - FINGU-AI/FinguAI-Chat-v1OverviewThe FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
 * ğŸ“¥ 3720 [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK) - ğŸˆ FlexDreamHKFlexDreamHKã¯ãƒªãƒ¼ã‚¯ã•ã‚ŒãŸNovelAIãƒ¢ãƒ‡ãƒ«ã®å…¥ã£ã¦ã„ãªã„ã€ã‚ã‚‹ã„ã¯ãã®ãƒªã‚¹ã‚¯ã‚’å¯èƒ½ãªé™ã‚Šä½ãã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 3582 [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) - bilingual-gpt-neox-4bOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * ğŸ“¥ 3507 [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base) - rinna/japanese-hubert-baseOverviewThis is a Japanese HuBERT Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Base model, which contains 12 transformer layers with 12 attention heads.
 * ğŸ“¥ 3472 [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - Japanese SimCSE (BERT-base)æ—¥æœ¬èªã®README/Japanese READMEsummarymodel name: pkshatech/simcse-ja-bert-base-clcmlpThis is a Japanese SimCSE model.
 * ğŸ“¥ 3388 [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - shisa-base-7b-v1shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
 * ğŸ“¥ 3350 [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - LINE DistilBERT
 * ğŸ“¥ 3220 [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - Shisa 7BShisa 7B (shisa-7b-v1) is a bilingual Japanese and English (JA/EN) general-purpose chat model that aims to achieve strong Japanese language performance while retaining robust English capabilities, using a synthetic-data driven approach.
 * ğŸ“¥ 3117 [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b) - CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets.
 * ğŸ“¥ 3094 [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 3081 [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - Model Card for Japanese DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶ è¨€èª
 * ğŸ“¥ 3058 [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * ğŸ“¥ 3028 [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k) - bilingual-gpt-neox-4b-8kOverviewNotice: This model requires transformers&gt;=4.31.0 to work properly.
 * ğŸ“¥ 2867 [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - japanese-gpt2-xsmallThis repository provides an extra-small-sized Japanese GPT-2 model.
 * ğŸ“¥ 2783 [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) - japanese-gpt-neox-3.6bOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * ğŸ“¥ 2674 [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct) - c4ai-command-r-v01-japanese-instructGGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF versionæ¦‚è¦CohereForAI/c4ai-command-r-v01ã‚’ã€ichikara-instructionã‚’ä½¿ã£ã¦è¿½åŠ ã§æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å­¦ç¿’ã®è¨­å®šRunpodã§GPUã‚µãƒ¼ãƒã‚’å€Ÿã‚Šã€A6000x4ã§å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚ä¸»ãªå­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚lora_r: 64lisa_alpha: 128lora_dropout: 0.05lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]learning_rate: 2e-5num_train_epochs: 10epochsbatch_size: 50max_seq_length: 2048è©•ä¾¡jsquad(jsquad-1.1-0.3, 2-shots)ã€jcommonsenseqa(jcommonsenseqa-1.1-0
 * ğŸ“¥ 2665 [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 2661 [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - japanese-gpt2-mediumThis repository provides a medium-sized Japanese GPT-2 model.
 * ğŸ“¥ 2658 [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator) - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
 * ğŸ“¥ 2599 [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
 * ğŸ“¥ 2525 [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf) - ELYZA-japanese-Llama-2-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruc
 * ğŸ“¥ 2510 [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo) - bilingual-gpt-neox-4b-instruction-ppoOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * ğŸ“¥ 2394 [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - Japanese-StableLM-Instruct-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * ğŸ“¥ 2312 [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - hotchpotch/japanese-reranker-cross-encoder-large-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfro
 * ğŸ“¥ 2283 [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-gg
 * ğŸ“¥ 2238 [SakanaAI/EvoLLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B) - ğŸŸ EvoLLM-JP-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦
 * ğŸ“¥ 2160 [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) - japanese-gpt-neox-3.6b-instruction-ppoOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * ğŸ“¥ 2154 [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr) - hubert-base-asrThis model is a fine-tuned version of rinna/japanese-hubert-base on the common_voice_11_0 dataset for ASR tasks.
 * ğŸ“¥ 2153 [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF) - Local-Novel-LLM-projectæ§˜ã® Assistance ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 2104 [SakanaAI/EvoVLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B) - ğŸŸ EvoVLM-JP-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦
 * ğŸ“¥ 2070 [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-novel-gpt-j-6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚æ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 2031 [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF) - Japanese-Starling-ChatV-7B-GGUFGGUF conversion of "Japanese-Starling-ChatV-7B""Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.I applied the chat vector acquired by subtracting the weights of Mistral-7B-v0.1 from the weights of "Starling-LM-7B-beta" to this model.
 * ğŸ“¥ 2019 [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - wav2vec2-base-asrThis model is a fine-tuned version of rinna/japanese-wav2vec2-base on the common_voice_11_0 dataset for ASR tasks.
 * ğŸ“¥ 2007 [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr) - hubert-large-asrThis model is a fine-tuned version of rinna/japanese-hubert-large ASR.
 * ğŸ“¥ 1905 [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - Japanese-StableLM-Base-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * ğŸ“¥ 1813 [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - OpenCALM-1BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * ğŸ“¥ 1780 [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - rinna/japanese-cloob-vit-b-16This is a Japanese CLOOB (Contrastive Leave One Out Boost) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models.
 * ğŸ“¥ 1753 [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - stockmark/stockmark-13bStockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
 * ğŸ“¥ 1749 [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - japanese-sentiment-analysisThis model was trained from scratch on the chABSA dataset.
 * ğŸ“¥ 1696 [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ï¼˜ã¤ã®æ„Ÿæƒ…ï¼ˆå–œã³ã€æ‚²ã—ã¿ã€æœŸå¾…ã€é©šãã€æ€’ã‚Šã€æã‚Œã€å«Œæ‚ªã€ä¿¡é ¼ï¼‰ã®å†…ã€ã©ã®æ„Ÿæƒ…ãŒæ–‡ç« ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã‹åˆ†æã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯wrimeãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆhttps://huggingface.co/datasets/shunk031/wrimeï¼‰ã‚’ç”¨ã„ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚This model is based on Luke-japanese-large-liteThis model is fine-tuned model which besed on studio-ousia/Luke-japanese-large-lite.
 * ğŸ“¥ 1689 [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 1591 [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - stockmark-gpt-neox-japanese-1.4b-ggufstockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gpt-neox-japanese-1.4bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚æ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 1573 [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2) - Japanese-StableLM-Instruct-Alpha-7B-v2"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
 * ğŸ“¥ 1551 [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct) - Japanese StableLM-3B-4E1T InstructModel DescriptionThis is a 3B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese StableLM-3B-4E1T Base.
 * ğŸ“¥ 1549 [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old) - oldï¼Ÿ
 * ğŸ“¥ 1542 [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - OpenCALM-SmallModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * ğŸ“¥ 1527 [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - japanese-gpt-1bThis repository provides a 1.3B-parameter Japanese GPT model.
 * ğŸ“¥ 1453 [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - BERTã«ã‚ˆã‚‹æ—¥æœ¬èªå›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«BertForTokenClassificationã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã®æ–‡ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ã€‚æŠ½å‡ºã•ã‚Œã‚‹å›ºæœ‰è¡¨ç¾ã®ã‚¿ã‚¤ãƒ—ã¯ã€ä»¥ä¸‹ã®8ç¨®é¡ã§ã™ã€‚äººåæ³•äººåï¼ˆæ³•äººã¾ãŸã¯æ³•äººã«é¡ã™ã‚‹çµ„ç¹”ï¼‰æ”¿æ²»çš„çµ„ç¹”åï¼ˆæ”¿æ²»çš„çµ„ç¹”åã€æ”¿å…šåã€æ”¿åºœçµ„ç¹”åã€è¡Œæ”¿çµ„ç¹”åã€è»éšŠåã€å›½éš›çµ„ç¹”åï¼‰ãã®ä»–ã®çµ„ç¹”å	ï¼ˆç«¶æŠ€çµ„ç¹”åã€å…¬æ¼”çµ„ç¹”åã€ãã®ä»–ï¼‰åœ°åæ–½è¨­åè£½å“åï¼ˆå•†å“åã€ç•ªçµ„åã€æ˜ ç”»åã€æ›¸ç±åã€æ­Œåã€ãƒ–ãƒ©ãƒ³ãƒ‰åç­‰ï¼‰ã‚¤ãƒ™ãƒ³ãƒˆåä½¿ç”¨æ–¹æ³•å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆtransformersã€unidic_liteã€fugashiï¼‰ã‚’pipãªã©ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã€ä¸‹è¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã™ã€‚from transformers import BertJapaneseTokenizer, BertForTokenClassificationfrom transformers import pipelinemodel = BertForTokenClassification.from_pretrained("jurabi/bert-ner-japanese")tokenizer = BertJapaneseTokeniz
 * ğŸ“¥ 1427 [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 1400 [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
 * ğŸ“¥ 1332 [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - bert-finetuned-japanese-sentimentThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
 * ğŸ“¥ 1332 [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft) - japanese-large-lm-3.6b-instruction-sftThis repository provides a 3.6B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
 * ğŸ“¥ 1331 [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
 * ğŸ“¥ 1259 [KBlueLeaf/guanaco-7b-leh-v2](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2) - Guanaco-leh-V2: A Multilingual Instruction-Following Language Model Based on LLaMA
 * ğŸ“¥ 1214 [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 1175 [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - nlp-waseda/roberta-large-japanese-seq512Model descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")se
 * ğŸ“¥ 1165 [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - bert-base-japanese-v3-marc_jaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®MARC-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinetext_classification_pipeline
 * ğŸ“¥ 1142 [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b) - Japanese-StableLM-Instruct-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
 * ğŸ“¥ 1142 [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 1127 [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 1115 [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF) - Japanese-WizardLM2-ChatV-7B-GGUFGGUF conversion of "Japanese-WizardLM2-ChatV-7B"This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.We aimed to add the high performance of WizardLM-2 to the Japanese language capability of ChatNTQ.ã“ã®ãƒ¢ãƒ‡ãƒ«ã€Japanese-WizardLM2-ChatV-7Bã¯ã€â€chatntq-ja-7b-v1.0â€ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€"WizardLM-2-7b"ã‹ã‚‰"Mistral-7B-v0.1"ã‚’å·®ã—å¼•ã„ã¦ä½œã£ãŸChatVectorã‚’1.0å€ã§è¶³ã—ã¾ã—ãŸã€‚ChatNTQã®æ—¥æœ¬èªèƒ½åŠ›ã«WizardLM
 * ğŸ“¥ 1095 [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1) - Swallow-MX-8x7b-NVE-v0.1Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
 * ğŸ“¥ 1049 [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) - japanese-gpt-neox-3.6b-instruction-sftOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * ğŸ“¥ 1039 [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) - Japanese-StableLM-Base-Alpha-7B"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * ğŸ“¥ 1037 [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - japanese-gpt-neox-smallThis repository provides a small-sized Japanese GPT-NeoX model.
 * ğŸ“¥ 1029 [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha
 * ğŸ“¥ 1021 [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional) - Additional pretrained BERT base Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 1020 [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese) - gpt2-large-japaneseThis repository provides a large sized Japanese GPT-2 model.
 * ğŸ“¥ 1012 [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1) - albert-base-japanese-v1æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«Sentencepieceã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ãã®ã¾ã¾ã§ã¯[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚ã¨ã«ä½™è¨ˆãªãƒˆãƒ¼ã‚¯ãƒ³ãŒæ··å…¥ã™ã‚‹å•é¡ŒãŒã‚ã‚‹ã®ã§ã€åˆ©ç”¨ã™ã‚‹éš›ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™for PyTorchfrom transformers import (AlbertForMaskedLM, AlbertTokenizerFast)import torchtokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")
 * ğŸ“¥ 1006 [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf) - ELYZA-japanese-Llama-2-7b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-c
 * ğŸ“¥ 1004 [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct) - PLaMo-13B-InstructModel DescriptionPLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
 * ğŸ“¥ 1003 [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese) - nlp-waseda/bigbird-base-japaneseModel descriptionThis is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/bigbird-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/bigbird-base-japanese")sentence = '[MASK] å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’
 * ğŸ“¥ 992 [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)CoolJapanDiffusion 2.1.1ã¨WaifuDiffusion 1.4 anime epoch2ã®ãƒãƒ¼ã‚¸ã€‚
 * ğŸ“¥ 972 [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf) - karakuri-lm-70b-chat-v0.1-ggufkarakuri-aiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹karakuri-lm-70b-chat-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
 * ğŸ“¥ 941 [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b) - stockmark/gpt-neox-japanese-1.4bThis repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
 * ğŸ“¥ 938 [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B) - Japanese-Starling-ChatV-7Bã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.è©³ç´°ã¨GGUFç‰ˆã¯ã“ã¡ã‚‰ã€‚Details and GGUFs are here.
 * ğŸ“¥ 934 [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 924 [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - japanese-large-lm-3.6bThis repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation.
 * ğŸ“¥ 922 [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
 * ğŸ“¥ 921 [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0) - ChatNTQ JA 7B V1.0Model
 * ğŸ“¥ 908 [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 904 [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr) - uniTKU-hubert-japanese-asrThis model was fine-tuned on a dataset provided by uniTKU, and it has maintained the original performance metrics on the common_voice_11_0 dataset.
 * ğŸ“¥ 899 [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large) - luke-japanese-largeluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * ğŸ“¥ 878 [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * ğŸ“¥ 856 [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b) - rinna/nekomata-14bOverviewWe conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
 * ğŸ“¥ 851 [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - Llama 3 Youko 8B (rinna/llama-3-youko-8b)OverviewWe conduct continual pre-training of meta-llama/Meta-Llama-3-8B on 22B tokens from a mixture of Japanese and English datasets.
 * ğŸ“¥ 835 [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b) - rinna/nekomata-7bOverviewWe conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
 * ğŸ“¥ 822 [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf) - ELYZA-japanese-CodeLlama-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-ins
 * ğŸ“¥ 811 [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos) - roberta-small-japanese-luw-uposModel
 * ğŸ“¥ 805 [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF) - c4ai-command-r-v01-japanese-instruct-GGUFæ¦‚è¦Aratako/c4ai-command-r-v01-japanese-instructã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
 * ğŸ“¥ 795 [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc) - PLaMo-13B-Instruct-NCModel DescriptionPLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
 * ğŸ“¥ 783 [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base) - Japanese StableLM-3B-4E1T BaseModel DescriptionThis is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * ğŸ“¥ 767 [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b) - Japanese-StableLM-Base-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * ğŸ“¥ 764 [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 751 [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli) - bert-base-japanese-v3-jnliã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(è‡ªç„¶è¨€èªæ¨è«–)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®MARC-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinenli_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jnli")text = "äºŒäººã®ç”·æ€§ãŒã‚¸ã‚§ãƒƒãƒˆæ©Ÿã‚’è¦‹ã¦ã„ã¾ã™"entailment_text = "ã‚¸ã‚§ãƒƒãƒˆæ©Ÿã‚’è¦‹ã¦ã„ã‚‹äººãŒäºŒäººã„ã¾ã™"# textã¨entailment_textã®è«–ç†é–¢ä¿‚ã‚’äºˆæ¸¬print(nli_pipeline({"text": text, "text_pair": entailment_text}))# {'label': 'enta
 * ğŸ“¥ 721 [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - c4ai-command-r-plus-ggufCohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹c4ai-command-r-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚åˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦q6_kã‚„q8_0ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µã‚¤ã‚ºãŒå¤§ããåˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ã®ã§çµåˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚cat c4ai-command-r-plus-Q5_K_M.gguf.* &gt; c4ai-command-r-plus-Q5_K_M.ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * ğŸ“¥ 703 [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite) - luke-japaneseluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * ğŸ“¥ 687 [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft) - japanese-large-lm-1.7b-instruction-sftThis repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
 * ğŸ“¥ 659 [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - bert-base-japanese-v3-jstsã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã‚’JGLUEã®JSTSãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆè¨“ç·´ï¼‰Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinetext_sim_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jsts",function_to_apply="none",)text = "å·ã¹ã‚Šã§ã‚µãƒ¼ãƒ•ãƒœãƒ¼ãƒ‰ã‚’æŒã£ãŸäººãŸã¡ãŒã„ã¾ã™"sim_text = "ã‚µãƒ¼ãƒ•ã‚¡ãƒ¼ãŸã¡ãŒå·ã¹ã‚Šã«ç«‹ã£ã¦ã„ã¾ã™"# textã¨sim_textã®é¡ä¼¼åº¦ã‚’è¨ˆç®—result = text_sim_pipeline({"text": text, "text_pair": sim_text
 * ğŸ“¥ 658 [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 654 [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) - Model Card for Tanrei/GPTSAN-japaneseGeneral-purpose Swich transformer based Japanese language modelGPTSAN has some unique features.
 * ğŸ“¥ 652 [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - nlp-waseda/roberta-base-japaneseModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-base-japanese")sentence = 'æ—©ç¨²ç”° å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’
 * ğŸ“¥ 643 [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * ğŸ“¥ 637 [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b) - japanese-large-lm-1.7bThis repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
 * ğŸ“¥ 611 [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - Evaluation on MIRACL japaneseThese models don't train on the MIRACL training data.
 * ğŸ“¥ 605 [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf) - ELYZA-japanese-Llama-2-13b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japa
 * ğŸ“¥ 588 [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf) - rinna/nekomata-14b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b-instruction.
 * ğŸ“¥ 585 [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf) - ELYZA-japanese-Llama-2-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k
 * ğŸ“¥ 582 [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 baseModel
 * ğŸ“¥ 579 [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base) - Original ModelOptical character recognition for Japanese text, with the main focus being Japanese manga.
 * ğŸ“¥ 543 [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf) - rinna/japanese-gpt-neox-3.6b-instruction-pporinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6b-instruction-ppoã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰mmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 542 [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf) - rinna/japanese-gpt-neox-3.6brinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰mmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneoxãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 536 [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese) - COMET-T5 jaFinetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
 * ğŸ“¥ 536 [mmnga/gemma-1.1-7b-it-gguf](https://huggingface.co/mmnga/gemma-1.1-7b-it-gguf) - gemma-1.1-7b-it-ggufgoogleã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gemma-1.1-7b-itã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚Licencegemma-terms-of-use åˆ©ç”¨è¦ç´„ã‚’ã”åˆ©ç”¨å‰ã«å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«mmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * ğŸ“¥ 525 [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - hotchpotch/japanese-reranker-cross-encoder-small-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfro
 * ğŸ“¥ 505 [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2) - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 481 [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 479 [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2) - japanese-gpt-neox-3.6b-instruction-sft-v2OverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * ğŸ“¥ 475 [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base) - rinna/japanese-wav2vec2-baseOverviewThis is a Japanese wav2vec 2.0 Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original wav2vec 2.0 Base model, which contains 12 transformer layers with 12 attention heads.
 * ğŸ“¥ 457 [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF) - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUFOriginal Modelelyza/ELYZA-japanese-Llama-2-13b-fast-instructRun with LlamaEdgeLlamaEdge version: v0.2.8 and abovePrompt templatePrompt type: llama-2-chatPrompt string&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{{ system_prompt }}&lt;&lt;/SYS&gt;&gt;{{ user_msg_1 }}
 * ğŸ“¥ 456 [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 450 [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1) - Kotoba-Whisper-v1.1Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, withadditional postprocessing stacks integrated as pipeline.
 * ğŸ“¥ 438 [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese) - mt5_summarize_japanese(Japanese caption : æ—¥æœ¬èªã®è¦ç´„ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization.
 * ğŸ“¥ 431 [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 431 [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for VecTeus-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the f
 * ğŸ“¥ 426 [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) - bert-base-japanese-v3-unsup-simcse-jawikiã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬8ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3 ã‚’
 * ğŸ“¥ 424 [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web) - t5-base-japanese-web (with Byte-fallback, 32K)Descriptionmegagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
 * ğŸ“¥ 415 [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - Model Card for Japanese DeBERTa V3 baseModel
 * ğŸ“¥ 414 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-instruct - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-instructStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
 * ğŸ“¥ 414 [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * ğŸ“¥ 413 [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja) - æ—¥æœ¬èªå‘ã‘ Llama 3 8Bã¯ã˜ã‚ã«ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯Llama 3ã‚’æ—¥æœ¬èªåŒ–ã—ã‚ˆã†ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
 * ğŸ“¥ 408 [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 403 [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf) - ELYZA-japanese-CodeLlama-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰é€šå¸¸ç‰ˆ: llama2ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFastç‰ˆ æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’æ¸›ã‚‰ã—ã€1.8å€é«˜é€ŸåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«mmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellamaç‰ˆ GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellamaç‰ˆ GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPT
 * ğŸ“¥ 402 [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube) - hubert-base-jtubeThis repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.Scroll down for the model usageFAQQ. 
 * ğŸ“¥ 401 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new format introduced
 * ğŸ“¥ 397 [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char) - Model Card for Japanese character-level GPT-2 SmallModel descriptionThis is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-small-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * ğŸ“¥ 390 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-base - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-baseStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
 * ğŸ“¥ 375 [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 367 [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - hotchpotch/japanese-bge-reranker-v2-m3-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfrom sentence
 * ğŸ“¥ 363 [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1) - fio-base-japanese-v0.1æ—¥æœ¬èªç‰ˆã¯è¿‘æ—¥å…¬é–‹äºˆå®šã§ã™ï¼ˆæ—¥æœ¬èªã‚’å‹‰å¼·ä¸­ãªã®ã§ã€é–“é•ã„ã¯ã”å®¹èµ¦ãã ã•ã„ï¼ï¼‰fio-base-japanese-v0.1 is a proof of concept, and the first release of the Fio family of Japanese embeddings.
 * ğŸ“¥ 359 [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf) - SakanaAI-EvoLLM-JP-A-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-A-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
 * ğŸ“¥ 354 [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese) - roberta_qa_japanese(Japanese caption : æ—¥æœ¬èªã® (æŠ½å‡ºå‹) è³ªå•å¿œç­”ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of rinna/japanese-roberta-base (pre-trained RoBERTa model provided by rinna Co., Ltd.) trained for extractive question answering.
 * ğŸ“¥ 348 [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick) - Japanese Natural Language Inference ModelThis model was trained using SentenceTransformers Cross-Encoder class, gradient accumulation PR, and the code from CyberAgentAILab/japanese-nli-model.
 * ğŸ“¥ 344 [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’é‹è»¢ãƒ‰ãƒ¡ã‚¤ãƒ³QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆDDQAï¼‰ï¼ˆã€€https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasetsã€€ï¼‰ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚Question-Answeringã‚¿ã‚¹ã‚¯ï¼ˆSQuADï¼‰ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Question-Answering which is based on luke-japanese-base-liteThis model is fine-tuned by using DDQA dataset.
 * ğŸ“¥ 343 [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 341 [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese) - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 333 [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 331 [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head) - bert-base-japanese-wikipedia-ud-headModel
 * ğŸ“¥ 329 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1) - Heron GIT Japanese StableLM
 * ğŸ“¥ 329 [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf) - rinna/nekomata-14b-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b.
 * ğŸ“¥ 328 [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - Model Card for Japanese DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-large-japanese')sentence = 'äº¬éƒ½ å¤§å­¦ ã§ è‡ªç„¶
 * ğŸ“¥ 327 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new f
 * ğŸ“¥ 317 [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf) - line-corporation/japanese-large-lm-3.6b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 315 [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese) - XLNet-japaneseModel descriptionThis model require Mecab and senetencepiece with XLNetTokenizer.
 * ğŸ“¥ 314 [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * ğŸ“¥ 310 [webbigdata/C3TR-Adapter](https://huggingface.co/webbigdata/C3TR-Adapter) - ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰(Model Card for Model ID)C3TR-Adapterã¯GoogleãŒç™ºè¡¨ã—ãŸLLMã§ã‚ã‚‹gemma-7bã®æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹QLoRA Adapterã§ã™ã€‚C3TR-Adapter is a QLoRA Adapter that improves the Japanese-English and English-Japanese translation performance of gemma-7b released by Google.
 * ğŸ“¥ 306 [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus) - llm-book/t5-base-long-livedoor-news-corpusã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬7ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹è¦ç´„ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 303 [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en) - WS TCG Card Text TranslatorA Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
 * ğŸ“¥ 297 [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) - nlp-waseda/roberta-large-japaneseModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese")sentence = 'æ—©ç¨²ç”° å¤§å­¦ ã§ è‡ªç„¶ è¨€èª å‡¦ç† ã‚’
 * ğŸ“¥ 293 [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja) - Stanza model for Japanese (ja)Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages.
 * ğŸ“¥ 288 [SakanaAI/EvoLLM-JP-v1-10B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B) - ğŸŸ EvoLLM-JP-v1-10BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦
 * ğŸ“¥ 286 [SakanaAI/EvoLLM-JP-A-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B) - ğŸŸ EvoLLM-JP-A-v1-7BğŸ¤— Models | ğŸ“š Paper | ğŸ“ Blog | ğŸ¦
 * ğŸ“¥ 283 [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps) - whisper-large-v2-japanese-5k-stepsThis model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
 * ğŸ“¥ 280 [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf) - line-corporation/japanese-large-lm-1.7b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆline-gpt2_convert-hf-to-gguf.pyUsage (è©¦ç”¨)git clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * ğŸ“¥ 276 [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf) - line-corporation/japanese-large-lm-1.7bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆline-gpt2_convert-hf-to-gguf.pyUsagegit clone --branch
 * ğŸ“¥ 270 [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head) - deberta-base-japanese-aozora-ud-headModel
 * ğŸ“¥ 266 [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char) - Model Card for Japanese character-level GPT-2 LargeModel descriptionThis is a Japanese character-level GPT-2 Large (717M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-large-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * ğŸ“¥ 260 [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b) - Japanese-StableLM-Base-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * ğŸ“¥ 257 [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf) - shisa-7b-v1-ggufaugmxntã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹shisa-7b-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
 * ğŸ“¥ 245 [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b) - Japanese-StableLM-Instruct-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * ğŸ“¥ 235 [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 229 [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF) - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
 * ğŸ“¥ 228 [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking) - japanese-gpt-1b-PII-maskingModel Descriptionjapanese-gpt-1b-PII-masking ã¯ã€ æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿1B GPTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ—¥æœ¬èªã®æ–‡ç« ã‹ã‚‰å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 227 [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on luke-japanese-baseThis model is fine-tuned by using Wikipedia dataset.
 * ğŸ“¥ 223 [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 215 [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf) - SakanaAI-EvoLLM-JP-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
 * ğŸ“¥ 214 [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * ğŸ“¥ 211 [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese) - Wav2Vec2-Large-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
 * ğŸ“¥ 206 [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis) - bert-japanese_finetuned-sentiment-analysisThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
 * ğŸ“¥ 203 [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1) - æ—¥æœ¬èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ«This is a CLIP text/image encoder model for Japanese.
 * ğŸ“¥ 203 [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf) - Deepreneur-blue-lizard-ggufDeepreneurã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹blue-lizardã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
 * ğŸ“¥ 202 [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite) - luke-japanese-large-liteluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * ğŸ“¥ 202 [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 201 [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 200 [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * ğŸ“¥ 199 [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion) - One more step before getting this model.
 * ğŸ“¥ 190 [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next) - reazonspeech-espnet-nextReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team.
 * ğŸ“¥ 186 [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 185 [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1) - hotchpotch/japanese-reranker-cross-encoder-base-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ålayershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker ã«ã¤ã„ã¦ã‚„ã€æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãƒ»è©•ä¾¡ç­‰ã¯ä»¥ä¸‹ã‚’å‚è€ƒãã ã•ã„ã€‚æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã®Rerankerã‚’ãƒªãƒªãƒ¼ã‚¹ / ãã‚‚ãã‚‚ Reranker ã¨ã¯?æ—¥æœ¬èª Reranker ä½œæˆã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä½¿ã„æ–¹SentenceTransformersfrom
 * ğŸ“¥ 183 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’é‹è»¢ãƒ‰ãƒ¡ã‚¤ãƒ³QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆDDQAï¼‰ï¼ˆã€€https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasetsã€€ï¼‰ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚Question-Answeringã‚¿ã‚¹ã‚¯ï¼ˆSQuADï¼‰ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for Question-Answering which is based on deberta-v2-base-japaneseThis model is fine-tuned by using DDQA dataset.
 * ğŸ“¥ 173 [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf) - line-corporation/japanese-large-lm-3.6bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¯ã“ã¡ã‚‰GPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-ggufæ³¨æ„:ã“ã¡ã‚‰ã¯ãƒ–ãƒ©ãƒ³ãƒã§è©¦ç”¨ã«ãªã‚Šã¾ã™ã€‚llama.cppæœ¬å®¶ã«gptneox, gpt2ãŒå®Ÿè£…ã•ã‚ŒãŸæ™‚ã«ã€ã“ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GitHubãƒªãƒã‚¸ãƒˆãƒªã® readme ã¯ã“ã¡ã‚‰Usage (è©¦ç”¨)git clone --branch
 * ğŸ“¥ 171 [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 166 [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base) - luke-japaneseluke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
 * ğŸ“¥ 165 [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JSTS(æ–‡ç« ã®é¡ä¼¼åº¦è¨ˆç®—)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’yahoo japan/JGLUEã®JSTS( https://github.com/yahoojapan/JGLUE )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚æ–‡ç« ã®é¡ä¼¼åº¦(5ãŒæœ€é«˜å€¤)ã‚’è¨ˆç®—ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for JSTS which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE JSTS dataset.
 * ğŸ“¥ 161 [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hackathon.
 * ğŸ“¥ 161 [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")sentence
 * ğŸ“¥ 161 [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ã‚„Instaç³»ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒãƒ»Instaç³»ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
 * ğŸ“¥ 160 [rinna/nue-asr](https://huggingface.co/rinna/nue-asr) - rinna/nue-asrOverview[Paper][GitHub]We propose a novel end-to-end speech recognition model, Nue ASR, which integrates pre-trained speech and language models.
 * ğŸ“¥ 155 [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf) - Japanese-LLaMA-2-13B-GGUFJapanese-LLaMA-2-13B-GGUFã¯Japanese-LLaMA-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«URLï¼šhttps://huggingface.co/owner203/japanese-llama-2-13b
 * ğŸ“¥ 155 [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF) - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUFThis model was converted to GGUF format from inu-ai/alpaca-guanaco-japanese-gpt-1b using llama.cpp via the ggml.ai's GGUF-my-repo space.
 * ğŸ“¥ 153 [sappho192/jesc-ja-en-translator](https://huggingface.co/sappho192/jesc-ja-en-translator) - Japanese to English translatorJapanese to English translator model based on EncoderDecoderModel(bert-japanese+GPT2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/jesc-ja-en-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferenceimport transformersimport torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "openai-community/gpt2"src_tokenizer = transformers.
 * ğŸ“¥ 153 [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix) - Yaki-Dofu-Mixæ¦‚è¦ / OverviewYaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ / Yaki-Dofu-Mix is a merge model that specializes in an anime-like painting style.
 * ğŸ“¥ 145 [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard) - Deepreneur-blue-lizardModel DescriptionDeepreneur-blue-lizardã¯ã€Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 138 [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf) - rinna/nekomata-7b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b-instruction.
 * ğŸ“¥ 138 [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * ğŸ“¥ 137 [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation) - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
 * ğŸ“¥ 137 [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - DeBERTa V2 base JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
 * ğŸ“¥ 136 [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 134 [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b) - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
 * ğŸ“¥ 130 [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0) - LLaVA-JP Model CardModel detailModel type:LLaVA-JP is a vision-language model that can converse about input images.
 * ğŸ“¥ 128 [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 127 [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) - DeBERTa V2 small JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
 * ğŸ“¥ 121 [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small) - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics.
 * ğŸ“¥ 120 [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion) - SFCOCO Stable Diffusion Model CardSFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
 * ğŸ“¥ 120 [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1-128kThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja-128k has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hack
 * ğŸ“¥ 119 [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - Model Card for Japanese BART baseModel
 * ğŸ“¥ 116 [Local-Novel-LLM-project/Ninja-v1-GGUF](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-GGUF) - Ninja-v1 ã®GGUFç‰ˆOur Models for GGUFVecteus-GGUFNinja-v1-GGUFNinja-v1-NSFW-GGUFNinja-v1-128k-GGUFNinja-v1-NSFW-128k-GGUF
 * ğŸ“¥ 116 [aerner/lm-v2](https://huggingface.co/aerner/lm-v2) - Aerner LM-v2äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã§ã™ã€‚
 * ğŸ“¥ 115 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1) - Heron BLIP Japanese StableLM
 * ğŸ“¥ 115 [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base) - SpiralAI Spiral-RetNet-3b-baseWe have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621) architecture model 3b using a mixed dataset of Japanese and English.
 * ğŸ“¥ 114 [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc) - ã¯ã˜ã‚ã«Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å°å‹ãªã®ã§ã‚¹ãƒãƒ›ã‚„å®¶é›»ãªã©ã«å‘ã„ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€Instruction tuningãŒå›°é›£ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚Colabã§è©¦ã™mmngaã•ã‚“ãŒä½œã£ãŸè»½é‡ç‰ˆã‚’Colabã§è©¦ã™Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("alfredplpl/suzume-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/suzume-poc")input_text = """äººå·¥çŸ¥èƒ½ã¨ã¯"""input_ids = tokenizer(input_text, return_tensors="pt")outputs = model.generate(**input_ids,max_new_tokens=64)print(tokenizer.decode(outputs[0])
 * ğŸ“¥ 112 [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
 * ğŸ“¥ 108 [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct) - Stockmark-13b-instructStockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
 * ğŸ“¥ 107 [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b) - æ›´æ–°å±¥æ­´2023å¹´5æœˆ7æ—¥ã€Œoasst1-89k-jaã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 106 [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - Model Card for Model IDå®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ /
 * ğŸ“¥ 100 [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius) - spekulatiusãƒãƒ¼ã‚¸ã—ã¦ã„ã‚‹ã¨ãŸã¾ã«å‡ºã¦ãã‚‹ã€Œç›®çš„ã®æ„å›³ã¨ã¯é•ã†ã®ã ã‘ã©ãªã‚“ã ã‹æ¶ˆã™ã«ã¯ã‚‚ã£ãŸã„ãªã„ãƒ¢ãƒ‡ãƒ«ã€ã‚’ãŠã™ãåˆ†ã‘ã™ã‚‹ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
 * ğŸ“¥ 100 [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base) - recruit-jp/japanese-clip-vit-b-32-roberta-baseOverviewDeveloped by: Recruit Co., Ltd.Model type: Contrastive Language-Image Pretrained ModelLanguage(s): JapaneseLICENSE: CC-BY-4.0More details are described in our tech blog post.æ—¥æœ¬èªCLIPå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¬é–‹Model
 * ğŸ“¥ 99 [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa) - Kokuwalamettaã®æ”¹è‰¯ã§ãƒãƒ¼ã‚¸ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«æ¢ã—ã‚’ã—ã¦ã„ãŸã‚‰KiwiMixã¨ã„ã†é¢ç™½ãã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚
 * ğŸ“¥ 97 [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char) - Model Card for Japanese character-level
 * ğŸ“¥ 97 [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2) - japanese-sexual-moderation-v2ã¯ã€studio-ousia/luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚çŸ­æ–‡ãŒæ€§çš„ã‹ã©ã†ã‹ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚regressionã§å­¦ç¿’ã—ã¦ãŠã‚Šã€å‡ºåŠ›ã™ã‚‹ã‚¹ã‚³ã‚¢ã¯ãŠãŠã‚€ã­0-1ã®ç¯„å›²ã‚’å–ã‚Šã¾ã™ãŒè² ã®å€¤ã‚„1ã‚’è¶…ãˆã‚‹å€¤ãŒå‡ºã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚é•·ã„æ–‡ç« ã¯å­¦ç¿’ã—ã¦ãŠã‚‰ãšã€å…¥åŠ›ã¯æ”¹è¡Œå˜ä½ã§åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚0.0-0.2: å…¨ãæ€§çš„ã§ã¯ãªã„0.2-0.4: ã»ã¨ã‚“ã©æ€§çš„ãªå†…å®¹ã‚’å«ã¾ãªã„0.4-0.6: æ€§çš„ãªå†…å®¹ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹0.6-0.8: æ€§çš„ãªå†…å®¹ã‚’å«ã‚“ã§ã„ã‚‹0.8-1.0: éå¸¸ã«æ€§çš„ãªå†…å®¹ã§ã‚ã‚‹Usageimport torchfrom transformers import AutoModelForSequenceClassification, AutoTokenizermodel_id
 * ğŸ“¥ 96 [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL) - ã“ã¡ã‚‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã€civitaiã«ã¦å…ˆã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 94 [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja) - MobileBERT æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çˆ†èª•ï¼ï¼
 * ğŸ“¥ 93 [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 92 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2) - Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Swallow-MX-8x7b-NVE-v0.1 + 0.8*(Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1)aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®èªå½™ãŒãŠã‹ã—ã„å ´åˆã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªãŒã‚ˆã‚Šè‡ªç„¶ã«ãªã‚Šã¾ã™context size 32k tokenä½¿ç”¨å¯èƒ½ãªæ—¥æœ¬èªå¯¾å¿œãƒ­ãƒ¼ã‚«ãƒ«ç”¨LLMã¨ã—ã¦ã¯2024å¹´3æœˆæ™‚ç‚¹ã§ã¯æœ€é«˜ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½ã§ã™
 * ğŸ“¥ 88 [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts) - SpeechT5 (TTS task) for JapaneseSpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech) on JVS.This model utilizes the JVS dataset which encompasses 100 speakers.
 * ğŸ“¥ 84 [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1) - Kotoba-Speech-v0.1Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
 * ğŸ“¥ 84 [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3ã€‚
 * ğŸ“¥ 83 [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion) - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
 * ğŸ“¥ 83 [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b) - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦AWSã®trn1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦é–‹ç™ºã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚äº‹å‰å­¦ç¿’å¾Œã«å¤§å–œåˆ©ãƒ‡ãƒ¼ã‚¿ã§Fine-tuningã—ã¦ã„ã¾ã™ã€‚Architecture: GPT2Vocab size: 44880Model size: 6B paramsLicense: Apache License 2.0Library: aws-neuron-reference-for-megatron-lmå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰å­¦ç¿’ã‚’è¡Œã„ã¾ã—ãŸã€‚ãã®éš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯477å„„ãƒˆãƒ¼ã‚¯ãƒ³ã§ã—ãŸã€‚C4ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿CC-100ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿OSCARã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿Wikipediaã®æ—¥æœ¬èªãƒ€ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿Fine-tuningã¯ã€693ä¸‡ä»¶ã®å¤§å–œåˆ©ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¡Œã„ã¾ã—ãŸã€‚ä½¿ç”¨æ–¹æ³•import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "watashiha/watashiha-gpt-6b"tokenizer = AutoTokenizer.from_pretrained(model_name, u
 * ğŸ“¥ 81 [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel) - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset )ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒãƒ™ãƒ«é¢¨ç”»åƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§naver-clova-ix/donut-baseã‚’è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 80 [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - Japanese to Korean translatorJapanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/aihub-ja-ko-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferencefrom transformers import(EncoderDecoderModel,PreTrainedTokenizerFast,BertJapaneseTokenizer,)import torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "skt/kogpt2-base-v2"src_tokenizer = BertJapaneseTokenizer.from_pre
 * ğŸ“¥ 79 [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha) - Japanese InstructBLIP AlphaModel DetailsJapanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
 * ğŸ“¥ 79 [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese) - wav2vec2-live-japanesehttps://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using thecommon_voiceJSUTCSS10TEDxJP-10KJVSJSSSInference#usageimport torchimport torchaudiofrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processormodel = Wav2Vec2ForCTC.from_pretrained("ttop324/wav2vec2-live-japanese")processor = Wav2Vec2Processor.from_pretrained("ttop324/wav2vec2-live-japanese")test_dataset = load_dataset("commo
 * ğŸ“¥ 79 [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor) - Model Card for Model IDFine tunned ASR model from distil-whisper/distil-large-v2.This model aimed to transcribe japanese audio especially visual novel.
 * ğŸ“¥ 78 [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot) - yuyuyui-chatbotThis model is based on rinna/japanese-gpt2-medium and finetuned on Yuyuyui scenario corpus.
 * ğŸ“¥ 77 [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese) - Mixtral-8x7B-Instruct-v0.1-japaneseMixtral-8x7B-Instruct-v0.1-japaneseã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ABEJAã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸMetagton-LMã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯ã“ã¡ã‚‰ã§ã™ã€‚ä½¿ã„æ–¹import torchfrom transformers import AutoModelForCausalLM,
 * ğŸ“¥ 73 [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf) - Japanese-Alpaca-2-13B-GGUFJapanese-Alpaca-2-13B-GGUFã¯Japanese-Alpaca-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«URLï¼šhttps://huggingface.co/owner203/japanese-alpaca-2-13b
 * ğŸ“¥ 73 [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece) - alabnii/jmedroberta-base-manbyo-wordpieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * ğŸ“¥ 72 [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
 * ğŸ“¥ 72 [alfredplpl/gemma-2b-it-ja-poc-3](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-3) - ã¯ã˜ã‚ã«ãªã‚“ã‹æ—¥æœ¬èªãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚å°å‹ãªã®ã§ã‚¹ãƒãƒ›ã‚„å®¶é›»ãªã©ã«å‘ã„ã¦ã„ã¾ã™ã€‚Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchfrom peft import PeftModel# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™tokenizer = AutoTokenizer.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = AutoModelForCausalLM.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = PeftModel.from_pretrained(model = model, model_id = "alfredplpl/gemma-2b-it-ja-poc-3")# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™prompt="""ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‹±èªã¯å–‹ã‚‰ãšã€æ—¥æœ¬èªã ã‘å–‹ã£ã¦ãã ã•ã„ã€‚&lt;start_of_turn&gt;us
 * ğŸ“¥ 70 [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony) - BERT Base Japanese for IronyThis is a BERT Base model for sentiment analysis in Japanese additionally finetuned for automatic irony detection.
 * ğŸ“¥ 69 [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer) - albert-base-japanese-v1-with-japaneseæ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«BertJapaneseTokenizerã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™albert-base-japanese-v1ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒæ¥½ã«ãªã£ã¦ã„ã¾ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskfor PyTorchfrom transformers import (AutoModelForMaskedLM, AutoTokenizer)tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")model = AutoModelForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")tex
 * ğŸ“¥ 67 [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens) - This is a Japanese+English sentence-BERT model.
 * ğŸ“¥ 65 [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf) - rinna/nekomata-7b-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b.
 * ğŸ“¥ 65 [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese) - nlp-waseda/gpt2-small-japaneseThis model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
 * ğŸ“¥ 64 [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã§åˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚2021å¹´ã«ä½œã£ãŸå°èª¬ç”¨è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Model DetailsGPT-J-6Bã‚’TPUã§2é€±é–“æ—¥æœ¬èªtokenizerã‚’ç”¨ã„ã¦æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã—ã€ãã®å¾Œ2é€±é–“å°èª¬ãƒ‡ãƒ¼ã‚¿ã§è»¢ç§»å­¦ç¿’ã—ãŸã‚‚ã®ã§ã™ã€‚UsesGoogle colabã®T4 High-RAMã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚pip install transformers sentencepiece acceleratefrom transformers import GPTJForCausalLM, AlbertTokenizerimport torchtokenizer = AlbertTokenizer.from_pretrained('AIBunCho/japanese-novel-gpt-j-6b', keep_accents=True, remove_space=False)model = GPTJForCausalLM.from_pretrained("AIBunCho/japanese-novel-gpt-j-6b", torch_
 * ğŸ“¥ 62 [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b) - ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ calm-2-7b-chat ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 61 [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens) - This is a Japanese sentence-T5 model.
 * ğŸ“¥ 61 [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - This is a model for named entity recognition of Japanese medical documents.
 * ğŸ“¥ 60 [Helsinki-NLP/opus-mt-ja-it](https://huggingface.co/Helsinki-NLP/opus-mt-ja-it) - jpn-itasource group: Japanesetarget group: ItalianOPUS readme: jpn-itamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): itamodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus-2020-06-17.ziptest set translations: opus-2020-06-17.test.txttest set scores: opus-2020-06-17.eval.txtBenchmarkstestsetBLEUchr-FTatoeba-test.jpn.ita22.80.460System Info:hf_name: jpn-itasource_languages: jpntarg
 * ğŸ“¥ 59 [mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis) - Sentiment Analysis in Japanese - PhÃ¢n tÃ­ch cáº£m xÃºc trong tiáº¿ng Nháº­tBert phÃ¢n tÃ­ch cáº£m xÃºcModel descriptionMÃ´ hÃ¬nh cÃ³ tÃ¡c dá»¥ng xÃ¡c Ä‘á»‹nh cáº£m xÃºc cá»§a Ä‘oáº¡n vÄƒn.
 * ğŸ“¥ 57 [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine) - Model Card for Model IDæ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã‹ã‚‰ã€æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚ã‚‹å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™Model DetailsModel Descriptionä¾‹ãˆã°ã€ã€Œæ±äº¬ã®è‚‰æ–™ç†ã§ã€æ˜¥ã«é£Ÿã¹ã‚‰ã‚Œã‚‹ã€é¶è‚‰ã‚’ä½¿ã£ãŸæ–™ç†ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã€Œæ±äº¬ã€€â†’ã€€éƒ½é“åºœçœŒ/åœ°æ–¹(AREA)ã€ã€€ã€Œè‚‰æ–™ç†ã€€â†’ã€€ç¨®é¡(TYPE)ã€ã€€ã€Œæ˜¥ã€€â†’ã€€å­£ç¯€(SZN)ã€ã€€ã€Œé¶è‚‰ã€€â†’ã€€é£Ÿæ(INGR)ã€ã®ã‚ˆã†ã«ã€å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™æŠ½å‡ºå¯¾è±¡ã¯ã€AREAã€TYPEã€SZNã€INGRã®ï¼”ã¤ã§ã™Language(s) (NLP): æ—¥æœ¬èªLicense: mitFinetuned from model: tohoku-nlp/bert-base-japanese-v2Model SourcesRepository: wolf4032/nlp-token-classificationãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã‚¢ãƒ—ãƒªã®ä½œæˆã«ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ãŒæ²è¼‰ã•ã‚Œã¦ã„ã¾ã™Paper: [More Information Needed]Demo: wolf4032/japanese-token-classification-s
 * ğŸ“¥ 56 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-base-gamma-7bğŸ§© Configurationslices:- sources:-
 * ğŸ“¥ 56 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-instruct-gamma-7bğŸ§© Configurationslices:- sources:-
 * ğŸ“¥ 56 [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
 * ğŸ“¥ 55 [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - WRIME-fine-tuned BERT base JapaneseThis model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
 * ğŸ“¥ 55 [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - BERT base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer BERT base Japanese for details about the pre-training model.
 * ğŸ“¥ 52 [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo) - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šmicrosoft/Phi-3-mini-4k-instructå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-jaå­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«import torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",trust_remote_code=True,)model = AutoModelForCausalLM.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",device_map="auto",torch_dtype='auto',trust_remote_code=True,)text = "&lt;|user|&gt;\nä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦è‹±èªã§æ€è€ƒã—ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚æ±äº¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n&lt;|end|&gt;\n&lt;|assistant
 * ğŸ“¥ 52 [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese) - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)This is the longer input version of RoBERTa Japanese model pretrained on approximately 200
 * ğŸ“¥ 52 [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese) - è‹±èª+æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
 * ğŸ“¥ 52 [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
 * ğŸ“¥ 51 [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja) - GPT-2 small Japanese modelThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
 * ğŸ“¥ 50 [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-baseFine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 49 [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000) - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * ğŸ“¥ 45 [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos) - bert-base-japanese-unidic-luw-uposModel
 * ğŸ“¥ 44 [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese) - COMET-GPT2 jaFinetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
 * ğŸ“¥ 43 [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L) - Model Card for Model IDThis is a small T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese and English corpus.
 * ğŸ“¥ 42 [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps) - whisper-large-v3-japanese-4k-stepsThis model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
 * ğŸ“¥ 42 [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * ğŸ“¥ 41 [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k) - t5-base-japanese-web-8k (with Byte-fallback, 8K)Descriptionmegagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
 * ğŸ“¥ 41 [alfredplpl/gemma-2b-it-ja-poc](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc) - Noteã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¦ãƒã‚°ã£ã¦ã„ã‚‹ãŸã‚ã€ã“ã¡ã‚‰ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚Google ColabUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torch# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™tokenizer = AutoTokenizer.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™prompt="""ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‹±èªã¯å–‹ã‚‰ãšã€æ—¥æœ¬èªã ã‘å–‹ã£ã¦ãã ã•ã„ã€‚&lt;start_of_turn&gt;useräººç”Ÿã§å¤§åˆ‡ãªã“ã¨ã¯ãªã‚“ã§ã™ã‹ï¼Ÿ&lt;end_of_turn&gt;&lt;start_of_turn&gt;model"""# æ¨è«–ã®å®Ÿè¡Œinput_ids = tokenizer(prompt, return_tensors="pt").to(model.device
 * ğŸ“¥ 40 [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese) - æ—¥æœ¬èªByT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * ğŸ“¥ 40 [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli) - bert-base-japanese-jsnliThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
 * ğŸ“¥ 40 [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363) - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59363Validation MetricsLoss: 0.12651239335536957Accuracy: 0.9532079853817648Precision: 0.9729688278823665Recall: 0.9744633462616643AUC: 0.9717333684823413F1: 0.9737155136027014UsageYou can use cURL to access this model:$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}'
 * ğŸ“¥ 39 [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base) - studio-ousia/luke-japanese-baseã«å¯¾ã—ã¦æ¬¡ã®å¤‰æ›´ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’RoBERTaã‹ã‚‰æ—¥æœ¬èªBERTã«åˆ‡ã‚Šæ›¿ãˆã€ãã‚Œã«ä¼´ã£ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒSentencepieceã‹ã‚‰WordPieceã«ãªã‚Šã¾ã—ãŸ2023å¹´7æœˆ1æ—¥æ™‚ç‚¹ã®æ—¥æœ¬èªWikipediaã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã‚’ãŠã“ãªã„ã¾ã—ãŸ[UNK] (unknown) ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸè©³ç´°ã¯ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„ã€‚ä½¿ç”¨æ–¹æ³•from transformers import AutoTokenizer, AutoModel# æœ¬ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€trust_remote_code=True ã®æŒ‡å®šãŒå¿…è¦ã§ã™tokenizer = AutoTokenizer.from_pretrained("uzabase/luke-japanese-wordpiece-base", trust_remote_code=True)model = AutoModel.from_pretrained("uzabase/luke-japanese-wordpiece-base")æ›´æ–°æƒ…å ±2023/11
 * ğŸ“¥ 38 [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta) - æ—¥æœ¬èªåŒ»ç™‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«æ¦‚è¦ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶å®¤ã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹MedTxt-CRã‚’ç”¨ã„ã¦ã€alabniiã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹RoBERTaã‚’fine-tuningã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 37 [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base) - recruit-jp/japanese-typo-detector-roberta-baseãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦æ—¥æœ¬èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨å„æ–‡å­—ã”ã¨ã«èª¤å­—è„±å­—ã§ã‚ã‚‹ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¾ã™å„ãƒ©ãƒ™ãƒ«ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™idlabelmeaning0OKèª¤å­—ãªã—1deletion1æ–‡å­—ã®æŠœã‘2insertion_aä½™åˆ†ãª1æ–‡å­—ã®æŒ¿å…¥3insertion_bç›´å‰ã®æ–‡å­—åˆ—ã¨ä¸€è‡´ã™ã‚‹ï¼’æ–‡å­—ä»¥ä¸Šã®ä½™åˆ†ãªæ–‡å­—ã®æŒ¿å…¥4kanji-conversion_aåŒä¸€ã®èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰5kanji-conversion_bè¿‘ã„èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰6substitution1æ–‡å­—ã®å…¥ã‚Œæ›¿ãˆ7transpositionéš£æ¥ã™ã‚‹ï¼’æ–‡å­—é–“ã®è»¢ç½®8othersãã®ä»–ã®å…¥åŠ›èª¤ã‚Šèª¤ã‚Šç¨®é¡ã®è©³ç´°ã«ã¤ã„ã¦ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…ƒè«–æ–‡ã‚’ã”å‚ç…§ãã ã•ã„æ—¥æœ¬èª Wikipedia ã®ç·¨é›†å±¥æ­´ã«åŸºã¥ã å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ãã®ä»–ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯å½“ç¤¾ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„èª¤å­—è„±å­—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Hubã«å…¬é–‹ã—ã¾ã—ãŸ (Recruit Data Blog)å­¦ç¿’ãƒ‡ãƒ¼ã‚¿äº¬éƒ½å¤§å­¦å¤§å­¦é™¢æƒ…å ±å­¦ç ”ç©¶ç§‘çŸ¥èƒ½æƒ…
 * ğŸ“¥ 36 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k) - Heron BLIP Japanese StableLM
 * ğŸ“¥ 36 [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large) - rinna/japanese-hubert-largeOverviewThis is a Japanese HuBERT Large model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Large model, which contains 24 transformer layers with 16 attention heads.
 * ğŸ“¥ 36 [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model) - Model overviewThis model is the baseline model for awesome-japanese-nlp-classification-dataset.
 * ğŸ“¥ 36 [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm) - ku-nlp/roberta-large-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
 * ğŸ“¥ 36 [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps) - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.Usage:Install pip install faster-whisper (Check faster-whisper for detailed instructions.)from faster_whisper import WhisperModelmodel = WhisperModel('zh-plus/faster-whisper-large-v2-japanese-5k-steps', device="cuda", compute_type="float16")segments, info = model.transcribe("audio.mp3", beam_size=5)print("Detected language '%s' with probability
 * ğŸ“¥ 35 [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja) - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™samplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)prompt = "ãã‚Œã¯ä¹æœˆåˆæ—¬ã®ã‚ã‚‹è’¸ã—æš‘ã„æ™©ã®ã“ã¨ã§ã‚ã£ãŸã€‚ç§ã¯ã€ï¼¤å‚ã®"inputs = tokenizer(prompt, return_tensors="pt")generate_ids = model.generate(inputs.input_ids,max_length=30,top_k=30,top_p=0.95,temperature=0.6,repetition_penalty=1.2,do_sample=True,)tokenizer.decode(gen
 * ğŸ“¥ 34 [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1) - swallow-hermes-st-v1ç‰©èªä½œæˆã«å¼·ã‚ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºæ¥ãªã„ã‹ã¨è€ƒãˆã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 34 [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese) - Model Card for Japanese BART largeModel descriptionThis is a Japanese BART large model pre-trained on Japanese Wikipedia.
 * ğŸ“¥ 33 [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix) - â—†REV-Mix"ãƒ¬ãƒœãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 32 [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator) - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
 * ğŸ“¥ 32 [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner) - bert-japanese-nerã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã‚¿ã‚¹ã‚¯ã‚’ç›®çš„ã¨ã—ã¦ã€äº¬éƒ½å¤§å­¦ é»’æ©‹ãƒ»è¤šãƒ»æ‘è„‡ç ”ç©¶å®¤ãŒå…¬é–‹ã—ã¦ã„ã‚‹BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ãŒå…¬é–‹ã—ã¦ã„ã‚‹ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 32 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct) - æ›´æ–°æƒ…å ±æ—¥æœ¬èªæ©Ÿèƒ½ã¨instructãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã—ãŸver.2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ãƒ¢ãƒ‡ãƒ«æ¦‚è¦Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 31 [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt) - æ—¥æœ¬èªT5 Prefix Language ModelThis is a T5 (Text-to-Text Transfer Transformer)
 * ğŸ“¥ 31 [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯
 * ğŸ“¥ 31 [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese) - nlp-waseda/gpt2-xl-japaneseThis is Japanese GPT2 with approximatelyã€€1.5B parameters pretrained on Japanese Wikipedia and CC-100The model architecture of the model are based on Radford+ 2019.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
 * ğŸ“¥ 30 [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft) - The English document is here.
 * ğŸ“¥ 30 [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 29 [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚cl-tohoku/bert-base-japanese-v3ã®å‡ºåŠ›å±¤ã«CRFå±¤ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ãƒ‡ãƒ«ã‚’llm-book/ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚é–¢é€£ãƒªãƒ³ã‚¯GitHubãƒªãƒã‚¸ãƒˆãƒªColabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆAmazon.co.jpï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ï¼ˆgihyo.jpï¼‰ä½¿ã„æ–¹from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«ã®ãƒ—ãƒ­é‡çƒé¸æ‰‹"# textä¸­ã®å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºpprint(ner_pipe
 * ğŸ“¥ 29 [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin) - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤— HuggingFace Mainpage | ğŸ¤– ModelScope MainpageğŸ¬ HuggingFace Demo | ğŸ« ModelScope DemoğŸ˜º GitHubğŸ“– Tech ReportTable of ContentsğŸ“– Model IntroductionğŸ”— Model DownloadğŸ”– Model BenchmarkğŸ“Š Model InferenceğŸ“œ Declarations &amp; LicenseğŸ¥‡ Company Introduction1.
 * ğŸ“¥ 29 [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering) - RoBERTa base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model.
 * ğŸ“¥ 29 [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation) - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
 * ğŸ“¥ 28 [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa) - bert-base-japanese-v3-jcommonsenseqaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(å¤šè‚¢é¸æŠå¼è³ªå•å¿œç­”)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 28 [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese) - bart-base-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University.
 * ğŸ“¥ 26 [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B) - GPT-Neo 1.3B pre-trained model for JapaneseModel DescriptionGPT2/GPT3 like model trained on Japanese.corpus.
 * ğŸ“¥ 26 [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base) - SambaLingo-Japanese-BaseSambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
 * ğŸ“¥ 25 [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€MARC-ja(positive or negativeã®äºŒå€¤åˆ†é¡)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’yahoo japan/JGLUEã®MARC-ja( https://github.com/yahoojapan/JGLUE )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚positive or negativeã®äºŒå€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚This model is fine-tuned model for MARC-ja which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE MARC-ja dataset.
 * ğŸ“¥ 25 [jovyan/Swallow-MS-7b-v0.1-ChatVector](https://huggingface.co/jovyan/Swallow-MS-7b-v0.1-ChatVector) - Swallow-MS-7b-v0.1-ChatVectorJapanese "instruction tuned" model made by the technique of Chat VectorThe weights of this model are obtained not by any instruction tuning but by the following arithmetic:Swallow-MS-7b-v0.1 + Mistral-7B-Instruct-v0.2 - Mistral-7B-v0.1Chat Vectorã®æ‰‹æ³•ã‚’ä½¿ã£ã¦ã€å­¦ç¿’æ¸ˆã¿é‡ã¿ã®è¶³ã—å¼•ãã®ã¿ã§Swallow-MS-7b-v0.1ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å¯¾è©±èƒ½åŠ›ã‚’ä¸ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ã“ã¡ã‚‰ã®æ—¥æœ¬èªè¨˜äº‹ã§è§£èª¬ã—ã¦ã„ã¾ã™ã€‚Instruction formatThe promot format should be the same as Mistral-7B-Instruct-v0.2.E.g.text = "&lt;s&gt;[INST]
 * ğŸ“¥ 25 [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1.
 * ğŸ“¥ 24 [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese) - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
 * ğŸ“¥ 24 [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider) - Japanese CLIP ViT-H/14 (Wider)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
 * ğŸ“¥ 24 [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese) - Mixtral-8x7B-v0.1-japaneseMixtral-8x7B-v0.1-japaneseã¯Mixtral-8x7B-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è©³ç´°ã¯ABEJAã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸMetagton-LMã®ãƒ¬ãƒã‚¸ãƒˆãƒªã¯ã“ã¡ã‚‰ã§ã™ã€‚ä½¿ã„æ–¹import torchfrom transformers import AutoModelForCausalLM,
 * ğŸ“¥ 23 [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition) - This is for (private) DEMO only.
 * ğŸ“¥ 23 [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base) - Finance Sentiment JA (base)Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
 * ğŸ“¥ 23 [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseConsidering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
 * ğŸ“¥ 22 [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation) - å›ç­”ã¨å›ç­”ãŒå‡ºã¦ãã‚‹ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ä¸ãˆã‚‹ã¨è³ªå•æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://github.com/sonoisa/deep-question-generationæœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚¹ãƒ†ãƒƒãƒ—æ¦‚è¦SQuAD 1.1ã‚’æ—¥æœ¬èªã«æ©Ÿæ¢°ç¿»è¨³ã—ã€ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯ç´„åŠåˆ†ï¼‰ã€‚
 * ğŸ“¥ 22 [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 22 [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - luke-large-defamation-detection-japaneseæ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºå™¨This model is a fine-tuned version of studio-ousia/luke-japanese-large for the Japanese language finetuned for automatic defamation detection.
 * ğŸ“¥ 21 [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha
 * ğŸ“¥ 21 [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
 * ğŸ“¥ 21 [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1.
 * ğŸ“¥ 21 [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm) - ku-nlp/roberta-base-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
 * ğŸ“¥ 20 [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small) - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
 * ğŸ“¥ 20 [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese) - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset.
 * ğŸ“¥ 20 [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora) - Japanese DialoGPT trained with Aozora(ja) é’ç©ºæ–‡åº«ã®ã‚»ãƒªãƒ•ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®DialoGPT Smallã§ã™(en) Japanese DialoGPT Small trained on Aozora Bunko.
 * ğŸ“¥ 19 [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset )
 * ğŸ“¥ 19 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0) - Heron GIT Japanese StableLM
 * ğŸ“¥ 19 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ç¤¾ã€https://github.com/stockmarkteam/ner-wikipedia-dataset )ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚This model is fine-tuned model for Named Entity Recognition (NER) which is based on deberta-v2-base-japaneseThis model is fine-tuned by using Wikipedia dataset.
 * ğŸ“¥ 19 [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news) - bart-base-japanese-news(base-sized model)This repository provides a Japanese BART model.
 * ğŸ“¥ 18 [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos) - bert-large-japanese-luw-uposModel
 * ğŸ“¥ 18 [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss) - æ—¥æœ¬èªã§trainingã—ãŸllama2model size:  130.78Mtrainingã¯ä»¥ä¸‹ã®scriptå‚ç…§https://github.com/Lightning-AI/lit-gpt/tree/mainusefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")import torchfrom transformers import GenerationConfigprompt="ã‚ã®ã‚¤ãƒ¼ãƒãƒˆãƒ¼ãƒ´ã‚©ã®ã™ãã¨ãŠã£ãŸé¢¨ã€"inputs = tokenizer(prompt, return_tensors="pt")input_ids = inputs["input_ids"]generation_config = Gener
 * ğŸ“¥ 18 [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator) - ELECTRA base Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
 * ğŸ“¥ 17 [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos) - deberta-large-japanese-unidic-luw-uposModel
 * ğŸ“¥ 17 [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2) - In-progess long-context Japanese-English translation model based on tinyllama.
 * ğŸ“¥ 17 [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert) - nagisa_bertA BERT model for nagisa.
 * ğŸ“¥ 17 [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite) - Japanese transformer pipeline (bert-base).
 * ğŸ“¥ 16 [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja) - BERT base Japanese modelThis repository contains a BERT base model trained on Japanese Wikipedia dataset.
 * ğŸ“¥ 16 [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 16 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0) - Heron BLIP Japanese StableLM
 * ğŸ“¥ 16 [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator) - ELECTRA small Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
 * ğŸ“¥ 16 [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This pretrained model is almost the same as cl-tohoku/bert-base-japanese-char-v2 but do not need fugashi or unidic_lite.
 * ğŸ“¥ 15 [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base) - MPT-7B-baseã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 15 [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst) - MPT-7B-instã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7b-instructã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
 * ğŸ“¥ 15 [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’yahoo japan/JGLUEã®JCommonsenseQA( https://github.com/yahoojapan/JGLUE ) ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-tiny-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset.
 * ğŸ“¥ 15 [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying) - yacis-electra-small-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
 * ğŸ“¥ 14 [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-wase
 * ğŸ“¥ 14 [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * ğŸ“¥ 14 [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False) - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 1.7B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation.
 * ğŸ“¥ 14 [mpasila/calm2-7b-safetensors](https://huggingface.co/mpasila/calm2-7b-safetensors) - This is a conversion of cyberagent/calm2-7b  to safetensors so you don't have to worry about getting hacked by downloading dirty pickled files.
 * ğŸ“¥ 14 [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b) - Leia-Swallow-7BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
 * ğŸ“¥ 14 [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja) - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
 * ğŸ“¥ 14 [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos) - deberta-base-japanese-wikipedia-luw-uposModel
 * ğŸ“¥ 14 [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp) - Genji-JP 6BPlease check our blog post for more details, samples, evaluations and more:BlogpostModel DescriptionGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
 * ğŸ“¥ 13 [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-smallFine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 13 [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram) - Japanese BERT-base (Nothing + Unigram)How to load the tokenizerPlease download the dictionary file for Nothing + Unigram from our GitHub repository.
 * ğŸ“¥ 13 [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork) - LINE DistilBERT Japanese (forked by liwii)This is a forked version of DistilBERT model pre-trained on 131 GB of Japanese web text.
 * ğŸ“¥ 13 [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base) - Japanese CLIP ViT-H/14 (Base)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
 * ğŸ“¥ 13 [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient) - outputç­‘æ³¢ 2.0035860538482666ã¤ãã° 1.6586617231369019ç ”ç©¶ 1.6227693557739258å¤§å­¦ 1.3798155784606934å®Ÿé¨“ 0.5522942543029785å­¦ç”Ÿ 0.42351895570755005åˆ†æ 0.37844282388687134å›½ç«‹ 0.3685397505760193ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ 0.36495038866996765èŒ¨åŸ 0.3056415021419525ç§‘å­¦ 0.2876652181148529é–¢æ± 0.24301066994667053åœ°åŸŸ 0.21340851485729218å®Ÿæ–½ 0.1976248174905777å…ˆç«¯ 0.192025288939476ã‚µã‚¤ãƒˆ 0.11629197001457214èª¿æŸ» 0.09159307181835175ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ 0.08552580326795578è­°è«– 0.07484486699104309æ¤œè¨ 0.007034890353679657
 * ğŸ“¥ 13 [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0) - (English part follows Japanese one.
 * ğŸ“¥ 13 [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram) - Japanese BERT-base (Juman++ + Unigram)How to load the tokenizerPlease download the dictionary file for Juman++ + Unigram from our GitHub repository.
 * ğŸ“¥ 13 [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese) - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 largeFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * ğŸ“¥ 13 [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT
 * ğŸ“¥ 13 [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese) - åè¨€æ¨è«–ãƒ¢ãƒ‡ãƒ«
 * ğŸ“¥ 12 [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo) - Tanuki-ZeroBase model: llm-jp/llm-jp-13b-v1.0Instruction data: Randomly sampled, 15k Jaster dataset (train)Code is here.
 * ğŸ“¥ 12 [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0) - Heron GIT Japanese ELYZA Llama 2 Fast 7BModel
 * ğŸ“¥ 12 [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰è©³ç´°ã¯å…ƒãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ç¾åœ¨ã¯Q4_K_Mã®ã¿ã§ã™ã€‚éœ€è¦ã‚ã‚Šãã†ã§ã‚ã‚Œã°ä»–ã®ã‚‚ã®ã‚‚ç”¨æ„ã—ã¾ã™ã€‚DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1.
 * ğŸ“¥ 12 [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf) - Japanese-TextGen-Kage-v0.1-2x7BThis is a merge model using Mergekit-Evolve.We merged our model and Ninja-v1 with Mergekit-Evolve and then franken MoE.This model has been made more powerful by merging Ninja-v1!
 * ğŸ“¥ 12 [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZA-japanese-Llama-2-13b-fast-instructã® GGUF
 * ğŸ“¥ 12 [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp) - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
 * ğŸ“¥ 12 [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
 * ğŸ“¥ 12 [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp) - Model Card for Japanese DeBERTa V2 baseModel
 * ğŸ“¥ 12 [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp) - whisper-large-v2-mix-jp model for CTranslate2This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
 * ğŸ“¥ 12 [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora) - deberta-large-japanese-aozoraModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts.
 * ğŸ“¥ 12 [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese) - yacis-electra-smallThis is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.
 * ğŸ“¥ 11 [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char) - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * ğŸ“¥ 11 [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA) - friendly_JA-Modelã€€(T5 fine-tuned model)MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexiconExamplesinputoutputæœ€é©åŒ–ã‚’å¿œç”¨ã—ãŸæ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾åº¦ã ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿œç”¨ã—ãŸãƒã‚·ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ã å½¼ã¯æ¶ç©ºã®ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹å½¼ã¯ã‚¤ãƒã‚¸ãƒŠãƒªãƒ¼ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«æ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«ã‹ã‹ã£ã¦ã—ã¾ã£ãŸæ·±å±¤å­¦ç¿’ã¯é›£ã—ã„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚€ãšã‹ã—ã„æ–°ãŸãªæ¦‚å¿µã‚’ç´¹ä»‹ã™ã‚‹æ–°ã—ã„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç´¹ä»‹ã™ã‚‹æ´¥æ³¢ã®è­¦å ±ãŒæµã‚ŒãŸãƒ„ãƒŠãƒŸã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒæµã‚ŒãŸå—æµ·ãƒˆãƒ©ãƒ•ã®ç½å®³ã¯éœ‡æºåœ°ã«ã‚ˆã‚‹å—æµ·ãƒˆãƒ©ãƒ•ã®ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒ¼ã¯ã‚¨ãƒ”ã‚»ãƒ³ã‚¿ãƒ¼ã«ã‚ˆã‚‹æ¯å­ã¯éš›ã©ã„å†…å®¹ã®æœ¬ã‚’
 * ğŸ“¥ 11 [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying) - electra-base-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
 * ğŸ“¥ 11 [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - alabnii/jmedroberta-base-sentencepieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * ğŸ“¥ 11 [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa) - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’yahoo japan/JGLUEã®JCommonsenseQA( https://github.com/yahoojapan/JGLUE ) ã‚’ç”¨ã„ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-base-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset.
 * ğŸ“¥ 11 [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * ğŸ“¥ 11 [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b) - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
 * ğŸ“¥ 11 [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b) - Leia-Swallow-13BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
 * ğŸ“¥ 11 [AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer](https://huggingface.co/AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer) - Adapter bert-base-multilingual-cased-ja-wiki_pfeiffer for bert-base-multilingual-casedPfeiffer Adapter trained with Masked Language Modelling on Japanese Wikipedia Articles for 250k steps and a batch size of 64.This adapter was created for usage with the Adapters library.
 * ğŸ“¥ 11 [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseThis model was trained using SentenceTransformers Cross-Encoder class.
 * ğŸ“¥ 11 [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels) - NLLB 1.3B fine-tuned on Japanese to English Light Novel translationThis model was fine-tuned on light and web novel for Japanese to English translation.
 * ğŸ“¥ 11 [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200) - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g. FrenchFine-tuned facebook/wav2vec2-large-xlsr-53 on {language} using the Common Voice, ... and ... dataset{s}.

## Datasets

This list is sorted by downloads as of May 13, 2024.

 * ğŸ“¥ 27586 [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - Please feel free to open an issue or pull request.
 * ğŸ“¥ 11209 [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - JMTEB:
 * ğŸ“¥ 2884 [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
 * ğŸ“¥ 1805 [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 1684 [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - AutoWikiQAæ±å·¥å¤§ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MXã‚’ç”¨ã„ã¦ã€Wikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€Œè³ªå•(query)ã€ã¨ã€Œå›ç­”(answer)ã€ã‚’ç”Ÿæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã¨å›ç­”ã«ã¤ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 1392 [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - range3/wiki40b-jaThis dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
 * ğŸ“¥ 1259 [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ELYZA-tasks-100: æ—¥æœ¬èªinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆData Descriptionæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 1186 [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
 * ğŸ“¥ 861 [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - Please feel free to open an issue or pull request.
 * ğŸ“¥ 813 [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
 * ğŸ“¥ 779 [llm-jp/oasst1-21k-ja](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) - oasst1-21k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
 * ğŸ“¥ 729 [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - databricks-dolly-15k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
 * ğŸ“¥ 728 [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en) - The dataset was constructed in 3 steps:selecting business scenes,writing monolingual conversation scenarios according to the selected scenes, andtranslating the scenarios into the other language.
 * ğŸ“¥ 539 [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - Japanese Anime Speech Datasetæ—¥æœ¬èªã¯ã“ã¡ã‚‰japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
 * ğŸ“¥ 496 [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - JMMLUJapanese Massive Multitask Language Understanding BenchmarkJMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
 * ğŸ“¥ 453 [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA) - Dataset DetailsDataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
 * ğŸ“¥ 450 [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - LLM ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆèƒ½åŠ›ã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ HumanEval ã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ã€‚æ©Ÿæ¢°ç¿»è¨³(DeepL, GPT-4)ã®ç¿»è¨³çµæœã‚’å…¨ã¦äººæ‰‹ã«ã‚ˆã£ã¦å†ä¿®æ­£ã—ã€ è¨³æ–‡ã‚’æ—¥æœ¬äººã®ãƒ—ãƒ­ã‚°ãƒ©ãƒãŒèª­ã‚“ã§ç†è§£ã—ã€ã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‘ã‚‹å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã€‚ãŸã ã—ã€è‹±èªç‰ˆ HumanEval ã®é–“é•ã„ã¯ã€ä¿®æ­£ã›ãšã«æ®‹ã—ã¦ã€ HumanEval åŒæ§˜ã«ä¸å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ç”Ÿæˆèƒ½åŠ›ã‚’è¦‹ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªLLM ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ãŠä½¿ã„ãã ã•ã„ã€‚LanguagesThe programmin
 * ğŸ“¥ 373 [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023) - Danbooru2023:
 * ğŸ“¥ 353 [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja) - hh-rlhf-12k-jaThis repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
 * ğŸ“¥ 351 [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 340 [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - Rakuda - Questions for Japanese modelsRepository:
 * ğŸ“¥ 320 [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - OverviewThis dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.[For Japanese] æ—¥æœ¬èªã§ã®æ¦‚è¦èª¬æ˜ã‚’ Qiita ã«è¨˜è¼‰ã—ã¾ã—ãŸ: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodologyThe code to reproduce this dataset is made available on GitHub: globis-org/aozorabunko-exctractor.1.
 * ğŸ“¥ 284 [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset) - Githubãƒªãƒã‚¸ãƒˆãƒªstockmarkteam/ner-wikipedia-datasetã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 279 [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
 * ğŸ“¥ 276 [neulab/odex](https://huggingface.co/datasets/neulab/odex) - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
 * ğŸ“¥ 258 [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction) - ichikara-instruction (Non Commercial)LLMã®ãŸã‚ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›å¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
 * ğŸ“¥ 239 [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - OpenMathInstruct-1 ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãª180ä¸‡ä»¶ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 225 [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["æ–‡å­—é£ã„ç¨®åˆ¥"] == "æ–°å­—æ–°ä»®å"
 * ğŸ“¥ 222 [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus) - The corpus has 50,000 manually simplified and aligned sentences.
 * ğŸ“¥ 192 [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli) - Dataset PreprocessingSupported Tasks and LeaderboardsLanguagesæ³¨é‡ˆã¯ã™ã¹ã¦æ—¥æœ¬èªã‚’ä¸»è¦è¨€èªã¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 189 [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - llm-japanese-datasetLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³(ãƒãƒ£ãƒƒãƒˆ)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸»ã«ï¼Œè‹±èªã§æ§‹ç¯‰ã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼
 * ğŸ“¥ 187 [transformersegmentation/CHILDES](https://huggingface.co/datasets/transformersegmentation/CHILDES) - Phonemized Child Directed Speech DatasetThis dataset contains utterance downloaded from CHILDES which have been pre-processed and converted to phonemic transcriptions by this processing script.
 * ğŸ“¥ 185 [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - JQaRA : Japanese Question Answering with Retrieval Augmentation - æ¤œç´¢æ‹¡å¼µ(RAG)è©•ä¾¡ã®ãŸã‚ã®æ—¥æœ¬èª Q&amp;A ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜æ€§èƒ½ãª LLM ã®å°é ­ã«ä¼´ã„ã€LLM ã‚’ç”¨ã„ãŸè³ªç–‘å¿œç­”ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 182 [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA) - è‡ªå‹•ç”ŸæˆQ&amp;Aç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡äºŒç¨®é¡ã®è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒå­˜åœ¨ã—ã¾ã™CommonCrawlã¾ãŸã¯ã€CC-BYç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 174 [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
 * ğŸ“¥ 166 [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - range3/cc100-jaThis dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
 * ğŸ“¥ 163 [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset) - Dataset overviewThis dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).The labels are categorized as "Relevant (1)" and "Not Relevant (0)".
 * ğŸ“¥ 160 [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - Update:2023/12/25oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸoasst2-chat-68k-jaã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚This dataset was created by automatically translating "OpenAssistant/oasst2" into Japanese by DeepL."OpenAssistant/oasst2" ã‚’ DeepLç¿»è¨³ã‚’ç”¨ã„ã¦æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ Instruction ã¨ Output ï¼ˆprompterã®å‘½ä»¤ã¨assistantã®å›ç­”ï¼‰ã®å½¢å¼ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã“ã¡ã‚‰ã®ã‚³ãƒ¼ãƒ‰ã§å¤‰æ›ã—ã¦ä¸‹ã•ã„ï¼ˆå¤‰æ›ã«ã¯5åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ï¼‰ã€‚å¤‰æ›ã‚³ãƒ¼ãƒ‰å‚è€ƒhttps://github.com/h2oai/h2o-llmstudio/blob/5ebfd3879e226b4e1afd0a0b45eb632e60412129/app_utils/utils.py#L1888pip install datasetsfrom datasets import l
 * ğŸ“¥ 158 [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences) - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 154 [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹2010ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’huggingfaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ï½¡2009 å¹´åº¦ã«ãŠã‘ã‚‹è‘—ä½œæ¨©æ³•ã®æ”¹æ­£ï¼ˆå¹³æˆ21å¹´é€šå¸¸å›½ä¼šã€€è‘—ä½œæ¨©æ³•æ”¹æ­£ç­‰ã«ã¤ã„ã¦ | æ–‡åŒ–åºï¼‰ã«åŸºã¥ãï¼Œæƒ…å ±è§£æç ”ç©¶ã¸ã®åˆ©ç”¨ã«é™ã£ã¦åˆ©ç”¨å¯èƒ½ã§ã™ï½¡å½¢æ…‹ç´ è§£æã‚’ç”¨ã„ã¦ï½¤è‡ªå‹•ã§å¥ç‚¹ã‚’ã¤ã‘ã¾ã—ãŸï½¡å¤‰æ›ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆå½¢æ…‹ç´ è§£æãªã©
 * ğŸ“¥ 131 [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - [github].
 * ğŸ“¥ 111 [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - Japanese stopwords for nagisaThis is a stopword list of frequently used words in the Japanese language, created according to the tokenization rules of the Japanese text analysis library, nagisa.
 * ğŸ“¥ 107 [Nexdata/multi_language](https://huggingface.co/datasets/Nexdata/multi_language) - SummaryThe dataset contains 25,000 hours of multi-language reading speech data.
 * ğŸ“¥ 107 [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - Dataset detailsEach entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
 * ğŸ“¥ 106 [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese) - The annotation is by majority decision by 5 - 10 crowd workers.
 * ğŸ“¥ 106 [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese) - fungi_indexed_mycological_papers_japaneseå¤§èŒè¼ªã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰====LanguagesJapaneseThis dataset is available in Japanese only.
 * ğŸ“¥ 106 [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese) - fungi_diagnostic_chars_comparison_japaneseå¤§èŒè¼ªã€Œè­˜åˆ¥å½¢è³ªã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰====LanguagesJapaneseThis dataset is available in Japanese only.
 * ğŸ“¥ 105 [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ãŸã‚‚ã®ã§ã™ã€‚
 * ğŸ“¥ 105 [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database) - fungi_trait_circus_databaseå¤§èŒè¼ªã€ŒTrait Circusã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆçµ±åˆ¶å½¢è³ªï¼‰æœ€çµ‚æ›´æ–°æ—¥ï¼š2023/12/29====LanguagesJapanese and EnglishPlease do not use this dataset for academic purposes for the time being. 
 * ğŸ“¥ 103 [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - J-ResearchCorpusUpdate:2024/3/16è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š(NLP2024)ã‚’å«ã‚€ã€è«–æ–‡ 1,343 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 2024/2/25è¨€èªå‡¦ç†å­¦ä¼šèªŒã€Œè‡ªç„¶è¨€èªå‡¦ç†ã€ã®ã†ã¡ CC-BY-4.0 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ 360 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ æ¦‚è¦CC-BY-* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè«–æ–‡ã‚„å­¦ä¼šèªŒç­‰ã‹ã‚‰æŠœç²‹ã—ãŸé«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 81 [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - llm-japanese-dataset-vanillaLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆizumi-lab/llm-japanese-dataset ã‹ã‚‰ï¼Œæ—¥è‹±ç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ã‚’æŠœã„ãŸã‚‚ã®ã§ã™ï¼
 * ğŸ“¥ 81 [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus) - IntroductionThis is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
 * ğŸ“¥ 74 [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - JA-VG-VQA-500Dataset DescriptionJA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
 * ğŸ“¥ 57 [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - range3/wikipedia-ja-20230101This dataset consists of a parquet file from the wikipedia dataset with only Japanese data extracted.
 * ğŸ“¥ 56 [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions) - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja ã®ä¸­ã‹ã‚‰ JGLUEï¼ˆ JcommonsenseQA , MARC-ja , JSQuAD ï¼‰ã®è¦³ç‚¹ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 55 [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli) - JSNLI Version 1.1 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆ (train_w_filtering)
 * ğŸ“¥ 53 [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick) - Dataset.
 * ğŸ“¥ 52 [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja) - Dataset used to train PokÃ©mon text to image model, add a Japanese Column of PokÃ©mon BLIP captionsBLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
 * ğŸ“¥ 52 [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped) - chatbot-arena-ja-calm2-7b-chatã‹ã‚‰promptãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 51 [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki) - JaWikiWikipediaã®HTMLå½¢å¼ã®ãƒ€ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 51 [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja) - mmarcoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦ã€queryã‚’keyã¨ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 46 [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever) - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 46 [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats) - OverviewThis dataset is of conversations extracted from Aozora Bunko (é’ç©ºæ–‡åº«), which collects public-domain books in Japan, using a simple heuristic approach.[For Japanese] æ—¥æœ¬èªã§ã®æ¦‚è¦èª¬æ˜ã‚’ Qiita ã«è¨˜è¼‰ã—ã¾ã—ãŸ: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodFirst, lines surrounded by quotation mark pairs (ã€Œã€) are extracted as utterances from the text field of globis-university/aozorabunko-clean.
 * ğŸ“¥ 45 [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja) - oasst1-89k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 42 [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset) - Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆGitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/LICENSE: CC-BY-SA 3.0Developed by Stockmark Inc.
 * ğŸ“¥ 42 [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation) - Japanese-Vietnamese Translated Sentence Pairs.
 * ğŸ“¥ 38 [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - CABank Japanese CallHome CorpusParticipants:   120Type of Study:  phone callLocation:   United StatesMedia type: audioDOI:    doi:10.21415/T5H59VWeb: https://ca.talkbank.org/access/CallHome/jpn.htmlCitation informationSome citation here.
 * ğŸ“¥ 33 [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law) - Japanese LawsThis dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. Each entry furnishes comprehensive details about a particular law, encapsulating its number, title, unique ID, the date it came into effect, and its complete text.
 * ğŸ“¥ 32 [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct) - Amenokaku-Code-InstructUpdate:2023/12/27ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« JaxTon , ãƒ—ãƒ­ã«ãªã‚‹Java ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ 180 ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
 * ğŸ“¥ 32 [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR) - JaCWIR: Japanese Casual Web IR - æ—¥æœ¬èªæƒ…å ±æ¤œç´¢è©•ä¾¡ã®ãŸã‚ã®å°è¦æ¨¡ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªWebã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿‘å¹´ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å°é ­ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚’ç”¨ã„ãŸè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§è³ªå•ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 29 [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - CoTangentã¯äººæ‰‹ã§ä½œæˆã•ã‚ŒãŸé«˜å“è³ªã§ã‚¯ãƒªãƒ¼ãƒ³ãª100ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªCoTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 28 [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset) - å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã¯llm-book/ner-wikipedia-datasetã¨åŒæ§˜ã®ã‚‚ã®ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å…¨éƒ¨ã§8ç¨®é¡ (äººåã€æ³•äººåã€åœ°åã€è£½å“åã€æ”¿æ²»çš„çµ„ç¹”åã€æ–½è¨­åã€ãã®ä»–ã®çµ„ç¹”åã€ã‚¤ãƒ™ãƒ³ãƒˆå)ã‚ã‚Šã¾ã™ã€‚
 * ğŸ“¥ 28 [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - Japanese-Heron-BenchDataset DescriptionJapanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
 * ğŸ“¥ 26 [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives) - [Under Construction]This is a repository containing all the queries from the Japanese part of the MMarco dataset, the multilingual version of the MSMarco dataset.
 * ğŸ“¥ 25 [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp) - JBLiMPThis is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
 * ğŸ“¥ 23 [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs) - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
 * ğŸ“¥ 23 [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01) - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
 * ğŸ“¥ 21 [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO) - Chatbot Arena Conversationsã®è³ªå•æ–‡ã‹ã‚‰ã€aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ã‚’ä½¿ç”¨ã—ã¦å¿œç­”æ–‡ã‚’ä½œæˆã—ã¾ã—ãŸè³ªå•æ–‡ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®Promptéƒ¨åˆ†ã‚’ä½¿ç”¨ã—ã¾ã—ãŸChatbot Arena Conversations JA (calm2)ä»¥ä¸‹å¼•ç”¨ã§ã™ã€‚
 * ğŸ“¥ 20 [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs) - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
 * ğŸ“¥ 20 [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja) - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦æ‰‹å‹•ã§ä½œæˆã—ãŸDatabricksã«é–¢ã™ã‚‹è³ªå•ã¨å›ç­”ãƒšã‚¢ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 19 [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - Sakura_datasetå•†ç”¨åˆ©ç”¨å¯èƒ½ãªè¶…å°è¦æ¨¡é«˜å“è³ªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚categoryã¯ä»¥ä¸‹commonsense_qa: å¸¸è­˜å•é¡ŒCalc-ape210k: æ•°å­¦å•é¡Œjapanese-commonsense-openqa: æ—¥æœ¬ã®å¸¸è­˜å•é¡Œ(è‡ªä½œ)ä¸‹è¨˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚commonsense_qaMU-NLPC/Calc-ape210kLICENSEThis dataset is licensed under Database Contents License (DbCL)
 * ğŸ“¥ 17 [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - mqaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
 * ğŸ“¥ 17 [RyokoAI/Syosetu711K](https://huggingface.co/datasets/RyokoAI/Syosetu711K) - Not all information here may be accurate or accessible.
 * ğŸ“¥ 15 [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored) - For the English version, please click here.
 * ğŸ“¥ 14 [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja) - Japanese Wikipedia Human Retrieval datasetThis is a Japanese question answereing dataset with retrieval on Wikipedia articlesby trained human workers.
 * ğŸ“¥ 14 [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese) - Sorry, it's no longer available on Hugging Face.
 * ğŸ“¥ 14 [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja) - ApolloCorpus-jaæ¦‚è¦å¤šè¨€èªåŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® ApolloCorpus ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸ 525k ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
 * ğŸ“¥ 14 [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja) - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
 * ğŸ“¥ 14 [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k) - cosmopedia-japanese-20kã®ãƒ‡ãƒ¼ã‚¿ã«ã€kunishouæ§˜ã‹ã‚‰20k-100kã‚’ã”æä¾›ã„ãŸã ã‘ã‚‹ã“ã¨ã«ãªã‚Š100kã¾ã§æ‹¡å¤§ã—ã¾ã—ãŸã€‚
