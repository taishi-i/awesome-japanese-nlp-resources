# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1361 models and 529 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents üìñ

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).

Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Multimodality](#Multimodality)
	 * [Text Generation](#Text-Generation)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Reasoning](#Reasoning)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## The latest additions üéâ

**Models**
8 models have been added.

- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)


**Datasets**
7 datasets have been added.

- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)


## Models üß†

This list is sorted by downloads as of May 27, 2025.
1361 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 3,705,897
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from xlm-roberta-base using a dataset from Japanese Wikipedia, classifying tokens into PER and ORG entities.
  - Downloads: 627,855
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M parameter text embedding model by Retrieva, Inc., optimized for Japanese and English utilizing a modernbert-ja-310m architecture.
  - Downloads: 435,709
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri provides Japanese general text embeddings with v3 models (30M-315M parameters, max length 8192) optimized for performance and easily usable with Sentence Transformers and libraries like Fugashi and Unidic-lite.
  - Downloads: 314,990
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - This repository provides a Japanese BERT model pretrained using Unidic-lite word-level tokenization and whole word masking on CC-100 and JA Wiki datasets.
  - Downloads: 300,460
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - Rinna‚Äôs japanese-cloob-vit-b-16 is a Japanese contrastive language-image pre-training (CLIP) model, installable via pip and accessible through the `japanese-clip` package.
  - Downloads: 296,321
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - This repository provides a Japanese-specific DeBERTa V3 model designed for inference without a morphological analyzer and with improved handling of word boundaries.
  - Downloads: 156,863
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This repository provides a refined Japanese Sentence-BERT model (v2) ‚Äì built on cl-tohoku/bert-base-japanese-whole-word-masking and utilizing MultipleNegativesRankingLoss for improved accuracy over v1, requiring fugashi and ipadic for inference.
  - Downloads: 144,216
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B is a top-performing Japanese text embedding model by Preferred Networks, excelling in tasks like information retrieval and achieving state-of-the-art results on the JMTEB benchmark.
  - Downloads: 140,666
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with IPA dictionary-based word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 120,990
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - This repository provides a Japanese BERT base model pretrained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved masked language modeling.
  - Downloads: 120,360
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - This repository provides a Japanese BERT base model pre-trained on Japanese text, utilizing word and character tokenization based on the IPA dictionary, with a standard 12-layer, 768-dimensional architecture.
  - Downloads: 103,991
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - This repository provides a Japanese BERT base model pretrained on CC-100 and JAWIKI corpora, utilizing character and word-level (Unidic 2.1.2) tokenization with whole word masking for improved language modeling.
  - Downloads: 102,293
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - This repository provides a Japanese BERT base model pretrained on Japanese text using word and WordPiece tokenization based on the IPA dictionary, mirroring the original BERT base architecture.
  - Downloads: 59,556
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This repository provides a Japanese Sentence-BERT model (version 1) for generating sentence embeddings, with a more accurate version 2 also available, utilizing the `transformers` library and PyTorch.
  - Downloads: 48,016
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri provides pre-trained Japanese text embeddings using the `sentence-transformers` library, requiring installation of `fugashi`, `sentencepiece`, and `unidic-lite`, and specifying "„ÇØ„Ç®„É™: " or "ÊñáÁ´†: " prefixes for queries and passages.
  - Downloads: 46,496
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - This repository provides a pre-trained Japanese DeBERTa V2 tiny model, trained on Japanese Wikipedia, CC-100, and OSCAR, for masked language modeling tasks.
  - Downloads: 41,268
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 37,896
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese sentence embedding model built on LUKE, trained on diverse data for tasks like semantic similarity and search.
  - Downloads: 36,206
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 30,950
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced large language model based on Meta Llama 3, fine-tuned by ELYZA for improved performance in Japanese language tasks.
  - Downloads: 30,181
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository hosts a medium-sized Japanese GPT-2 model trained by rinna Co., Ltd., and readily usable with the `transformers` library for causal language modeling.
  - Downloads: 24,562
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a state-of-the-art Japanese text embedding model, built on Sarashina2.1-1B, that generates 1792-dimensional vectors for semantic similarity and search tasks, achieving top performance on JMTEB.
  - Downloads: 23,420
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - This repository provides a BERT-based model fine-tuned for Japanese sentiment analysis of Amazon product reviews, classifying sentence polarity as positive or negative.
  - Downloads: 21,351
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - This repository provides a Japanese BERT model pretrained on Japanese text using Unidic-lite word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 18,188
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow enhances the Japanese language capabilities of Meta‚Äôs Llama 3.1 models (8B, 70B) through continual pre-training on a large Japanese web corpus, while maintaining English proficiency.
  - Downloads: 18,145
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jnli) fine-tuned on the JGLUE MARC-ja dataset for natural language inference tasks, as detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 17,160
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - This repository provides a Japanese DeBERTa V2 large language model, pre-trained on diverse Japanese text corpora using character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 15,009
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, trained on 312.5B Japanese tokens and achieving 8.68 perplexity, based on the EleutherAI/gpt-neox architecture.
  - Downloads: 14,650
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitions‚Äô repository hosts Japanese autoregressive language models, including sarashina2.2-3b-instruct-v0.1, evaluated on Japanese and English benchmarks like Elyza-tasks-100 and MT Bench.
  - Downloads: 13,368
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a multilingual BERT-based model providing sentence embeddings for 109 languages, trained with masked and translation language modeling for tasks like bi-text retrieval, and now available in PyTorch.
  - Downloads: 12,561
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring installation of `sentence-transformers`, `fugashi`, `sentencepiece`, and `unidic-lite`, and specifying prefixes ("„ÇØ„Ç®„É™:" or "ÊñáÁ´†:") for effective query/passage processing.
  - Downloads: 11,754
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model built on Whisper v2.0, enhanced with speaker diarization, punctuation, and integrated into Hugging Face Transformers (v4.39+).
  - Downloads: 10,787
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese is a pre-trained DistilBERT model for Japanese natural language processing, built by LINE Corporation on 131GB of web text, and based on their in-house BERT-base model.
  - Downloads: 10,504
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering expanded vocabulary, longer sequence support (up to 8192 tokens), and FlashAttention for improved performance and efficiency.
  - Downloads: 9,646
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - This repository provides a Japanese sentence-transformers model, `sbert-jsnli-luke-japanese-base-lite`, for generating 768-dimensional sentence embeddings useful for semantic search and clustering, fine-tuned on JSNLI data.
  - Downloads: 8,937
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - This repository provides a Japanese BERT large model, pretrained using Unidic-lite word-level tokenization and whole word masking on CC-100 and JA Wiki data.
  - Downloads: 8,594
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jsts) fine-tuned on the JSTS dataset for calculating semantic similarity, as detailed in chapter 5 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 7,577
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - This repository provides a tiny, Japanese character-level DeBERTa V2 model pre-trained on diverse Japanese text corpora for masked language modeling tasks.
  - Downloads: 7,046
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T is a 3B-parameter decoder-only language model pretrained on Japanese data to achieve high performance in Japanese language modeling and downstream tasks.
  - Downloads: 6,951
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository hosts a base-sized Japanese RoBERTa model, trained with rinna‚Äôs code and readily loadable via the `transformers` library for masked language modeling.
  - Downloads: 6,661
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - LUKE-Japanese is a pre-trained Japanese language model utilizing knowledge-based embeddings of words and entities from Wikipedia to generate contextualized representations.
  - Downloads: 6,627
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Meta's Llama 3, enhanced with Japanese language data and available in 8B and 70B parameter Instruct and Chat versions released July 1, 2024.
  - Downloads: 6,379
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - LLM-jp-3-7.2b-instruct3 is a 7.2 billion parameter, instruction-tuned Japanese large language model developed by NII, available in Hugging Face Transformers format with PyTorch support.
  - Downloads: 6,316
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and distributed as a Python package for advanced Japanese natural language processing with GiNZA v5.
  - Downloads: 6,253
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - Rinna‚Äôs japanese-gpt-1b is a 1.3B-parameter Japanese GPT model for causal language modeling, easily usable with Hugging Face Transformers.
  - Downloads: 6,041
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri provides pre-trained Japanese text embeddings (v3 models ranging from 30M to 315M parameters) optimized for performance and utilizing libraries like Sentence Transformers, Fugashi, and SentencePiece.
  - Downloads: 5,960
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M is a computationally efficient Japanese BERT model trained on 4.09T tokens, utilizing local & global attention and RoPE for improved long sequence handling with a 102,400 vocabulary.
  - Downloads: 5,679
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1 billion parameter English/Japanese language model using a hybrid Mamba/attention architecture‚Äîlike Samba, but with added normalization for stability‚Äîdeveloped by Preferred Elements, Inc.
  - Downloads: 5,641
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 5,587
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 5,386
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - This repository provides a 32K vocabulary T5 model pre-trained on extensive Japanese web text corpora (mC4, wiki40b) for text-to-text tasks, with accompanying training code and a smaller 8K version also available.
  - Downloads: 5,319
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, offering models for improved Japanese text retrieval and ranking.
  - Downloads: 5,244
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - This repository hosts a 3.6B parameter Japanese GPT-NeoX model finetuned with translated Anthropic and FLAN datasets to function as an instruction-following conversational AI.
  - Downloads: 5,192
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow are 8B/70B language models continually pre-trained on Llama 3.1, significantly enhancing Japanese language capabilities alongside retained English proficiency using a 200 billion token corpus.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M is a computationally efficient Japanese BERT model trained on 4.39T tokens, utilizing local & global attention with RoPE for improved long sequence handling and featuring a 102,400 vocabulary.
  - Downloads: 4,655
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-14B and 32B Japanese language models, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 4,360
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - This repository provides a GGUF-formatted version of the AXCXEPT-phi-4-deepseek-R1K-RL-EZO model, utilizing the TFMC/imatrix dataset for Japanese language processing and intended for use with llama.cpp.
  - Downloads: 4,108
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - Rinna‚Äôs Japanese wav2vec2.0 Base model is a 12-layer transformer trained on nearly 19,000 hours of Japanese speech data, replicating the original architecture and training process.
  - Downloads: 4,040
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - llava-calm2-siglip is an experimental vision-language model enabling Japanese-language question answering about images, utilizing PIL, requests, and transformers.
  - Downloads: 3,879
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct versions) developed by NII, utilizing Hugging Face Transformers and requiring specific library versions like PyTorch, Transformers, and Accelerate.
  - Downloads: 3,818
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the deepseek-r1-distill-qwen2.5-bakeneko-32b language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 3,795
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository hosts a small Japanese GPT-2 model trained by rinna Co., Ltd., readily usable with the `transformers` library for causal language modeling.
  - Downloads: 3,664
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper provides fast, distilled Japanese Automatic Speech Recognition (ASR) models‚Äîbuilt on OpenAI‚Äôs Whisper large-v3‚Äîoptimized for speed and accuracy using knowledge distillation and incorporating stable-ts/punctuation pipeline.
  - Downloads: 3,624
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - This repository offers a 3.6 billion parameter Japanese GPT-NeoX language model, fine-tuned with Reinforcement Learning from Human Feedback (RLHF) to function as an instruction-following conversational agent.
  - Downloads: 3,335
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri provides large Japanese text embeddings using Sentence Transformers, requiring installation of supporting libraries and a specific prefix ("„ÇØ„Ç®„É™:" or "ÊñáÁ´†:") for optimal query and passage text processing.
  - Downloads: 3,293
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - Rinna's Japanese HuBERT Base model is a 12-layer transformer trained on 19,000 hours of Reazon Japanese speech data, mirroring the original HuBERT Base architecture.
  - Downloads: 3,246
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - ELYZA‚Äôs Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter language model built on Meta Llama 3, provided in quantized GGUF (Q4_K_M) format for use with llama.cpp.
  - Downloads: 3,215
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides GGUF quantized versions of the 32B rinna/qwen2.5-bakeneko-instruct model, optimized for use with llama.cpp-based applications.
  - Downloads: 3,003
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly improved accuracy, particularly in qualitative assessments.
  - Downloads: 2,679
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository provides a series of Japanese cross-encoder rerankers of varying sizes (xsmall to large) for improved information retrieval, including a high-performance model and technical documentation.
  - Downloads: 2,502
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - This repository provides a high-performance Japanese SPLADE v2 model with a WebUI demo for converting text to sparse vectors, utilizing YAST/YASEM for training and easy inference/token inspection.
  - Downloads: 2,407
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 2,401
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository provides a Japanese-tuned version of the DeepSeek-R1-Distill-Qwen-14B language model, enabling it to generate thought processes and text directly in Japanese.
  - Downloads: 2,277
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) fine-tuned on JA Wiki for unsupervised SimCSE sentence embeddings, as detailed in chapter 8 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 2,252
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - This repository provides Japanese cross-encoder rerankers (v1 & v2-m3) of varying sizes (xsmall to large) for improved sentence ranking, offering models with 6-24 layers and 384-1024 hidden sizes.
  - Downloads: 2,232
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to improve retrieval relevance using the `cl-nagoya/ruri-reranker-large` model from Hugging Face.
  - Downloads: 2,230
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository hosts rinna‚Äôs extra-small Japanese GPT-2 model for causal language modeling, providing code for usage with the `transformers` library.
  - Downloads: 2,219
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 2,212
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Meta‚Äôs Llama 3.3, significantly improving Japanese language capabilities while maintaining English proficiency using a 315B token corpus.
  - Downloads: 2,210
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - This repository hosts the llm-jp-3-1.8b-instruct3, a 1.8 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 2,185
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - This repository provides a fine-tuned BERT-base Japanese language model (llm-book/bert-base-japanese-v3-marc_ja) for sentiment analysis, trained on the JGLUE MARC-ja dataset and detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 2,121
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - This repository provides a Japanese DeBERTa V3 base model pre-trained on the LLM-jp corpus, suitable for masked language modeling tasks using the `transformers` library.
  - Downloads: 2,060
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering improved performance and efficiency with support for 8192 tokens and a 100K vocabulary, utilizing FlashAttention.
  - Downloads: 2,042
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository provides GGUF-formatted versions of the Fugaku-LLM-13B-instruct model, utilizing the TFMC/imatrix dataset and requiring agreement to the terms of use for usage with llama.cpp.
  - Downloads: 2,032
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing 7B Japanese language model built upon the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 2,010
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 base model pre-trained on large Japanese text corpora for masked language modeling tasks.
  - Downloads: 1,983
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese sentence embedding model based on RoFormer, optimized for long sequence retrieval using distillation and RoPE, and runnable on CPUs.
  - Downloads: 1,979
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a fine-tuned 13B parameter Japanese large language model optimized for instruction-following, building on RakutenAI-2.0-8x7B to improve fluency and contextual understanding.
  - Downloads: 1,950
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - Cyberagent‚Äôs DeepSeek-R1-Distill-Qwen-14B-Japanese is a Japanese-finetuned 14B language model based on DeepSeek-R1-Distill, usable with the `transformers` library.
  - Downloads: 1,930
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base is a Japanese language model based on modernBERT, trained on a 3.4TB corpus with support for 8192 sequence length, requiring the transformers library.
  - Downloads: 1,860
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri provides Japanese general text embeddings with v3 models (30M-315M parameters, max length 8192) achieving up to 77.24 JMTEB, utilizing Sentence Transformers, Fugashi, Sentencepiece, and Unidic-lite.
  - Downloads: 1,844
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with character and word-level tokenization, and whole word masking for improved language understanding.
  - Downloads: 1,816
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - This repository details an experiment applying differences extracted from the smaller Japanese Suzume-Llama-3-8B model to the larger Meta-Llama-3-70B-Instruct model using a chat-vector approach, yielding minimal changes and suggesting future experimentation with scaling factors.
  - Downloads: 1,814
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - This repository provides GGUF format conversions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama models, including a fast-instruct version optimized for speed and reduced token cost.
  - Downloads: 1,700
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring extended sequence length support (8192 tokens), a larger 100K vocabulary, and FlashAttention for improved performance and efficiency.
  - Downloads: 1,686
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B is a high-performing 7B Japanese language model based on Mistral-7B, achieving top scores on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,675
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Meta‚Äôs Llama 3, enhanced with Japanese language data and available in 8B and 70B Instruct/Chat versions released July 1, 2024.
  - Downloads: 1,637
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model, fine-tuned for instruction following using datasets like Dolly-15k, also available in 7B and optimized versions.
  - Downloads: 1,618
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is a 13B LLaMA-based language model pre-trained on English and Japanese datasets, released by Preferred Networks under an Apache 2.0 license and usable via the `transformers` pipeline.
  - Downloads: 1,610
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities for instruction-following tasks.
  - Downloads: 1,584
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture with GEGLU activation and no pre-training dropout‚Äîoptimized for fine-tuning with potential improvements over the original T5.
  - Downloads: 1,576
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 is a Japanese language model based on Qwen2.5-7B-Instruct, enhanced with instruction-following via distillation from a larger model and ChatVector differences, without post-training.
  - Downloads: 1,569
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to select the most relevant response from a set of candidates given a query.
  - Downloads: 1,554
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - LINE Corporation‚Äôs repository hosts a 3.6B parameter Japanese language model for text generation, utilizing the Transformers library and offering detailed technical documentation.
  - Downloads: 1,533
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - This repository hosts ABEJA's large Japanese GPT-2 model for text generation, requiring sentencepiece for installation and use with the transformers pipeline.
  - Downloads: 1,515
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B is a continually pre-trained and instruction-tuned 8B parameter language model based on Meta-Llama-3-8B, optimized for improved performance on both Japanese and English tasks.
  - Downloads: 1,502
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - This repository provides a RoBERTa-small model pre-trained on Japanese Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit-words.
  - Downloads: 1,472
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository provides a Japanese Sentence BERT base model, pretrained on colorfulscoop/bert-base-ja v1.0 and fine-tuned using the Japanese SNLI dataset for sentence similarity tasks.
  - Downloads: 1,454
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - This repository hosts ABEJA's 2.7B-parameter Japanese GPT-NeoX model, compatible with transformers v4.23+, for text generation tasks.
  - Downloads: 1,422
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct versions) developed by NII's R&D center, formatted for Hugging Face Transformers and requiring PyTorch >= 2.3.0.
  - Downloads: 1,418
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, offering easy usage via Hugging Face Transformers.
  - Downloads: 1,408
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, enhanced with additional pre-training for improved Japanese proficiency and designed for instruction-following tasks.
  - Downloads: 1,407
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow is a series of 8B and 70B large language models continually pre-trained on a massive Japanese web corpus, enhancing Japanese language capabilities while maintaining English proficiency based on Meta‚Äôs Llama 3.1.
  - Downloads: 1,399
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - LINE Corporation‚Äôs repository hosts a 3.6 billion parameter, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 1,365
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x1.8b-instruct3 Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp.
  - Downloads: 1,364
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - This repository provides a GGUF-formatted conversion of the aixsatoshi Llama-3-8b-Cosmopedia-japanese model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and offering links to other related Japanese language models.
  - Downloads: 1,330
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Stability AI's Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, decoder-only Japanese language model fine-tuned for instruction following, based on the Gamma 7B base model, and requiring Transformers 4.34.0+.
  - Downloads: 1,329
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B Japanese language model, trained with the imatrix dataset and compatible with llama.cpp.
  - Downloads: 1,302
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - This repository provides a pretrained Japanese RoBERTa base model for masked language modeling, trained on Japanese Wikipedia and CC-100 data.
  - Downloads: 1,287
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,252
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - Youri-7b is a continually pre-trained 7-billion parameter language model based on Llama 2-7b, enhanced with 40B Japanese/English tokens for improved Japanese language performance.
  - Downloads: 1,247
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - This repository details a 717M parameter Japanese character-level GPT-2 language model pre-trained on large-scale Japanese text datasets and usable for text generation via the Hugging Face `transformers` pipeline.
  - Downloads: 1,219
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of the Mistral-7B-Instruct-v0.3 model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and demonstrating usage with llama.cpp for Japanese language tasks.
  - Downloads: 1,213
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M is a computationally efficient Japanese BERT model trained on 4.39T tokens with RoPE attention, designed for long sequences and featuring a 102,400 vocabulary.
  - Downloads: 1,207
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - This repository provides GGUF-formatted versions of the Vecteus-v1 Japanese language model, trained with TFMC/imatrix data and compatible with llama.cpp for inference.
  - Downloads: 1,206
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, a Japanese language model based on Llama 2, optimized for speed and reduced token cost.
  - Downloads: 1,197
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - LINE Corporation‚Äôs repository hosts a 1.7B parameter Japanese language model, fine-tuned for instruction following and improved conversational ability.
  - Downloads: 1,172
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - CyberAgent‚Äôs Mistral-Nemo-Japanese-Instruct-2408 is a continually pre-trained Japanese language model built upon Mistral-Nemo-Instruct-2407, usable with the `transformers` library.
  - Downloads: 1,106
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - LINE Corporation‚Äôs japanese-large-lm-1.7b is a 1.7 billion parameter Japanese language model for text generation, readily usable with Hugging Face Transformers and detailed in a tech blog.
  - Downloads: 1,102
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - This repository provides a GGUF-formatted version of Stability AI's japanese-stablelm-2-instruct-1_6b, trained with imatrix data, requiring agreement to the terms of use and membership for commercial applications.
  - Downloads: 1,090
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring expanded vocabulary, longer sequence support (8192 tokens), and FlashAttention for improved performance and efficiency.
  - Downloads: 1,087
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7B-parameter language model pre-trained on Japanese and English data to excel in Japanese language modeling and downstream tasks, with an instruction-tuned version also available.
  - Downloads: 1,080
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x13b-instruct3 Japanese language model, trained on the TFMC/imatrix-dataset, and usable with llama.cpp.
  - Downloads: 1,078
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - This repository provides a Japanese SimCSE model (pkshatech/simcse-ja-bert-base-clcmlp) for generating sentence embeddings from Japanese text, built on cl-tohoku/bert-base-japanese-v2 and trained with JSNLI, requiring fugashi and unidic-lite for tokenization.
  - Downloads: 1,067
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, enhanced with additional pre-training for improved Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 1,063
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - Rinna‚Äôs nekomata-14b is a continually pre-trained version of Qwen-14b, enhanced with 66B Japanese/English tokens to substantially improve Japanese language performance and leverage Qwen‚Äôs efficient, large vocabulary.
  - Downloads: 1,063
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 1,060
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B is a 7B-parameter decoder-only language model pretrained on Japanese data, building upon Mistral-7B-v0.1 to achieve strong performance in Japanese language modeling and tasks.
  - Downloads: 1,039
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository hosts rinna‚Äôs 3.6 billion parameter Japanese GPT-NeoX model, finetuned for instruction-following conversations using a new training data split.
  - Downloads: 1,031
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM is a Japanese-enhanced Llama 2 language model, pretrained on mixed Japanese/multilingual data and fine-tuned with continual learning via SteerLM to create a chat-optimized version.
  - Downloads: 1,023
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 7B models, including instruct and fast-tuned variants, and CodeLlama versions for improved performance and reduced token costs.
  - Downloads: 1,021
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is a 13 billion parameter Japanese language model, instruction-tuned by Stockmark Inc. using the Japanese Instruction data for LLM project.
  - Downloads: 1,007
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - Karakuri LM is a Japanese-enhanced language model built on Llama 2, with a fine-tuned chat version (Karakuri LM Chat) leveraging continual learning and the SteerLM technique for improved performance on Japanese and multilingual tasks.
  - Downloads: 1,006
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B is a 7B-parameter decoder-only language model pre-trained on 1.3T Japanese and English tokens, offering a chat variant and requiring transformers >= 4.34.1 for usage.
  - Downloads: 1,003
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter language model fine-tuned on Japanese data, built upon Llama-2-70B, for optimal performance in Japanese language tasks.
  - Downloads: 999
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the Moonlight-16B-A3B-Instruct language model, trained with Japanese data from TFMC/imatrix-dataset-for-japanese-llm, and intended for testing with llama.cpp.
  - Downloads: 999
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository hosts the llm-jp-3-13b-instruct3, a 13 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 999
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, accessible through Hugging Face Transformers.
  - Downloads: 996
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B is a 7B-parameter language model, fine-tuned for instruction-following using datasets like Dolly-15k, with larger 70B and faster tokenizer versions also available.
  - Downloads: 988
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-7B is a 7 billion parameter Llama-2-based language model fine-tuned on diverse Japanese data and featuring an expanded Japanese vocabulary for improved performance on Japanese language tasks.
  - Downloads: 984
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained for enhanced instruction-following capabilities and optimized for fast inference.
  - Downloads: 983
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter large language model pretrained on a 220B Japanese token corpus by Stockmark Inc., with an instruction-tuned version (stockmark-13b-instruct) and AWS support.
  - Downloads: 982
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository provides GGUF-formatted conversions of the open-calm-7b language model, compatible with llama.cpp, with potential future incompatibility pending updates to llama.cpp‚Äôs gptneox implementation.
  - Downloads: 978
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - Stockmark‚Äôs GPT-NeoX-Japanese-1.4b is a 1.4 billion parameter language model pre-trained on a 20 billion token Japanese corpus, offering strong performance for Japanese natural language processing tasks.
  - Downloads: 974
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - This repository provides a GGUF conversion of the AIBunCho japanese-novel-gpt-j-6b language model, intended for use with llama.cpp, though compatibility may change with future updates.
  - Downloads: 970
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Nekomata-7B is a continually pre-trained version of Qwen-7B on 30B Japanese/English tokens, enhancing Japanese language performance with an expanded vocabulary and long sequence support.
  - Downloads: 969
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B is a 7B-parameter language model fine-tuned on diverse Japanese data for improved performance on Japanese language tasks, with a corresponding instruction-following model also available.
  - Downloads: 955
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built on Llama 2 with additional pre-training to enhance its Japanese capabilities, optimized for fast inference.
  - Downloads: 955
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-7B is a 7 billion parameter language model fine-tuned for instruction following, utilizing an expanded Japanese vocabulary for improved text generation.
  - Downloads: 946
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository hosts a small Japanese GPT-NeoX model, trained with EleutherAI‚Äôs code and compatible with Hugging Face‚Äôs transformers library for causal language modeling.
  - Downloads: 946
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - Stockmark-100b is a 100 billion parameter language model pretrained by Stockmark Inc. on a 910 billion token Japanese/English corpus, with an instruction-tuned version available and support from GENIAC.
  - Downloads: 929
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - Rinna's Japanese HuBERT Large model is a 24-layer transformer trained on 19,000 hours of Japanese speech data from the Rea corpus, mirroring the original HuBERT architecture.
  - Downloads: 913
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - This repository provides GGUF-formatted conversions of the Honyaku-13b language model and related models by aixsatoshi, built using the TFMC/imatrix-dataset-for-japanese-llm dataset.
  - Downloads: 890
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - LLM-jp provides collaboratively developed, Japanese and English instruction-tuned large language models, including 13B parameter versions like llm-jp-13b-instruct-full, with varying training methods (DPO-LoRA, LoRA, and full fine-tuning) and versions (v1.0 & v1.1).
  - Downloads: 886
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - Cyberagent‚Äôs DeepSeek-R1-Distill-Qwen-32B-Japanese is a Japanese-finetuned 32B language model based on DeepSeek-R1-Distill-Qwen, optimized for usage with the `transformers` library.
  - Downloads: 877
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides a quantized, llama.cpp-compatible version of the Mistral-Nemo-Japanese-Instruct-2408 model for efficient Japanese language processing.
  - Downloads: 860
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Umievo-itr012-Gleipnir-7B is a merged 7B Japanese language model evolved from Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004.
  - Downloads: 856
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-13b-instruct-v0.1 model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and includes links to related models.
  - Downloads: 850
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, offering various quantization levels (including IQ1_S at 2.0GB) for use with tools like those detailed in TheBloke's READMEs.
  - Downloads: 847
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B model, created with support from a16z and hardware from Massed Compute.
  - Downloads: 815
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs fast, Japanese-language Llama 2 and CodeLlama 7b models, optimized for speed and reduced token cost.
  - Downloads: 812
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository provides quantized GGUF versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 7B model, created with support from a16z and Massed Compute for enhanced accessibility.
  - Downloads: 793
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - Luke-Japanese-Large-Lite is a lightweight, pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia embeddings.
  - Downloads: 780
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository provides a series of Japanese cross-encoder rerankers‚Äîincluding xsmall, small, base, and large models with varying layer and hidden size configurations‚Äîfor improved information retrieval, along with technical reports and usage examples.
  - Downloads: 777
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted version of the TokyoTech-LLM Llama-3.1-Swallow-8B-Instruct-v0.3 model, trained with the imatrix Japanese dataset and usable with llama.cpp.
  - Downloads: 773
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - This repository provides a GGUF-formatted version of the r1-1776-distill-llama-70b language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 754
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - This repository provides GGUF-formatted versions of the c4ai-command-r-plus large language model, trained on the imatrix dataset, requiring file concatenation for larger quantization levels and usage with llama.cpp.
  - Downloads: 737
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8 billion parameter large language model, pre-trained on 1.3T tokens and fine-tuned for dialogue using SFT and DPO, with available quantized versions including AWQ, GPTQ, and GGUF (though GGUF performance is potentially degraded).
  - Downloads: 737
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct, enhanced for instruction following using ChatVector and available via Hugging Face Transformers.
  - Downloads: 732
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B Japanese language model, with links to a potentially better 32B version.
  - Downloads: 685
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - This repository provides a GGUF-formatted version of Microsoft‚Äôs Phi-3-mini-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 667
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 is a 7B parameter language model fine-tuned for instruction following, built upon Japanese-StableLM-Base-Alpha-7B and designed for Japanese text generation.
  - Downloads: 662
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - This repository provides GGUF-formatted versions of the Ninja-v1 Japanese language model, built with the TFMC/imatrix-dataset and compatible with llama.cpp for local inference.
  - Downloads: 642
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Instruct Beta 70B language model, supported by a16z and utilizing hardware from Massed Compute.
  - Downloads: 638
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - This repository provides a 3B-parameter Japanese decoder-only language model, fine-tuned for instruction following, based on the StableLM-3B-4E1T base model.
  - Downloads: 635
- [tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2‚Äôs Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 623
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a 13B parameter, Apache 2.0 licensed, instruct-tuned language model built on PLaMo-13B and fine-tuned with Japanese datasets, offering an 8192 context length.
  - Downloads: 616
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - This repository provides a GGUF quantized version of the Aratako/calm3-22b-RP-v2 model, distributed under a CC-BY-NC-SA 4.0 license due to training data derived from OpenAI's GPT-4o-mini and Anthropic's Claude 3.5 Sonnet.
  - Downloads: 603
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - This repository provides a Japanese BERT large language model, pretrained on Jawiki-20200831 using Unidic-lite word-level tokenization and whole word masking for improved performance.
  - Downloads: 593
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA‚Äôs Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter large language model based on Meta Llama 3, quantized with AutoAWQ for improved performance.
  - Downloads: 590
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - This repository provides a T5-base model fine-tuned on the Livedoor News corpus for text summarization, as presented in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 585
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository hosts the llm-jp-3-980m-instruct3, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 569
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on the Japanese Aozora corpus for dependency parsing and head-detection, optimized for handling ambiguous words using `[MASK]` tokens.
  - Downloads: 555
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow is a series of 8B and 70B large language models continually pre-trained on a massive Japanese web corpus and multilingual data to enhance Japanese language capabilities while preserving English proficiency.
  - Downloads: 547
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a non-commercial, 13B parameter, Japanese language model fine-tuned for instruction following, built on PLaMo-13B and released under a CC-BY-NC-4.0 license.
  - Downloads: 543
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the qwq-bakeneko-32b language model, trained on the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 538
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese-language CodeLlama-7b-instruct model, alongside related Japanese Llama 2 models optimized for speed and token cost.
  - Downloads: 530
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - This repository hosts the llm-jp-3-440m-instruct3, a 440M parameter Japanese large language model developed by NII, utilizing the Hugging Face Transformers format and requiring PyTorch 2.3.0+ and Transformers 4.40+.
  - Downloads: 524
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - This repository provides a Japanese BERT base model fine-tuned on the WRIME dataset for predicting emotion intensities (eight emotions) in Japanese tweets, as detailed in a related research paper.
  - Downloads: 516
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - This repository provides a GGUF-formatted conversion of the Japanese Llama-3-8B-Instruct model by alfredplpl, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 504
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - This repository provides a Japanese-specific DeBERTa V3 model designed for inference without a morphological analyzer and with improved handling of word boundaries.
  - Downloads: 503
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - This repository hosts the llm-jp-3-150m-instruct3, a 150 million parameter Japanese language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 499
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - This repository hosts llm-jp-3-980m, a 980 million parameter Japanese large language model developed by NII, utilizing Hugging Face Transformers and requiring specific torch, transformers, and tokenizers versions.
  - Downloads: 485
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-13b-instruct3 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp (excluding custom chat templates like -cvn).
  - Downloads: 473
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation's japanese-large-lm-1.7b-instruction-sft language model, enabling its use with llama.cpp.
  - Downloads: 449
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa base model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 448
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-mini-instruct language model, built with data from TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 445
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted conversions of the tokyotech-llm Swallow-7b-instruct-v0.1 large language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and links to related model variations.
  - Downloads: 436
- [mmnga/cyberagent-open-calm-1b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-1b-gguf)
  - This repository provides a GGUF-formatted conversion of the cyberagent-open-calm-1b language model, intended for use with llama.cpp and potentially subject to future incompatibility with updates to llama.cpp‚Äôs gptneox implementation.
  - Downloads: 433
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository provides GGUF-converted versions of LINE Corporation's 1.7B Japanese language model, along with conversion scripts and usage instructions for llama.cpp.
  - Downloads: 418
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - This repository provides a GGUF-formatted version of the Qwen1.5-110B-Chat large language model, utilizing the TFMC/imatrix-dataset for Japanese and licensed under tongyi-qianwen.
  - Downloads: 412
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the TokyoTech-LLM Swallow-70b-instruct-v0.1 model, trained with the TFMC/imatrix-dataset-for-japanese-llm, alongside links to related model variants.
  - Downloads: 411
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - This repository hosts llm-jp-3-150m, a 150 million parameter Japanese large language model developed by NII‚Äôs R&D Center, utilizing the Hugging Face Transformers format and requiring specific torch, transformers, and tokenizers versions.
  - Downloads: 408
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight, transformer-based Japanese language foundation model optimized for resource-constrained environments, serving as a base for instruct models like RakutenAI-2.0-mini-instruct.
  - Downloads: 406
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - This repository provides a Japanese BERT model, pretrained on Japanese Wikipedia data, featuring a small architecture (12 layers, 256 dimensions, 4 attention heads) for finance-related natural language processing.
  - Downloads: 395
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides GGUF conversions of Japanese-Starling-ChatV-7B, a 7B Japanese chat model built on chatntq-ja-7b-v1.0 and enhanced with a chat vector derived from Starling-LM-7B-beta.
  - Downloads: 393
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - This repository provides GGUF quantized versions of Google's Gemma-2-2b-jpn-it model, enabling its use with tools like llama.cpp, LM Studio, and LLMFarm, built using a process adapted from npaka's LLM-jp-3.
  - Downloads: 390
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the haqishen Llama-3-8B-Japanese-Instruct model, trained with the imatrix dataset and optimized for use with llama.cpp.
  - Downloads: 385
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This repository provides a Japanese XLNet language model requiring Mecab and Sentencepiece, utilizing NFKD normalization which removes Japanese accent marks for text processing.
  - Downloads: 378
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted conversion of the TokyoTech-LLM Swallow-MS-7b-instruct-v0.1 model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and linking to related model variations.
  - Downloads: 374
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - This repository provides GGUF-formatted versions of Meta-Llama-3-8B-Instruct, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 369
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitions‚Äô repository hosts sarashina2.2-1b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English tasks alongside other models like Qwen and RakutenAI.
  - Downloads: 368
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1)
  - Gemma-2-Llama-Swallow improves Gemma 2's Japanese language capabilities via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while maintaining English proficiency.
  - Downloads: 367
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M is a computationally efficient Japanese BERT model‚Äîtrained on 4.39T tokens with RoPE and a 8192 sequence length‚Äîdesigned for handling long sequences with local and global attention.
  - Downloads: 366
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Base Beta 70B language model, optimized using hardware from Massed Compute.
  - Downloads: 365
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - This repository provides GGUF-formatted conversions of ryota39‚Äôs Phi-3-mini-4k-instruct-dpo model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and demonstrating usage with llama.cpp.
  - Downloads: 356
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - This repository provides a GGUF-formatted version of the karakuri-lm-32b-thinking-2501-exp large language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 353
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - Luke-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities from Wikipedia.
  - Downloads: 350
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository hosts llm-jp-3-3.7b-instruct3, a 3.7 billion parameter Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 349
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of the ELYZA-japanese-Llama-2-7b language model, including standard, fast, and CodeLlama variations, optimized for Japanese language processing.
  - Downloads: 346
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including small, base, and large models with varying layer and hidden size configurations, for improved information retrieval.
  - Downloads: 345
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides GGUF quantized versions of the rinna/qwen2.5-bakeneko-32b-instruct-v2 Japanese language model, compatible with llama.cpp applications, including merged reasoning models.
  - Downloads: 344
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct versions) developed by NII, formatted for Hugging Face Transformers and requiring specific library versions like PyTorch, Transformers, and Tokenizers.
  - Downloads: 321
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - This repository provides a pre-trained Japanese ALBERT model (‚Äúken11/albert-base-japanese-v1‚Äù) intended for fine-tuning on various tasks, requiring specific handling of the [MASK] token with Sentencepiece tokenizer.
  - Downloads: 308
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - This repository provides a Japanese extractive question answering model fine-tuned from rinna/japanese-roberta-base on the JaQuAD dataset, enabling question answering from Japanese Wikipedia articles.
  - Downloads: 282
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - This repository provides GGUF format conversions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama models, including a fast-tokenized 13b version for improved speed and efficiency.
  - Downloads: 276
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - This repository provides a GGUF format conversion of the open-calm-3b language model, intended for use with llama.cpp, but may become incompatible upon GPTNeox implementation within llama.cpp.
  - Downloads: 269
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - This repository provides a Japanese Llama-3-ELYZA-JP-8B model finetuned on a specific dataset, trained with Unsloth and TRL for 2x faster performance, under the Apache-2.0 license.
  - Downloads: 266
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - Luke-Japanese is a lightweight, pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 262
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 259
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - This repository provides a Japanese BERT base model further pretrained on financial text data, utilizing the architecture of BERT small and building upon Tohoku University's base Japanese model.
  - Downloads: 248
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides a GGUF quantized version of the Llama-3-8B-Japanese-Instruct model, optimized for use with LlamaEdge (v0.10.1+) and a specific llama-3-chat prompt template.
  - Downloads: 246
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLS-R-53 speech recognition model for Japanese, trained on publicly available datasets like Common Voice, and requiring 16kHz sampled input.
  - Downloads: 226
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides a GGUF-formatted version of the Qwen2.5-bakeneko-32b-instruct language model, trained with the imatrix Japanese dataset and compatible with llama.cpp.
  - Downloads: 226
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B is a 1 billion parameter language model pre-trained on 100 billion tokens, utilizing a Differential Transformer architecture with Differential Attention to improve focus and reduce noise, and optimized for efficient training with techniques like patch-level training and the Muon optimizer.
  - Downloads: 226
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100, suitable for masked language modeling and other NLP tasks.
  - Downloads: 224
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - JMedRoBERTa-base-sentencepiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, licensed under CC BY-NC-SA 4.0.
  - Downloads: 220
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - This repository provides a GGUF-formatted conversion of the stockmark-gpt-neox-japanese-1.4b language model, intended for use with llama.cpp, but potentially incompatible with future llama.cpp updates following native GPT-Neox implementation.
  - Downloads: 219
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - This repository hosts the llm-jp-3-8x13b-instruct3, a large language model developed by NII's Research Center for Large Language Models, utilizing the Hugging Face Transformers format and requiring PyTorch 2.3.0+.
  - Downloads: 219
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - This repository hosts llm-jp-3-8x1.8b-instruct3, a Japanese large language model from NII's LLM-jp-3 series, utilizing the Hugging Face Transformers format and requiring PyTorch >=2.3.0 and Transformers.
  - Downloads: 217
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - This repository provides GGUF-formatted versions of the RakutenAI-2.0-8x7B-instruct Japanese language model, trained with imatrix data and compatible with llama.cpp for inference.
  - Downloads: 216
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B is an instruction-tuned, Japanese language model built on rinna/qwen2.5-bakeneko-32b and optimized for superior reasoning and chat performance using Chat Vector and ORPO.
  - Downloads: 214
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - This repository provides the ELYZA-japanese-Llama-2-13b-fast-instruct model in GGUF format, optimized for use with LlamaEdge (v0.2.8+) and featuring a 5120 context size with a specific llama-2-chat prompt template.
  - Downloads: 211
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository provides a Japanese typo detection model‚Äîbased on RoBERTa‚Äîthat identifies and scores the probability of various character-level errors like deletions, insertions, substitutions, and incorrect kanji conversions within text.
  - Downloads: 210
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - This repository provides GGUF-converted versions of rinna's japanese-gpt-neox-3.6b model, intended for use with llama.cpp and potentially subject to compatibility changes with future updates.
  - Downloads: 208
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - This repository provides a large Japanese DeBERTa V2 model pre-trained on extensive Japanese text data for tasks like masked language modeling.
  - Downloads: 208
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - This repository provides a Japanese-optimized, quantized GGUF version of the Gemma 2B-it model, leveraging an iMatrix with Japanese terms and supporting accelerated performance via speculative decoding in llama.cpp.
  - Downloads: 204
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese/English generation with a 128k context window and memory, including NSFW versions.
  - Downloads: 201
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - This repository provides a Japanese BERT model, pretrained on Wikipedia data and fine-tuned for dependency parsing and question answering, utilizing masked input for disambiguation.
  - Downloads: 197
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - This repository provides a Japanese instruction-tuned language model, based on Meta-Llama-3.1-70B-Instruct, for use with the `transformers` library.
  - Downloads: 197
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-14B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 197
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - AMBER is a 132M parameter text embedding model by Retrieva, primarily for Japanese but also supporting English, built on the modernbert-ja-130m architecture.
  - Downloads: 195
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, offering various size/quality trade-offs with recommendations for usage and multi-part file handling.
  - Downloads: 187
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training to enhance its Japanese capabilities, designed for code generation and instruction following.
  - Downloads: 185
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - This repository provides a Japanese biomedical named entity recognition (NER) model, fine-tuned RoBERTa on MedTxt-CR, to identify and tag entities like diseases, drugs, and body parts using the IOB2 format.
  - Downloads: 182
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Llama 3.3, significantly enhancing Japanese language capabilities while preserving English proficiency using a 315B token corpus.
  - Downloads: 180
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - This repository provides a pre-trained RoBERTa-small model for Japanese, specifically trained on the Aozora text corpus with a character-level tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 174
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 172
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA's Japanese-specific CodeLlama-7b and Llama-2-7b models, including both base and instruction-tuned variants, optimized for speed and reduced token cost.
  - Downloads: 168
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and maintains reasonable word boundary respect during inference.
  - Downloads: 165
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-3.7b-instruct large language model, compatible with llama.cpp, LM Studio, and LLMFarm for various usage methods.
  - Downloads: 160
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - This repository provides a GGUF-formatted version of the EZO-phi-4-v2_900 language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and is usable with llama.cpp.
  - Downloads: 158
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - This repository provides a Japanese BERT large language model, pretrained with character and word-level tokenization (using Unidic) and whole word masking on CC-100 and JAwiki datasets.
  - Downloads: 155
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - This repository provides a Japanese speech recognition model, `japanese-wav2vec2-large-rs35kh`, fine-tuned on the ReazonSpeech v2.0 corpus using the wav2vec 2.0 architecture and accessible via the transformers library.
  - Downloads: 154
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-32B language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and designed for use with llama.cpp.
  - Downloads: 153
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository provides GGUF versions of Stability AI‚Äôs Japanese-StableLM-3B-4E1T-base language model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 152
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - This repository provides a fine-tuned Japanese BERT model (‚Äúbert-base-japanese-jsnli‚Äù) for zero-shot text classification, achieving 92.88% accuracy on the JSNLI dataset.
  - Downloads: 150
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„Å´chat vector„ÅßÂØæË©±ËÉΩÂäõ„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 150
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-1.5B language model, trained with imatrix data and usable with llama.cpp for tasks like recipe generation.
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - This repository provides GGUF quantized versions of rinna's Japanese/Italian Gemma 2B model, built using npaka's LLM-jp-3 conversion process, and intended for use with tools like llama.cpp and LM Studio.
  - Downloads: 147
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, alongside a potentially preferred 14B version.
  - Downloads: 145
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is a 47B parameter language model, fine-tuned for dialogue using SFT and DPO, with available AWQ, GPTQ, and GGUF (potentially lower performance) quantized versions requiring flash attention for inference.
  - Downloads: 144
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository provides GGUF versions of Stability AI's Japanese-StableLM-3B-4E1T-Instruct language model, noting a current 34-layer GPU offloading limit with Llama.cpp.
  - Downloads: 142
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1 is a Japanese language model fine-tuned from Google‚Äôs Gemma-3-4B-it, specializing in multi-turn, prompt-following conversational ability for AI VTubers with a focus on lightweight performance.
  - Downloads: 141
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 141
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository likely contains research and experimental data related to the Tsukuba region (including its university and campus) in Ibaraki, Japan, potentially involving student projects and scientific analysis.
  - Downloads: 139
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 138
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - This repository provides a GGUF-formatted conversion of the AXCXEPT-phi-4-open-R1-Distill-EZOv1 language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 135
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository hosts Japanese large language models‚Äîincluding 1.8b, 3.7b, 13b, and 172b parameter versions, both base and instruction-tuned‚Äîdeveloped by NII and partially supported by GENIAC.
  - Downloads: 134
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 129
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri is a series of pretrained Japanese text embedding models based on ModernBERT-Ja, available in varying sizes (30M-70M parameters) and evaluated using JMTEB benchmarks.
  - Downloads: 127
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed collaboratively using the Fugaku supercomputer, permitting commercial and non-commercial use, modification, and redistribution under the specified conditions.
  - Downloads: 126
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - This repository provides a base model card template and details a small, pretrained Japanese/English T5 text-to-text transformer model, requiring further information on its developer, type, language specifics, and license.
  - Downloads: 126
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - This repository provides quantized GGUF versions of the Llama-3-8B-Japanese-Instruct model, optimized for use with GaiaNet, including a Q2_K 2-bit quantized model with a 4096 context size.
  - Downloads: 125
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - This repository provides a 6B-parameter Japanese GPT-2 language model, ‚Äúwatashiha-gpt-6b‚Äù, fine-tuned on 6.93 million *okashi* (humorous response) examples and pre-trained on a 47.7 billion token Japanese corpus, utilizing AWS Neuron for optimized performance.
  - Downloads: 124
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - LLM-jp-3-7.2b-instruct offers large language models developed by NII‚Äôs R&D center, utilizing Hugging Face Transformers with specific library version requirements for usage and building upon related LLM-jp-3 pre-trained/fine-tuned models.
  - Downloads: 121
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - This repository provides a pre-trained Japanese BART base model, `ku-nlp/bart-base-japanese`, for conditional generation tasks, requiring Japanese text to be pre-segmented with Juman++.
  - Downloads: 120
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and recommending dropout re-enablement for fine-tuning.
  - Downloads: 118
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-small-japanese) for improved speech recognition, built on OpenAI's whisper-small and trained with Japanese datasets, requiring 16kHz sampled audio input.
  - Downloads: 115
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - This repository provides a RoBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 113
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 113
- [tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2's Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - This repository provides a 32B Qwen2.5-Bakeneko instruction-tuned model quantized to 8-bit using AutoGPTQ for reduced memory usage and faster inference.
  - Downloads: 113
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - This repository provides a Japanese BigBird base model pre-trained on large-scale Japanese text data for masked language modeling tasks.
  - Downloads: 110
- [tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1](https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1)
  - Gemma-2-Llama-Swallow enhances Gemma 2's Japanese language skills via continual pre-training on a 200 billion token corpus of Japanese web data, Wikipedia, and code, while preserving English capabilities.
  - Downloads: 110
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - This repository provides a Japanese question answering model fine-tuned on the JaQuAD dataset, achieving F1 scores of 77.35/78.92 and Exact Match scores of 61.01/63.38 on development/test sets, based on the BERT base Japanese model.
  - Downloads: 108
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Asagi-14B is a 14B-parameter Japanese Vision & Language Model (VLM) trained on a large, diverse dataset‚Äîprimarily synthesized using CALM3-22B-Chat and Phi3.5-vision-instruct‚Äîwithout restrictions on output usage.
  - Downloads: 108
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed collaboratively using the Fujaku supercomputer, permitting commercial and non-commercial use including modification, redistribution, and service implementation.
  - Downloads: 106
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Llama-8B language model, trained with the imatrix Japanese dataset and designed for use with llama.cpp.
  - Downloads: 104
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - This repository provides GGUF-quantized versions of the LLM-jp-3-1.8b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for conversion.
  - Downloads: 100
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - This repository provides a GGUF-formatted version of the WabiSabi-V1 Japanese language model, built using the TFMC/imatrix-dataset, and designed for use with llama.cpp.
  - Downloads: 99
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - This repository provides a Japanese BERT small model pretrained on Wikipedia and financial text data, utilizing a 12-layer, 256-dimensional architecture.
  - Downloads: 98
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the `transformers` library for implementation.
  - Downloads: 97
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - This repository provides a Waseda RoBERTa model finetuned for evaluating truthfulness in generated answers on the JTruthfulQA dataset.
  - Downloads: 96
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model pretrained on Japanese Wikipedia and CC-100, designed for masked language modeling tasks.
  - Downloads: 95
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - This repository provides a pre-trained Japanese character-level GPT-2 Medium model (310M parameters) for text generation, trained on Japanese Wikipedia, CC-100, and OSCAR data.
  - Downloads: 93
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - This repository provides a pretrained DeBERTa V2 base model for Japanese language tasks, including masked language modeling, with accompanying pretraining code available elsewhere.
  - Downloads: 92
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - This repository provides a question encoder based on the bert-base-japanese-v3 model, fine-tuned with llm-book/aio-retriever for document retrieval as detailed in chapter 9 of ‚ÄúIntroduction to Large Language Models‚Äù.
  - Downloads: 92
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This repository merges intfloat‚Äôs E5-Mistral-7B-Instruct and Stability AI‚Äôs Japanese-StableLM-Base-Gamma-7B models, providing instructions for usage and detailing the merging process to address class incompatibility.
  - Downloads: 91
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned Wav2Vec2-large-xlsr-53 model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets with a 16kHz sampling rate requirement.
  - Downloads: 89
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository provides HuBERT-base model weights trained on JTubeSpeech, an encoder-type model for speech recognition and embedding speech into latent variables, but *not* for speech generation.
  - Downloads: 89
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 88
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - This repository provides a RoBERTa-large Japanese character-level model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing with long-unit-word features.
  - Downloads: 87
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Asagi-8B is a large-scale Japanese Vision & Language Model trained on extensive, largely synthesized Japanese data, avoiding LLMs with restrictive output usage policies.
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository hosts llm-jp-3-13b-instruct2, a 13 billion parameter Japanese large language model developed by NII, requiring PyTorch and Transformers libraries for use.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the cyberagent/Mistral-Nemo-Japanese-Instruct-2408 model, compatible with llama.cpp and runnable on the TensorBlock client.
  - Downloads: 87
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - This repository provides a Japanese pre-trained MobileBERT model‚Äîa faster alternative to BERT‚Äîbuilt for use with the `transformers` library and leveraging the Tohoku University Japanese BERT tokenizer.
  - Downloads: 86
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - This repository provides the llm-jp-1.3b-v1.0-aya model, a 1.3 billion parameter Japanese language model fine-tuned on Cohere's aya dataset, with example usage for text generation via Hugging Face Transformers.
  - Downloads: 86
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B is a 14B parameter vision language model from the National Institute of Informatics, Japan, requiring Python 3.10.12 and specific library installations (including flash-attention) for usage.
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - This repository releases a 4-bit quantized version of the llm-jp-3-172b-instruct3 large language model, reducing GPU/memory usage while maintaining performance through BitsAndBytes (fp4) quantization and float16/float32 computation.
  - Downloads: 83
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - This repository provides a Japanese RoBERTa-base model pre-trained on Japanese Wikipedia and CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 82
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - Rinna/nekomata-7b-instruction-gguf provides a GGUF quantized version of the 7B instruction-tuned Nekomata model, optimized for lightweight inference with llama.cpp, with a recommendation for GGUF q4_K_M quantization.
  - Downloads: 79
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted versions of the Japanese-StableLM-base-gamma-7B-Mistral-7B-Instruct-v0.1 model, enabling easier use with tools supporting the GGUF format.
  - Downloads: 76
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted versions of the japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 model, enabling easier use with tools supporting the GGUF format.
  - Downloads: 76
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [espnet/kan-bayashi_jsut_transformer](https://huggingface.co/espnet/kan-bayashi_jsut_transformer)
  - This repository provides an ESPnet2 text-to-speech (TTS) model‚Äîjsut_transformer‚Äîtrained on the JSUT dataset and originally from Zenodo, with demo instructions pending and citing the ESPnet framework.
  - Downloads: 70
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model, with options tailored for different VRAM availability (8GB-16GB).
  - Downloads: 68
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT model‚Äîbased on bert-base-japanese-v2‚Äîpre-trained for Universal Part-Of-Speech tagging and dependency parsing using Japanese Wikipedia and the UniDic long-unit-word vocabulary.
  - Downloads: 66
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a pre-trained, Japanese-language Transformer encoder built with Megatron-LM, featuring PreNorm and addressing a recent bias initialization bug.
  - Downloads: 66
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - This repository provides a QLoRA-fine-tuned version of cyberagent/calm3-22b-chat, specifically for role-playing using the ChatML prompt format and available in GGUF format.
  - Downloads: 65
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - This repository provides Llama 3.1-70B-EZO-1.1-it, a Japanese language model fine-tuned from Meta's Llama 3.1 achieving top performance‚Äîsurpassing gpt-4o-mini‚Äîon ElyzaTasks-100, under the Llama 3 Community License.
  - Downloads: 65
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - This repository details a 90M parameter Japanese character-level GPT-2 Small language model pre-trained on large Japanese text corpora and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 64
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture featuring GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 63
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository provides a fine-tuned luke-japanese-base model for binary classification of MARC-ja data (positive/negative) achieving 0.9 accuracy.
  - Downloads: 63
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard is a high-performing, 7 billion parameter Japanese language model based on Llama-2-7b, exceeding ChatGPT-3.5 on the JGLUE benchmark through additional pre-training and fine-tuning on unique data.
  - Downloads: 62
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-base-japanese) for improved speech recognition, trained on Common Voice, JVS, and JSUT datasets with a 16kHz sampling rate requirement.
  - Downloads: 62
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - This repository provides a Japanese BERT model pre-trained for Universal Part-Of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia data.
  - Downloads: 62
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This repository hosts a ChatML model created by SLERP merging yamatazen/Himeyuri-Magnum-12B with shisa-ai/shisa-v2-mistral-nemo-12b, designated as HMS.
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - This repository provides a pre-trained GPT2-Japanese-lyric-medium model and code for generating Japanese song lyrics from a title and/or prompt text.
  - Downloads: 61
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - This repository provides a Japanese BERT model (bert-base-japanese-v3) fine-tuned on the JGLUE JCommonsenseQA dataset for multiple-choice question answering, as detailed in the book ‚ÄúÂÖ•ÈñÄ‚Äù (Introduction to Large Language Models).
  - Downloads: 61
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository hosts a base-sized Japanese BART model, pre-trained by Stockmark Inc. for sequence-to-sequence tasks, detailed in the linked article.
  - Downloads: 58
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - This repository provides a GGUF-formatted version of the Stockmark-2-100B-Instruct-beta language model, trained with the imatrix Japanese dataset and usable with llama.cpp.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - This repository provides a JaQuAD-fine-tuned RoBERTa base model for Japanese question answering, offering code and usage examples via the `transformers` library.
  - Downloads: 57
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a continually pre-trained 32B parameter language model based on Qwen2.5, enhanced for improved performance on Japanese language tasks using an 18B token dataset.
  - Downloads: 57
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted weights for ELYZA-japanese-Llama-2-13b-fast-instruct, a Japanese language model based on Llama 2, optimized for use with llama.cpp.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Asagi-4B is a large-scale Japanese Vision & Language Model trained on a diverse, primarily synthesized, dataset‚Äîavoiding outputs from restrictive LLMs‚Äîand built leveraging CALM3-22B-Chat and Phi3.5-vision-instruct.
  - Downloads: 56
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - This repository provides a GPT-2 small language model trained on a 540M token subset of the Japanese Wikipedia dataset (Aug 2021) for Japanese text generation.
  - Downloads: 55
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - This repository provides a fine-tuned Qwen3-8B model‚ÄîQwen3-8B-RP-v0.1‚Äîspecifically for role-playing, utilizing system prompts to define character settings and dialogue scenarios, with an example using Ollama.
  - Downloads: 54
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Sentence Text Similarity (JSTS) using the JGLUE dataset, achieving a Pearson correlation of 0.8971 for sentence similarity scoring (maximum 5).
  - Downloads: 53
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - This repository provides a merged language model created with `mergekit` using the TIES method, building upon `shisa-ai/shisa-v2-mistral-nemo-12b` and incorporating `natong19/Mistral-Nemo-Instruct-2407-abliterated`.
  - Downloads: 52
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - This repository offers weighted and static GGUF quantized versions of the japanese-llama-3-8b-instruct-v2 model, with various quant levels (including IQ1_S) detailed for size and usage.
  - Downloads: 52
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - Rinna/nekomata-14b-instruction-gguf provides a GGUF quantized version of the 14 billion parameter rinna/nekomata-14b-instruction model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization.
  - Downloads: 49
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm is a Japanese RoBERTa-large model pre-trained on Japanese Wikipedia and CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 49
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - This repository hosts an 8-layer reduced version of the oshizo/japanese-e5-mistral-7b_slerp model, trained on 800,000 Japanese sentences, with usage details available via intfloat/e5-mistral-7b-instruct.
  - Downloads: 49
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - This project provides code and a LoRA/GRPO-tuned Unsloth/Phi-4 model for converting standard Japanese text into the distinctive, emoji-filled "Ojisan" (middle-aged man) writing style common in online communication.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - Llama-3.3-SuperSwallow-70B-Instruct-v0.1 is a mergekit-created, 70B parameter language model demonstrated with Japanese-RP conversation examples, showcasing interaction with a Gemini-2.0-flash-exp model.
  - Downloads: 47
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - This repository provides a pre-trained Japanese BART large model, fine-tuned on Japanese Wikipedia, for conditional generation tasks using the `transformers` library.
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ](https://huggingface.co/Aratako/DeepSeek-R1-Distill-Qwen-32B-Japanese-AWQ)
  - This repository provides a 32B Japanese language model, built upon DeepSeek-R1-Distill-Qwen, and quantized using AWQ for improved efficiency, trained with TFMC/imatrix data.
  - Downloads: 45
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - This repository provides a Japanese novel-focused language model, GPT-J-6B, pre-trained on Japanese data and fine-tuned on novel text, runnable via Google Colab.
  - Downloads: 44
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - This repository details a 1.3B parameter Japanese GPT-2 model ("dolly-japanese-gpt-1b") instruction-tuned with RLHF on datasets like "databricks-dolly-15k-ja" and "oasst1-89k-ja" for interactive AI, requiring 7GB VRAM/RAM, though recent updates show decreased QA accuracy.
  - Downloads: 43
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - This repository provides a GGUF conversion of Japanese-WizardLM2-ChatV-7B, a 7B language model combining the Japanese capabilities of ChatNTQ with the performance of WizardLM-2.
  - Downloads: 43
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - DataPilot/Llama3.1-ArrowSE-v0.4 is a Japanese language model fine-tuned from Llama3.1-8B-instruct using Mergekit to enhance performance, designed as a helpful and honest Japanese assistant.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed via distributed parallel learning on the Fugaku supercomputer, permitting commercial and non-commercial use including modification, reproduction, redistribution, and service implementation.
  - Downloads: 41
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - This repository provides a Japanese natural language inference (NLI) model, built on BERT, that classifies sentence pair relationships as entailment, neutral, or contradiction using the SentenceTransformers Cross-Encoder.
  - Downloads: 41
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 is a Japanese language LLaMA-based model‚Äîtrained on primarily Wikipedia data with 76,000 steps and utilizing Flash Attention‚Äîdesigned to run on 24GB VRAM, exhibiting improved conceptual understanding over its V1 counterpart.
  - Downloads: 39
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository provides a Japanese-localized version of the Llama 3 8B model, commercially usable under the Llama 3 license, with instructions for use via demo, Colab, or local installation using transformers and accelerate.
  - Downloads: 39
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository provides GGUF-converted versions of Line Corporation's japanese-large-lm-3.6b-instruction-sft model, intended for use with llama.cpp but potentially incompatible with future updates.
  - Downloads: 39
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a Japanese language model based on a switched transformer with a Prefix-LM structure, utilizing a unique "Spout" vector for controllable text generation and fine-tuning.
  - Downloads: 38
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - REV-Mix is an anime and realistic style diffusion model for Stable Diffusion, inspired by DateMix and RDtMix, optimized with specific settings like DDIM/DPM++ SDE Karras samplers and recommending the use of embeddings.
  - Downloads: 38
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from Kyoto University‚Äôs BERT Japanese Pretrained model using the ner-wikipedia-dataset, requiring separate downloads for the tokenizer and installation of Juman++ & pyknp.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - This repository details a 4.11GB Japanese-language quantized version of Meta's Llama 2 7B model, optimized for speed with some performance trade-offs after pre-training and tuning.
  - Downloads: 37
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - ArrowNeo-AME-4x3B-v0.1 is a 4x3B Mixture-of-Experts (MoE) model, built upon sarashina-2.2-instruct-v0.1 and utilizing Mergekit-MoE, comprising a base model and three specialized experts‚Äîcoding, prompt-following, and multi-turn conversation‚Äîoptimized for a lightweight AI "AItuber" persona.
  - Downloads: 37
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - This repository provides a Japanese language model based on Mixtral 275.86M, pre-trained on Japanese datasets and usable with the Hugging Face Transformers library.
  - Downloads: 37
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - This repository provides GGUF conversions of Line Corporation‚Äôs Japanese large language model (3.6B parameters) for use with llama.cpp, acknowledging potential incompatibility with future llama.cpp updates.
  - Downloads: 36
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - Nekomata-7b-gguf provides a GGUF quantized version of the rinna/nekomata-7b model, optimized for lightweight inference with llama.cpp, with a recommended 4-bit quantization of q4_K_M.
  - Downloads: 36
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - This repository offers a GGUF quantized version of the 32B-parameter DeepSeek-R1-Distill-Qwen-Japanese large language model, licensed under MIT.
  - Downloads: 36
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - This repository provides a pretrained Japanese GPT-2 model, trained on Japanese Wikipedia and CC-100, for text generation and fine-tuning, requiring word segmentation with Juman++.
  - Downloads: 34
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100B-parameter Japanese-focused large language model pre-trained on 1.5T tokens and instruction-tuned with synthetic data generated by Qwen2.5.
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - This repository provides a pre-trained Japanese ELECTRA-small model, built on Japanese Wikipedia subword units with MeCab tokenization, for use with the `transformers` library.
  - Downloads: 31
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - TohokuNLP‚Äôs BERT-alpha 500M is a Japanese BERT model based on the Llama architecture, enabling long sequence input (up to 8,192 tokens) as an encoder-type language model.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a 370 million parameter Japanese chat language model built using the Mamba state-space architecture for efficient, linear-time sequence modeling.
  - Downloads: 31
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - This repository provides a t5-base model fine-tuned on the xlsum dataset for Japanese summarization, achieving specific Rouge scores on the evaluation set with a learning rate of 0.0001.
  - Downloads: 30
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - This repository provides a Japanese pre-trained ALBERT model (‚Äúken11/albert-base-japanese-v1-with-japanese-tokenizer‚Äù) optimized with BertJapaneseTokenizer for easier fine-tuning on various Japanese NLP tasks.
  - Downloads: 30
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This repository provides phi-4-open-R1-Distill-EZOv1, a Japanese-focused reasoning model distilled from Deepseek-R1, primarily generating responses in Japanese but adaptable with English integration.
  - Downloads: 30
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, achieving comparable or slightly improved accuracy, particularly in qualitative evaluations, and requiring SentencePiece for inference.
  - Downloads: 30
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA model (sudachitra-wordpiece, mC4) tokenized with SudachiTra, ready for use with the Hugging Face Transformers library.
  - Downloads: 30
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - This repository provides a GGUF-formatted version of the Matsuo Lab's Weblab-10B model, compatible with llama.cpp examples, utilizing a modified branch for faster development.
  - Downloads: 29
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 29
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository provides a Japanese GPT-2 distillation model trained on Wikipedia data, achieving a perplexity of around 40, using rinna/japanese-gpt2-medium as the teacher model and HuggingFace Transformers code.
  - Downloads: 28
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - This repository provides a Japanese language model, fine-tuned from studio-ousia/luke-japanese-large, for automatic defamation detection, classifying comments into categories like threat, insult, or reputational damage.
  - Downloads: 28
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - Sarashina 2.2-3b-RP-v0.1 is a 3B parameter model fine-tuned for role-playing, utilizing a system prompt to define character and scenario within a fantasy setting, and is available in GGUF format for use with tools like Ollama.
  - Downloads: 28
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - Fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding model, based on bert-base-japanese-v3 and trained on limited data for similarity/entailment and retrieval tasks like JSTS, JSNLI, and MMARCO.
  - Downloads: 27
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - This repository provides a 4-bit MLX quantized version of the DeepSeek-R1-Distill-Qwen-32B-Japanese language model, runnable with `mlx-lm` for efficient inference.
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - This repository hosts the llm-jp-3-980m-instruct2 large language model‚Äîpart of the LLM-jp-3 series developed by NII‚Äîcompatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - LINE Corporation‚Äôs repository hosts a 1.7B parameter, 8-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - This repository provides a Japanese-StableLM-Base-Alpha-7B model fine-tuned to emulate the speech patterns of Reimu Hakurei from *Touhou Project*, enabling conversational interactions via prompts.
  - Downloads: 25
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - This repository provides a Japanese Natural Language Inference (NLI) model, trained with SentenceTransformers on JSNLI data and based on bert-base-japanese-v3, outputting entailment scores for sentence pairs.
  - Downloads: 25
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This repository provides a Japanese BERT-base model finetuned for automatic irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 25
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This repository provides a Japanese+English Sentence-BERT model (‚Äúsonoisa/sentence-bert-base-ja-en-mean-tokens‚Äù) pre-trained on cl-tohoku/bert-base-japanese-whole-word-masking, achieving improved English STS benchmark scores with a slight reduction in Japanese accuracy, and requiring fugashi/ipadic for inference.
  - Downloads: 25
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a large 890GB corpus, requiring fine-tuning for specific tasks and acknowledging potential biases in generated text.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - This repository provides an uncensored (abliterated) version of vecteus v1, a high-performance Japanese large language model specializing in novel-style text generation and understanding, enabling moreËá™Áî±„Å™ (free) text output while disclaiming responsibility for generated content.
  - Downloads: 25
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository provides the 32B Japanese language model DeepSeek-R1-Distill-Qwen converted to the MLX format for use with the `mlx-lm` library.
  - Downloads: 25
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Asagi-2B is a large-scale Japanese Vision & Language Model trained on a diverse, primarily synthesized, dataset‚Äîavoiding outputs from restrictive LLMs‚Äîto enable robust Japanese VLM capabilities.
  - Downloads: 25
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository hosts a fine-tuned Japanese language model (based on sonoisa/sentence-luke-japanese-base-lite) for detecting aggression in social media comments, achieving a 71.3% F1-score for non-aggressive content, and presented at NLP2024.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - This repository details swallow-hermes-st-v1, a Japanese language model created by merging English language model vectors (chat & story) using an evolutionary strategy to generate bedtime stories, aiming for a more natural and less rigidly positive conversational style than models like GPT-4.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - DeBERTa-v2 pre-trained on Japanese Wikipedia and Aozora Bunko texts provides a foundation for fine-tuning various downstream Japanese NLP tasks.
  - Downloads: 23
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Instruct Beta 7B, offering an efficient and accurate large language model, supported by a16z and Massed Compute.
  - Downloads: 23
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM provides open-source, locally deployable Japanese-to-Chinese translation models‚Äîfine-tuned on general and ACGN-specific data‚Äîfor light novels and galgames, released under a non-commercial license.
  - Downloads: 23
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - RoBERTa-long-japanese is a Japanese language model pre-trained on 200M sentences, extending the base RoBERTa model with a 1282 max position embedding for longer inputs and requiring Juman++ pre-tokenization with SentencePiece.
  - Downloads: 23
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - This repository provides a Japanese T5 language model fine-tuned on a 100GB corpus of Japanese text‚Äîincluding Wikipedia, OSCAR, and CC-100‚Äîfor adapted language modeling (next token prediction).
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja is a Japanese language LayoutLM model pretrained for token classification tasks and finetuned from cl-tohoku/bert-base-japanese-v2, released under CC BY-SA 3.0.
  - Downloads: 23
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - This repository provides a 4-bit AutoAWQ quantized version of the phi-4 language model, calibrated with both Japanese and English data for enhanced multilingual performance.
  - Downloads: 23
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card Ëã±Êó•„ÄÅÊó•Ëã±ÁøªË®≥Áî®„É¢„Éá„É´C3TR-Adapter„ÅÆGPTQ4bitÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - This repository provides a pretrained ESPnet2 text-to-speech (TTS) model, jsut_vits_accent_with_pause, trained on the JSUT dataset and imported from Zenodo.
  - Downloads: 22
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository provides a Japanese language model, ‚Äúluke-japanese-wordpiece-base‚Äù, based on Japanese BERT and pre-trained on July 2023 Japanese Wikipedia data, enhanced to handle unknown entities and utilizing WordPiece tokenization.
  - Downloads: 22
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - This repository provides a Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, created via differential merging of intermediate models and evaluated on the ABEJA tech blog.
  - Downloads: 22
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 22
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - jp-ModernBert-base-preview is a 150M parameter Japanese language model, fine-tuned on Japanese data with 8192 context length and 300B training tokens, utilizing BertJapaneseTokenizer and optimized for FlashAttention.
  - Downloads: 22
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - This repository provides a Japanese ELECTRA model, pretrained on 200M sentences with SudachiTra-WordPiece tokenization, for use with the `transformers` library.
  - Downloads: 22
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - This repository provides a transformer-aligned Japanese-to-Hebrew translation model (jpn-heb) trained on OPUS data, utilizing SentencePiece preprocessing and including BLEU/chr-F benchmarks on the Tatoeba test set.
  - Downloads: 21
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - KoichiYasuoka's roberta-large-japanese-aozora is a pre-trained RoBERTa model for Japanese, trained on the Aozora corpus and tokenized with Japanese-LUW, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 21
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - This repository details a fine-tuned Japanese GPT-2 model for generating *ES* (job application essays) using over 20,000 entries from successful candidates, built upon rinna's Japanese pretrained models and accessible via a web application.
  - Downloads: 21
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon CodeLlama, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 21
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository provides AWQ-quantized versions of Stability AI's Japanese StableLM Base Beta 70B language model, optimized for efficiency using hardware from Massed Compute.
  - Downloads: 21
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - This repository provides japanese-gpt-1b-PII-masking, a 1B Japanese GPT model fine-tuned to mask personal information ‚Äì names, birthdays, phone numbers, addresses, and more ‚Äì within text.
  - Downloads: 21
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - This repository provides a BERT model, fine-tuned on Japanese novel data, for classifying text (titles & summaries) into literary genres, utilizing cl-tohoku/bert-base-japanese-char-v3 as its base.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - This repository offers a 1.7B parameter, 4-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation, with usage examples provided.
  - Downloads: 20
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - This repository provides Japanese-finetuned versions of Meta-Llama-3-8B-Instruct, trained on a 49k conversation dataset using LLaMA-Factory, and compatible with both transformers and the original llama3 codebase with an 8192 context length.
  - Downloads: 20
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - This repository provides a Japanese BERT base model and tokenizer trained on the June 2021 Japanese Wikipedia dataset for use in natural language processing tasks.
  - Downloads: 20
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Zenz-v1 is a 90M parameter Japanese GPT-2 language model finetuned for kana-to-kanji conversion, intended for use with the Zenzai neural conversion system and licensed under CC-BY-SA 4.0.
  - Downloads: 20
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - This repository hosts JP-ModernBERT-Large, a 396M parameter Japanese language model trained on ~100B tokens with an 8192 context length, built by Algomatic and utilizing the BertJapaneseTokenizer.
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - This repository provides GGUF quantized weights for Llama-3.3-Swallow-70B-Instruct-v0.4, fine-tuned with the imatrix Japanese language dataset.
  - Downloads: 20
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository provides a Japanese BERT model (cl-tohoku/bert-large-japanese-v2) fine-tuned on the JGLUE/JCommonsenseQA dataset for commonsense question answering tasks.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - This repository provides a ctranslate2-compatible dataset converted from AIBunCho's Japanese novel GPT-J-6B model, currently 8-bit quantized with unquantified accuracy measurements.
  - Downloads: 20
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - This model generates Japanese novels, fine-tuned via QLoRA on 216 highly-rated web novels, Aozora Bunko texts, and Wikipedia, and is steerable using genre, keywords, and prompt instructions‚Äîthough it's a prototype with unverified behavior.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - This repository provides a pre-trained RoBERTa-large model for Japanese, specifically trained on the Aozora corpus with a character-level tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 19
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer architecture with 64-dimensional hidden states and 1 attention head.
  - Downloads: 19
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Vecteus Ninja is a fine-tuned Mistral-7B-based LLM excelling in high-quality Japanese & English generation with strong long-context memory capabilities, developed with LocalAI hackathon support.
  - Downloads: 19
- [arvine111/japanese-grammar-classification](https://huggingface.co/arvine111/japanese-grammar-classification)
  - This repository provides a fine-tuned Japanese BERT model for multi-class classification of grammar points, trained on dictionary data and augmented with LLM-generated examples for use in language learning.
  - Downloads: 19
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - This repository provides a Japanese SPLADE model (aken12/splade-japanese) fine-tuned on the mMARCO dataset for sparse retrieval, built upon tohoku-nlp/bert-base-japanese-v2.
  - Downloads: 19
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - This repository provides a 130.78M parameter Llama 2 model trained on Japanese text, utilizing the `transformers` library and a specific training script from Lightning-AI's `lit-gpt`.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora Bunko texts for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - Sarashina2.2-3Bx8-moe is a Japanese Mixture of Experts language model built upon sbintuitions/sarashina2.2-3b-instruct-v0.1 using mergekit-moe, offering diverse and high-quality text generation through the integration of eight specialized models.
  - Downloads: 19
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI‚Äôs Japanese StableLM Base Beta 70B language model, offering various parameter permutations for optimized performance.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - This repository provides a 4-bit quantized, Japanese-fine-tuned version of Llama-2-Chat 70B, built upon Meta‚Äôs LLaMA license and the alpaca_ja dataset.
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Vecteus Ninja models are Mistral-7B-based LLMs fine-tuned for high-quality Japanese and English generation with an extended 128k context window and improved long-context memory.
  - Downloads: 18
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b via direct preference optimization, built on a base model trained with 42 billion Japanese tokens from Cultura-X.
  - Downloads: 18
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - This repository provides a pretrained DeBERTa V2 small model for Japanese masked language modeling, with associated pretraining code available elsewhere.
  - Downloads: 18
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - This repository fine-tunes a Japanese GPT-2 model specifically for generating application essays (ES) focused on the IT industry, building upon the work of rinna's japanese-pretrained-models.
  - Downloads: 18
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B parameter Japanese GPT2 model finetuned by jweb (based on rinna's work) in both PyTorch and Rust formats, requiring T5Tokenizer for use.
  - Downloads: 18
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - This repository provides a Japanese BERT large model pre-trained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved language understanding.
  - Downloads: 18
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and Retrieval-Augmented Generation (RAG) tasks with Japanese context, and quantized using both GPTQ and AutoAWQ for improved performance.
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for real-time Japanese speech recognition, trained on diverse datasets like Common Voice and CSS10, enabling hiragana-based transcription.
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz v2.5 is a finetuned GPT-2-based Japanese conditional language model specialized for Kana-to-Kanji conversion, offered in small, medium, and large sizes under a CC-BY-SA 4.0 license.
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a fine-tuned Mistral-7B-based LLM boasting a 128k context window, high-quality Japanese & English generation, and enhanced long-context memory capabilities, even for NSFW content.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is a 90M parameter Japanese language model based on the GPT-2 architecture, finetuned for high-performance kana-to-kanji conversion, and built upon ku-nlp/gpt2-small-japanese-char under a CC-BY-SA 4.0 license.
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-large model pretrained on Japanese text (ÈùíÁ©∫ÊñáÂ∫´) for dependency parsing and question answering, utilizing the UD_Japanese-GSDLUW dataset and addressing long-unit-word head detection.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - This repository provides a fine-tuned LLM for Whisper, specifically trained to accurately transcribe audio containing Dominion (board game) card terminology, including challenging words and phrases, as of December 19, 2023.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - This repository provides a 3.89GB AWQ-quantized version of the Japanese instruction-tuned ELYZA-japanese-Llama-2-7b model, based on Meta‚Äôs Llama 2, optimized for use with Colab A100 or RTX 3000 series GPUs.
  - Downloads: 17
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This repository details a 6.8 billion parameter Japanese language model, built on EleutherAI's Mesh Transformer JAX (like GPT-J-6B), utilizing T5Tokenizer and SentencePiece for Japanese tokenization and requiring SentencePiece normalization.
  - Downloads: 17
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This repository provides a Japanese BART large language model, converted from Kyoto University‚Äôs original, and compatible with the `BartJapaneseTokenizer` and `transformers` pipeline for sequence-to-sequence tasks.
  - Downloads: 17
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - DeBERTa-large-japanese-juman-ud-goeswith is a pretrained Japanese DeBERTa(V2) model for POS-tagging and dependency parsing, utilizing the goeswith subword approach and requiring fugashi.
  - Downloads: 17
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository provides a GPTQ quantized version of ELYZA-japanese-CodeLlama-7b-instruct, calibrated with a 1k sample from Japanese Wikipedia and ELYZA-tasks-100 data for improved performance.
  - Downloads: 17
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 17
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-instruct-gamma-7b using Slerp merging across the first 32 layers, based on Mistral-7B as the base model.
  - Downloads: 17
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B language model, offering various parameter options for optimized performance.
  - Downloads: 17
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This repository provides a JAX/Flax-based transformer language model (0.1B parameters) trained on Japanese data, built upon the Flax lm1b example, and includes benchmark scores and FlaxAutoModel support.
  - Downloads: 17
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 17
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - KoichiYasuoka/bert-base-japanese-char-extended is a pre-trained Japanese BERT model, extending bert-base-japanese-char-v2 with enhanced character embeddings for improved performance on downstream tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository provides a pre-trained medium-sized Japanese GPT-2 model with a BERT-like tokenizer (Unidic) built on PyTorch and Hugging Face Transformers.
  - Downloads: 17
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - This repository provides a fine-tuned DistilHuBERT speech recognition model for Japanese, trained on a combined corpus including JVS, Tsukuyomi-Chan, Amitaro's ITA, and a custom dataset, requiring adherence to the JVS corpus terms of use.
  - Downloads: 17
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - ModernBERT-large-japanese-aozora is a pre-trained Japanese language model based on ÈùíÁ©∫ÊñáÂ∫´ texts, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, mirroring the original ELECTRA small architecture with 12 layers and 256 hidden dimensions.
  - Downloads: 17
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on the Japanese Common Voice 8.0 dataset, achieving a Word Error Rate of 1.0132 with a learning rate of 7.5e-05 and batch size of 32.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora texts for Universal Part-of-Speech (UPOS) tagging and dependency parsing of character-level long-unit-words.
  - Downloads: 17
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-large model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword approach.
  - Downloads: 17
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA-adapted checkpoints in Hugging Face format.
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B is a 7 billion parameter Japanese language model fine-tuned for generating ACG-related fanfiction, built upon Japanese Stable LM Base Gamma 7B as an experimental, quality-focused project.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - This repository provides a Japanese RoBERTa-base model pretrained for universal dependency parsing and POS-tagging, utilizing the goeswith subword tokenizer and requiring the fugashi library.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model, trained with the Heron library, capable of image-based conversation and implemented using GitGPTNeoX.
  - Downloads: 16
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 70B language model, offering various parameter configurations.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - This repository provides a Japanese language-extended version of Mixtral-8x7B-Instruct-v0.1, fine-tuned by ABEJA and built upon the Metagton-LM framework.
  - Downloads: 16
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 16
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 16
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-7B language model, available via Hugging Face.
  - Downloads: 16
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - Rinna/nekomata-14b-gguf provides a GGUF quantized version of the rinna/nekomata-14b model, optimized for lightweight inference with llama.cpp, and recommends GGUF q4_K_M for 4-bit quantization due to potential stability issues with other methods.
  - Downloads: 16
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 16
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - This repository provides a fine-tuned Japanese GPT-2 model for generating *ES* (r√©sum√©s/cover letters) using a dataset of 140,000 examples, built upon rinna's japanese-pretrained-models.
  - Downloads: 16
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B is a pre-trained Japanese text generation model, similar to GPT2/GPT3, trained on a large Japanese corpus and usable via the `transformers` library.
  - Downloads: 16
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - KoichiYasuoka/bert-large-japanese-char-extended is a pre-trained Japanese BERT model, based on bert-large-japanese-char, with enhanced character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 16
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - This repository provides a Japanese ELECTRA-small model pre-trained on Japanese Wikipedia using Byte-Pair Encoding and mecab-ipadic-NEologd tokenization for improved performance.
  - Downloads: 16
- [yamatazen/NeonMaid-12B](https://huggingface.co/yamatazen/NeonMaid-12B)
  - NeonMaid-12B is a 12B parameter language model merged from ForgottenMaid-12B, Francois-PE-V2-Huali-12B, and Ohashi-NeMo-12B using the Model Stock merge method with Orihime-12B as the base.
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - DeBERTa-large-japanese-wikipedia is a pre-trained Japanese language model based on DeBERTa V2, trained on Wikipedia and Aozora Bunko texts for tasks like POS-tagging and dependency parsing.
  - Downloads: 16
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specializing in Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 16
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - This repository provides a Japanese BERT base model pre-trained on jawiki-20200831 using character-level tokenization and whole word masking, offering an alternative to `cl-tohoku/bert-base-japanese-char-v2` without requiring external tools like Fugashi or Unidic.
  - Downloads: 16
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - This repository provides a Japanese BERT-based model finetuned for cyberbullying detection using a unified dataset of online comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - DeBERTa-base-Japanese-juman-ud-goeswith is a pretrained DeBERTa(V2) model for Japanese POS-tagging and dependency parsing, utilizing the goeswith subword tokenizer and requiring fugashi.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese NLP model, pretrained on the Aozora corpus and fine-tuned for part-of-speech tagging and dependency parsing using the goeswith subword tokenizer.
  - Downloads: 16
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised Chinese-Japanese masked language model pretraining framework leveraging the Unihan database for improved cross-lingual understanding through a coarse-to-fine training approach.
  - Downloads: 16
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - MegagonLabs‚Äô t5-base-japanese-web-8k is a Japanese T5 model pre-trained on 8K vocabulary web text data (mC4, Wiki40b) with available training code.
  - Downloads: 16
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - This repository details hyperparameter optimization using Optuna for a Japanese sentiment analysis model (based on cl-tohoku/bert-base-japanese) trained on the multilingual-sentiments dataset, achieving optimal settings of cosine learning rate schedule (2.82e-05), gradient accumulation of 1, and weight decay of 0.00017.
  - Downloads: 16
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - This repository details Japanese sentiment analysis experiments using BERT, WRIME dataset, Adafactor optimization, and Optuna-tuned hyperparameters‚Äîincluding learning rate and batch size‚Äîwith weight decay regularization.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pre-trained on Japanese Wikipedia, designed for text generation or fine-tuning after word segmentation with Juman++.
  - Downloads: 15
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Beta 7B language model, offering various parameter configurations.
  - Downloads: 15
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - This repository provides a 7B language model, Youri, fine-tuned for Q&A and RAG tasks with context, quantized using AutoGPTQ/AutoAWQ, and aiming for performance exceeding GPT-3.5.
  - Downloads: 15
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero is a 13B Japanese language model fine-tuned on 15k samples from the Jaster dataset for instruction following.
  - Downloads: 15
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - This repository provides an early-stage, Japanese-enhanced version of the Mixtral-8x7B-Instruct-v0.1 language model, evaluated and released as part of ABEJA‚Äôs tech blog, built on the Metagton-LM framework.
  - Downloads: 15
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - This repository provides a Japanese-tuned version of Meta-Llama-3-8B-Instruct, finetuned on a 49k dataset using h2o-llmstudio with an 8k context length, and usable with the transformers library.
  - Downloads: 15
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0 or newer.
  - Downloads: 15
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - This repository provides a fine-tuned RoBERTa-base-Japanese model for zero-shot text classification on the JSNLI dataset, requiring Juman++ word segmentation for input.
  - Downloads: 15
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - DeBERTa-large-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora text corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - DeBERTa-base-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - DeBERTa-small-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 15
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - This repository provides a 1.5B parameter Japanese GPT2 model pretrained on Japanese Wikipedia and CC-100, requiring Juman++ word segmentation for text generation or fine-tuning.
  - Downloads: 15
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - This repository provides a fine-tuned Wav2Vec2 model (facebook/wav2vec2-large-xlsr-53) for Japanese accent recognition, achieving a 15.82% Word Error Rate on 16kHz audio input.
  - Downloads: 15
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 15
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-of-Speech tagging and dependency parsing on Japanese Wikipedia with long-unit-word support.
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - This repository hosts a fine-tuned Japanese language model, llm-jp-3-3.7b-instruct, optimized for long-form text generation using the Japanese-LongWriter-3k dataset, trained with a learning rate of 1e-05 and specific batch sizes.
  - Downloads: 15
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - This repository provides a XLM-RoBERTa-base model fine-tuned on a Japanese mMARCO dataset using ANCE, offering a checkpoint at 50k steps for optimal MRR@100 performance, with dataset preparation scripts linked.
  - Downloads: 15
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - Bert-base-sudachitra-v11 is a Japanese language model based on SudachiTra, differing from v1.1 by using surface word forms and modified vocabulary handling for improved tokenization.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Rinna's Llama 3 Youko 70B is a continually pre-trained version of Meta Llama 3 70B, enhanced for Japanese language performance with 5B tokens, and available in 8B quantized formats for instruction-tuning.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, featuring 12 layers, 64 hidden dimensions, and 1 attention head.
  - Downloads: 14
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - This repository provides the GGUF-formatted weights for the Japanese-LLaMA-2-13B language model, available on Hugging Face.
  - Downloads: 14
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and WordPiece for effective Japanese text processing.
  - Downloads: 14
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - jmedroberta-base-manbyo-wordpiece is a Japanese RoBERTa base model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 14
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This repository provides a Japanese sentence-T5 model pre-trained with `sonoisa/t5-base-japanese`, requiring `sentencepiece` for inference and achieving comparable accuracy to `sonoisa/sentence-bert-base-ja-mean-tokens`.
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository provides a fine-tuned modernbert-ja-310m Reward Model for evaluating the quality of Japanese novels, designed for use with reinforcement learning of text generation models, and predicts user ratings as a regression score‚Äîthough potentially biased by factors beyond text quality.
  - Downloads: 14
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - This repository provides a Japanese-enhanced, fine-tuned version of Mistral-Nemo (v0.2) for role-playing, trained on multiple datasets and optimized for Japanese output with a recommended temperature of 0.3.
  - Downloads: 14
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2-base-japanese-v2 is a Japanese language model based on GPT-2, trained on Japanese Wikipedia and web crawl data with a 60,000-token BPE tokenizer for text generation.
  - Downloads: 14
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - This repository shares a merged Stable Diffusion model (‚ÄúKokuwa‚Äù) built upon KiwiMix and other sources, specializing in uniquely stylized, slightly quirky character generation with some seed-dependent instability, inspired by the small kiwi-like "kokuwa" fruit.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 14
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - This repository provides a Japanese language model (mt5-base fine-tuned) for error detection and correction, trained on a 20,000-pair dataset with a "correction: " prefix for text-to-text tasks.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-large Japanese model pre-trained on the Aozora corpus for dependency parsing and question answering, built upon existing character-level and UD Japanese resources.
  - Downloads: 14
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - KoichiYasuoka/bert-large-japanese-luw-upos is a BERT-large model pre-trained on Japanese Wikipedia for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository provides a Japanese-finetuned version of the DeepSeek-R1-Distill-Qwen-14B model, optimized for Japanese language output and assistant-style responses.
  - Downloads: 14
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - This repository provides a fine-tuned T5-base-japanese model for Japanese title generation from input text, pre-trained on a 100GB corpus including Wikipedia and OSCAR data.
  - Downloads: 14
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This repository provides a fine-tuned open-calm-7b model, trained with H2O LLM Studio on a Japanese quiz dataset, and demonstrates its usage with the transformers pipeline for text generation.
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 3.6B parameter, instruction-tuned, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for efficient inference.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - This repository provides a Japanese BERT-base tokenizer trained with Nothing + Unigram, requiring a downloaded dictionary file for loading and use with Transformers.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - This repository provides a small, pre-trained RoBERTa model for Japanese natural language processing, specifically trained on the Aozora text corpus and using the Japanese-LUW tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - This repository provides a 1.3B NLLB-200 model fine-tuned for Japanese-to-English translation specifically tailored to the ‚ÄúAscendance of a Bookworm‚Äù web novel.
  - Downloads: 13
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech recognition, outputting continuous character sequences from 16kHz audio inputs.
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - This repository provides a Japanese RoBERTa-large model pretrained for universal dependency parsing and POS-tagging, utilizing the goeswith subword approach and requiring the fugashi library.
  - Downloads: 13
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - This repository provides an inference model cloned from rinna‚Äôs japanese-gpt-1b, fine-tuned using the databricks-dolly-15k-ja dataset, and details the environment (Windows 10, RTX4070, Python 3.9.6) used for its recreation and inference.
  - Downloads: 13
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model‚Äîpretrained on 200M Japanese sentences (mC4) and distributed as a Python package‚Äîfor advanced Japanese natural language processing with GiNZA v5.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer, utilizing Juman++ and WordPiece, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 13
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and WordPiece for efficient Japanese text processing within the Transformers library.
  - Downloads: 13
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - This repository provides a 1.3B parameter Japanese GPT model (‚Äúalpaca-guanaco-japanese-gpt-1b‚Äù) trained on alpaca_ja and GuanacoDataset, requiring 7GB VRAM or RAM for conversational AI applications.
  - Downloads: 13
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese BERT base model specifically for super short unit words (SSUW), requiring full-width character conversion and SSUW segmentation as pre-processing steps.
  - Downloads: 13
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - This repository provides a small GPT2 model fine-tuned for generating Japanese lyrics, with example code and a live demo available at lyric.fab.moe.
  - Downloads: 13
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using the Japanese-LUW-Tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following, based on Japanese Stable LM Base Gamma and requiring Transformers 4.34.0+.
  - Downloads: 13
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - This repository hosts a merged language model‚Äîbuilt with mergekit from Aratako/Ninja-v1-RP-WIP‚Äîoptimized for roleplaying through task vector addition and model stock merging, utilizing the Vicuna chat template and requiring an `eos_token` for multi-turn conversations.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - DeBERTa-v2 pre-trained on Japanese ÈùíÁ©∫ÊñáÂ∫´ texts with BertJapaneseTokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing using the `transformers` library.
  - Downloads: 13
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese RoBERTa base model specifically for super short unit word (SSUW) processing, requiring full-width conversion and SSUW segmentation as pre-processing steps.
  - Downloads: 13
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja is a fine-tuned version of OpenAI's whisper-small model, trained on the Common Voice 17.0 dataset with specific hyperparameters for improved Japanese speech recognition.
  - Downloads: 13
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA-adapted checkpoints, developed through a collaborative Japanese project.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa(V2) model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependency parsing and part-of-speech tagging, built upon existing Japanese UD models and utilizing the goeswith subword approach.
  - Downloads: 13
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - This repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal POS-tagging and dependency parsing with long-unit-word tagging.
  - Downloads: 13
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - This repository provides a DeBERTa(V2) model pre-trained on Japanese ÈùíÁ©∫ÊñáÂ∫´ texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit-words.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-based model pre-trained on Japanese text (Wikipedia & Aozora Bunko) for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 13
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - This repository provides a BERT-large model pretrained on Japanese Wikipedia data specifically for dependency parsing and question answering, building upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - Isekai-BERT-v1 is a Japanese BERT model fine-tuned on an unknown dataset with a reported loss of 1.9164, utilizing hyperparameters like a 0.0005 learning rate and batch sizes of 8.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-large Japanese model pre-trained on the Aozora corpus for universal dependency parsing and part-of-speech tagging, utilizing the goeswith subword tokenization.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-based question answering model, pretrained on the Japanese Aozora literature corpus with UD-based dependency parsing for improved long-unit word handling.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora Bunko for universal part-of-speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - This repository provides a Japanese ELECTRA base discriminator model, pretrained on Japanese Wikipedia data with a 12-layer, 768-dimension architecture.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-large Japanese language model pre-trained on Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 13
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts LINE Corporation's 1.7B parameter, 4-bit quantized, instruction-tuned Japanese language model for causal language modeling.
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Instruct Beta 70B language model, optimized with support from Massed Compute and funding from a16z.
  - Downloads: 13
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction-following tasks.
  - Downloads: 12
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - This repository provides a 417.12M parameter Llama 2 model trained on Japanese text, utilizing the `lit-gpt` script and `transformers` for inference with a Japanese tokenizer.
  - Downloads: 12
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned using the JaQuAD dataset, offering a non-transformer alternative for sequence modeling.
  - Downloads: 12
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained, bilingual Japanese-English language model adapting Llama-2-7b with 42 billion tokens from Cultura-X, achieving state-of-the-art results in perplexity and translation.
  - Downloads: 12
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - This model merges differences between Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1, building upon Swallow-MX-8x7b-NVE-v0.1 to improve Japanese naturalness and utilizes a 32k context size for top-performing local LLM capabilities as of March 2024.
  - Downloads: 12
- [atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-clpp-untied-ja)
  - This repository provides a Japanese-adapted version of Mistral-7B, fine-tuned with LAPT and CLP techniques, and accessible via PEFT and Transformers libraries.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 model, currently available in Q4_K_M format.
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - This repository reproduces a 7B-parameter Japanese language model, fine-tuned for instruction following using translated Databricks Dolly-15k and other datasets, built upon Japanese Stable LM Base Gamma 7B with the notus codebase.
  - Downloads: 12
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-base-gamma-7b using Slerp merging, focusing on layers 0-32 with specific parameter tuning for improved performance.
  - Downloads: 12
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - This repository provides a Japanese instruction-tuned Llama 2 model (if001/llama2_ja_small) fine-tuned on an instruction dataset, utilizing the Lit-GPT training script and compatible with Hugging Face Transformers.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer (Vaporetto + Unigram) requiring a downloaded dictionary file for processing.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - This repository provides a Japanese BERT-base tokenizer utilizing Juman++ and Unigram, requiring a downloaded dictionary file for implementation via `transformers` and `pyknp`.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-large Japanese language model‚Äîbased on the ÈùíÁ©∫ÊñáÂ∫´ corpus and BertJapaneseTokenizer‚Äîsuitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - This repository provides a Rust implementation of the Tohoku University BERT large Japanese model, enabling its use in Rust projects with instructions for cloning, project setup, and basic usage via the `rust-bert` crate.
  - Downloads: 12
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - Zenz v2.5 is a Japanese GPT-2-based conditional language model specialized for Kana-to-Kanji conversion, offered in small, medium, and large sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 12
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 12
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - This repository provides a Japanese ELECTRA model, pretrained and finetuned on disaster tweets, for information triage tasks, licensed under CC BY-SA 4.0.
  - Downloads: 12
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 12
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - This repository provides a Japanese language ELECTRA base model pretrained on Japanese Wikipedia data, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing, specifically handling long-unit-words.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is a full, instruction-following language model built upon Meta's Llama-3-8B-Instruct and developed/tested on ConoHa VPS with NVIDIA H100 GPUs.
  - Downloads: 12
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-based question answering model pretrained on Japanese Wikipedia and Aozora Bunko texts, specifically designed for dependency parsing and handling ambiguity with the [MASK] token.
  - Downloads: 12
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-of-Speech tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 12
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - This repository provides a fork of LINE's DistilBERT model, pre-trained on a large Japanese web corpus, with an updated tokenizer compatible with transformers v4.34+.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - This repository provides a Japanese language model exhibiting inconsistent persona traits (shifting gender/personality) but generally cheerful, intended for experimentation‚Äînot merging‚Äîwith specific settings for text generation (layer adjustments, sampling parameters) and demonstrating a self-introduction example.
  - Downloads: 12
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - This repository hosts a Japanese chat model, a variant of ebisuke/liz-nojaloli-ja, fine-tuned from abeja/gpt-neox-japanese-2.7b for personal use, and accepts input formatted with speaker attribution for conversational turns.
  - Downloads: 12
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This repository showcases a QLoRA-fine-tuned Llama-2-13b-chat-hf model, trained on a large Japanese/Chinese dataset of chat and non-chat samples, and offers a `test.py` script for evaluation with recommended generation parameters.
  - Downloads: 12
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - This repository provides a fine-tuned Japanese language model based on japanese-novel-gpt-j-6b, enabling conversation with the Touhou Project character Marisa Kirisame, utilizing 4-bit quantization for efficient inference.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - This repository provides a Japanese BERT-base model with a Nothing + BPE tokenizer, requiring a downloaded dictionary file for loading and use with Transformers.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - ModernBERT-base-japanese-char is a pre-trained Japanese language model, trained on Wikipedia and Aozora Bunko texts, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - This repository provides a Japanese natural language processing pipeline based on bert-base, incorporating transformer, parser, and NER components with a CC BY-SA 4.0 license.
  - Downloads: 11
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - Nagisa_bert is a BERT model and tokenizer for Japanese text, readily usable with Transformers, requiring Python 3.7+ on Linux or macOS and installable via pip.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa-based model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependency parsing and part-of-speech tagging, building upon existing Japanese UD resources.
  - Downloads: 11
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 is a Japanese language model finetuned from GPT-2, trained on the ATOMIC dataset using causal language modeling for text generation, and easily usable via the Hugging Face `transformers` pipeline.
  - Downloads: 11
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This repository details a Japanese GPT-1B-based model fine-tuned for extractive question answering and answer refinement within the gpt-index (v0.2.5) framework, utilizing specific prompt templates for context-based responses.
  - Downloads: 11
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM is a suite of Japanese decoder-only language models, specifically a LoRA fine-tune of open-calm-large using PyTorch, Transformers, and PEFT.
  - Downloads: 11
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - This repository provides a 6.3GB GPTQ-quantized version of Weblab-10B-instruction-sft, a 10 billion parameter Japanese-centric multilingual GPT-NeoX model, offering faster speed with a slight performance trade-off.
  - Downloads: 11
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base is a 3 billion parameter language model pre-trained on Japanese and English data, utilizing the RetNet architecture and intended for research into retention mechanisms.
  - Downloads: 11
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - This repository provides a 7B Japanese chat model‚Äîumievo-itr004-7b‚Äîcreated by linearly merging Aratako/Antler-7B-RP-v2, NTQAI/chatntq-ja-7b-v1.0, and TFMC/Japanese-Starling-ChatV-7B with a bfloat16 dtype and int8 parameter quantization.
  - Downloads: 11
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Instruct Gamma 7B, optimized with hardware from Massed Compute and supported by a16z grant.
  - Downloads: 11
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts LINE Corporation‚Äôs 3.6B-parameter Japanese language model, quantized to 8-bit with 1GB activation order, fine-tuned for instruction following.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - This repository provides a Japanese BERT-base model (‚ÄúVaporetto + BPE‚Äù) and instructions for loading its associated tokenizer using a downloaded dictionary file.
  - Downloads: 11
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small, fast, Japanese language LLaMA-based model pre-trained entirely on Japanese text, producing surprisingly coherent‚Äîthough not always helpful‚Äîresponses with a focus on generation configuration for optimal performance.
  - Downloads: 11
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - This repository provides a Japanese chat model, ‚Äúliz-nojaloli-ja‚Äù, fine-tuned from rinna/japanese-gpt-neox-3.6b, designed for personal study and featuring a specific conversational style, and requiring input formatting with speaker attribution.
  - Downloads: 11
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - Rinna‚Äôs japanese-gpt-1b is a 1.3B-parameter Japanese GPT model utilizing the T5Tokenizer for text generation and inference.
  - Downloads: 11
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - This repository distributes a spaCy v3-finetuned ELECTRA model (ja_ginza_electra) for Japanese, pretrained on mC4 and utilizing SudachiTra tokenization via the GiNZA v5 pipeline.
  - Downloads: 11
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 11
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - This repository provides a pretrained FastText word embedding model for Japanese, with a Google Colaboratory example and installation instructions for necessary dependencies like MeCab and PyTorch.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Unigram, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Byte-Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - This repository provides a Japanese BERT-base tokenizer utilizing MeCab and Unigram, requiring a downloaded dictionary file for implementation via `transformers` and `tokenizers`.
  - Downloads: 11
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and omitting dropout during pre-training for improved quality.
  - Downloads: 11
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository provides a medium-sized, right-to-left generative Japanese GPT-2 model utilizing a BERT-like tokenizer with dependencies on PyTorch, Fugashi/Unidic, and Hugging Face Transformers.
  - Downloads: 11
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository provides the DeepSeek-R1-Distill-Qwen-32B-Japanese language model converted to MLX format, enabling its use with the `mlx-lm` library for inference and generation.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Byte Pair Encoding (BPE), requiring a downloaded dictionary file for usage.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository provides a fine-tuned modernbert-ja-130m reward model for evaluating the quality of Japanese novels, intended for use with reinforcement learning in text generation, by predicting user ratings as a regression task.
  - Downloads: 11
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, featuring 12 layers, 256 hidden dimensions, and 4 attention heads.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, particularly suited for handling long-unit words and ambiguous terms using masked input.
  - Downloads: 11
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - This repository provides a BART-base model pretrained on Japanese text, compatible with the Fairseq model and usable with `BartJapaneseTokenizer` via the Hugging Face `transformers` library for sequence-to-sequence tasks.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-base model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing Japanese RoBERTa and UD_Japanese-GSDLUW resources.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - DeBERTa-large-japanese-wikipedia-ud-head is a pretrained Japanese DeBERTa(V2) model for dependency parsing and question answering, built upon Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts, and optimized for handling long-unit words with the use of [MASK] tokens.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - This repository provides a DeBERTa-based model pre-trained on Japanese ÈùíÁ©∫ÊñáÂ∫´ text for Universal Part-Of-Speech (UPOS) tagging and dependency parsing.
  - Downloads: 11
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - This repository provides a Japanese language model, fine-tuned from Google‚Äôs mt5-base, specifically for summarizing patent claims within the pharmaceutical domain.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - This repository provides a Japanese RoBERTa model, pretrained on the Aozora corpus and fine-tuned for dependency parsing and question answering, leveraging UD_Japanese-GSDLUW and employing [MASK] tokens for context clarity.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - This repository provides a Japanese language ELECTRA small discriminator model, pretrained on Japanese Wikipedia data, mirroring the original ELECTRA small architecture (12 layers, 256 dimensions, 4 attention heads).
  - Downloads: 11
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU is an experimental Japanese-Ainu machine translation model fine-tuned from T5, providing example translations and built upon the sonoisa/t5-base-japanese model.
  - Downloads: 11
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - This repository details Japanese sentiment analysis training using the cl-tohoku BERT base model on the wrime-sentiment dataset, optimized with AdamW and tuned via Optuna to achieve best performance with a cosine learning rate schedule, learning rate of 3.9e-5, batch size of 128, weight decay of 5.2e-5, 100 epochs, and early stopping.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - This repository provides a Japanese BERT-base model ("Vaporetto") with a WordPiece tokenizer, requiring a downloaded dictionary file for loading and use with transformers.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing the Nothing + WordPiece method, requiring a downloaded dictionary file for proper loading and use.
  - Downloads: 11
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - This repository provides a Japanese ELECTRA model, pretrained on 200M sentences and tokenized with SudachiTra/WordPiece, readily usable with Hugging Face Transformers.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset for conversational ability.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - This repository provides a pretrained VITS prosody model (kan-bayashi/jsut_full_band_vits_prosody) for ESPnet2 text-to-speech, trained on the JSUT dataset and imported from Zenodo.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter decoder-only Japanese language model fine-tuned for instruction following, based on the Japanese Stable LM Base Gamma 7B.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - Stability AI‚Äôs Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, instruction-following Japanese language model fine-tuned from its base Gamma 7B version, requiring Transformers 4.34.0+.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Syntactic Text Processing
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., readily usable with Hugging Face Transformers for text generation.
  - Downloads: 3,254
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 3,129
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - This repository provides the BracingEvoMix_v2 model, licensed under CreativeML Open RAIL-M with additional authorship by sazyou_roukaku, and explicitly disclaims responsibility for outputs, particularly those violating the license‚Äôs restriction A (prohibiting criminal or specialized uses like medical imaging).
  - Downloads: 2,746
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-NSFW-128k language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 2,589
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B is a suite of 7-billion parameter decoder-only language models pre-trained by CyberAgent on Japanese datasets, accessible via Hugging Face Transformers.
  - Downloads: 2,238
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7B, 13B, and 70B instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 2,190
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7B, 13B, and 70B instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 1,887
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,787
- [Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-30B-A3B-ERP-v0.1 language model, released under the MIT license.
  - Downloads: 1,549
- [Aratako/Qwen3-8B-ERP-v0.1-GGUF](https://huggingface.co/Aratako/Qwen3-8B-ERP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/Qwen3-8B-ERP-v0.1 large language model, released under the MIT license.
  - Downloads: 1,470
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B is a suite of 3 billion parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and readily usable with Hugging Face Transformers.
  - Downloads: 1,406
- [Kotajiro/fuduki_mix](https://huggingface.co/Kotajiro/fuduki_mix)
  - This model, licensed under CreativeML Open RAIL++-M, generates images with recommended settings (DPM++ 2M SDE karras, 30-40 steps, 1152x896 resolution) but prohibits generation of violent, sexual, or exploitative content, especially involving minors or real individuals without consent.
  - Downloads: 1,358
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 is a Japanese-enhanced language model continuously pre-trained from Mixtral-8x7B, utilizing the same tokenizer and achieving improved Japanese performance.
  - Downloads: 1,260
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, offering readily usable models and tokenizers via Hugging Face Transformers.
  - Downloads: 1,242
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,188
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - This repository provides GGUF-formatted conversions of the Ninja-v1-NSFW language model, trained on the TFMC/imatrix-dataset for Japanese LLM applications and runnable with llama.cpp.
  - Downloads: 1,185
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,155
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,090
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 1,028
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium is a suite of decoder-only language models pre-trained by CyberAgent on Japanese datasets, readily usable with Hugging Face Transformers for text generation.
  - Downloads: 1,008
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7b, 13b, and 70b instruct-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 981
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 950
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 947
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 863
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - This repository provides GGUF-formatted versions of rinna's llama-3-youko-8b and other models, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 800
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores older, experimental, and merged Stable Diffusion 1.5 models ‚Äì primarily a ‚Äúlametta-like‚Äù deformation material (v1745 + littleMonsters_anime) ‚Äì for playful, highly-deformed image generation, licensed under the lametta terms.
  - Downloads: 765
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - This repository provides a GGUF-formatted version of the Ninja-v1-128k language model, trained on the imatrix dataset and intended for use with llama.cpp for Japanese novel generation.
  - Downloads: 552
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-7.2b-instruct3 language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp (excluding custom chat templates/`-cvn`).
  - Downloads: 548
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - This repository provides statically quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model in GGUF format, offering various quantization levels (like Q2_K) for efficient use.
  - Downloads: 396
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - This repository provides a Stanza NLP model for Japanese, offering tools for linguistic analysis including syntax and entity recognition, automatically prepared via Hugging Face integration.
  - Downloads: 379
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [kawaimasa/wanabi_24b_v1_GGUF](https://huggingface.co/kawaimasa/wanabi_24b_v1_GGUF)
  - wanabi-24B is a continuously-trained, Japanese large language model fine-tuned for fiction writing support‚Äîincluding idea generation and text continuation‚Äîbased on Mistral-Small-24B and currently available in GGUF format with multiple quantization levels.
  - Downloads: 348
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - This repository provides a faster, CTranslate2-converted Japanese speech transcription model based on OpenAI‚Äôs Whisper-large-v2, optimized for use with the `faster-whisper` library.
  - Downloads: 263
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - This repository provides a GGUF formatted conversion of the SakanaAI EvoLLM-JP-A-v1-7B base model, utilizing models like Shisa, Gamma, and Mistral 7B, for use with llama.cpp.
  - Downloads: 220
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - This repository provides a GGUF format conversion of the matsuo-lab weblab-10b-instruction-sft model, optimized for use with llama.cpp, specifically the mmnga-dev branch.
  - Downloads: 214
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 198
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - Wanabi-24B is a preview of a 24B language model fine-tuned for Japanese novel writing assistance, built on Mistral-Small-24B and specializing in idea generation, plot creation, and contextual continuation, currently available in GGUF format as a limited, proof-of-concept alpha.
  - Downloads: 197
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - This repository provides GGUF-formatted versions of the shisa-7b-v1 language model, demonstrated with example prompts for Pok√©mon-related questions and Japanese-to-English translation using `llama.cpp`.
  - Downloads: 190
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - This repository presents evaluation results for several Japanese SPLADE models (including versions optimized for efficiency and document-level retrieval) on MIRACL and JQaRA datasets, reporting metrics like nDCG, Recall, and MRR.
  - Downloads: 186
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - This repository provides a GGUF-formatted version of the SakanaAI EvoLLM-JP-v1-7B base model, a merged model utilizing Shisa Gamma, WizardMath, and Abel, optimized for use with llama.cpp.
  - Downloads: 160
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 133
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - This repository provides a research-focused, 5000-step fine-tuned version of OpenAI‚Äôs whisper-large-v2 specifically for Japanese speech recognition using the CommonVoice v11 dataset, achieving a 0.7449 WER.
  - Downloads: 119
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha is a vision-language model generating Japanese descriptions from images and optional text inputs, built using Llama, BLIP, and requiring dependencies like SentencePiece and Einops.
  - Downloads: 103
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - This repository offers statically quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, including Q2_K and Q3_K_S quants for use with tools like llama.cpp, with details on file concatenation available in TheBloke's READMEs.
  - Downloads: 91
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - This repository provides GGUF-formatted conversions of the Tora-7B-v0.1 language model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt text dataset.
  - Downloads: 88
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakuten„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãRakutenAI-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 85
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews since April 2024.
  - Downloads: 84
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/Ninja-v1-RP-expressive model, requiring users to check the original model for licensing details.
  - Downloads: 75
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository releases a Japanese text-to-speech model, re-trained from Parler-TTS-Large-v1, offering lightweight, high-quality voice generation with a custom, incompatible tokenizer, currently in beta.
  - Downloads: 69
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - This repository provides quantized GGUF versions of the Aratako/Ninja-v1-RP language model, requiring users to refer to the original model for licensing and details.
  - Downloads: 66
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - This repository provides a GGUF-formatted, 7B parameter version of the Deepreneur Blue Lizard language model, compatible with llama.cpp and licensed under the Llama 2 license.
  - Downloads: 65
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on Mistral-7B-v0.1 and fine-tuned from chatntq-ja-7b-v1.0, with details and GGUF versions available.
  - Downloads: 64
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - This repository provides GGUF quantized versions of the Tora-7B-v0.2 language model, enhanced with iMatrix for improved Japanese text generation using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 54
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 54
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - This repository provides a GGUF conversion of ChatNTQ-JA-7b-v1.0, a Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b (based on Mistral 7B).
  - Downloads: 52
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - This repository offers statically quantized versions of the Japanese-LLaMA-3-8B-instruct-v2 model in GGUF format, including various quantizations (like Q2_K) for different size/performance trade-offs.
  - Downloads: 41
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 40
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - „ÅØ„Åò„ÇÅ„Å´ „Å™„Çì„ÅãÊó•Êú¨Ë™û„ÅåË©±„Åõ„ÇãÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™AI„Åß„Åô„ÄÇ
  - Downloads: 35
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps speech recognition model to the CTranslate2 format for faster inference with CTranslate2 and projects like faster-whisper.
  - Downloads: 33
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - This repository provides a GGUF-formatted version of the Tanuki-ZeRo base model for use with llama.cpp, demonstrated with a natural language processing prompt.
  - Downloads: 30
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository provides a Japanese novel quality assessment Reward Model, fine-tuned from sbintuitions/sarashina2.1-1b for applications like reinforcement learning of text generation, predicting user evaluations as a regression task while acknowledging potential biases beyond text quality.
  - Downloads: 30
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - Shisa-base-7b-v1 is a 7B language model built upon Mistral 7B, enhanced with 8B Japanese tokens from MADLAD-400 and extended to a 120k tokenizer for improved Japanese language efficiency (~2.3 characters/token).
  - Downloads: 25
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAE„ÅÆÂÜÖËáì„ÅØ„Å™„ÅÑ„ÅûÔºÅ„Å®Ë®Ä„Çè„Åõ„Å™„ÅÑ„ÅûÔºÅÔºÅÔºÅÔºÅ
  - Downloads: 21
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - This repository provides Mixtral-8x7B-v0.1-japanese, a Japanese language model built upon Mixtral-8x7B-v0.1 with expanded vocabulary and continued pre-training.
  - Downloads: 18
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - This repository implements Named Entity Recognition (NER) using the modernBERT model (sbintuitions/modernbert-ja-130m) with a custom tokenizer and defined label set for recognizing people, organizations, locations, facilities, products, and events in Japanese text.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B „Äå„Å©„ÅÜ„Åã„ÅäÊÖàÊÇ≤„Çí „ÇÇ„ÅÜ Áñ≤„ÇåÊûú„Å¶„Åæ„Åó„Åü„Äç ÁîüÊàê‰æã [Â§™Â≠ó‰ª•Èôç„ÅåAIÁîüÊàê] „Äå„Å©„ÅÜ„Åã„Äç ‚Äù„Åù„Çå‚Äù„ÅØÊááÈ°ò„Åó„Åü„ÄÇ
  - Downloads: 17
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-heuristics-untied-ja)
  - TigerBot-7B is a 7 billion parameter Japanese language model, based on LAPT and incorporating heuristics, accessible via PEFT and Transformers for causal language modeling.
  - Downloads: 13
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - This repository provides a model for generating article bodies from titles, as detailed in the linked Qiita article.
  - Downloads: 13
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - This repository provides a Japanese language model example utilizing in-context and instruction learning with a specific Unifine input/output format for AutoTokenizer and AutoModelForCausalLM.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - This repository provides a Japanese SentencePiece tokenizer with a 52,000 vocabulary, specifically trained for the AI Novelist SuperTrin and Damsel 20B language models to enhance creative writing.
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - Suzume-POC is a commercially usable, Japanese-adapted base model built from Google's Gemma-2B, optimized for small devices despite potential instruction-tuning difficulties, and demonstrated with a Transformers usage example.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on Mistral-7B-v0.1 and chatntq-ja-7b-v1.0, with available 6-bit quantized and GGUF versions.
  - Downloads: 12
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - This repository implements a reinforcement learning approach to generate more character-consistent conversational responses.
  - Downloads: 12
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - This repository provides a Japanese language Bloom model with 10,000 vocabulary size, a hidden size of `hidden_size`, 8 attention heads, and 12 layers.
  - Downloads: 12
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - Sarashina2.2-3Bx4-moe is a 12B parameter Mixture of Experts (MoE) model created by merging one base model with three ‚Äúsbintuitions/sarashina2.2-3b-instruct-v0.1‚Äù expert models to improve performance and generate high-quality responses.
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - This repository details a QLoRA fine-tuned language model (sarashina2.2-3b-instruct-v0.1) enhanced with the ability to call Python functions using techniques from Kendamarron/jimba-instruction-all and SousiOmine/Japanese-Pythonic-FunctionCall, formatted with specific tags for tool interaction.
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This model fine-tunes SakanaAI/TinySwallow-1.5B-Instruct using reinforcement learning to automatically generate numerous, concise (max 15 characters), bullet-point slides in the style of the ‚ÄúTakahashi Method‚Äù for impactful presentations.
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - DataPilot's Arrival-32B-Instruct-v0.4 is a Japanese-enhanced, fine-tuned language model built upon Abeja‚Äôs Qwen2.5-32B base (using negative chat vectors due to limited access) and incorporating DeepSeek-R1-Distill-Qwen-32B-Japanese, designed for instruction-following but not long-form generation.
  - Downloads: 12
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - This repository implements a quote reasoning model for inferring relationships and insights from famous quotes.
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - This model adapts Vecteus to support LLava, maintaining expressiveness while enabling LLava-compatible usage via a combination of Vecteus-v1 and EvoVLM-JP-v1-7B.
  - Downloads: 11
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository links to the AfterRealXL_beta2 model‚Äîlicensed under CreativeML Open RAIL++-M‚Äîand details its usage restrictions, disclaiming responsibility for generated content beyond the license's stipulations, while listing merged checkpoints like nightvisionXLPhotorealisticPortrait and cbluePencilXL.
  - Downloads: 11
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares experimental, lightly-adjusted merged language models (based on lametta_v1921 and others like vorpal_v1 & snowpearAnime_v10) originally from the spekulatius project, aiming to provide unique models distinct from those in lametta.
  - Downloads: 11
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - YACIS-ELECTRA-Small is a pretrained Japanese language model based on ELECTRA Small, trained on 5.6 billion words from the YACIS blog corpus using MeCab and WordPiece tokenization.
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is an English-to-Japanese translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via Hugging Face pipelines.
  - Downloads: 57,309
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT is a Japanese-to-English translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via a Hugging Face pipeline.
  - Downloads: 55,827
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - Shisa-gamma-7b-v1 is a Japanese language model fine-tuned from Stable LM Base Gamma 7B, showcasing strong performance based on JA MT-Bench results and built upon the Shisa 7B model family.
  - Downloads: 19,450
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T multilingual corpus, supporting Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and technical documentation.
  - Downloads: 5,533
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - Ultralytics YOLO11 is a state-of-the-art, fast and flexible model for object detection, tracking, segmentation, classification, and pose estimation, building on previous YOLO versions.
  - Downloads: 4,539
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a 2B parameter SLM delivering near-7B model translation quality for Japanese-English and English-Japanese tasks with a compact 2GB file size.
  - Downloads: 2,738
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - This repository provides a GGUF-formatted version of the Aya-23-8B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and demonstrates its usage with llama.cpp.
  - Downloads: 2,719
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - Rinna‚Äôs bilingual-gpt-neox-4b is a 3.8 billion parameter English-Japanese transformer model trained on 524B tokens from diverse corpora, based on EleutherAI‚Äôs GPT-NeoX.
  - Downloads: 2,196
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - This repository provides a 4-bit quantized version of Google‚Äôs Gemma-3-12B-it model, optimized with a Japanese-rich imatrix for use with the latest llama.cpp and supporting image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 1,587
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - This repository provides a Japanese-to-Korean translation model built on BERT (Japanese) and KoGPT-2, with a hosted demo and dependencies including PyTorch and Transformers.
  - Downloads: 1,394
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL is a 260M parameter translator model, finetuned from sbintuitions/modernbert-ja-130m and Dart v3, that converts Japanese and English into Danbooru tags, licensed under Apache-2.0.
  - Downloads: 1,204
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - This repository provides GGUF-formatted conversions of the lightblue Suzume-Llama-3-8B Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset, alongside links to other related lightblue/mmnga models.
  - Downloads: 881
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - This repository provides a 1.3B parameter NLLB model fine-tuned for Japanese to English translation of light novels, capable of processing up to 512 tokens.
  - Downloads: 847
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - This repository provides GGUF-formatted conversions of the multilingual Suzume-Llama-3-8B model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, alongside links to other related language models.
  - Downloads: 729
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FinguAI-Chat-v1 is a multilingual (English, Korean, Japanese) AI model designed to improve language proficiency and provide financial/legal knowledge for global finance and investment.
  - Downloads: 710
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin-inst-merge language model, utilizing imatrix data and licensed under Tongyi-Qianwen, intended for use with llama.cpp.
  - Downloads: 677
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 614
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT-BT-ja-en is an openly licensed Japanese-to-English translation model fine-tuned from ElanMT-base-ja-en using Wikipedia back-translation and exclusively open corpora, avoiding web-crawled or machine-translated data.
  - Downloads: 599
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - This repository provides a quantized version of Google‚Äôs Gemma 3B model‚Äîspecifically `gemma-3-27b-it-qat-q4_0`‚Äîusing a Japanese-rich imatrix for use with the latest llama.cpp and supports image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 596
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source, multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 568
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - This repository provides the GGUF version of ascktgcc/Mistral-nemo-ja-rp-v0.2, a Japanese language model‚Äîrefer to the original model for details.
  - Downloads: 486
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - This repository provides a GGUF-formatted conversion of the pfnet-nekomata-14b-pfn-qfin language model, trained on the TFMC/imatrix dataset and licensed under Tongyi-Qianwen, with usage instructions for llama.cpp.
  - Downloads: 332
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - This repository details llm-jp-clip-vit-large-patch14, a 467M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset, enabling zero-shot image classification with `open_clip_torch`.
  - Downloads: 277
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - This model is based on CreativeML Open RAIL-M, adding ‚Äúsazyou_roukaku‚Äù as an additional author, and its use is subject to the original license‚Äôs restrictions, particularly prohibiting criminal or specialized applications like medical imaging.
  - Downloads: 230
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - QuinceMix ‚ÄúDefacta‚Äù is a merged Stable Diffusion model excelling in backgrounds and effects, optimized for settings like DDIM/DPM++ SDE Karras samplers, 20-30 steps, and utilizing EasyNegative with recommended prompts focused on detailed scenes and lighting.
  - Downloads: 194
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository provides GGUF conversions of rinna's japanese-gpt-neox-3.6b-instruction-ppo model, intended for use with llama.cpp, but potentially incompatible after native GPT-Neox support is added.
  - Downloads: 179
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - This repository provides a VAE-integrated model, SakuraMixSeries, focusing on high background and character quality, licensed under a modified CreativeML OpenRAIL-M allowing commercial use and modification with specific conditions regarding crediting and resale.
  - Downloads: 146
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - This repository provides GGUF-formatted versions of the Karasu-Mixtral-8x22B-v0.1 language model, along with other related lightblue and mmnga models, utilizing data from TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 142
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 137
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT-BT-en-ja is an English-to-Japanese translation model built on openly licensed data and Wikipedia back-translation, derived from the ElanMT-base family and avoiding web-crawled or machine-translated corpora.
  - Downloads: 121
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 105
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - This repository provides a work-in-progress Japanese-to-English translation model built on tinyllama, optimized for long-context inputs (500-1000 tokens) and requiring deterministic inference settings.
  - Downloads: 90
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, built with improved Japanese language capabilities and efficiency over previous open-weight models.
  - Downloads: 83
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models from Shisa.AI, improved for superior Japanese language performance with increased training data and efficiency.
  - Downloads: 74
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP Ê¶ÇË¶Å Local-Novel-LLM-project/Ninja-v1-NSFW„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 71
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medLLM is a trilingual (English, Japanese, Chinese) biomedical large language model built on Llama-3-8B, trained via continued pre-training and supervised fine-tuning.
  - Downloads: 70
- [kawaimasa/wanabi_mini_12b_GGUF](https://huggingface.co/kawaimasa/wanabi_mini_12b_GGUF)
  - wanabi-mini-12B-GGUF is a fine-tuned, 12 billion parameter Japanese language model optimized for novel writing, offering similar functionality to wanabi-24B with lower VRAM requirements (8GB+) and improved performance through a high-quality, focused dataset, available in quantized GGUF format.
  - Downloads: 68
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models developed by Shisa.AI, emphasizing improved Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 61
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - This repository provides a Japanese-input-compatible refiner model for Stable Diffusion XL 1.0, achieved by fine-tuning the OpenCLIP-ViT/G & CLIP-ViT/L text encoders using English-Japanese parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 59
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for improved Japanese language performance.
  - Downloads: 57
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on 2.5T of data including Chinese, English, and Japanese, with demos, benchmarks, and technical documentation available.
  - Downloads: 48
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 39
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 39
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean, with associated demos, benchmarks, and documentation.
  - Downloads: 38
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - This repository details llm-jp-clip-vit-base-patch16, a 248M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset, enabling zero-shot image classification with `open_clip_torch`.
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a DPO-trained, Japanese-language conversational AI model based on Qwen/Qwen2.5-7B-Instruct, achieving improved performance on multilingual chat benchmarks.
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling is a 7B multilingual language model continually pretrained on Korean, English, Chinese, Japanese, and a 500-language corpus, based on Google's Gemma-7B.
  - Downloads: 25
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - This repository provides the GGUF format of the Japanese-Alpaca-2-13B language model, sourced from Hugging Face.
  - Downloads: 24
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - This repository details a Japanese large language model built upon Llama2-13b, enhanced with a 13,046-token Japanese vocabulary and fine-tuned on *okashi* (humorous responses) data, developed with AWS support and utilizing Trainium instances for pre-training on a 65 billion token corpus.
  - Downloads: 22
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source multilingual large language model series developed by OrionStarAI, trained on 2.5T of diverse text including Chinese, English, and Japanese, with available demos, benchmarks, and a technical report.
  - Downloads: 22
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - This repository provides a Japanese-input-capable version of the SDXL 1.0 base model, fine-tuned using a novel approach with OpenCLIP/CLIP-ViT text encoders and a Japanese tokenizer to align English and Japanese text embeddings via parallel data.
  - Downloads: 22
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - This project provides a Japanese-to-English machine translation model, fine-tuned for Weiss Schwarz (WS) trading card text, with a deployable Gradio app and local running instructions.
  - Downloads: 21
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 20
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 20
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba is a multilingual NLI/text classification model based on XLM-RoBERTa, served via TensorFlow, and trained on GLUE, CLUE, JGLUE, KLUE, and private datasets.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 is a 70B language model currently exhibiting performance issues‚Äîpotentially due to repetition penalties and high temperatures‚Äîresulting in lower benchmark scores than its predecessor, Swallow, though testing suggests parameter adjustments may improve results.
  - Downloads: 18
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B is a Japanese-English bilingual LLM built on LLaMA 2, trained with the LEIA technique to significantly improve cross-lingual transfer and Japanese question answering performance.
  - Downloads: 18
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository provides a quantized (.exl2) merge of Qwen-14B-vntl and Qwen1.5-14B-Chat, specifically designed for translating Japanese game scripts into natural-sounding Chinese.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - This repository hosts a 2x7B Mixture-of-Experts (MoE) Japanese language model built by merging and fine-tuning the elyza/ELYZA-japanese-Llama-2-7b-fast and elyza/ELYZA-japanese-Llama-2-7b-fast-instruct models, licensed under the Llama 2 Community License.
  - Downloads: 18
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved performance in both languages.
  - Downloads: 17
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on a 2.5T corpus with support for Chinese, English, Japanese, and Korean, and accessible via Hugging Face and ModelScope.
  - Downloads: 17
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - This repository presents Swallow-MoE-2x13B-v0.1, a merged Mixture-of-Experts model created from tokyotech-llm/Swallow-13b-instruct-hf and nitky/Superswallow-13b-v0.2, inheriting both Llama 2 and potentially AI2 ImpACT licenses.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository provides a Mixture-of-Experts (MoE) model‚ÄîELYZA-japanese-Llama-2-MoE-2x7B‚Äîcreated by merging and instruction-tuning the ELYZA-japanese-Llama-2-7B and ELYZA-japanese-Llama-2-7b-instruct models under the Llama 2 license.
  - Downloads: 16
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 15
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 15
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a bilingual (Japanese/English) chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for strong performance in both languages.
  - Downloads: 15
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T multilingual corpus, with resources and demos available on Hugging Face and ModelScope.
  - Downloads: 15
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model with an extended 8192 context length, built upon EleutherAI‚Äôs GPT-NeoX and fine-tuned with RoPE interpolation.
  - Downloads: 15
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - ByT5-small-ain-jpn-mt is a pretrained and fine-tuned machine translation model for Ainu-to-Japanese translation, leveraging web-crawled bilingual data.
  - Downloads: 15
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - This repository provides a mT5-based doc2query model for enhancing document retrieval by generating diverse, synonym-rich queries to expand indexing and improve lexical search relevance.
  - Downloads: 15
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - This repository provides a T5-based machine translation model fine-tuned on the friendly_JA corpus to simplify Japanese for English speakers by prioritizing Latin/English-derived *katakana* over Sino-Japanese vocabulary.
  - Downloads: 15
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - This repository provides base and instruction-tuned LLaMA 2 models (13B parameters) for Japanese language processing, including full and LoRA versions.
  - Downloads: 14
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is a Japanese-English bilingual LLM based on LLaMA 2, improved via the LEIA training technique to enhance cross-lingual transfer and performance on Japanese question-answering tasks.
  - Downloads: 14
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This repository provides a fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en, trained on the bsd_ja_en dataset.
  - Downloads: 14
- [shisa-ai/shisa-v2-mistral-small-24b](https://huggingface.co/shisa-ai/shisa-v2-mistral-small-24b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance through increased pre-training and tokenizer efficiency.
  - Downloads: 14
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for strong performance in both languages.
  - Downloads: 12
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B is an open-source, multilingual large language model series by OrionStarAI, trained on a 2.5T corpus encompassing Chinese, English, Japanese, and Korean, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 12
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 12
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - This repository provides a fine-tuned MPT-7B base model (Jumtra/mpt-7b-base) with 47% accuracy on a 100QA dataset, requiring `trust_remote_code=True` due to its custom MPT architecture and featuring training efficiency options like FlashAttention.
  - Downloads: 12
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - This repository provides a compiled version of the Watashiha-Llama-2-13B-Ogiri-sft model optimized for AWS Inf2 instances using Neuron, requiring an Inf2.xlarge instance with at least 256GB storage and the Deep Learning AMI Neuron PyTorch 1.13.
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - This repository provides full and LoRA models for Japanese-Alpaca-2-13B, an instruction-following model built upon the Japanese-LLaMA-2-13B base model.
  - Downloads: 12
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B provides both a base foundational language model and a LoRA-adapted version for Japanese language processing.
  - Downloads: 11
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a Marian-NMT translation model leveraging transformers and sentencepiece to translate from German, English, Spanish, French, Italian, Russian, and Ukrainian to Japanese, trained with ParaCrawl data.
  - Downloads: 11
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides a transformer-aligned Japanese-to-Malay machine translation model, trained on OPUS data with normalization and SentencePiece preprocessing, requiring a language ID token for input.
  - Downloads: 11
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - This repository provides a Mixture-of-Experts (MoE) model‚ÄîELYZA-japanese-Llama-2-MoE-2x13B‚Äîcreated by merging and instruction-tuning the elyza/ELYZA-japanese-Llama-2-13b and elyza/ELYZA-japanese-Llama-2-13b-instruct models, licensed under Llama 2 Community License.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool leveraging a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga‚Äîhandling vertical/horizontal text, furigana, varied fonts, and low-quality images.
  - Downloads: 141,356
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - This repository provides a GGUF version of DeepSeek-V3, specifically sliced and optimized for Japanese language tasks using frequently used Mixture of Experts layers, built with data from TFMC/imatrix-dataset-for-japanese-llm and requiring llama.cpp for usage.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - Rinna‚Äôs Japanese CLIP model, `rinna/japanese-clip-vit-b-16`, enables contrastive language-image pre-training for Japanese text and images, installable via pip and utilizing PyTorch.
  - Downloads: 25,032
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - This repository provides a Japanese CLIP model, trained on a billion web image-text pairs, for zero-shot image classification and multimodal retrieval tasks.
  - Downloads: 7,143
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet-tdt_ctc-0.6b-ja is a 0.6B parameter Japanese Automatic Speech Recognition (ASR) model developed by NVIDIA NeMo, utilizing a Hybrid FastConformer TDT-CTC architecture to transcribe speech with punctuation.
  - Downloads: 5,097
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - ReazonSpeech-Nemo-v2 is a 619M parameter, subword-based RNN-T ASR model utilizing a Longformer-enhanced Conformer architecture for efficient, long-form Japanese audio transcription.
  - Downloads: 3,430
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is a Japanese Automatic Speech Recognition (ASR) model building on kotoba-tech/kotoba-whisper-v2.0, enhanced with integrated punctuation and postprocessing pipelines developed in collaboration with Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,956
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - This repository provides a GGUF-formatted version of the Umievo-itr012-Gleipnir-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and runnable with llama.cpp.
  - Downloads: 2,518
- [turing-motors/Heron-NVILA-Lite-15B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-15B)
  - Heron-NVILA-Lite-15B is a Japanese and English vision-language model built on the NVILA-Lite architecture using Qwen2.5-14B-Instruct and a paligemma-siglip vision encoder, requiring specific dependencies like transformers==4.45.0.
  - Downloads: 1,593
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - This repository provides a Japanese CLIP model for encoding text and images, enabling tasks like similarity calculation, embedding, and multimodal search, with accompanying tutorials and a demo utilizing illustrations.
  - Downloads: 1,061
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - Karamaru is a conversational AI model by Sakana AI fine-tuned on a large Edo-period Japanese dataset‚Äîincluding both human-transcribed and AI-OCR-converted historical text‚Äîto generate responses in that style.
  - Downloads: 1,058
- [turing-motors/Heron-NVILA-Lite-2B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-2B)
  - Heron-NVILA-Lite-2B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing Qwen2.5-1.5B-Instruct and a paligemma-siglip-so400m-patch14-448 vision encoder, requiring transformers 4.45.0 or similar.
  - Downloads: 1,023
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-performing Japanese large vision language model, built on Sarashina2-7B and Qwen2-VL-7B, achieving state-of-the-art results on multiple benchmarks.
  - Downloads: 1,015
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - NABLA-VL is a Japanese vision-language model by NABLAS that processes images, multiple images, and videos to understand and generate text for multimodal tasks.
  - Downloads: 913
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a multilingual speech foundation model offering ASR, LID, SER, and AED capabilities, with models available on ModelScope & Hugging Face and online demos.
  - Downloads: 903
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted version of the ELYZA-Shortcut-1.0-Qwen-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and runnable with llama.cpp.
  - Downloads: 783
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - This repository provides a Japanese CLIP model‚Äîpretrained for mapping Japanese text and images to a shared embedding space‚Äîsuitable for zero-shot classification, retrieval, and feature extraction, released by Recruit Co., Ltd. under CC-BY-4.0.
  - Downloads: 700
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - This repository provides a GGUF formatted version of the DataPilot-ArrowPro-7B-KUJIRA language model, trained with TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 683
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is a Japanese ASR model building on kotoba-whisper-v1.0, enhanced with punctuation restoration via an integrated postprocessing pipeline developed collaboratively by Asahi Ushio and Kotoba Technologies.
  - Downloads: 518
- [turing-motors/Heron-NVILA-Lite-1B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-1B)
  - Heron-NVILA-Lite-1B is a Japanese and English vision language model built on the NVILA-Lite architecture, utilizing Qwen2.5-0.5B-Instruct and a paligemma-siglip vision encoder, requiring specific transformer and library versions for setup.
  - Downloads: 460
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - This repository provides a GGUF formatted version of the DataPilot-ArrowPro-7B-RobinHood Japanese language model, trained with TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 395
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - This repository provides a fine-tuned OpenAI Whisper-large-v3 model on the Common Voice 16.1 dataset, achieving a loss of 0.4057 and demonstrating some overfitting during 4000 training steps.
  - Downloads: 339
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B is a text-to-speech model built on sarashina2.2-0.5b-instruct-v0.1 and XCodec2, offering fine-grained voice control via prompt-based adjustments to pitch, gender, and noise, similar to Parler-TTS.
  - Downloads: 326
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a high-performing Japanese Vision Language Model built upon Sarashina2-13B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 316
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - This repository provides a GGUF format conversion of the cyberagent's Mistral-Nemo-Japanese-Instruct-2408 model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 312
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut is a base-sized model fine-tuned on a synthetic dataset of visual novel images for document visual question answering, demonstrated with a Colab notebook and sample Japanese visual novel recognition results.
  - Downloads: 280
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - This repository provides a GGUF-formatted version of the stockmark-100b language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and runnable with llama.cpp.
  - Downloads: 205
- [turing-motors/Heron-NVILA-Lite-33B](https://huggingface.co/turing-motors/Heron-NVILA-Lite-33B)
  - Heron-NVILA-Lite-33B is a Japanese & English vision language model built on the NVILA-Lite architecture, utilizing a siglip2 vision encoder and Qwen2.5-32B-Instruct LLM, with specific compatibility noted for Transformers versions 4.45.0, 4.46.0, and 4.49.0.
  - Downloads: 181
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - This repository provides a Japanese text-to-speech model, SpeechT5 fine-tuned on the JVS dataset of 100 speakers, utilizing 16-dimensional speaker embeddings for voice independence.
  - Downloads: 171
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - This repository provides a GGUF-formatted version of the QwQ-32B language model, utilizing data created with TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 156
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a multilingual text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio, with a demo available at Fish Audio.
  - Downloads: 151
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - This repository provides GGUF-formatted, K-quantized language models derived from Local-Novel-LLM, enhanced with iMatrix using the TFMC c4_en_ja_imatrix.txt text dataset.
  - Downloads: 138
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, mapping Japanese text and images into a shared embedding space under a CC BY-NC-SA 4.0 license.
  - Downloads: 128
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, enabling joint embedding of Japanese text and images under a CC BY-NC-SA 4.0 license.
  - Downloads: 126
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Hakuhodo Technologies‚Äô Japanese CLIP ViT-H/14 model enables zero-shot image classification and multimodal tasks by aligning Japanese text and images in a shared embedding space, licensed under CC BY-NC-SA 4.0.
  - Downloads: 126
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - This repository provides a GGUF version of the Ocuteus model, optimized for use with Kobold cpp, and recommends lowering image resolution due to token limits, with a suggested context size of 16384.
  - Downloads: 125
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input audio.
  - Downloads: 122
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - This repository provides optical character recognition (OCR) specifically designed for Japanese text, particularly within Japanese manga.
  - Downloads: 105
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT corpora, requiring 16kHz sampled audio input.
  - Downloads: 83
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - This repository provides GGUF-formatted conversions of the Japanese-Chat-Umievo-itr004-7b model, including K-quantized versions with iMatrix applied using the c4_en_ja_imatrix.txt text.
  - Downloads: 59
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - This repository provides GGUF quantized versions of the ArrowPro-7B-KUJIRA language model, including K-quantized models enhanced with iMatrix text based on TFMC's c4_en_ja_imatrix.txt.
  - Downloads: 53
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool utilizing a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga‚Äîhandling vertical/horizontal text, furigana, diverse fonts, and low-quality images‚Äîbut also functional as a general printed Japanese OCR.
  - Downloads: 49
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max is a 1.3B parameter Japanese vision-language model built on LLaVA with a ConvNeXt Large vision encoder, trained on a custom dataset, and licensed under Apache 2.0.
  - Downloads: 40
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - This repository provides a fine-tuned wav2vec2-base model for Japanese Automatic Speech Recognition (ASR), specifically predicting Hiragana, trained on the common_voice_11_0 dataset and achieving a WER of 1.0 after 1000 steps.
  - Downloads: 39
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository provides a CTranslate2-formatted version of the whisper-large-v2-jp speech recognition model, enabling faster transcription with CTranslate2 and projects like faster-whisper.
  - Downloads: 36
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - This repository presents Canary-TTS-150M, a Japanese Text-to-Speech model built on llm-jp-3-150m-instruct3 and XCodec2, enabling fine-grained voice control via prompt engineering ‚Äì designed as an experimental predecessor to the Canary-TTS 0.5B model.
  - Downloads: 36
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - This repository provides a fine-tuned XLSR-53 model for Japanese two-speaker speech diarization, specifically trained on phone call data using the CallHome dataset.
  - Downloads: 35
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model‚Äîtrained with the Heron library‚Äîcapable of image-based conversation, utilizing Llama Tokenizer and deployable on CUDA devices.
  - Downloads: 33
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository provides a CTranslate2-compatible version of the whisper-large-v2-mix-jp speech recognition model, enabling faster transcription via CTranslate2 and projects like faster-whisper.
  - Downloads: 31
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR is a fine-tuned Whisper-large-v3 model for accurate Japanese speech recognition, including non-speech sound detection and improved punctuation, requiring specific post-processing for optimal performance.
  - Downloads: 28
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - ReazonSpeech-ESPnet-Next provides cutting-edge Japanese Automatic Speech Recognition (ASR) models and datasets, actively incorporating community feedback for rapid research advancement.
  - Downloads: 27
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - This repository provides a Japanese language Stable Diffusion model for generating Pok√©mon images from text prompts, trained with the diffusers library and licensed under CreativeML OpenRAIL-M.
  - Downloads: 26
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on ~60 hours of combined Japanese voice datasets (Common Voice, JUST, JSSS, CSS10) achieving benchmark WER results.
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - This repository provides a fine-tuned Whisper Large V3 model for Japanese speech transcription into Katakana with pitch accent annotation, trained on Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 23
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech recognition, achieving 9.34% CER on Common Voice Japanese data with 16kHz audio input and continuous sentence output.
  - Downloads: 20
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - This repository provides a Japanese Text-to-Speech (TTS) model, Amitaro, finetuned from Plachtaa's VITS using free voice data, offering 600-epoch training and sample usage.
  - Downloads: 19
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - Rinna‚Äôs Japanese data2vec Audio Base model is a 12-layer transformer trained on 19,000 hours of Japanese audio, replicating the original data2vec architecture and training process.
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO is a Japanese-specific Stable Diffusion model fine-tuned for photo-realistic text-to-image generation using the ü§ó Diffusers library.
  - Downloads: 19
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR is a Japanese text recognition system built on a Vision Encoder Decoder framework, specializing in accurately extracting text from manga, including vertical writing, furigana, and varied font styles, while also functioning as a general Japanese OCR solution.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - This repository provides an ESPnet-based Japanese automatic speech recognition (ASR) model trained on the 15,000-hour ReazonSpeech corpus, requiring 16kHz sampled audio for input.
  - Downloads: 16
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model, trained with the Heron library, capable of image-based conversation and demonstrated through this demo repository.
  - Downloads: 15
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B Japanese text-to-speech model leveraging end-to-end transformers for fluent generation and one-shot voice cloning, building upon the metavoice framework.
  - Downloads: 14
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned, real-time Japanese Automatic Speech Recognition model based on OpenAI's Whisper-tiny, trained on the Common Voice dataset with a learning rate of 1e-05, achieving a WER of 225.233037.
  - Downloads: 14
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech model for a specific language, trained on datasets like Common Voice, requiring 16kHz sampled input.
  - Downloads: 14
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This repository provides a Style Bert VITS2 voice clone capable of text-to-speech generation in English, Japanese, and Chinese, featuring a young, neutral voice suitable for diverse applications like virtual YouTubers.
  - Downloads: 14
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - This repository provides a Japanese VITS-TTS voice model finetuned on Sakura Miko‚Äôs voice data, offering a hobby-use resource compliant with Cover Corporation‚Äôs secondary creation guidelines.
  - Downloads: 14
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - This repository provides a fine-tuned Japanese Hubert-base model for Automatic Speech Recognition (ASR) predicting only Hiragana, trained on the common_voice_11_0 dataset and inspired by wav2vec2 methodologies.
  - Downloads: 13
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a Japanese vision-language model built by fine-tuning llm-jp/llm-jp-1.3b-v1.0 with the LLaVA method on datasets like LLaVA-CC3M-Pretrain and LLaVA-Instruct-150K-JA for image-based conversation.
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool, built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga content‚Äîhandling vertical/horizontal text, furigana, varied fonts, and low-quality images.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for Japanese audio transcription directly into Hiragana, achieving a 0.2227 Character Error Rate on the Common Voice dataset after converting all text to Hiragana with pykakasi and tokenizing with fugashi.
  - Downloads: 12
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned wav2vec2-xls-r-300m model for Japanese speech recognition, trained on Mozilla Common Voice 8.0 with Kanji-to-Hiragana conversion and evaluated using Character Error Rate (CER).
  - Downloads: 12
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository provides a free, commercially-usable ASMR voice model‚Äîa ‚ÄúRikkaBotan‚Äù variant‚Äîspecializing in childish, gentle voices, with optional ‚Äúsweet,‚Äù ‚Äúenglish,‚Äù ‚Äúcool,‚Äù and ‚ÄúChinese‚Äù versions for varied emotional and linguistic styles.
  - Downloads: 11
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a 7B vision-language model built with the Heron library, enabling image-based conversations and utilizing Llama Tokenizer for conditional generation.
  - Downloads: 11
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - This repository provides a Japanese VL-T5 model, pretrained on a Japanese corpus for unifying vision-and-language tasks via text generation, with inference examples available.
  - Downloads: 11
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned Whisper tiny model for real-time Japanese Automatic Speech Recognition, trained on the Common Voice dataset and achieving a 30.16% Word Error Rate.
  - Downloads: 11
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA is a fine-tuned speech recognition model for Japanese, based on the SVJ dataset and Common Voice 11.0, achieving a CER of 17.7261 with a loss of 0.5596.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a 100GB corpus of Japanese text, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in the training data.
  - Downloads: 8,544
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - This repository hosts the v2 release of chilled_remix and reversemix models, licensed under CreativeML Open RAIL-M with added authorship by sazyou_roukaku, and disclaims responsibility for outputs beyond the license's restrictions.
  - Downloads: 2,455
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality voice synthesis with a custom, incompatible tokenizer.
  - Downloads: 2,351
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitions‚Äô sarashina2.2-0.5b-instruct-v0.1 is a Japanese autoregressive language model evaluated on Japanese and English tasks, achieving competitive results against other models like Qwen and RakutenAI.
  - Downloads: 2,280
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - This repository provides a set of image generation models (BraV6, XXMix_9, Soda Mix) licensed under CreativeML Open RAIL-M, prohibiting use for violent, sexually explicit, or exploitative content, especially involving minors, and restricting public dissemination of images resembling real individuals without consent.
  - Downloads: 1,983
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a 100GB corpus of Japanese text data, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 1,188
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-7b-Japanese-v0.1 large language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 1,123
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - This repository provides a GGUF-formatted version of Microsoft's Phi-3-medium-128k-instruct model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and compatible with llama.cpp for inference.
  - Downloads: 989
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides a GGUF-formatted version of the qwen2.5-bakeneko-32b-instruct-v2 language model, trained with the imatrix Japanese dataset, and usable with llama.cpp.
  - Downloads: 841
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - This repository provides a fine-tuned mt5-small model for Japanese text summarization, trained on BBC news articles utilizing the headline as a summary of the body text.
  - Downloads: 797
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/gemma-3-4b-it-RP-v0.1 model, inheriting the Gemma Terms of Use and Prohibited Use Policy.
  - Downloads: 782
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen3-30B-A3B large language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 678
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - This repository provides a quantized, GGUF version of Qwen/Qwen2.5-3B-Instruct, optimized with a Japanese-focused importance matrix (iMatrix) to enable accurate summarization of extremely long texts exceeding 32K tokens.
  - Downloads: 655
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the Cogito-v1-preview-Qwen-32B language model, trained with imatrix data and usable with llama.cpp for tasks like recipe generation.
  - Downloads: 571
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual provides faster, distilled Whisper models for Japanese & English speech recognition and translation, built upon OpenAI's large-v3 and developed by Asahi Ushio & Kotoba Technologies.
  - Downloads: 527
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - This repository details a Japanese question generation model fine-tuned on a cleaned and translated SQuAD 1.1 dataset, using a Japanese T5 model to generate questions from given answers and contexts.
  - Downloads: 516
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a Japanese/English instruction-tuning dataset (Sarashina2.2) built from fineweb-edu and fineweb-2, designed for higher accuracy than traditional imatrix datasets and optimized for Q5_K_M quantization with Ollama.
  - Downloads: 404
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) model, fine-tuned from rinna/japanese-hubert-large on reazonspeech and common_voice_11_0, specifically for predicting Hiragana.
  - Downloads: 259
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - This repository provides a GGUF-formatted conversion of the QwQ-32B-Preview language model, utilizing TFMC/imatrix-dataset-for-japanese-llm data and intended for use with llama.cpp.
  - Downloads: 244
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository provides GGUF quantizations of a VNTL LLaMA 3 8B QLoRA merge, featuring a new chat mode optimized for Japanese grammar and including translation prompt examples.
  - Downloads: 241
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF format conversions of the RakutenAI-2.0-mini-instruct model for use with tools like llama.cpp and text-generation-webui.
  - Downloads: 202
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality voice synthesis with a custom, incompatible tokenizer and currently in beta.
  - Downloads: 182
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 v2 is a finetuned, large-version GPT-2 model based on the ATOMIC dataset, enabling reproducible text generation via causal language modeling.
  - Downloads: 141
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„ÇíÊó•Êú¨Ë™ûinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 139
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the sarashina2.2-3b-instruct-v0.1 Japanese language model, trained with imatrix data and usable with llama.cpp.
  - Downloads: 133
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-32b-Japanese-v0.1 large language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 132
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 91
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - This repository provides a T5 model pretrained on a balanced 500GB English-Japanese corpus‚Äîincluding Wikipedia and OSCAR/CC-100 datasets‚Äîrequiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 90
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - This repository provides GGUF-quantized versions of the Aratako/c4ai-command-r-v01-japanese-instruct model, a Japanese instruction-following language model; refer to the original model for licensing details.
  - Downloads: 75
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8b„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂öÂ≠¶Áøí„É¢„Éá„É´„Å´ChatVector„ÇíÈÅ©Áî®„Åó„ÄÅ„Åï„Çâ„Å´QLora„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository provides GGUF quantized versions of the VNTL Gemma 2 27B model, featuring a new chat mode primarily focused on Japanese grammar and including translation examples.
  - Downloads: 59
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides statically quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model in GGUF format, offering various quantization levels for different size/quality trade-offs.
  - Downloads: 56
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - This repository provides a Japanese ByT5 model‚Äîa tokenizer-free Text-to-Text Transfer Transformer‚Äîpretrained on a 100GB corpus including Wikipedia, OSCAR, and CC-100, requiring fine-tuning for specific tasks and acknowledging potential biases in generated text.
  - Downloads: 52
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Kage-v0.1-2x7B is a merged 2x7B Japanese text generation model, enhanced with Ninja-v1 and utilizing the Vicuna prompt format for improved performance.
  - Downloads: 49
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - This repository provides a GGUF-formatted conversion of the Sarashina 2.1 1B SFT Japanese language model, trained with imatrix data, and intended for use with llama.cpp.
  - Downloads: 47
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 is a Japanese T5 model finetuned on the ATOMIC dataset for text-to-text generation, offering a pipeline for reproducible results with a set seed.
  - Downloads: 46
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - This repository details EZO2.5-gemma-3-12b-it, a Japanese language model achieving performance improvements on benchmarks like Japanese MT Bench and Elyza Tasks100 through a novel, low-cost training method‚ÄîEZO‚Äîcombining GRPO/PPO concepts for LLM self-improvement.
  - Downloads: 44
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - This repository provides a model for automatically generating titles from article text, as detailed in the linked Qiita article.
  - Downloads: 38
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - This repository provides a Hubert-based Japanese Automatic Speech Recognition (ASR) model fine-tuned on the uniTKU dataset, predicting only Hiragana and achieving low Word Error Rates (WER) as detailed in the training results.
  - Downloads: 36
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - This repository details a fine-tuned Japanese Automatic Speech Recognition (ASR) model, based on distil-whisper/distil-large-v2, specifically for transcribing visual novel audio.
  - Downloads: 32
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - This repository details a modified CreativeML OpenRAIL-M license allowing commercial use, image selling, and model merging/redistribution‚Äîeven with altered permissions‚Äîwithout requiring creator credit.
  - Downloads: 27
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest „Åì„ÅÆ„É¢„Éá„É´„ÅØÁîüÁâ©Â≠¶„ÉªÂåªÂ≠¶„Å´Á≤æÈÄö„Åó„ÅüOpenBioLLM-8B„Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™ûÂØæÂøú„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´Llama-3-youko-8b-instruct-chatvector„Å®„Éû„Éº„Ç∏„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a 6 billion parameter Japanese language model finetuned from EleutherAI‚Äôs GPT-J 6B on a dataset of Japanese web novels, utilizing RoPE embeddings and a 50,400 GPT-2/3 vocabulary.
  - Downloads: 22
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri is a Japanese, vision-language model fine-tuned with LLaVA on STAIR Captions and Japanese Visual Genome VQA data for image-based *oogiri* (pun/gag) generation, utilizing a CLIP-ViT-B-32 vision encoder and released under the LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 21
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - Saikyou Shield 30M is a lightweight (30M parameter) Japanese text classification model designed to classify *all* prompts as dangerous, including jailbreaks and prompt injections, for ultimate safety ‚Äì released as an April Fool's joke.
  - Downloads: 20
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - This repository provides a fully instruction-tuned 1.7 billion parameter Japanese language model built upon line-corporation/japanese-large-lm-1.7b.
  - Downloads: 19
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository hosts a 32B Japanese language model, built by merging pre-trained models with mergekit, optimized for code generation with parameters based on FuseO1-Preview.
  - Downloads: 17
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This repository provides a QLoRA-finetuned LLaMA2-7B model, trained on the Guanaco dataset with 49,000 chat samples, exhibiting improved Chinese and Japanese performance and testable via the provided script.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on machine-translated preference datasets (Ultrafeedback & hh-rlhf) based on the STF/Japanese Stable LM Instruct Gamma 7B model.
  - Downloads: 15
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - Gendec is a machine learning framework for gender detection from Japanese names (provided in ROMAJI) presented in a research paper accepted to ISDA'23, offering male/female predictions.
  - Downloads: 15
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 is a Japanese language model based on DeepSeek-V3, optimized for stability and performance by selectively reconstructing each layer with the 64 most frequently used experts from its Mixture of Experts (MoE) architecture.
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - This repository details a modified OpenRAIL-M license permitting commercial use, including selling generated images, commercial services, and model merges, even with differing permissions, without requiring creator credit.
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - This repository details the karakuri-midroze-CV model.
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on a machine-translated Ultrafeedback dataset, building upon the Japanese Stable LM Instruct Gamma 7B model.
  - Downloads: 12
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - This repository provides a finetuned GPT-2 XL model (COMET-GPT2) trained on the ATOMIC dataset using causal language modeling for text generation with reproducibility features.
  - Downloads: 12
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository hosts a fine-tuned version of MosaicML‚Äôs MPT-7B-instruct model, evaluated on the Jumtra/test_data_100QA dataset and requiring `trust_remote_code=True` due to its custom architecture and training efficiency features like FlashAttention.
  - Downloads: 12
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - This repository provides an alpha version of a Japanese-language AI assistant, fine-tuned from calm2-7b-chat, designed to continue writing provided text, trained on ~150M novel tokens and usable with TextGen-WebUI.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - This repository provides a Japanese-language AI model‚Äîfine-tuned for a girl-like speaking style‚Äîwith specific settings for merging and text generation (max_length 150, top_p 0.8, temperature 0.7).
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Natural Language Interfaces
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with limitations in instruction-following.
  - Downloads: 646
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - This repository provides statically quantized versions of the DeepSeek-R1-Distill-Qwen-7B-Japanese model in GGUF format, with community requests welcome for imatrix quants not currently available.
  - Downloads: 375
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers statically quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model in GGUF format, with potential for imatrix quants based on community requests, and references TheBloke‚Äôs resources for usage instructions.
  - Downloads: 309
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - This repository provides a question-answering model fine-tuned from luke-japanese-large-lite using the DDQA dataset, achieving 86.3% exact match accuracy.
  - Downloads: 126
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the DDQA dataset, achieving an exact match accuracy of 0.845933.
  - Downloads: 119
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - This repository provides GGUF-formatted conversions of the ArrowPro-7B-RobinHood model, including K-quantized versions enhanced with iMatrix using the c4_en_ja_imatrix.txt dataset.
  - Downloads: 102
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - This repository demonstrates using GRPO to solve simple arithmetic problems with a specific prompt format involving `<think>` and `<answer>` blocks for reasoning and final solutions.
  - Downloads: 102
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This repository provides a Japanese question-answering model fine-tuned from DeBERTa-v2-base-japanese using the DDQA dataset, compatible with SQuAD-style tasks and requiring transformers, PyTorch, sentencepiece, and Juman++ for use.
  - Downloads: 81
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and further trained on Japanese conversational data to enable realistic, overlapping, real-time dialogue.
  - Downloads: 31
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 is a merged language model excelling in helpful, harmless responses, role-playing (particularly as a Japanese speaker), and multi-turn conversations, optimized with a low temperature/high top_p setting.
  - Downloads: 30
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - This repository provides a Japanese DialoGPT Small model trained on dialogue extracted from Aozora Bunko, a collection of public domain Japanese literature, due to GPU memory limitations.
  - Downloads: 24
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - Karasu-LoRA-JP-QA-chat is a LoRA-fine-tuned Japanese Q&A model (based on Karasu-1.1B & Karasu-sake-qa-v0.1) optimized for RAG systems using a dedicated question/answer dataset.
  - Downloads: 20
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - This repository hosts Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, a Japanese language model merging Mixtral-8x7B, extending context length to 32K while enhancing instruction-following capabilities.
  - Downloads: 18
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - This repository details a finetuned, MIT-licensed causal language model (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) for Japanese conversational chat, acknowledging limitations due to its small training dataset.
  - Downloads: 16
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This repository provides a Japanese GPT-2 model finetuned on the Yuyuyui scenario corpus for generating character-specific dialogue based on provided conversational context, utilizing special tokens for character and end-of-sequence markers.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - Lightblue's QLoRA finetune specializes in Japanese closed-question answering, trained on a 13,167-sample dataset combining SNOW TyDiQA, and XLSUM (Ja) based on reference text input.
  - Downloads: 14
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository provides a Japanese DeBERTa-v2-tiny model fine-tuned on the DDQA dataset for Question-Answering tasks, compatible with transformers and PyTorch.
  - Downloads: 13
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - This repository provides a Qwen2.5-7B-Instruct fine-tuned language model designed to generate chain-of-thought reasoning from given questions and answers, using a specific XML-like tagging format for input and output.
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 „Åì„ÅÆ„É¢„Éá„É´„ÅØtokyotech-llm/Swallow-MS-7b-instruct-v0.1„ÅÆtokenizer.chat_template„Çí‰ª•‰∏ã„Å´Â§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the JSQuAD dataset, achieving an F1 score of 0.876 and exact match of 0.758.
  - Downloads: 12
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 is a question-answering model built on japanese-stablelm-instruct-gamma-7b, designed to help users learn Japanese in English using a specific prompt format and requiring Transformers 4.34.0 or newer.
  - Downloads: 12
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - This repository hosts a Japanese instruction-tuned language model (c4ai-command-r-v01) further refined with ichikara-instruction, trained on Runpod with specific LoRA parameters and evaluated on JSquad and JCommonsenseQA datasets.
  - Downloads: 12
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - This repository provides a Japanese instruction-tuned version of the TinyMixtral model, trained on a dataset and available on Hugging Face.
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using the llm-book/ner-wikipedia-dataset, as presented in the book ‚ÄúIntroduction to Large Language Models‚Äù.
  - Downloads: 80,328
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - This repository provides a pre-trained BERT model (‚Äújurabi/bert-ner-japanese‚Äù) for Japanese Named Entity Recognition (NER), extracting eight types of entities including person, organization, location, and product names.
  - Downloads: 2,496
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository requires acceptance of conditions for public access to its files and content.
  - Downloads: 714
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from luke-japanese-base using a Wikipedia dataset, achieving 77% precision, recall, and F1-score for organizational names.
  - Downloads: 359
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This repository provides a Japanese medical named entity recognition model trained on MedTxt-CR-JA, along with a prediction script outputting XML-tagged entities and their normalized forms.
  - Downloads: 254
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This repository provides a Japanese medical named entity recognition (NER) model and script (`predict.py`) for identifying disease, medication, and key attributes in medical text.
  - Downloads: 254
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - Kurumi-flux-lora-v1.0 is a non-commercial LoRA model based on flux1-dev, optimized for realistic, beautiful girl depictions, and subject to Black Forest Labs‚Äô terms & restrictions‚Äîusers are responsible for its use and must credit the model when sharing images with #kurumi_flux.
  - Downloads: 218
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune, leveraging an expanded VNTL dataset to enhance English translation accuracy and stability for Japanese visual novels, excluding chat mode.
  - Downloads: 217
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the TFMC/Japanese-Starling-ChatV-7B large language model, offering various quantizations (including IQ1_S at 1.7GB) for efficient usage.
  - Downloads: 154
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - This repository details a binary classification model (ID 59362) trained with AutoNLP for Japanese sentiment analysis, achieving 95.27% accuracy, and accessible via a Hugging Face API endpoint.
  - Downloads: 153
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - This repository features a LLaMA 3 Youko QLoRA fine-tune on an expanded VNTL dataset, specifically designed to enhance English translation accuracy and stability for Japanese visual novels, excluding chat mode.
  - Downloads: 121
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository provides fastText classifiers‚Äîtrained on Wikipedia and LLM annotations‚Äîto assess the educational value of Japanese web pages under a CC BY-SA 4.0 license.
  - Downloads: 108
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - This repository provides a fine-tuned BERT-large-Japanese-v2 model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.862.
  - Downloads: 105
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - This repository provides a Japanese BERT-based model fine-tuned for Named Entity Recognition (NER) using a Wikipedia-derived dataset from Stockmark Inc.
  - Downloads: 60
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - `ja_core_news_lg` is a CPU-optimized spaCy 3.7+ Japanese NLP pipeline featuring tok2vec, morphological analysis, parsing, NER, and trained on UD Japanese GSD v2.8 data with 300-dimensional vectors.
  - Downloads: 53
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - This repository provides a Japanese language model, fine-tuned from `studio-ousia/luke-japanese-large-lite`, that scores short texts for sexual content on a 0-1 scale, intended for moderation purposes.
  - Downloads: 53
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model‚Äîbuilt by fine-tuning a BERT base model with a CRF layer on the NER Wikipedia dataset‚Äîas featured in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 49
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - This repository provides a Japanese text classifier, finetuned from cl-tohoku-bert-japanese-v3, that predicts JLPT levels (N1-N5) with up to 95% precision on similar data.
  - Downloads: 40
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - This repository provides a Fully Convolutional Neural Network (FCN) model trained for classifying images of 49 Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This repository provides a pre-trained model and script for named entity recognition (NER) specifically tailored to Japanese medical text, identifying entities like diseases, treatments, and time expressions.
  - Downloads: 22
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - This repository provides the WRIME dataset, a large-scale, weakly-supervised relational image matching evaluation benchmark with 50K+ images and relational annotations.
  - Downloads: 21
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from deberta-v2-base-japanese using a Wikipedia dataset.
  - Downloads: 20
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - `ja_core_news_trf` is a spaCy 3.7+ Japanese NLP pipeline featuring a `cl-tohoku/bert-base-japanese-char-v2` transformer model for tasks like morphological analysis, parsing, and named entity recognition.
  - Downloads: 20
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from deberta-v2-large-japanese using a Wikipedia dataset.
  - Downloads: 16
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - This repository provides a model for automatically generating article titles from text content, as detailed in the linked Qiita article.
  - Downloads: 14
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - This repository details a binary classification model (ID 59363) trained with AutoNLP for Japanese sentiment analysis, achieving 97% accuracy, precision, recall, and F1-score, accessible via a Hugging Face API endpoint.
  - Downloads: 14
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - `ja_core_news_md` is a CPU-optimized spaCy 3.7+ pipeline for Japanese natural language processing, featuring tokenization, morphology, parsing, sentence segmentation, named entity recognition, and attribute rules, trained on UD Japanese GSD v2.8 data with 200-dimensional vectors.
  - Downloads: 13
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This repository provides a language model trained on 2022 Japanese parliamentary proceedings, built as a multi-GPU/node training example for the #ABCILLM hackathon, and utilizes data from the National Diet Library API.
  - Downloads: 13
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This So-vits-svc 4.0 model creates natural, relatable female voices‚Äînot artificial or overly stylized‚Äîusing a dataset synthesized from the author's own voice and includes essential checkpoints & notebooks for inference and training, while acknowledging potential pronunciation quirks and similarities to existing voices.
  - Downloads: 13
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - This repository provides a fine-tuned luke-japanese-large model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.845.
  - Downloads: 12
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This repository provides a baseline transformer model trained and evaluated on the awesome-japanese-nlp-classification-dataset, achieving 97% accuracy with precision, recall, and F1-scores detailed in the description.
  - Downloads: 12
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Aratako/Japanese-Novel-Reward-TinySwallow-1.5B is a finetuned TinySwallow-1.5B reward model for evaluating the quality of Japanese novels, intended for use with reinforcement learning for text generation, and predicts user ratings via regression while acknowledging potential biases.
  - Downloads: 11
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - This repository provides a 4-bit quantized, Japanese-instruction-tuned version of Llama-2-Chat 70B, fine-tuned on the CC-BY-SA 4.0 licensed izumi-lab/llm-japanese-dataset.
  - Downloads: 11
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This repository provides a QLoRA-finetuned LLaMA2-7B model, trained on the full 49000 chat/280000 non-chat Guanaco dataset, exhibiting improved Chinese and Japanese performance and testable via the included `test.py` script.
  - Downloads: 11
### Responsible NLP
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - This model is a personally-merged, stylized checkpoint focused on generating low-height, young female characters with preserved eye highlights, requiring careful age adjustments and potentially limited compatibility with LoRAs, best used with DPM++ 2M Karras sampling and potentially adjusted Hires steps.
  - Downloads: 8,799
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a Japanese speech recognition model fine-tuned on a large anime voice acting dataset, excelling in that domain but also offering unique performance on other audio, and requiring use *without* an initial prompt to avoid hallucinations.
  - Downloads: 2,155
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - This repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime models (with specified ratios) for use in Stable Diffusion, noting potential over-saturation when merged with realistic models and compatibility issues with SD 2.1 768, alongside download commands.
  - Downloads: 292
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - Suzume_mix_v1.0 is a non-commercial, merged Stable Diffusion model based on flux1-dev, designed to soften facial features and intended for personal use with attribution when sharing generated images.
  - Downloads: 149
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - This repository details a Stable Diffusion model merging YaguruMagiku and AbyssOrangeMix2 to prioritize generating black-haired ponytail hairstyles, with adjustments to mitigate instruction-following issues, and is intended for SFW content despite potential occasional NSFW outputs.
  - Downloads: 27
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - This repository hosts Llama-3-Umievo-Shizuko-sqlcoder-2x8B, a Mixture of Experts (MoE) language model created with MergeKit, combining Japanese language ability and SQL generation by merging rdefog/llama-3-sqlcoder-8b with umiyuki's Llama-3-Umievo-itr014-Shizuko-8b, and offering gguf quantization.
  - Downloads: 24
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - Sarashina2.2-3B-Instruct, a Japanese language model, is based on sbintuitions' v0.1 and trained with the imatrix dataset.
  - Downloads: 23
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 merges DreamShaper 3.3 with WaifuDiffusion/Stable Diffusion VAEs to improve color vibrancy and generate highly realistic, beautiful images, though it may contain NAI/Insta-style leaks and is compatible with Colab WebUI.
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 11
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - Ê¶ÇË¶Å „ÄåLOCAL AI HACKATHON„Äç„Å´„Åä„Åë„Çã„ÄÅ„ÉÅ„Éº„É†DataPilot,4„Å§„ÇÅ„ÅÆÊàêÊûúÂìÅ„Åß„Åô„ÄÇ
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
### Sentiment Analysis
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model trained on the chABSA dataset, achieving 1.0 accuracy and F1 score.
  - Downloads: 2,962
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This repository provides a Japanese emotion analysis model, fine-tuned from Luke-japanese-large-lite using the wrime dataset, to detect eight emotions‚Äîjoy, sadness, anticipation, surprise, anger, fear, disgust, and trust‚Äîwithin text.
  - Downloads: 1,853
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - This repository provides a Japanese BERT Base model finetuned for both sentiment analysis and automatic irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 906
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - This repository provides a GGUF-formatted version of the umyuki Japanese-Chat-Umievo-itr001-7b language model, built using the TFMC/imatrix-dataset, and demonstrates its usage with llama.cpp.
  - Downloads: 549
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model finetuned from scratch on the Japanese Sentiment Polarity Dictionary dataset, building upon the jarvisx17/japanese-sentiment-analysis pretrained model.
  - Downloads: 469
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - This repository provides a BERT model finetuned for Japanese Twitter sentiment analysis (positive, negative, neutral) using the JTS1k dataset, built upon Tohoku NLP‚Äôs Japanese BERT.
  - Downloads: 254
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This repository provides a Japanese BERT-based model fine-tuned for classifying blog posts into ten distinct emotion categories‚Äîamaze, anger, dislike, excite, fear, joy, like, relief, sad, and shame.
  - Downloads: 210
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - This repository provides a Japanese BERT Base model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 55
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 37
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - This repository provides a Japanese BERT-based model, trained on Financial PhraseBank, for analyzing the sentiment (positive, negative, or neutral) of Japanese financial news.
  - Downloads: 36
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - This model is a fine-tuned version of calm-2-7b-chat using the Tsukuyomi corpus, freely usable under the specified Tsukuyomi character and AI training plan licenses.
  - Downloads: 18
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - This repository provides a sentiment analysis model trained on Japanese stock comments to classify discussions as either bullish or bearish, aiding investors and analysts.
  - Downloads: 18
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - This repository provides a Japanese BERT model pretrained on a Twitter corpus, optimized for Japanese social media tasks like sentiment analysis and defamation detection, and used as a base for further finetuned models.
  - Downloads: 16
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - This repository provides a free, commercially-usable voice generation model specializing in cute, gentle speech, with variations available for different tones (cool, ASMR, English, Chinese) and expressive reading.
  - Downloads: 16
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - This repository provides a Japanese ELECTRA-based model finetuned for irony detection using a dataset of ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 14
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository provides a Japanese language ELECTRA model, finetuned on ironic tweets, for automatic irony detection and licensed under CC BY-SA 4.0.
  - Downloads: 13
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese is a finetuned language model for detecting irony in Japanese text, built on ELECTRA and trained with ironic/sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 11
### Reasoning
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-QwQ32b-Reasoning-Japanese-v1.0 language model, trained on the TFMC/imatrix-dataset, and usable with llama.cpp for Japanese reasoning tasks.
  - Downloads: 1,112
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - This repository provides ABEJA-QwQ32b-Reasoning-Japanese-v1.0, a Japanese reasoning model built upon ABEJA-Qwen2.5-32b-Japanese-v0.1 by merging Qwen/QwQ-32B‚Äôs chat vector and further training, utilizing `<think>` tags to elicit a step-by-step thought process.
  - Downloads: 604
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B is a Japanese reasoning model built on Qwen/Qwen2.5-32B-Instruct and improved with post-training using synthetic, long Chain of Thought data generated via Monte Carlo Tree Search.
  - Downloads: 564
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - This repository provides a GGUF-formatted conversion of Microsoft's Phi-4-reasoning-plus model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and designed for use with llama.cpp.
  - Downloads: 529
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - This repository provides a GGUF-formatted version of YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1, a Japanese language model for mathematical tasks, built using the TFMC/imatrix-dataset dataset and usable with llama.cpp.
  - Downloads: 496
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - This repository provides a Japanese-focused fine-tune of the DeepSeek-R1 model, addressing its inconsistency in generating Japanese output and improving performance for Japanese language tasks.
  - Downloads: 395
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is an anime-style diffusion model merging and fine-tuned Stable Diffusion/Wifu Diffusion with LoRA distilled from styles of Niji Journey and everyday use, prioritizing transparency through disclosed datasets and processes, and excelling at solo female character illustrations with a defined, bold aesthetic.
  - Downloads: 372
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository provides a fine-tuned version of luke-japanese-large for the JCommonsenseQA task, achieving a high accuracy of 83.82% on commonsense question answering.
  - Downloads: 99
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference model, trained on JGLUE-JNLI & JSICK datasets, that outputs contradiction, entailment, and neutral scores for given sentence pairs using SentenceTransformers.
  - Downloads: 65
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - This repository provides a fine-tuned luke-japanese-base model achieving 89.77% accuracy on the JGLUE JNLI dataset for natural language inference, specifically determining textual entailment, contradiction, or neutrality.
  - Downloads: 55
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a Japanese commonsense knowledge model finetuned from COMET on the Japanese TimeATOMIC dataset using causal language modeling, detailed in a LREC-COLING2024 paper, and preprocessed with Juman++ and SentencePiece.
  - Downloads: 42
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-Japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 20
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - This model merges reasoning capabilities from DeepSeek-R1-Distill-Llama-8B into the Japanese Llama-3.1-Swallow-8B-v0.2, enhancing its reasoning skills.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman for morphological analysis.
  - Downloads: 16
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA tasks, trained on the JGLUE/JCommonsenseQA dataset using transformers, PyTorch, and requiring sentencepiece & Juman++.
  - Downloads: 15
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - This repository provides Polyglot-math-4x7b-24b, a multilingual Mixture of Experts model merging Chinese, Japanese, and English capabilities, fine-tuned on GSM8k with a 20GB VRAM footprint, and includes inference code.
  - Downloads: 12
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - This repository provides a highly accurate (80.07%) Japanese language model, fine-tuned from luke-japanese-base using the JGLUE JCommonsenseQA dataset for multiple-choice question answering.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository provides a private demonstration project.
  - Downloads: 515
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - This repository requires agreement to a License and Privacy Policy from Stability AI before access.
  - Downloads: 389
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 132
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Twitter/twhin-bert-large model for Japanese social media comment offensiveness detection, achieving a macro-averaged F1-score of 64.8% on a manually labeled dataset.
  - Downloads: 109
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese language model‚Äîbased on studio-ousia/luke-japanese-large-lite‚Äîfor classifying online comment offensiveness with 64.0% macro-averaged F-score and 65.0% accuracy.
  - Downloads: 106
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - This repository provides a fine-tuned Twitter/twhin-bert-base model for detecting offensive language in social media comments, achieving a macro-averaged F1-score of 64.7% on a manually labeled dataset.
  - Downloads: 106
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 88
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 68
- [Kotajiro/tsubaki_mix](https://huggingface.co/Kotajiro/tsubaki_mix)
  - This commercially usable Stable Diffusion model, licensed under CreativeML Open RAIL++-M, generates images with hires support but prohibits depictions of violence, explicit or exploitative content, and likenesses of individuals without consent‚Äîplease tag creations with #tsubaki_mix.
  - Downloads: 52
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model with a CreativeML OpenRAIL-M license, granting usage rights while prohibiting illegal/harmful outputs and clarifying user accountability for generated content.
  - Downloads: 46
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - This repository provides a Japanese language ELECTRA Small model finetuned for cyberbullying detection using data from BBS comments and Twitter, built upon a YACIS corpus pretraining.
  - Downloads: 26
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 25
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged model‚Äîsimilar to HimawariMix‚Äîfocused on strong backgrounds and detail, tuned with ideas from "Riga," and includes a standard VAE, but prohibits commercial use, resale, illegal outputs, and altered permissions upon sharing.
  - Downloads: 22
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - This repository provides a Japanese ELECTRA-Small model finetuned for cyberbullying detection using a combined dataset of online comments and tweets, licensed under CC BY-SA 4.0.
  - Downloads: 15
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - This repository provides instruction-tuned language models for Japanese (llm-jp).
  - Downloads: 14
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA Base model finetuned for cyberbullying detection using a combined dataset of online comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performing, CPU-runnable Japanese text embedding model optimized for semantic similarity and retrieval tasks, achieving state-of-the-art results on benchmarks like MIRACL.
  - Downloads: 15,538
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - This repository distributes LoRA models created by Hotaru Jujo, released under dual MIT/CreativeML Open RAIL-M licenses with no usage restrictions, but social media credit is appreciated.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT v1 is a Japanese document retrieval model based on ColBERT, achieving near-multilingual performance on standard benchmarks despite out-of-domain evaluation data.
  - Downloads: 504
- [hotchpotch/japanese-reranker-tiny-v2](https://huggingface.co/hotchpotch/japanese-reranker-tiny-v2)
  - This repository provides a series of small and fast Japanese reranker models (v2) with varying sizes and speeds, offering performance scores and GPU processing times for comparison.
  - Downloads: 429
- [hotchpotch/japanese-reranker-xsmall-v2](https://huggingface.co/hotchpotch/japanese-reranker-xsmall-v2)
  - This repository provides a series of fast and very small Japanese reranker models (v2) with varying sizes and performance metrics, including speed and scores, for tasks like search ranking.
  - Downloads: 105
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - This repository provides a passage encoder for a BERT-based Bi-directional Preference Ranking (BPR) document retrieval model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 42
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - This repository provides a Japanese NLP model, finetuned from bert-base-japanese-v2, for extracting key entities (AREA, TYPE, SZN, INGR) from cooking-related questions to generate search keywords.
  - Downloads: 12
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 is a 7 billion parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma 7B, achieving a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 12
## Datasets üß†

This list is sorted by downloads as of May 27, 2025.
529 datasets are listed.

### Information Extraction & Text Mining
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset is the livedoor News Corpus, used in the book *Introduction to Large Language Models*, providing news articles from livedoor News under a Creative Commons Attribution-NoDerivs 2.1 Japan license.
  - Downloads: 3,530
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a multilingual sentence dataset enabling customizable language pair loading (e.g., English-Hebrew) with optional version specification via the `load_dataset` function.
  - Downloads: 2,463
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - FineWeb2 Edu Japanese is a high-quality, 89.3 billion token dataset of 120 million filtered educational Japanese texts, including subsets for sampling and shorter text lengths.
  - Downloads: 881
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - WRIME is a dataset for emotional intensity estimation, featuring both self-reported and reader-annotated intensity levels of social media posts from 50 crowd-sourced participants.
  - Downloads: 627
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - JSICK is a Japanese natural language inference and semantic textual similarity dataset created by translating the English SICK dataset, designed for researching multilingual compositional inference and model stress-testing.
  - Downloads: 416
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This dataset provides 8.75K comprehensive records of Japanese laws ‚Äì including number, title, effective date, and full text ‚Äì sourced from e-Gov, deduplicated to the latest effective versions as of August 1, 2023.
  - Downloads: 375
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - This repository provides a cleaned, 612M token Japanese news corpus extracted from Common Crawl (July-October 2024) using Uzushio, filtered with pipeline_03a.conf, and tokenized for llm-jp/llm-jp-13b-v1.0.
  - Downloads: 368
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This dataset offers a machine learning-ready collection of public-domain Japanese literary works from Aozora Bunko, sourced and processed using the globis-org/aozorabunko-exctractor code.
  - Downloads: 341
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository provides a sharded, Japanese-language subset of the CC100 dataset in Parquet format.
  - Downloads: 313
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - This dataset provides 5,000 annotated Japanese tweets (Feb-Jun 2022) for detecting online defamation, labeling both the target of abuse and the type of defamatory content.
  - Downloads: 267
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository provides the Japanese named entity recognition (NER) dataset created by Stockmark Inc. and used in the book *Introduction to Large Language Models*, licensed under CC-BY-SA 3.0.
  - Downloads: 248
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully is a publicly available, commercially usable dataset in Japanese and other languages designed to improve LLM safety, but its use is restricted to safety enhancement and redistribution is prohibited‚Äîderivatives are allowed with attribution and adherence to the terms.
  - Downloads: 230
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository provides the Japanese subset of the wiki40b dataset, formatted as three parquet files generated using the `datasets` library.
  - Downloads: 191
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - This dataset classifies GitHub repository descriptions as relevant (1) or not relevant (0) to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing, sourced from an "awesome-japanese-nlp-resources" list.
  - Downloads: 168
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This repository provides a filtered Japanese subset of the XL-Sum dataset, reduced via PaLM 2's 15-gram overlap filter, containing 4215 training, 758 validation, and 766 test examples.
  - Downloads: 163
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - This repository provides a dataset card for ‚Äújapanese_alpaca_data,‚Äù built upon the japanese-alpaca-lora project and requiring further information to be complete.
  - Downloads: 162
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - This dataset provides Japanese named entity recognition (NER) data extracted from Wikipedia, licensed under CC-BY-SA 3.0 and developed by Stockmark Inc.
  - Downloads: 148
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - This repository provides a Japanese Wikipedia dataset from January 1, 2023, formatted as Parquet files generated using the `datasets` library.
  - Downloads: 146
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - This dataset provides images for evaluating Japanese image classification models across four tasks, including 101 types of Japanese food, released by Recruit Co., Ltd. under a CC-BY-4.0 license.
  - Downloads: 135
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - This Japanese dataset provides multi-label annotations of NLP research fields for GitHub repository data (descriptions, READMEs, PDFs, images) to facilitate training data for NLP research.
  - Downloads: 126
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This dataset provides named entity recognition (NER) labels for Wikinews articles in Japanese, featuring 8 entity types and licensed under CC BY 2.5, serving as a test set for large language model learning‚Äîspecifically used in the book *Introduction to Large Language Models*.
  - Downloads: 125
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - This dataset extracts news articles from July-October 2024 (focusing on September-October), prepends dates to the text, and adjusts length to ~1000 tokens for efficient continued pre-training with the llm-jp/llm-jp-3-13b tokenizer, assuming an output token count of 1024.
  - Downloads: 102
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository provides a dataset of voice clips with labels for Nene Kusakabe (CV: Machico) from *Project Sekai: Colorful Stage! feat. Hatsune Miku*, with plans for future expansion and standardization, and a related QQ group (691795641) for a full character dataset.
  - Downloads: 89
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This repository provides a dataset of extracted sections from Japanese securities reports (Êúâ‰æ°Ë®ºÂà∏Â†±ÂëäÊõ∏) submitted to EDINET between 2014 and 2022, including company information, document details, and reporting periods.
  - Downloads: 87
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - This dataset, zenz-v2.5, comprises 190M pairs of kana-to-kanji conversions with left context for training conditional language models, and includes pre-trained models (small, medium, xsmall) alongside the AJIMEE-Bench evaluation benchmark.
  - Downloads: 84
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA is a Hugging Face mirror of the AWS Open Data Registry dataset containing Japanese captions and images for multimodal AI research.
  - Downloads: 78
- [hal-utokyo/MangaOCR](https://huggingface.co/datasets/hal-utokyo/MangaOCR)
  - MangaOCR is a 209K-instance dataset of narrative text extracted from manga, combining annotations from Manga109 and a dedicated onomatopoeia dataset to support manga text recognition research.
  - Downloads: 73
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - This Japanese dataset contains three-line summaries and indexing information for thousands of mycological taxonomy papers from the Daikinrin website, maintained by Atsushi Nakajima.
  - Downloads: 73
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - This repository provides a dataset governed by a specific LICENSE agreement that users must acknowledge and review before use.
  - Downloads: 73
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a list of 100 common Japanese stopwords sourced from CC-100 and Wikipedia, compatible with the nagisa text analysis library, and accessible via a provided Python script requiring the Huggingface `datasets` library.
  - Downloads: 71
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - This repository provides a JSONL conversion of the Dolly-15k-Japanese dataset for use with the SFTTrainer, licensed under CC BY SA 3.0.
  - Downloads: 69
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - This repository provides a JSON-based dataset of anime metadata with cross-references to popular anime platforms like MAL, AniList, and Kitsu, originally from the Manami Project.
  - Downloads: 65
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - This repository provides a Japanese cooking recipe search dataset with labeled entities (AREA, TYPE, SZN, INGR) and includes code for language model fine-tuning & application development using the dataset.
  - Downloads: 65
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - J-NER is a Japanese Named Entity Recognition (NER) dataset comprising 1,570 examples of 157 entity types sourced from Wikipedia, designed for Large Language Model (LLM) training and evaluation.
  - Downloads: 55
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench is a 102-question benchmark dataset, featuring 21 Japanese images categorized by seven subcategories, designed to evaluate the performance of Vision-Language Models (VLMs) on conversation, detail, and complex reasoning.
  - Downloads: 55
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences is a Japanese Wikipedia-based dataset of sentences with article and section titles, licensed under CC BY-SA 4.0 and GFDL, and generated using a provided script.
  - Downloads: 51
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - This Japanese dataset compiles manually extracted diagnostic characters comparing fungal species, sourced from summaries of thousands of mycological taxonomy papers on the Daikinrin website.
  - Downloads: 51
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - This project automatically generates question-and-answer pairs from Japanese Wikipedia articles using a 5-bit quantized Mixtral 8x22b model on the TSUBAME4.0 supercomputer, but requires filtering due to potential hallucinations in generated answers.
  - Downloads: 49
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - This repository provides the English/Japanese dataset used to train the shisa-7b-v1 language model, with further details available in that model's README.
  - Downloads: 48
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - This dataset provides anime song lyrics in a Parquet file, offering a resource for research and enthusiasts seeking structured lyrical data.
  - Downloads: 48
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - This dataset contains Japanese news articles from September and October 2024, cleaned and trimmed to approximately 1000 tokens‚Äîoptimized for efficient training with the llm-jp/llm-jp-3-13b tokenizer, assuming a 1024 output token limit.
  - Downloads: 47
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains extracted sections from 2024 Japanese securities reports (Êúâ‰æ°Ë®ºÂà∏Â†±ÂëäÊõ∏) filed via EDINET, providing company information like names, codes, financial periods, and JCN (corporate number) via fields such as `edinet_code`, `company_name`, and dates.
  - Downloads: 47
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - This dataset contains approximately 7,000 rows of disease-symptom relationships extracted from Japanese medical journal articles (2003-2023), including disease text, findings, supporting descriptions, and article metadata in JSON-Lines format.
  - Downloads: 46
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - This dataset provides approximately 10 billion Japanese tokens filtered from CommonCrawl to remove sensitive personal information (PPI) using rule-based and machine learning techniques, licensed under CC terms.
  - Downloads: 45
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - This repository provides a Japanese RLHF dataset reformatted as a classification task for reward model training, utilizing synthetically generated text from Phi-3-medium with labels indicating preferred (1) or rejected (0) sentences.
  - Downloads: 44
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - This dataset provides 53,640 annotated Japanese tweets (January-June 2020) related to COVID-19, designed for text classification tasks and requiring API access to retrieve original tweets.
  - Downloads: 44
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - This repository provides a dataset of inspiring anime quotes, including character attribution, sourced from Anime Motivation for analysis and enjoyment.
  - Downloads: 43
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully is a commercially-usable dataset in Japanese and other languages designed to improve LLM safety, with usage restricted to safety enhancement and prohibiting redistribution of the original data while allowing derivative works under specific conditions, and acknowledging potentially harmful content.
  - Downloads: 42
- [OmniAICreator/Japanese-Novels](https://huggingface.co/datasets/OmniAICreator/Japanese-Novels)
  - This repository provides a large dataset of Japanese web novels (80B+ characters, 55.4B tokens) intended solely for machine learning research, requiring justification for access.
  - Downloads: 41
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - This repository provides a Hugging Face-compatible version of the Kyoto University Japanese Wikipedia input error dataset (v2), licensed under CC-BY-SA 3.0.
  - Downloads: 41
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - This repository provides a sample dataset of Japanese Wikipedia articles related to the "Nikkei 225" stock market index, extracted from the January 2023 Wikipedia data in Parquet format.
  - Downloads: 37
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - This repository provides a collaborative, large-coverage French-Japanese dictionary and aligned bilingual corpus built from sources like Cesselin, Raguet-Martin, and JMdict, currently containing over 154,000 entries.
  - Downloads: 36
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a Japanese language benchmark evaluating long-context LLM performance on extractive QA and abstractive summarization tasks using data from websites and GPT-4/Claude-3.5-Sonnet generated question-answer pairs.
  - Downloads: 35
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - This repository provides a clustered dataset, derived from customs advance ruling data, for training and evaluating embedding models, featuring item classifications (‚Äúgeneral product names‚Äù) labeled by HS code section, split into train/test sets with class-balanced proportions.
  - Downloads: 35
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - This repository expands the Japanese Cosmopedia dataset to 100k entries, incorporating contributions from kunishou, and includes translated text generation prompts‚Äîdetails are available on Hugging Face.
  - Downloads: 30
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - This repository provides a work-in-progress dataset of Japanese-English sentence pairs sourced from Ruby and Rails documentation, including some synthetically generated data, for machine translation or similar tasks.
  - Downloads: 28
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - Giellm is a Japanese general information extraction dataset built from the Livedoor News corpus, detailed in the linked arXiv paper exploring mutual reinforcement learning for large language models.
  - Downloads: 27
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - This repository provides a cleaned, UTF-8 encoded, human-generated Japanese subtitle dataset from OpenSubtitles, including text, timing, and metadata in Parquet format, prepared for use with Open Assistant.
  - Downloads: 27
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - This repository provides a clustered dataset‚Äîderived from Japan's PMDA website‚Äîfor training and evaluating embedding models, featuring text data (‚Äúgeneric name‚Äù + ‚Äúgeneric name definition‚Äù) labeled with ‚Äúclassification codes‚Äù split into train/test sets with preserved class proportions.
  - Downloads: 24
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - JParaCrawl is a large, publicly available English-Japanese parallel corpus created by NTT through web crawling and automatic sentence alignment, accessible via the `datasets` library.
  - Downloads: 23
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This synthetic question-answering dataset, built from the sakura_japanese_dataset, was generated using Nurture-intelligence/Gemma-2-108B-DPO-v0.1 with Pro-type inference and is subject to both the original dataset's license and Gemma Terms of Use.
  - Downloads: 21
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - This repository provides a furigana dataset derived from bibliographic data of the National Diet Library, available for download as a ZIP file.
  - Downloads: 21
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - This repository provides scripts to download, parse, and preprocess the publicly available en-ja-alignÊó•Ëã±ÂØæË®≥Êñá dataset (Uchiyama et al., 2003) for machine translation, utilizing libraries like `datasets`, `bs4`, and `lxml`.
  - Downloads: 21
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is a JSONL dataset derived from the English Touhou Wiki, designed for unsupervised text generation and potentially text classification, containing primarily English with some Japanese content.
  - Downloads: 20
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides a JSONL dataset of validated linguistic minimal pairs for Japanese, used for benchmarking natural language processing models, as detailed in Someya and Oseki (2023).
  - Downloads: 18
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - This dataset contains crawled data from the ‚ÄúSenryu Marusen‚Äù Japanese haiku/senryu website, comprising 5346 submissions across 376 prompts, formatted for text-to-text tasks with accompanying prompt IDs and content.
  - Downloads: 17
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - This dataset provides manually translated, passage and sentence-level English-Japanese parallel text derived from Wikipedia introductions, created with a permissive license for machine learning use and minimizing reliance on restricted translation tools by leveraging open-source LLMs like CALM3 and Qwen.
  - Downloads: 17
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - This dataset provides raw, Unicode-normalized Japanese text from fineweb-2-edu-japanese, cleaned with noise inference (threshold 0.7, length >= 4) and marked with `noise_spans` (start/end positions), licensed under ODC-By.
  - Downloads: 17
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-following dataset containing 6,259 manually annotated input-output pairs for use in tasks like instruction tuning and language modeling.
  - Downloads: 17
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository provides the Japanese subset of the NTX dataset converted to the Aya instruction format, licensed under CC-BY-SA 4.0, and linked to the full instruction-formatted dataset and associated research paper.
  - Downloads: 15
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - This repository provides a working sample of Parquet files extracted from the Japanese Wikipedia (range3/wikipedia-ja-20230101) specifically containing articles categorized under "Investment."
  - Downloads: 15
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M is a corpus of 3.5 million unique Japanese light novel character names from syosetu.com, designed for culturally aware NLP applications like NER and name generation.
  - Downloads: 14
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - This repository provides the dataset and code for the SLG framework, a sentence-to-label generation approach for multi-task Japanese sentence classification and named entity recognition, detailed in the linked paper and arXiv publication.
  - Downloads: 13
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - This repository provides a Japanese text dataset ("Washi") created using DSIR sampling from CulturaX, focusing on documents similar to XLSum and Aozora Bunko for improved language model performance.
  - Downloads: 13
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - This dataset contains 221 award-winning haiku from the Itoen Shinhaiku competition, including author/judge comments, translations, language labels, and image URLs, structured with fields like title, ordinal, award, and comment.
  - Downloads: 13
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This AutoTrain dataset, generated for the ‚Äútam_jp‚Äù project, provides Japanese (ja) language data instances structured as context-based examples.
  - Downloads: 13
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset provides 600 Japanese sentences extracted from Wikipedia articles about Japanese racehorses, annotated with nine named entity types ‚Äì including a specific ‚Äúracehorse name‚Äù label ‚Äì for named entity recognition tasks, and lacks negative examples.
  - Downloads: 12
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - This dataset provides English-Japanese paragraph pairs with labels indicating whether they refer to the same or different entities, extending the original PubChem & Wikipedia classification task to a multilingual setting.
  - Downloads: 12
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíUVR„Çí‰ΩøÁî®„Åó„Å¶BGM„ÇÑ„Éé„Ç§„Ç∫Èô§Âéª„Åó„Åü„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset, crowdsourced from Danbooru with detailed, multi-label tagging (avg. 30 tags/image) suitable for training image classification and tagging models.
  - Downloads: 5,987
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - This dataset provides thousands of Japanese anime/visual novel audio clips with transcriptions to improve automatic speech recognition accuracy, particularly for models like Whisper, within this unique genre.
  - Downloads: 3,003
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST is a dataset of 70,000 handwritten Japanese characters (10 classes) for image classification tasks.
  - Downloads: 2,950
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese-Anime-Speech-V2 is a 292,637-clip audio-text dataset from visual novels created to improve automatic speech recognition accuracy, and is distinct from its V1 predecessor.
  - Downloads: 2,124
- [turing-motors/MOMIJI](https://huggingface.co/datasets/turing-motors/MOMIJI)
  - MOMIJI is a 56M-document, large-scale Japanese image-text dataset extracted from Common Crawl, designed for training vision-language models.
  - Downloads: 1,702
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - ReazonSpeech is a 35,000+ hour Japanese speech dataset in FLAC format, intended for Automatic Speech Recognition (ASR) research under the constraints of Japanese copyright law.
  - Downloads: 681
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - This dataset provides a diverse collection of 2020s Japan images‚Äîspanning landscapes, culture, and daily life‚Äîfor AI training and research.
  - Downloads: 542
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - This repository provides a CC-0 licensed dataset of AI-generated anime images with Japanese captions (including English translations) created for ethical machine learning, featuring image, prompt, and dense caption data.
  - Downloads: 530
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 provides a dataset of over 240,000 curated animation clips‚Äîprimarily Japanese anime‚Äîcollected from Sakugabooru.com, aiming to address the lack of resources for generative video and AI animation research.
  - Downloads: 440
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark designed to rigorously evaluate Large Multimodal Model (LMM) performance by addressing cultural dependencies present in existing datasets through expert-sourced, culture-agnostic questions.
  - Downloads: 406
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tags (averaging 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 330
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a 1.2 million+ image MIT-licensed anime illustration dataset with diverse, high-quality sourcing ‚Äì including keyframes and manga ‚Äì designed for research despite relatively few tags per image.
  - Downloads: 301
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset provides a refined version of the image, context, and question data from the Japanese-Heron-Bench, designed for evaluating vision-language models in Japanese.
  - Downloads: 243
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample subset of the Japanese Visual Genome VQA dataset, used to evaluate EvoVLM-JP-v1-7B and available under a CC BY 4.0 license.
  - Downloads: 230
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This dataset provides transcriptions and 77 characters of voice lines from the game *Umamusume*, totaling nearly 10,000 seconds of audio for characters like Sweep Tokai, Tokai Teio, and others.
  - Downloads: 224
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - This repository provides a large-scale Japanese speech dataset‚Äîcontaining 445,793 WAV files totaling 577 hours of audio‚Äîcreated with VOICEVOX using the ITA, Tsukuyomi, and ROHAN text corpora.
  - Downloads: 187
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This project creates a non-official, publicly available dataset of voice data from VTuber Sakura Miko (hololive) for use in speech recognition and other applications, adhering to hololive's secondary creation guidelines.
  - Downloads: 181
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - This repository provides Japanese Automatic Speech Recognition (ASR) transcriptions generated using Whisper, specifically the reazon_speech_all dataset, excluding the original audio files.
  - Downloads: 173
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - This dataset contains 2735 WAV audio clips extracted from the game *Project Sekai*'s character Emu Otori, intended for research use with So-vits-svc 4.0, and licensed under CC-BY-NC 4.0 with rights reserved by SEGA and voice actors.
  - Downloads: 123
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - This corpus provides 120 Japanese speakers‚Äô audio phone conversations recorded in the United States, distributed by the LDC and available via TalkBank with specific citation requirements.
  - Downloads: 110
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - This repository provides a split dataset derived from the joujiboi/japanese-anime-speech-v2 project, likely for easier access or specific training/testing purposes.
  - Downloads: 109
- [AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations](https://huggingface.co/datasets/AIxBlock/Human-to-machine-Japanese-audio-call-center-conversations)
  - AIxBlock provides a Japanese audio dataset of synthetic human-machine conversations simulating call center interactions, licensed under CC BY-NC 4.0.
  - Downloads: 100
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay collection (based on DeepSeek-V3-0324) by adding system messages and formatting, released under the MIT license.
  - Downloads: 100
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - This dataset provides approximately 39 million Japanese characters of high-quality text extracted from 1,924 research papers (including NLP2024) and 360 journal articles under CC-BY licenses, intended for use in language model pre-training and RAG applications.
  - Downloads: 100
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - This repository provides a comprehensive 86,000+ sample Japanese voice dataset‚Äîcombining StoryTTS, genshin-voice, and japanese-anime-speech‚Äîfor speech recognition, text-to-speech, and machine learning applications.
  - Downloads: 84
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - This dataset pairs images of PDF pages (resized to 896px, 700px, or 588px) with OCR-processed text (using NDLOCR, potentially including "„Äì" for failed reads) and questions generated by Qwen/Qwen2.5-14B-Instruct to train image retrieval models.
  - Downloads: 82
- [Rakuto/DailyTalkContiguous-ja](https://huggingface.co/datasets/Rakuto/DailyTalkContiguous-ja)
  - DailyTalkContiguous-ja is a Japanese multi-turn conversational speech dataset generated using translated DailyTalk data and synthesized speech from Zyphra/Zonos, featuring five distinct speaker voices.
  - Downloads: 77
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This corpus consists of Japanese text generated by Phi-3 from randomly sampled data sources, built upon the OpenMathInstruct-1-1.8m-ja code and utilizing computational resources from the TSUBAME4.0 supercomputer.
  - Downloads: 74
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This dataset contains Japanese *senryu* (short poems) crawled from two websites, offering 70 image-to-text and 30 text-to-text prompts with two curated responses each, for tasks involving generating poems from visual or textual cues.
  - Downloads: 66
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - This dataset provides approximately 1000 Japanese roleplay instruction prompts generated by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, created with DeepInfra, and may contain low-quality records due to limited post-processing.
  - Downloads: 66
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from *Project Sekai* for use with So-vits-svc 4.0, licensed under CC-BY-NC 4.0 with copyright remaining with SEGA and voice actors.
  - Downloads: 60
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - This repository provides translated speech instructions‚Äîsourced from the Common Voice dataset across 120 languages‚Äîto English, Arabic, Japanese, Mandarin, and French, intended for finetuning Speech LLMs.
  - Downloads: 60
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - This dataset provides English and Japanese captions generated by BLIP for Pok√©mon images from the FastGAN dataset, used for training Pok√©mon text-to-image models.
  - Downloads: 55
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - This repository presents speech quality analysis results‚Äîspecifically MOS scores and transcriptions‚Äîfor reazon-research/reazonspeech-v2, saved in a JSON file and visualized with histograms, utilizing resources provided by AiHUB.
  - Downloads: 54
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This dataset facilitates evaluation of large language models on three humorous response generation tasks ‚Äì text-to-text, image-to-text, and text-image-to-text ‚Äì using image and text prompts with corresponding IDs.
  - Downloads: 53
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - This dataset provides diverse, high-quality images and videos of Japan‚Äîcovering landscapes, culture, and daily life‚Äîcaptured primarily in the 2020s for AI training.
  - Downloads: 53
- [hal-utokyo/MangaVQA-train](https://huggingface.co/datasets/hal-utokyo/MangaVQA-train)
  - MangaVQA is a synthetic VQA dataset of 41,895 samples generated from 8,379 Manga109 images using GPT-4o, licensed under CC BY 4.0 and subject to OpenAI terms.
  - Downloads: 48
- [aidealab/aidealab-videojp-eval](https://huggingface.co/datasets/aidealab/aidealab-videojp-eval)
  - This repository provides data and instructions to reproduce the Fr√©chet Video Distance (FVD) evaluation for AIdeaLab VideoJP, including necessary files and a step-by-step guide for setup and execution.
  - Downloads: 48
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - The CABank Japanese Sakura Corpus provides audio data from 31 participants in Japan, requiring proper citation and adhering to TalkBank usage rules, and is a copy of the original dataset available at https://ca.talkbank.org/access/Sakura.html.
  - Downloads: 46
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - This dataset provides CC0-licensed images of places in Japan for training text-to-image models and other applications without copyright concerns.
  - Downloads: 43
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This dataset contains crawled data from the HomeMate Senryu Grand Prize's "Photo Senryu" contest, featuring 435 image prompts and 1767 corresponding entries, intended for use within the YANS hackathon.
  - Downloads: 40
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - This repository provides a Hugging Face Datasets version of the Tanaka Corpus, preprocessed into a dataset of Japanese-English sentence pairs with IDs.
  - Downloads: 40
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus is a dataset of 96kHz/16bit Japanese speech recordings by a virtual character ("Lux"), including both raw and cleaned audio files with corresponding transcripts in `metadata.csv` and dataset information in `dataset_infos.json`.
  - Downloads: 37
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This repository provides a 66.4-hour Japanese voice-text dataset of 30,800 records featuring single-actor performances from *Fate/Grand Order* characters, suitable for ASR/ASV model training & evaluation.
  - Downloads: 29
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - This repository presents the results of speech quality analysis using speechMOS on the Common Voice Corpus 17.0, providing a JSON file of MOS scores and corresponding audio file counts for various SNR thresholds.
  - Downloads: 29
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - This dataset provides a furigana-annotated speech corpus derived from Aozora Bunko and SAPIE audio Daisy data, containing over 3.3 million cleaned entries with kanji.
  - Downloads: 28
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella is a Japanese a cappella vocal ensemble corpus featuring musical scores and isolated audio tracks of six-part arrangements of public domain children's songs.
  - Downloads: 28
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - This repository provides a Japanese translation of the HelpSteer dataset, designed for testing and utilizing alignment with NVIDIA‚Äôs SteerLM for customizable LLM training and inference.
  - Downloads: 27
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage is a Japanese dataset derived from JDocQA, featuring 200dpi PNG images and corresponding questions/answers, specifically filtered for single-image, factoid, and yes/no question types to reduce size and enhance practicality.
  - Downloads: 25
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This synthetic dataset, built using images from ThePioneer/japanese-photos, was generated with Qwen/Qwen2-VL-7B-Instruct and Qwen/Qwen2.5-32B-Instruct-AWQ models.
  - Downloads: 25
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - This repository provides a 32K Japanese instruction dataset, automatically generated using Rakuten/RakutenAI-7B-chat based on the Stanford Alpaca methodology and community-sourced seed tasks, but requires filtering due to potentially low quality.
  - Downloads: 24
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - This dataset provides 6315 PNG images of Kanji characters (1024x1024) paired with their textual definitions, adapted from KanjiVG SVG files.
  - Downloads: 23
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This dataset, derived from CLoT-Oogiri-Go, contains Japanese humorous responses for three tasks ‚Äì text-to-text, image-to-text, and text-image-to-text ‚Äì sourced from the Bokete website, totaling 100 examples.
  - Downloads: 22
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - This dataset filters high-quality records from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja specifically for JGLUE benchmark tasks (JcommonsenseQA, MARC-ja, JSQuAD) with varying licenses (Apache 2.0, CC-BY-SA-3.0, MIT).
  - Downloads: 22
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - This dataset provides CC0-licensed images of Japanese scenery for training text-to-image models and other machine learning applications, serving as a template for new dataset creation.
  - Downloads: 21
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This dataset provides 330k Japanese web text samples (train/test) with noise spans identified by an LLM (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) in a strict JSON format, targeting elements like navigation, ads, and system messages for cleaner text processing.
  - Downloads: 20
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Japanese LLaVA Instruct 150K is a 150K Japanese-translated version of the LLaVA Visual Instruct dataset, licensed for non-commercial use and adhering to OpenAI policies.
  - Downloads: 19
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - This repository provides the LLaVA JP Instruct 108K dataset, a Japanese language instruction-tuned dataset created from Japanese Visual Genome VQA and docci_ja data, licensed under Apache 2.0.
  - Downloads: 19
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - This repository provides a dataset of emotion analysis results for Japanese music videos‚Äîincluding predicted moods, valence, and arousal‚Äîgenerated using the Music2Emotion tool and formatted as JSONL.
  - Downloads: 19
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This dataset contains 100 Japanese *senryu* (short poem) examples‚Äî70 image-to-text and 30 text-to-text‚Äîcrawled from photo and online *senryu* websites, intended for evaluating generative models with a leaderboard and human evaluation.
  - Downloads: 18
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - This repository provides Japanese MS MARCO data with hard negatives mined through normalization, filtering, and selection of queries and documents, alongside SPLADE model training and comparison with mMARCO for information retrieval.
  - Downloads: 18
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository provides a dataset of voice embeddings for Japanese members of parliament, created using speechbrain/spkrec-ecapa-voxceleb, suitable for speaker separation and analysis of parliamentary proceedings.
  - Downloads: 17
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - This dataset provides CC-BY-SA-4.0 licensed quiz data from the JAQKET AI-Owari competition, accessible via the `datasets` library, featuring questions, answers, and metadata like competition details and timestamps.
  - Downloads: 16
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - This repository provides a clustered dataset of 6,127 Japanese legal documents (XML format) collected from e-Gov, categorized by legal classification based on "Heisei" or "Reiwa" era, and split into train/test sets for embedding model learning and evaluation.
  - Downloads: 15
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - This dataset provides clustered text data‚Äîbusiness descriptions from EDINET filings‚Äîlabeled with industry codes for training and evaluating embedding models, split into train/test sets with balanced label representation.
  - Downloads: 15
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - This repository provides a cleaned and filtered version of the Aozora corpus‚Äîa 2.5M+ entry dataset of Japanese speech with furigana annotations‚Äîimproving upon the original by correcting Whisper transcription errors and enhancing data accuracy for training speech models like FLFL.
  - Downloads: 12
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - This dataset provides Japanese audio and transcriptions of veterinary medicine terms‚Äîdrugs, diseases, and symptoms‚Äîfor training speech recognition or natural language processing models.
  - Downloads: 12
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - This dataset provides multilingual Amazon product reviews (English, Japanese, German, French, Chinese, Spanish) collected between 2015-2019 for text classification, but is currently defunct and inaccessible.
  - Downloads: 2,131
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - This repository benchmarks Large Language Models on English translation of Japanese Visual Novels, providing a leaderboard and comparisons to existing translation tools with preliminary, evolving results.
  - Downloads: 521
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - JHumanEval is a manually revised, Japanese translation of the HumanEval code-solving dataset, designed as a benchmark for evaluating code generation capabilities of Japanese Large Language Models, intentionally retaining original errors for realistic assessment.
  - Downloads: 389
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - This repository provides a Japanese translation of the research-safe English subset of the ReLAION-5B dataset, generated using the Gemma-2-9b-it LLM and the vLLM inference library via the text2dataset tool.
  - Downloads: 381
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - This repository provides the filtered training and development datasets from JSNLI v1.1, used in the book *Introduction to Large Language Models*, licensed under CC BY-SA 4.0.
  - Downloads: 327
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - This repository provides Japanese queries from the MMarco dataset with hard negatives retrieved using both multilingual e5 and BM25 (Anserini) models for information retrieval research.
  - Downloads: 278
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - This dataset provides English translations of the Japanese text from the kaken subset of llm-jp-corpus-v3, created using Qwen/Qwen2.5-32B-Instruct and released as an open, parallel Japanese-English corpus under a CC-BY 4.0 license.
  - Downloads: 205
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - This dataset provides a cleaned Japanese corpus extracted from OSCAR-2301, containing 94 million Japanese words (181.2 GB) with several metadata files excluded due to unsuccessful cleaning.
  - Downloads: 160
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA2ZH-XS is a 30-hour, 10,000-sample paired dataset of Japanese speech and Simplified Chinese text designed for speech translation and multilingual speech understanding research, released under a CC B license.
  - Downloads: 131
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository hosts oasst2-33k-ja, a Japanese instruction tuning dataset translated from an English oasst2 subset by LLM-jp, built upon kunishou/oasst2-135k-ja data.
  - Downloads: 111
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - This repository provides a Japanese translation of the MS MARCO dataset, created using google/madlad400-3b-mt, structured like the original Hugging Face dataset, but with potentially lower translation quality compared to mMARCO.
  - Downloads: 110
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - This repository provides a Japanese translation of a cleaned subset of the bluemoon-fandom-1-1-rp dataset, utilizing the command-r-08-2024 model via the openrouter API for faster, resource-efficient, and uncensored NSFW translation.
  - Downloads: 110
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - This repository provides a Japanese translation of the GSM8K reasoning dataset, with answers extracted from translated descriptions, utilizing a quantized language model and acknowledging potential data inconsistencies.
  - Downloads: 109
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - This repository provides a deduplicated and randomized dataset of English-Japanese sentence pairs sourced from Tatoeba.org, suitable for machine translation or language learning.
  - Downloads: 101
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This repository provides a Japanese-English parallel text dataset for translation, licensed under Apache 2.0 with potential source-specific restrictions, and includes metadata detailing the origin of each text.
  - Downloads: 93
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This repository provides a sentence-aligned Japanese-English dataset of web novel chapters with metadata for document translation research, including series titles and NovelUpdates information.
  - Downloads: 91
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja is a 12,000-entry Japanese human preference dataset, translated from hh-rlhf via DeepL, comprising random samples from its harmless, helpful, and rejection-sampled training splits.
  - Downloads: 90
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset provides corrected translations and completed data for the Japanese portion of the multilingual LLAVA-Bench-in-the-wild benchmark.
  - Downloads: 80
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - This repository provides a sample of a 101,702-entry Japanese pronunciation dictionary created by linguists, intended for research and development of Japanese Automatic Speech Recognition (ASR) technology, with a link to the full paid dataset.
  - Downloads: 77
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - This repository provides a large 3.3M row Vietnamese-Japanese parallel corpus for machine translation and natural language processing tasks.
  - Downloads: 72
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl provides a Japanese information retrieval dataset converted to the BeIR format for streamlined evaluation with the mteb benchmark.
  - Downloads: 68
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This repository provides a JSON conversion of the NilanE/ParallelFiction-Ja_En-100k dataset, formatted for use with text-generation-webui in document translation and model training, containing sentence-aligned Japanese web novel chapters and English translations.
  - Downloads: 66
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - This repository provides paired Japanese and Vietnamese sentences for machine translation and language learning applications.
  - Downloads: 63
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - This repository provides a 380,000-sentence sample of a larger, paid Japanese-English parallel corpus suitable for machine translation, text analysis, and NLU tasks, excluding sensitive content.
  - Downloads: 60
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - This repository provides a free 850,000 sentence sample of a larger paid English-Japanese parallel corpus covering diverse fields like tourism and medicine, suitable for machine translation and text analysis.
  - Downloads: 56
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA is a 30-hour, 10,000-sample dataset of paired Japanese speech and Simplified Chinese text designed for speech translation and multilingual speech understanding research, released under CC BY 4.0.
  - Downloads: 56
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - This repository provides a filtered English-Japanese parallel corpus derived from Wikidata descriptions, formatted as a JSONL file optimized for training translation models with Hugging Face Transformers.
  - Downloads: 51
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual dataset for form understanding, providing human-labeled key-value pairs across seven languages to benchmark performance in form analysis.
  - Downloads: 50
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - This repository provides a sample of a 9.83 million sentence pair Chinese-Japanese parallel corpus in TXT format, covering general, IT, news, patent, and international engine domains, suitable for machine translation and text analysis.
  - Downloads: 49
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - This dataset provides the English-Japanese parallel corpus extracted from the Asian Language Treebank (ALT) project, sourced from the Hugging Face `alt` dataset, and cited in Riza et al. (2016).
  - Downloads: 49
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - This repository provides a 20,000-pair Japanese-English translation dataset generated using the Magpie technique applied to Nvidia's Nemotron-4-340B-Instruct, along with the dataset creation code, noting the potential for lower-quality records due to minimal post-filtering.
  - Downloads: 49
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - This repository provides a manually-created Japanese dataset (‚Äúliz-nojaloli-ja‚Äù) for preparing data for Reinforcement Learning from Human Feedback (RLHF), potentially referencing related code on Qiita.
  - Downloads: 47
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - This repository provides a question-answering dataset generated from the Japanese wiki40b corpus.
  - Downloads: 47
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - OPUS-MIT-5M is a balanced, 5-million sentence-pair dataset derived from the OPUS corpus, designed to facilitate robust multilingual image translation across 20 languages.
  - Downloads: 45
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Japanese LLaVA Pretrain is a Japanese-translated version of the original LLaVA dataset, enabling similar multimodal research applications in the Japanese language while adhering to the licenses of CC-3M and BLIP.
  - Downloads: 40
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - This dataset provides a Japanese translation of the FED dataset using the Google Cloud Translate API v2, noting potential inconsistencies in some dimensions due to machine translation.
  - Downloads: 40
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers the MBPP dataset translated into Japanese using LLM-jp and DeepL, created by Han, Otake, Ozaki, and Miyao, for evaluating code generation models in Japanese.
  - Downloads: 36
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a manually created, high-quality Japanese dataset of 100 Chain-of-Thought (CoT) examples, available in both connected and separated CoT/output formats, with the connected format exhibiting more natural flow.
  - Downloads: 34
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - This repository provides a Japanese translation of the MMLU dataset, created using GPT-3.5-turbo, for use in MultilingualSIFT research.
  - Downloads: 32
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP is a Japanese anime knowledge benchmark dataset comprising 100 quiz questions‚Äî20 per anime‚Äîfrom five popular series, designed to evaluate LLM understanding in Japanese.
  - Downloads: 32
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This dataset is a 49k Japanese reinforcement learning from human feedback (RLHF) collection derived from kunishou/hh-rlhf-49k-ja, excluding examples with `ng_translation == 1`.
  - Downloads: 32
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - LLM-jp‚Äôs Synthetic-JP-EN-Coding-Dataset is an instruction tuning dataset‚Äîa subset of Aratako‚Äôs 801k dataset‚Äîcreated through a Japanese collaborative project for Japanese-English coding tasks.
  - Downloads: 30
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - This repository provides a quality-improved subset of the JParaCrawl v3 English-Japanese parallel corpus, filtered using an LLM to address issues with alignment and completeness in the original 1M row dataset.
  - Downloads: 29
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - LIMA-JA is a Japanese translation of Meta‚Äôs LIMA dataset, manually refined after ChatGPT translation, and accessible via the `datasets` library for conversational AI research.
  - Downloads: 29
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This repository provides the Japanese language portion of the Guanaco dataset, similar to the alpaca-guanaco-japanese-gpt-1b dataset.
  - Downloads: 27
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This repository provides a Japanese translation of the ViQuAE dataset, focusing on translating the original answers into Japanese while leaving the answers themselves untranslated.
  - Downloads: 25
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - This repository provides a corrected Japanese translation of MT-Bench, incorporating questions from Stability AI‚Äôs Japanese MT-Bench dataset.
  - Downloads: 25
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - This repository provides the Japanese translation of the SciQ dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed under CC-BY-NC 3.0.
  - Downloads: 24
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - This dataset provides summaries for long texts sourced from the Aozora Bunko corpus, licensed under CC BY 4.0.
  - Downloads: 24
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This repository provides a multilingual translation dataset‚ÄîKorean, Chinese, and Japanese‚Äîderived from OpenOrca, matched by ID and refined using embedding similarity for best translation quality.
  - Downloads: 22
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - This repository provides a Faiss index and sentence embeddings of Japanese Wikipedia paragraphs, generated using the intfloat/multilingual-e5-base model for efficient semantic search.
  - Downloads: 22
- [jri-advtechlab/jsynflow](https://huggingface.co/datasets/jri-advtechlab/jsynflow)
  - JSynFlow is a 10,000-entry Japanese flowchart VQA dataset generated using Meta's Llama 3.1 405B, containing PNG images and JSON data describing job tasks and procedures in Mermaid format for research purposes.
  - Downloads: 21
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - This repository provides a Japanese translation of the PIQA dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed identically to the original PIQA.
  - Downloads: 20
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This repository provides a chunked, Alpaca-formatted dataset of 100k sentence-aligned Japanese web novel chapters and English translations, optimized for the augmxnt/shisa-base-7b-v1 model.
  - Downloads: 20
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - This repository provides a ~2.46M row Japanese instruction-tuning dataset in Aya format, converted from its original v1.0.0 version and licensed under CC-BY-SA 4.0.
  - Downloads: 19
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository provides 1k responses generated using DeepSeek-R1-Distill-Llama-8B on the aya-ja-evol-instruct-calm3-dpo-masked dataset with 8-bit quantization, noting potential accuracy loss and imperfect token generation for research/pre-processing purposes.
  - Downloads: 18
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - This repository provides a long-text instruction dataset built from the Aozora Bunko corpus, designed to challenge question answering performance on lengthy content without filtering for correctness, acknowledging its difficulty and dependence on the base model's capabilities.
  - Downloads: 17
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository provides the TaCo dataset used for research on improving cross-lingual transfer in low-resource language LLMs via translation-assisted chain-of-thought prompting, requiring citation if used.
  - Downloads: 16
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - This repository provides the Tanaka-corpus, a publicly available Japanese-English parallel text dataset compiled by Professor Yasuhito Tanaka and used for machine translation research.
  - Downloads: 16
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - This dataset provides Japanese-English translation pairs from the TALPCo corpus in Hugging Face format, with whitespace stemming from Japanese word segmentation removed, and licensed under CC-BY 4.0.
  - Downloads: 16
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - This repository provides a function to rigorously evaluate Japanese-to-English translations based on accuracy, completeness, grammar, and overall quality, rejecting flawed outputs.
  - Downloads: 15
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - This dataset is a 50k-sample English subset extracted from the larger 801k Synthetic JP-EN Coding Dataset, requiring review of the original dataset‚Äôs documentation for details and caveats.
  - Downloads: 14
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides the TaCo dataset for cross-lingual instruction tuning, formatted with instruction, input, and multi-lingual output pairs, as detailed in the linked research paper.
  - Downloads: 12
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - This repository provides Japanese-English translation resources licensed under Creative Commons BY 4.0.
  - Downloads: 11
### Semantic Text Processing
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - This repository provides Japanese Wikipedia embeddings and a FAISS index for RAG applications, including a Hugging Face Space demo, evaluation datasets, and comparisons of various Japanese embeddings like OpenAI‚Äôs `text-embedding-3-small` under CC-BY-SA-4.0 (except OpenAI files).
  - Downloads: 1,976
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - JGLUE is a Japanese NLU benchmark dataset, created by Yahoo Japan and Waseda University, designed to evaluate and advance general language understanding abilities in Japanese without relying on translation.
  - Downloads: 1,726
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a Japanese biomedical LLM benchmark dataset with a provided evaluation framework (med-eval) intended for assessing and improving performance in this domain.
  - Downloads: 1,153
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a Japanese text embedding benchmark comprising 24 datasets across 6 tasks for evaluating model performance on Japanese language understanding.
  - Downloads: 850
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth provides Japanese/English synthetic conversation datasets‚Äîderived from LMSYS-Chat-1M and used to post-train the Llama-3.1-Swallow and Gemma-2 models.
  - Downloads: 545
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - This repository provides a Japanese instruction-following (chat) dataset for fine-tuning large language models‚Äîlike LoRA adaptation of English LLMs‚Äîwith updates addressing licensing and data quality issues in resources like Alpaca, Wikipedia, and ALT.
  - Downloads: 461
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - This repository merges and extracts lines of 256 characters or less from the neody/c4-ja-cleaned, neody/cc100-ja-cleaned, and neody/oscar-ja-cleaned Japanese datasets.
  - Downloads: 225
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - This repository provides a dataset of Japanese Wikipedia sentences used in the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 204
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - This dataset provides simple, author-created conversational data for the ‚Äúzunda mon‚Äù character, formatted for LLM-jp and ChatGPT, intended for character LLM development and testing with a focus on respecting the license.
  - Downloads: 202
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - This repository provides a Japanese chat dataset, derived from izumi-lab/llm-japanese-dataset, designed for fine-tuning Japanese LLMs‚Äîparticularly for instruction-following and chat tasks using methods like LoRA‚Äîand details can be found in the linked research papers.
  - Downloads: 128
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - This repository provides a roughly 200GB Japanese subset of the large multilingual FineWeb2-HQ dataset, split into multiple jsonl chunks for easier handling.
  - Downloads: 117
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - This dataset provides 39.6k formatted Japanese roleplay conversations generated with gpt-4o-mini, including system messages, licensed under CC-BY-NC-SA 4.0, and prohibits use for developing competing OpenAI models.
  - Downloads: 107
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This dataset provides Japanese passage embeddings generated using llm-book/bert-base-japanese-v3-bpr-passage-encoder on the ‚ÄúAI King‚Äù competition passages (llm-book/aio-passages), licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 106
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - This dataset provides 190,854 Japanese preference labels generated using five open-source models (including Tanuki and Qwen) and judged by Qwen/Qwen2.5-72B, focusing on high-quality instructions and mitigating positional bias.
  - Downloads: 81
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - This repository provides a ggml-formatted Japanese GPT-2 model and executable for Windows, requiring both the model bin file and SentencePiece model for use with a command-line prompt.
  - Downloads: 74
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - This repository provides a clean dataset of 5.2 million Japanese sentences with context, suitable for unsupervised semantic similarity learning and related tasks.
  - Downloads: 69
- [SimpleStories/SimpleStories-JA](https://huggingface.co/datasets/SimpleStories/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, and built as an improvement upon TinyStories for NLP tasks.
  - Downloads: 67
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese temporal inference dataset designed to evaluate language model generalization, comprising templates, training, and test data categorized by tense, format, and span.
  - Downloads: 64
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - This repository provides a processed version of the ShareGPT52K dataset, converted to Markdown, with language detection and CJK/half-width whitespace adjustments using tools like polyglot and pangu.js.
  - Downloads: 57
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - This repository provides a Japanese fake news dataset converted for use with Hugging Face datasets, including article text, labels indicating truthfulness (real, partial_gpt2, full_gpt2), and character counts for real and fake portions.
  - Downloads: 54
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - This repository provides a Japanese translation of the Databricks Dolly 15k dataset, enhanced with BERTScore-calculated translation quality scores to identify and address issues like inconsistent phrasing and failed entity translations.
  - Downloads: 54
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - This repository provides document-level joined versions of the cc100/cc100-ja dataset, originally line-separated, while maintaining the original license.
  - Downloads: 51
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - This dataset provides Japanese Wikipedia paragraphs for use with the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 51
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository hosts databricks-dolly-15k-ja, a Japanese translation of the dolly-15k instruction tuning dataset created by the LLM-jp collaboration.
  - Downloads: 50
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - This dataset is a Japanese role-playing learning set‚Äîa translation of Bluemoon_Top50MB_Sorted_Fixed_ja using karakuri-lm-8x7b-chat-v0.1-awq, processed with 3-shot prompting and limited to 8000 tokens, with some records removed due to incomplete translation or LLM repetition, but containing inaccurate token count data.
  - Downloads: 49
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - llm-jp-instructions is a manually created Japanese instruction dataset, available via the `datasets` library with train, development, and test splits in version 1.0.
  - Downloads: 49
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - Kanalizer provides datasets for a library that predicts pronunciations from English words, with code available in VOICEVOX/kanalizer and pre-trained models in VOICEVOX/kanalizer-model.
  - Downloads: 48
- [ducalt/jcrrag](https://huggingface.co/datasets/ducalt/jcrrag)
  - JCrRAG is a 20,000-record Japanese benchmark dataset for evaluating the Retrieval-Augmented Generation (RAG) performance of Large Language Models, formatted as (Context, Question, Groundtruth Answer) triples.
  - Downloads: 42
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - This dataset, created by the University of Tokyo's Matsuo & Iwasawa Lab LLM course, provides human-generated inputs and outputs from two language models (watashiha-gpt-6b & Watashiha-Llama-2-13B-Ogiri-sft) for supervised fine-tuning (SFT) exercises, intended for educational and research use only.
  - Downloads: 42
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - This repository provides a continuously updated, GPT-4 generated Japanese question-answering dataset for fine-tuning open-source, non-English language models.
  - Downloads: 41
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - Shisa-base-7b-v1 is pre-trained on a dataset of 90% Japanese/10% English tokens sampled from MADLAD-400 using DSIR.
  - Downloads: 38
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - This repository provides a 69,000-record Japanese-English coding dialogue dataset generated using models like Nemotron, Phi-3, Mixtral, and Calm3 via the Magpie method, along with the generation code, noting potential quality variations due to minimal post-filtering.
  - Downloads: 36
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - This dataset contains conversational data generated by GPT-3.5-Turbo based on the Japanese Wikipedia (July 2023) dataset, and is not licensed for commercial use.
  - Downloads: 35
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - YokaiEval is a Japanese dataset of 810 multiple-choice questions designed to evaluate Large Language Models‚Äô knowledge of Japanese folklore, specifically focusing on *yokai* (supernatural creatures) across six knowledge categories.
  - Downloads: 34
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 30
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - This dataset comprises bullet-point lists generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and DeepSeek-R1-Distill-Qwen-32B-Japanese, licensed under CC-BY-SA 4.0.
  - Downloads: 30
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and soon other models) in English and Japanese, built upon TinyStories and designed for easy NLP data filtering.
  - Downloads: 30
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One designed to enhance realism, increase image complexity, and simplify anime-style illustration generation within the stable-diffusion-webui.
  - Downloads: 28
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - This dataset provides educational scores (0-4) for Japanese text from the fineweb-2 dataset, generated using the Deepseek API and a method inspired by the FineWeb-Edu classifier, comprising 280k training and 30k testing samples.
  - Downloads: 26
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - DataPilot converted the kinokokoro/ichikara-instruction-003 Japanese instruction dataset into the widely-used ShareGPT format, providing JSON Lines data ideal for fine-tuning conversational large language models.
  - Downloads: 26
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - This dataset converts the Open-Platypus-Japanese-masked format to OpenAI message format, referencing the original dataset for details and considerations.
  - Downloads: 24
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - This dataset records Pok√©mon VGC Regulation F team selection data collected from YouTube battle streams, including data from the author (trainer_id 13), and was used for a presentation at the Remote Pok√©mon Society in May 2024.
  - Downloads: 23
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 22
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - This dataset provides Japanese example sentences generated with calm3-22b, covering a range of grammatical patterns including polite forms, negation, desires, progressives, and various sentence structures for requests, permissions, obligations, and inferences.
  - Downloads: 22
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - This repository provides a filtered and modified Japanese/Chinese parallel dataset from WikiMatrix v1, enhanced by regex filtering, semantic similarity scoring (LaBSE, threshold 0.6), and Traditional to Simplified Chinese conversion using zhconv.
  - Downloads: 21
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - This repository provides a Japanese translation of the HelpSteer2 dataset, used for training and aligning large language models with NVIDIA‚Äôs SteerLM and potentially Nemotron-4-430B-Reward.
  - Downloads: 21
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - This repository provides human-generated responses to the ELYZA-tasks-100 benchmark for Japanese LLM evaluation, achieving an average GPT-4o score of 3.69 and outperforming Claude 3.5 Sonnet at 4.42 during automated scoring.
  - Downloads: 20
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese coding dialogue dataset generated using the Magpie method applied to Nvidia's Nemotron-4-340B-Instruct, along with the code used for its creation, noting potential quality variations due to minimal post-filtering.
  - Downloads: 20
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - This dataset provides a ShareGPT-formatted, Japanese language version of the OpenAssistant/oasst2 135k dataset, optimized for multi-turn conversation fine-tuning with significant computational resource requirements.
  - Downloads: 20
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - This repository provides a 26.5k Japanese instruction dataset created by applying Magpie methodology, clustering with Mini Batch K-Means, and then evolving instructions using Qwen2.5-72B, licensed primarily under Apache 2.0 with Qwen license considerations.
  - Downloads: 19
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository provides a 60k Japanese instruction dataset, Self-Instruct-Qwen2.5-72B-Instruct-60k, generated using Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8 and Self-Instruct techniques, based on and extending the Aratako/Magpie-Tanuki dataset, with licensing subject to both Apache 2.0 and the Qwen License.
  - Downloads: 17
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small is a Japanese synthetic dataset of high-quality prompts and AI outputs automatically generated using the Mistral Small 3.1 24B Instruct model, formatted as JSONL for supervised fine-tuning.
  - Downloads: 17
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - This dataset provides Japanese example sentences generated with calm3-22b, covering diverse grammatical patterns including polite forms, negation, desires, progressives, and various sentence types like requests, permissions, and opinions.
  - Downloads: 15
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset provides detailed, natural language overviews of VTubers ‚Äì including their character, activities, collaborations, and relationships ‚Äì gathered using GPT-4o Search Preview and costing $27.04 for 36,276 tokens.
  - Downloads: 15
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - This repository provides a model that generates and scores tweets, fine-tuned on both a publicly available dataset and a custom collection of 330 personality-driven tweets, with scores ranging from 10 (excellent) to 8 (meh).
  - Downloads: 15
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - This repository provides an experimental dataset of 50 queries and corresponding responses‚Äîgenerated with ChatGPT-4o and manually supplemented with patent data‚Äîfor evaluating performance across five perspectives, including patent attorney referral (excluding direct referrals in 10 queries).
  - Downloads: 13
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - This dataset comprises human-written (OSCAR) and LLM-generated (GPT-3.5 Turbo) Japanese text, created for evaluating the performance of LLMs in detecting machine-generated text.
  - Downloads: 12
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - This repository provides manually created variations of input method editor (IME)-style conversion candidate suggestion and bracket matching tasks, developed to address weaknesses in a model created for the 2024 University of Tokyo Matsuo Lab Deep Learning competition.
  - Downloads: 12
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - LLM-jp Corpus v3 (excluding Wikipedia due to its CC-BY-SA license) provides a mirrored dataset of Japanese text for large language model training and research.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This repository provides a Japanese translation of Databricks' Dolly project, licensed under CC BY-SA 3.0, utilizing and adapting content from sources like Wikipedia under the same license.
  - Downloads: 11
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - This project explores prompt extraction using the Magpie method and the rinna/llama-3-youko-8b language model, based on prompts from a research paper, despite the model not being instruction-tuned.
  - Downloads: 11
### Natural Language Interfaces
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda provides 40 Japanese-language questions‚Äîcovering history, society, government, and geography‚Äîto benchmark and rank the performance of Japanese Large Language Models.
  - Downloads: 711
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - This dataset contains 20,000 synthesized Japanese roleplay conversations (10-20 turns each) with detailed settings for genre, tags, world, characters, and dialogue tone, formatted for use with language models like DeepSeek-V3-0324.
  - Downloads: 604
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset designed to evaluate the effectiveness of Retrieval-Augmented Generation (RAG) for improving Large Language Model (LLM) answer accuracy by leveraging external knowledge retrieval.
  - Downloads: 574
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - This repository provides the Japanese question-answering benchmark dataset‚Äîused for evaluating large language models with llm-jp-eval and featured in the book *Large Language Model Introduction II*‚Äîunder the Apache 2.0 license.
  - Downloads: 266
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696 question-answer pair dataset for Japanese machine reading comprehension, based on Wikipedia contexts and designed to facilitate SQuAD-like QA research with models like BERT-Japanese (achieving 78.92% F1).
  - Downloads: 247
- [speed/relaion2B-multi-research-safe-ja](https://huggingface.co/datasets/speed/relaion2B-multi-research-safe-ja)
  - This repository provides the Japanese subset of the LAION-2B-multi-research-safe dataset, a large-scale, safe-for-research image-text dataset.
  - Downloads: 178
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - JAQKET is a Japanese open-domain question answering dataset, built from Wikipedia articles, designed to advance QA/machine reading research with multiple-choice questions across versions 1.0 and 2.0.
  - Downloads: 173
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIÁéã (AIO) is a Japanese quiz dataset (version 2.0 validation set) featuring questions with manually annotated, valid answers, including fields like question ID, competition name, timestamp, and section.
  - Downloads: 158
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The Business Scene Dialogue (BSD) dataset is a Japanese-English parallel corpus of written business conversations created through scenario writing and translation, offering a balanced representation of both languages.
  - Downloads: 140
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a Japanese question answering dataset of over 4K pharmacist licensing exam questions, answers, and commentaries from 2012-2024, now including image data, released under a CC-BY 4.0 license.
  - Downloads: 138
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository provides a Japanese translation of the everyday-conversations-llama3.1-2k dataset, formatted as topic-based dialogue pairs using DeepL, with potential minor translation imperfections and an Apache 2.0 license.
  - Downloads: 118
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval benchmark dataset of 5,000 questions and 500,000 web page titles/summaries, created using ChatGPT 3.5 and sourced from Hatena Bookmark RSS feeds to evaluate search systems with natural language queries.
  - Downloads: 110
- [AIxBlock/Japanese-short-utterances](https://huggingface.co/datasets/AIxBlock/Japanese-short-utterances)
  - AIxBlock provides a quality-assured dataset of 500k Japanese sentences for applications like speech data generation and Natural Language Processing (NLP).
  - Downloads: 104
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a Japanese dialogue corpus of approximately 14,000 conversations with associated speaker personas and personality traits, intended for research with ethical considerations regarding privacy and misuse.
  - Downloads: 96
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository provides the Kyoto University's Japanese Vicuna QA Benchmark, an 80-question dataset across 10 categories for evaluating Japanese LLM responses without references, licensed under Apache 2.0.
  - Downloads: 91
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This Japanese dataset provides translated data for dialogue summarization, sourced from both the DialogSum and CSDS datasets.
  - Downloads: 83
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a challenging Japanese visual question answering benchmark built on a food image dataset, featuring multiple-choice questions designed to test vision-language models.
  - Downloads: 80
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This repository provides 3,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential unreviewed, unusual dialogue.
  - Downloads: 76
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - JEMHopQA is a Japanese multi-hop question answering dataset with explainable derivation steps, focusing on compositional and comparison questions requiring information from linked Wikipedia articles, licensed under CC BY-SA 4.0.
  - Downloads: 70
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - This repository provides a multi-turn Q&A dataset automatically generated using Mixtral-8x22B with data sourced from oasst2, databricks-dolly, minnade, and cyberagent/chatbot-arena-ja, adhering to respective licenses.
  - Downloads: 66
- [jaeyong2/Qwen3-06B-Ja-DPO](https://huggingface.co/datasets/jaeyong2/Qwen3-06B-Ja-DPO)
  - This repository provides a 100k Japanese question-answering dataset with answer candidates generated and evaluated using Qwen models (0.6B & 14B) derived from hotchpotch/japanese-qa-reasoning-100k and kuotient/orca-math-word-problems-193k-korean.
  - Downloads: 59
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - This dataset is a manually cleaned, Japanese translation of the WikiHow-NFQA question-answering dataset.
  - Downloads: 58
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This repository provides a growing, manually-created dataset designed for training Japanese chatbots.
  - Downloads: 54
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository provides the 1.56B token Japanese text dataset (AKU-d_ms-0.5B-v0.1) used for pre-training the AKU-series chat model, comprising data from publicly available sources with their respective licenses (CC-BY-4.0, CC-BY-SA-4.0) and processing scripts.
  - Downloads: 51
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [llm-jp/llm-jp-chatbot-arena-conversations](https://huggingface.co/datasets/llm-jp/llm-jp-chatbot-arena-conversations)
  - This dataset provides approximately 1,000 Japanese conversations with human pairwise preferences collected from the LLM-jp Chatbot Arena, including model names, transcripts, votes, and metadata like language and OpenAI API usage.
  - Downloads: 46
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This repository provides a sample of the Nexdata Japanese Conversational Speech dataset, featuring approximately 1000 speakers engaging in natural, manually-transcribed dialogues on diverse topics recorded via mobile phone.
  - Downloads: 40
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - This repository provides a multi-turn Q&A dataset automatically generated using Calm3-22b from open-source data including oasst2, databricks-dolly, and minnade, with various licenses (Apache 2.0, CC-BY-SA-3.0, CC0, CC-BY-4.0).
  - Downloads: 40
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a 4,246-dialogue Japanese dataset for multi-domain task-oriented dialogue research, featuring Wizard-of-Oz collection, six domains, and annotations for dialogue state tracking and goal-oriented conversations.
  - Downloads: 39
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - This dataset contains Japanese conversations extracted from public-domain books in Aozora Bunko using a heuristic approach to identify utterances within quotation marks, with code provided for reproduction.
  - Downloads: 39
- [hal-utokyo/MangaVQA](https://huggingface.co/datasets/hal-utokyo/MangaVQA)
  - MangaVQA is a benchmark dataset of 526 manually-created question-answer pairs designed to evaluate manga understanding using images from Manga109.
  - Downloads: 38
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This dataset offers over 80,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia using Llama2Pro8B, with automated screening but potential for unusual dialogue.
  - Downloads: 38
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - This repository provides a Japanese QA dataset derived from Stack Overflow, featuring processed question-answer pairs with markdown formatting, code block handling, and base64 image replacement, offered in both detailed and simplified subsets.
  - Downloads: 34
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - This dataset consists of high-quality, 96k-context responses regenerated by Qwen2.5-72B-Instruct from excellent inputs extracted from Aratako/Magpie-Tanuki-8B, and is subject to both Apache 2.0 and Qwen license restrictions for model training.
  - Downloads: 34
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - This dataset provides question-answer pairs derived from the Japanese Stack Exchange data dump, with processed markdown text, base64 image replacement, and includes IDs for questions, answers, accepted, and popular responses, offered in both default and simplified formats.
  - Downloads: 32
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - This repository provides a filtered and original Japanese dialogue corpus sourced from role-playing forum conversations, excluding threads with single participants or very short posts.
  - Downloads: 32
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - This dataset provides 11,808 multi-turn conversational instructions about Japanese photos, generated using GPT-4o via the Azure OpenAI API from the ‚Äújapanese-photos‚Äù Hugging Face dataset.
  - Downloads: 31
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - This repository provides a Japanese translation of the Open-Orca dataset, currently about 20% complete, and is available for commercial use.
  - Downloads: 31
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - Tengentoppa is a large Japanese instruction-following dataset created by merging 16 diverse sources, formatted as JSON with instruction, input, and output fields, and built using code available in the linked `dataset-processor` repository.
  - Downloads: 30
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - mc4 is a massively multilingual machine translation dataset containing 100+ languages built from Common Crawl, designed for training and evaluating translation models.
  - Downloads: 29
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - This dataset provides approximately 39,600 synthetic Japanese roleplay conversations generated with gpt-4o-mini, each with detailed settings (genre, tags, world/scene/character setups, tone) and formatted for easy model training via system message creation.
  - Downloads: 27
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench is a Japanese multimodal QA dataset featuring geometry problems from entrance exams, comprising text, images, and corresponding answers for evaluating AI reasoning.
  - Downloads: 27
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 is a Japanese (ja-JP) VQA dataset derived from shunk031/JDocQA, containing 200 single-image question-answer pairs extracted from the test split, focused on practical use with reduced size.
  - Downloads: 24
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - This dataset contains approximately 1,000 synthetic Japanese roleplay dialogues, each with 10 turns, generated using Nvidia's Nemotron-4-340B-Instruct and Magpie, potentially including lower-quality records and a tendency to prematurely end longer conversations.
  - Downloads: 24
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - This repository provides a commercially-usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of the Orion-14B Series Models Community License and acknowledging computing resources from ABCI.
  - Downloads: 23
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - This dataset removes prompts matching those from the chatbot-arena-ja-calm2-7b-chat dataset.
  - Downloads: 22
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese instruction tuning dataset created by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, along with the dataset creation code, noting the absence of post-filtering and potential for lower-quality records.
  - Downloads: 22
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - This repository provides Japanese chatbot conversations generated using the Swallow-MX-8x7b model, based on single-turn prompts from the Chatbot Arena Conversations JA dataset (CC-BY 4.0 license) and translated with a Facebook translation model.
  - Downloads: 22
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - This repository provides a commercially-usable, multi-turn conversation dataset generated from Japanese Wikipedia using the Orion14B-Chat model, subject to the Orion-14B Series Models Community License and computed using ABCI resources.
  - Downloads: 21
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW is a synthetic evaluation dataset for benchmarking Japanese language model role-playing abilities, featuring key metadata like genre, setting, and character information, and built using Claude 3.5 Sonnet outputs.
  - Downloads: 21
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset provides 60,000 commercially-usable, multi-turn conversations generated from Japanese Wikipedia using Llama2Pro-8B, with automated screening but potential for unrefined dialogue, and is available on Hugging Face.
  - Downloads: 20
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - This repository provides a filtered subset of the Aozora Bunko Japanese text dataset, specifically containing texts written in modern orthography (Êñ∞Â≠óÊñ∞‰ªÆÂêç).
  - Downloads: 19
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - This dataset is a Japanese translation of the ‚Äúdatabricks-dolly-15k‚Äù dataset with sentence endings modified to ‚Äúnyan!‚Äù using ArrowPro-7B-KUJIRA, inheriting the original dataset's license and intended for stylistic alteration, not performance improvement.
  - Downloads: 17
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This repository provides 3,000 commercially usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential unreviewed, and occasionally strange, dialogue.
  - Downloads: 16
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - This dataset is a Japanese translation of the WikiHowNFAQ dataset, providing question-answer pairs for natural language question answering.
  - Downloads: 16
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - This repository provides a Japanese multi-turn conversation dataset, generated with Qarasu14B from Wikipedia data for non-commercial use and compatible with Axolotl, built using FreeAI Ltd.'s computational resources.
  - Downloads: 15
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - AI-generated Turkish and Japanese subtitle data, created with Gemini 2.0 Flash, is provided for chatbot training despite potential errors and unsuitability for translation AI.
  - Downloads: 15
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - This dataset comprises human-checked and corrected instructions for open-source LLMs, with outputs generated using Swallow-MX, though it may contain inaccurate answers and was created during the LOCAL AI HACKATHON #000.
  - Downloads: 14
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - This dataset provides simple Japanese example sentences created using the `if001/elementray_m calm3-22b` model, covering various grammatical patterns like polite forms, negatives, desires, and more.
  - Downloads: 14
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - This repository provides a Japanese language subset of the OpenAssistant Conversations Dataset, formatted as paired human-assistant messages, potentially lacking full conversational context per row.
  - Downloads: 13
### Text Generation
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - This repository provides a clustered Japanese text corpus of approximately 10,000 entries, created by cleaning web corpora like mc4-ja and applying unsupervised learning, intended for information analysis purposes with attention to partial Parquet format and Git LFS download requirements.
  - Downloads: 867
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese language benchmark, built from translated MMLU questions and culturally-specific prompts, to evaluate large language model performance in Japanese understanding.
  - Downloads: 729
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual NL-to-Code benchmark with 1,707 human-written test cases across English, Spanish, Japanese, and Russian, designed for evaluating open-domain execution-based code generation.
  - Downloads: 358
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard provides a Japanese RAG performance evaluation across five industries‚Äîfinance, IT, manufacturing, public sector, and retail‚Äîassessing Parser, Retrieval, and Generation components with a publicly available dataset for informed RAG solution consideration.
  - Downloads: 317
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 236
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This dataset from YANS-official/ogiri-bokete provides crawled data from the Bokete humor site‚Äîpart of the CLoT-Oogiri-Go dataset‚Äîfor three tasks: image-to-text, text-to-text, and text-image-to-text generation, totaling 500 images and 2355 responses as of August 30th.
  - Downloads: 212
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - This repository extracts text snippets of 256 characters or less from the cleaned OSCAR-JA dataset.
  - Downloads: 114
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository provides a Japanese preference dataset for Reinforcement Learning from Human Feedback (RLHF), created by scoring and ranking multiple responses generated from language models (Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter2 & Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8) based on a synthetic instruction dataset (Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k), and is subject to META LLAMA 3.1, Gemma, and Qwen licenses.
  - Downloads: 72
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - This 801k synthetic dataset enhances the original Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k with diverse, Evol-Instruct generated, Japanese/English code instructions and responses, including model provenance (Nemotron, Phi-3, Mixtral, Calm3) and evolution history.
  - Downloads: 63
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15/T23 is a 50,000-sentence simplified Japanese corpus with English translations, designed for research in automatic text simplification and Japanese-English translation using a 2,000-word vocabulary.
  - Downloads: 60
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - This 5.2K instruction dataset focuses on programming tasks‚Äîcode generation, behavior check, and bug fixing‚Äîsourced from commercially licensed and permissively authorized learning content, including a mix of original and translated (English to Japanese) examples.
  - Downloads: 60
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - This repository provides a templated dataset of ~40 Japanese downstream tasks‚Äîwith up to 20,000 samples per task‚Äîdesigned for instruction tuning LLMs using natively-written, non-translated Japanese data, including 0-shot and few-shot examples.
  - Downloads: 55
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - This repository provides a dataset of approximately 3000 Japanese children's stories generated by GPT-4o-mini, utilizing only simple vocabulary and based on the methodology detailed in the linked research paper.
  - Downloads: 48
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This dataset from Bokete, a Japanese humor site, provides text and image-based challenge data for three tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîtotaling 56 images/prompts and 112 responses, derived from the CLoT-Oogiri-Go dataset.
  - Downloads: 43
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - This dataset converts a subset of the LLM-JP-Corpus-v3 into Hugging Face format, adding article titles sourced from original URLs where available, all under a CC-BY 4.0 license.
  - Downloads: 39
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - This Japanese preference dataset, built upon llm-jp/magpie-sft-v1.0, consists of chosen/rejected response pairs judged by google/gemma-2-27b-it, comparing original Qwen/Qwen2.5-32B-Instruct outputs with those regenerated by Aratako/Llama-Gemma-2-27b-SFT-trial1, and is subject to META LLAMA 3.1, Gemma Terms of Use, and Qwen licensing (requiring attribution if used to train a model).
  - Downloads: 38
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends the CommonCatalog CC-BY dataset with English and Japanese captions generated by Phi-3 models, enabling easy integration via the 'photoid' primary key and commercial use under the CC BY license.
  - Downloads: 36
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This dataset is a Japanese translation of the Open_o1_sft_Pro dataset, generated using Qwen2.5-14B-Instruct, and formatted with user/assistant conversations including thoughts, adhering to the original dataset's license.
  - Downloads: 32
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - This dataset contains question-answer pairs generated using a Japanese LLM (CyberAgent's DeepSeek-R1-Distill-Qwen-32B-Japanese) based on Japanese Wikipedia content, and is released under the CC-BY-SA 4.0 license.
  - Downloads: 31
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - This repository presents a JSON dataset (reazonspeech-all-wada-snr.json) containing SNR values and transcriptions from analysis of the reazon-research/reazonspeech-v2[all] dataset, including a count of 1,208,360 entries with SNR values exceeding 100, and acknowledges AiHUB for providing computational resources.
  - Downloads: 29
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - This dataset provides 3,907 three-line summaries of Livedoor News articles, formatted with prompts for Llama v2 and recommending the addition of `[R_START]` and `[R_END]` as special tokens for training.
  - Downloads: 26
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This dataset facilitates evaluation of large language models for *okashiri* (humorous response) generation, encompassing both text-to-text and image-to-text tasks with corresponding IDs, file paths, and task types.
  - Downloads: 25
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - This repository provides automatically generated Q&A data sourced from Common Crawl using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, employing random text excerpts to minimize reliance on original content, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 24
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository provides a Japanese preference dataset for SimPO training, created by scoring and ranking responses generated from Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter1 using Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, and is subject to the META LLAMA 3.1 COMMUNITY LICENSE, Gemma Terms of Use, and Qwen licensing.
  - Downloads: 23
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - This repository provides automatically generated Japanese Q&A data sourced from both team-created data and Common Crawl, using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, with randomized text excerpts to reduce reliance on original sources, and recommends data cleaning due to potential unnatural phrasing.
  - Downloads: 22
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - This dataset converts level 2 filtered data from the llm-jp-corpus-v3 warp_html into Hugging Face format, adding article titles sourced from original URLs, and is licensed under CC-BY 4.0.
  - Downloads: 21
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - This repository provides a Japanese RLHF dataset, reformatted from open_preference_v0.1 as a classification task with chosen/rejected sentence labels (1/0), acknowledging potentially lower text quality due to synthetic and translated content.
  - Downloads: 18
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - This repository provides a Japanese-English glossary of generative AI terminology, intended to improve translation quality with models like GPT-4, though accuracy isn't guaranteed.
  - Downloads: 18
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset (BCP-47 ja-JP) intended for developing advanced text generation models.
  - Downloads: 18
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This dataset provides question data generated using Qwen/Qwen2.5-32B-Instruct via ollama, intended for building thinking models, but cautions against fine-tuning due to potentially non-Apache 2.0 licensed answers generated by Mistral Large.
  - Downloads: 17
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - This repository provides a prototype dataset, generated by Deepseek-V3-0324, designed to improve adherence to constrained system prompts in language models, with code and data licensed under MIT.
  - Downloads: 17
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset provides Japanese translations of English quotes from the original Hugging Face dataset, generated using the llm-jp/llm-jp-3-3.7b-instruct model and licensed under CC BY 4.0.
  - Downloads: 16
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This repository provides a Japanese translation of the NLVR (Natural Language for Visual Reasoning) dataset, originally available at https://github.com/lil-lab/nlvr.
  - Downloads: 14
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - This Japanese dataset, built using magnum-v4-12b, provides 10K-100K question-answer pairs labeled with "evil" and "justice" for both classification and generation tasks, released under the Apache-2.0 license.
  - Downloads: 14
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - This repository provides a Japanese dataset generated by ELYZA-japanese-Llama-2 for evaluating AI text detection and self-instruct methods, based on instructions from the GPT-4-Self-Instruct-Japanese dataset.
  - Downloads: 14
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset combines null-instruct-ja and DeepSeek-v2.5 (q4) models, generated using ollama and 7 A5000 GPUs in under 2 hours, and is licensed under the DeepSeek license.
  - Downloads: 13
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - This repository provides 1k responses generated using DeepSeek-R1-Distill-Qwen-32B on the aya-ja-evol-instruct-calm3-dpo-masked dataset with 8-bit quantization, exhibiting issues with `<think>` token generation and limited practical use.
  - Downloads: 13
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - This repository provides a Japanese dataset generated by Qwen/Qwen1.5-14B for evaluating AI text detection and facilitating self-instruct learning.
  - Downloads: 12
- [EQUES/AIME24-ja](https://huggingface.co/datasets/EQUES/AIME24-ja)
  - This repository provides Japanese translations of the aimo-validation-aime dataset, generated with ChatGPT-4o, featuring a unique 0-30 index differing from the standard AIME24 format.
  - Downloads: 12
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset provides Japanese instruction data, translated using KUJIRA, focused on investment, Berkshire Hathaway, and Warren Buffett, originally from the glaive-ai's in-foxhound dataset.
  - Downloads: 12
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository provides a dataset of all *Keitai Ookii* comedy challenge prompts and responses‚Äîsourced from a blog archive‚Äîstructured with IDs, episode numbers, prompts, and lists of responses.
  - Downloads: 12
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - This dataset contains synthetic instruction-following data generated using the CALM3-22B-Chat model and Japanese Wikipedia text, aiming for reduced hallucination but acknowledging its potential presence and unfiltered raw output.
  - Downloads: 12
### Syntactic Text Processing
- [preference-team/dataset-for-annotation-v2-annotated](https://huggingface.co/datasets/preference-team/dataset-for-annotation-v2-annotated)
  - This dataset provides human-annotated preferences‚Äîincluding intensity scores from -3 to 3‚Äîfor paired question-response sets in Japanese, covering diverse topics like general knowledge, history, medicine, coding, and creative writing, while noting limited overall and per-genre variation.
  - Downloads: 206
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - This repository provides Japanese datasets structured for contrastive learning with Sentence Transformers, filtered using Rerank scores and formatted as (anchor, positive/negative) pairs from sources like LLM-JP-Kaken and MS Marco-JA.
  - Downloads: 186
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - This dataset contains 2139 human-evaluated question-answer pairs and ratings from LLMChat, a system used to benchmark 13 LLMs (including Tanuki and Llama-3) by comparing their responses in a pairwise, human-preference setting between August 19-25, 2024.
  - Downloads: 100
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This repository integrates data from erai-raws and MyAnimeList for over 2056 anime, providing IDs and resources like Bangumi, RSS feeds, and tracking links for platforms including AniDB, Kitsu, and MAL, with publication dates detailed for 500+ entries.
  - Downloads: 93
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - This repository provides a dataset of approximately 150,000 Danbooru tag pairs‚Äîlinking English tags to Japanese translations‚Äîfiltered using fastText and the Calm3 LLM to improve accuracy and ensure at least one Japanese translation per tag.
  - Downloads: 90
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - This repository provides Japanese text data extracted from PDF documents found on Common Crawl, along with the extraction code.
  - Downloads: 84
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k is a 97,269-record Japanese dialogue dataset created by applying the Magpie method to weblab-GENIAC/Tanuki-8B-dpo-v1.0, potentially containing low-quality records due to lack of post-filtering.
  - Downloads: 79
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - This repository provides a cleaned, ~950-example Japanese mathematics question-answer dataset translated from MetaMathQA using RekaAI/reka-flash-3, focusing on format correction while leaving content accuracy unchecked.
  - Downloads: 63
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWiki is a Japanese Wikipedia text dataset extracted from HTML dumps (as of January 1, 2024) providing clean, structurally-preserved text and accompanying data/scripts for NLP tasks, with each record representing an article containing its title, text, and segmented paragraphs.
  - Downloads: 62
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - This repository provides a 26,728-example Japanese SFT dataset, derived from Aratako/Magpie-Tanuki-8B and regenerated with Qwen-2.5-turbo, intended for fine-tuning small Japanese chat LLMs focused on information seeking, reasoning, planning, and editing‚Äîexcluding coding.
  - Downloads: 53
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - This repository provides a multilingual dataset released under the MIT license.
  - Downloads: 51
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - This dataset annotates the Aratako/Magpie-Tanuki-8B-97k dataset‚Äîcreated by applying the Magpie method to Tanuki-8B‚Äîwith difficulty, quality, and category labels using cyberagent/calm3-22b-chat and specific prompting for each annotation type.
  - Downloads: 48
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - This dataset provides approximately 2800 high-quality (beauty score 87+) images of a generated virtual girlfriend (versions 2.1 & 2.6) designed to address portrait rights concerns, with over 1000 images scoring 90+ for beauty‚Äîa notably large collection for training AI models.
  - Downloads: 42
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - This repository provides a modified parsing and chunking method for Wikipedia data, built upon and pre-processed with a fork of singletongue/wikipedia-utils, using data crawled between December 5-8, 2023.
  - Downloads: 41
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - This repository provides the first 300k entries of a Japanese image-caption dataset, augmenting captions from alfredplpl/commoncatalog-cc-by-ja with images resized to 512px, with licensing details available in the source repository.
  - Downloads: 41
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - Alpaca-jp-python is a Japanese synthetic dataset created using the Stanford Alpaca method and Mistral AI's Mixtral-8x22B-Instruct-v0.1, refined with the same model, and available under an Apache 2.0 license via Deepinfra and the `datasets` library.
  - Downloads: 37
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - This repository provides a CBZ-formatted manga dataset from Nhentai‚Äîcontaining adult content‚Äîfor research in image analysis (segmentation, detection) and Japanese text recognition.
  - Downloads: 27
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - This repository provides a furigana dataset derived from Aozora Bunko and SAPIE braille data, with 307 validation-corrected mismatches identified in the original text.
  - Downloads: 24
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - This repository provides a 280-image dataset of female illustrations generated with NijiJourney v5, intended for LoRA model training and merging, including some copyrighted characters, with accompanying tags and a disclaimer against misuse.
  - Downloads: 24
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja is a 525k instruction-tuning dataset consisting of automatically translated Japanese versions of the open-source, quality-assured ApolloCorpus, specifically focusing on English medical question-answering data, with noted potential translation errors for LLM applications.
  - Downloads: 23
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This dataset, ‚ÄúTrait Circus,‚Äù provides automatically extracted and controlled vocabulary-based trait data for fungi, currently for casual use only, as described in a non-peer-reviewed article and potentially containing errors.
  - Downloads: 23
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - This repository provides a Japanese question-answering dataset of approximately 1,300 manually created pairs focused on Databricks, sourced from its official blog, FAQs, and Qitta posts by Databricks employees.
  - Downloads: 21
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Japanese LLaVA v1.5 Instruct 620K is a 620K-image Japanese translation of the LLaVA v1.5 Visual Instruct dataset, intended for Japanese language visual instruction tuning under a CC BY-NC-4.0 license.
  - Downloads: 21
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - This repository provides a validated furigana dataset derived from the National Diet Library's bibliographic data, with 5064 inconsistencies corrected.
  - Downloads: 21
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 20
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - This dataset contains freely reusable quiz questions sourced from Quiz Forest as of August 5, 2024, suitable for RAG, document search, and other applications, with a permissive license allowing commercial use and redistribution while respecting Quiz Forest and related parties.
  - Downloads: 19
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - This repository likely hosts resources related to the popular Japanese character "Chiikawa" („Å°„ÅÑ„Åã„Çè), potentially including assets or related projects inspired by the "hachiwari" creator.
  - Downloads: 18
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - This repository provides automatically generated question-answer pairs created using Mixtral-8x22B, derived from various openly licensed (CC-BY-SA 3.0/Apache 2.0) datasets and designed with low textual similarity to the source material, though cleaning is recommended.
  - Downloads: 17
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - This repository scrapes metadata (excluding full text) from the Dengeki Bunko novecomi website.
  - Downloads: 16
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthetic dataset, created with the Evol-Instruction method and Mistral-8x22B, based on Stanford Alpaca's seed tasks, and hosted with Deepinfra under an Apache 2.0 license.
  - Downloads: 16
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - This dataset contains freely reusable quiz questions sourced from Quiz Works as of August 4-5, 2024, suitable for RAG and document retrieval system development.
  - Downloads: 15
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - This repository provides stickers featuring "Ruri" as a personal project for enjoyment.
  - Downloads: 15
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSEC„Éõ„Éº„É†„Éö„Éº„Ç∏ is a website providing information and resources related to Japanese Security Conferences (JSEC).
  - Downloads: 14
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, formatted as JSON and licensed under Apache-2.0, designed for training and evaluating large language models.
  - Downloads: 14
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - This repository provides a refined dataset of 50,000 Delite posts by data creator t_w, corrected and restructured for embedding learning, with usage allowed for training but redistribution prohibited under Japanese law.
  - Downloads: 13
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - This repository provides a processed dataset of 50,000 Delite posts by t_w, optimized for embedding learning, with usage permitted for training but redistribution prohibited under Japanese law.
  - Downloads: 12
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - This repository provides speech data for Chiaki Nanami from the *Danganronpa* series.
  - Downloads: 11
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data is provided for unspecified use.
  - Downloads: 11
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - This repository provides a 3-turn multi-turn instruction dataset generated with Qwen/Qwen2.5-32B-Instruct-AWQ, containing mixed English and Chinese records requiring potential filtering, and leveraging instructions from Aratako/Magpie-Tanuki-8B-annotated-96k.
  - Downloads: 11
### Responsible NLP
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended solely for research purposes and requiring copyright permissions for other uses‚Äîwith gratitude to IPA (Information-technology Promotion Agency).
  - Downloads: 420
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository provides the Japanese Web Corpus 2010, a dataset for research use only, automatically punctuated with morphological analysis and includes conversion scripts.
  - Downloads: 368
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset comprises manually extracted FAQs from Japanese government websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models and serving as a linked resource.
  - Downloads: 361
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This dataset provides filtered Japanese Wikipedia typo data‚Äîspecifically kanji conversion errors‚Äîsplit into pre- and post-error text segments for use with Hugging Face models.
  - Downloads: 184
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - This repository provides text regenerated with Phi-3 from large (tens of GB) datasets‚ÄîWikibooks, Wikipedia, Cosmopedia, and legal case data‚Äîpotentially requiring Git LFS for full download, and utilizing supercomputing resources for computation.
  - Downloads: 106
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 78
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource provision.
  - Downloads: 77
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - This repository provides a deduplicated and preprocessed Japanese question-passage dataset (mqa) with cleaned text and NFKC normalization, where passage IDs correspond to indices within the `collection` file.
  - Downloads: 67
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - This dataset contains query-answer pairs generated by an LLM based on paraphrased text from Japanese Wikipedia, and is released under the CC-BY-SA 4.0 license.
  - Downloads: 61
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - This repository provides a large, automatically translated Japanese-English corpus generated by Phi-3 from Japanese texts sourced from Wikibooks, Wikipedia, and code, requiring Git LFS for full access due to its size (tens of GB).
  - Downloads: 61
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - This repository provides the Japanese prompts from the GuanacoDataset, identified using language detection.
  - Downloads: 60
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - This dataset contains Japanese translations of English Wikipedia text generated using DeepSeek-R1-Distill-Qwen-32B, including input/output pairs and few-shot examples, released under a CC-BY-SA 4.0 license.
  - Downloads: 55
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - This dataset contains Japanese sentences generated by Phi-3 based on ConceptNet 5.7 triples, aiming to express relationships between subjects, relations, and objects.
  - Downloads: 44
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - This dataset converts the oasst1-89k-ja Japanese dataset into a ShareGPT-formatted, multi-turn conversation structure suitable for fine-tuning large language models (requiring significant computational resources).
  - Downloads: 43
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - This repository provides automatically generated multi-turn dialogue data created with Calm3-22B-chat, based on randomly selected text from the Aozora Bunko library, specifically using a cleaned version of "I Am a Cat."
  - Downloads: 38
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst2 dataset, automatically translated with DeepL and formatted for instruction/output-based fine-tuning of language models, with provided conversion code.
  - Downloads: 34
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - This repository provides publicly available models and datasets under a license requiring users to agree to terms prohibiting enjoyment of expressed ideas/feelings and acknowledging no warranty or liability from the publisher.
  - Downloads: 32
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - This repository provides a deduplicated and preprocessed subset of the mmarco query-passage dataset, with IDs referencing corresponding passages in the `collection` subset for easy access, adhering to the original dataset's license.
  - Downloads: 24
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - This repository provides a dataset of English and Chinese translations for item descriptions and HS codes used by Japan Post for international shipping, based on data published May 9, 2024.
  - Downloads: 21
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 20
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - This repository provides a low-quality dataset of 1002 examples for enabling Python function calling in chat LLMs, generated with Qwen2.5 and Phi-4, and containing potential issues with missing/Chinese tools and repetitive/low-quality responses.
  - Downloads: 16
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset comprises 1243 carefully selected tweets (May 2022 - May 2024) known for expressing nuanced ideas or unique perspectives, intended for fine-tuning language models‚Äîspecifically for tasks involving personality and tweet generation with fixed system prompts and inputs.
  - Downloads: 15
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This repository provides a Japanese-only subset of the Open Assistant dataset, sourced from the timdettmers/openassistant-guanaco Hugging Face dataset.
  - Downloads: 14
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the LLM-JP 3.13B Instruct model, featuring instructions, optional Chain-of-Thought reasoning, and self-refined responses in JSON format.
  - Downloads: 14
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset provides question-answer pairs about characters from the Touhou Project's Tokama Club, suitable for training chatbots and question answering/machine learning systems.
  - Downloads: 14
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - This repository provides publicly released models and datasets subject to a license agreement prohibiting commercial use and requiring adherence to legal compliance and responsible distribution.
  - Downloads: 14
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - This repository provides publicly available models and datasets under a license requiring users to agree to terms prohibiting enjoyment of expressed ideas/emotions, accepting no warranty or liability from the publisher, and ensuring legal compliance in usage and sharing.
  - Downloads: 14
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This repository provides a 16,000-sample Japanese instruction-following dataset, automatically generated using Qwen2.5 72B, containing instructions, reasoning steps, initial responses, and refined answers in JSONL format for LLM training and evaluation.
  - Downloads: 13
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset contains cleaned dialogue from the anime ‚ÄúMy Next Life as a Villainess,‚Äù featuring lines spoken by characters like Ray and Claire, intended for use with a character-based AI assistant.
  - Downloads: 12
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - This repository provides automatically generated Japanese question-answer pairs for RAG systems, sourced from Wikibooks, Wikipedia, and case law data, intended for pre-training rather than direct instruction following.
  - Downloads: 12
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use, disclaiming warranty, and requiring adherence to all applicable laws.
  - Downloads: 12
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - This repository provides publicly available models and datasets under a license prohibiting commercial use, offering no warranty or liability, and requiring adherence to legal compliance by users and any recipients.
  - Downloads: 11
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and requiring adherence to legal compliance and responsible disclosure.
  - Downloads: 11
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - This repository provides publicly released models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permission for other uses, with gratitude to IPA for resource provision.
  - Downloads: 11
### Reasoning
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a high-quality, assured-correctness Japanese mathematical dataset generated by translating English datasets (PRM800K, GSM8K) and providing chain-of-thought reasoning.
  - Downloads: 158
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - JSNLI is a Japanese translation of the standard SNLI natural language inference benchmark, provided in TSV format with morphologically analyzed text using JUMAN++.
  - Downloads: 151
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - JSQuAD is a Japanese reading comprehension dataset, based on SQuAD 1.1 and using the 20211101 Japanese Wikipedia dump, designed for evaluating and reproducing scores with a clone of SB Intuitions.
  - Downloads: 120
- [DataPilot/Zero_SFT_Ja_v3_Reasoning](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v3_Reasoning)
  - DataPilot/Zero_SFT_Ja_v3_Reasoning is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created using the Qwen3-235B-A22B model, formatted as JSONL for training and reasoning tasks.
  - Downloads: 118
- [jaeyong2/Reason-Qwen3-14B-Ja](https://huggingface.co/datasets/jaeyong2/Reason-Qwen3-14B-Ja)
  - This repository provides a 100k Japanese question answering & reasoning dataset, evaluated with Qwen/Qwen3-14B, and acknowledges support from the TPU Research Cloud program.
  - Downloads: 99
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a human-crafted Japanese dataset of multi-turn conversations and passages for training and evaluating logical reasoning capabilities, demonstrated with Qwen2.5-7B models on Japanese MT-Bench.
  - Downloads: 91
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset is a commercially usable, small-scale, high-quality Japanese dataset for commonsense and mathematical question answering, built upon existing datasets and licensed under DbCL v1.0.
  - Downloads: 90
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - JCommonsenseQA is a Japanese multiple-choice question answering dataset, based on CommonsenseQA and ConceptNet, designed to evaluate commonsense reasoning abilities and publicly cloned from yahoojapan/JGLUE.
  - Downloads: 90
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - This dataset, ‚Äúabc-multiple-choice,‚Äù provides multiple-choice questions from the ‚Äúabc‚Äù quiz competition for Japanese question answering research, with non-commercial usage rights held by abc/EQIDEN.
  - Downloads: 84
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - This repository provides a commercially-usable, 1.8 million instruction-tuning dataset ‚Äì a Japanese translation of OpenMathInstruct-1 ‚Äì comprising math questions from GSM8K and MATH, paired with Mixtral-8x7B generated solutions validated for correctness, under a permissive NVIDIA license.
  - Downloads: 56
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - OpenO1-SFT (Japanese Translation) is a 77,312-sample dataset of Japanese-translated Chain of Thought reasoning examples, derived from the original OpenO1-SFT and intended for fine-tuning language models.
  - Downloads: 45
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - This repository provides a high-quality, ~1800-example Japanese instruction-following, reasoning, and answer dataset generated using the Qwen/Qwen2.5-32B-Instruct model, based on SkunkworksAI/reasoning-0.01.
  - Downloads: 40
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - This repository provides a 17k synthetic instruction dataset for Japanese mathematical reasoning, generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct and filtered for consistency across two distinct system prompts ‚Äì one for logical assistance and one for Python code execution.
  - Downloads: 39
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - This repository provides a 125,000-sample Japanese instruction-following dataset, automatically generated using the Qwen2.5-32B-instruct model with Chain-of-Thought reasoning and multi-persona prompting, formatted as JSONL under the Apache-2.0 license.
  - Downloads: 36
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - This dataset contains Japanese question-answer pairs, reasoning traces, and corresponding text passages generated by DeepSeek-R1, derived from the fineweb2-edu-japanese dataset and licensed under ODC-By.
  - Downloads: 36
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository provides a high-information-density, multi-turn Japanese conversational dataset, synthetically created from the Cosmopedia dataset, showcasing reasoning, knowledge, and interactive dialogue.
  - Downloads: 28
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suite‚Äîbased on and extending the FraCaS dataset‚Äîfor evaluating recognizing textual entailment through premise-hypothesis pairs and assessing entailment, neutrality, or contradiction.
  - Downloads: 26
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - This dataset contains OpenAI message-formatted data extracted from DeL-TaiseiOzaki's magpie-reasoning-llama-nemotron-70b-100k, filtering out entries lacking "ÊîπËâØ" (improvement) in the refined_answer column.
  - Downloads: 25
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - This dataset consists of synthetic instruction-following data generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, filtered to include only records where responses from two distinct system prompts (logic/math assistant vs. Python programmer) matched.
  - Downloads: 23
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - This repository offers a 50k-subset of the NuminaMath CoT dataset, enhanced with reasoning steps and designed to improve multistep, reflective problem-solving in Japanese language models.
  - Downloads: 22
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - JCQ is a Japanese NLP dataset comprising 700 creative thinking questions‚Äîinspired by the Torrance Test and Zhao et al. (2024)‚Äîassessing abilities like unusual uses, consequences, and hypothetical scenarios.
  - Downloads: 22
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - This repository provides a 100k-sample Japanese translation of the NuminaMath CoT dataset‚Äîcontaining math problems and Chain of Thought solutions‚Äîcreated using the Gemma 2B language model.
  - Downloads: 21
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - This repository provides a 100k question-answering dataset in Japanese, generated using Gemini Pro 2.5, and licensed under the Open Data Commons Attribution family.
  - Downloads: 19
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench is a Japanese reasoning benchmark using challenging 2023 Kyoto University math entrance exam questions to evaluate advanced problem-solving skills in Large Language Models.
  - Downloads: 13
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa is a 210-question, multi-turn Japanese language benchmark evaluating reasoning across seven diverse domains‚Äîmath, writing, coding, understanding, grammar, culture, and general logic‚Äîwith 30 questions per category.
  - Downloads: 13
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE (JNLI) is a Japanese Natural Language Inference dataset for evaluating understanding of relationships ‚Äì entailment, contradiction, or neutral ‚Äì between premise and hypothesis sentences.
  - Downloads: 13
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - This dataset comprises 200 simplified instruction prompts derived from Kendamarron/jimba-instuction-1k-beta, created to replicate the "Wizard LM" In-depth evolving process as a result of the LOCAL AI HACKATHON #000 collaboration.
  - Downloads: 13
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Responsible & Trustworthy NLP
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec is a machine learning framework and dataset for gender detection from Japanese names, detailed in a research paper accepted at ISDA'23, intended solely for research purposes.
  - Downloads: 107
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - This repository releases a 42k Japanese-English parallel dataset‚Äîa subset of Swallow-Magpie-Ultra-v0.1‚Äîdesigned for instruction tuning of TokyoTech‚Äôs Llama-3.1-Swallow language models.
  - Downloads: 60
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset is a Japanese language dataset for evaluating and mitigating toxicity in large language models, available at the provided link.
  - Downloads: 52
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - This dataset provides a 10,341-hour sample of unsupervised Japanese speech across 28 domains, offering quality-tested data for improving AI performance while prioritizing user privacy and legal compliance.
  - Downloads: 43
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA is a 1402-question Japanese red-teaming dataset designed to evaluate LLM vulnerability to generating harmful responses through adversarial testing, containing potentially offensive content.
  - Downloads: 42
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - JaNLI is a Japanese natural language inference dataset, inspired by HANS, built to challenge models with nuanced Japanese linguistics and reveal their weaknesses.
  - Downloads: 35
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - This repository provides a dataset of questions and answers from the Japanese National Medical Licensing Examination (NMLE) from the 110th to 117th exams, intended for model evaluation, RAG applications, and research‚Äîlicensed under CC-BY-NC-ND 4.0 for non-commercial use.
  - Downloads: 21
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository provides a dataset detailing unit states and game information‚Äîincluding IDs, timestamps, classes, layers, locations, and team affiliations‚Äîfor a competitive game with blue and red teams.
  - Downloads: 15
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - This repository provides access to code and instructions for a dataset intentionally kept private to prevent its inclusion in large language model training.
  - Downloads: 15
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - This repository contains pairwise comparison data of LLMChat model responses, generated using various models, to evaluate the consistency between human and open LLM automated evaluations, with licensing details mirroring the original team-hatakeyama-phase2/LLMChat dataset.
  - Downloads: 14
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - Cauldron-JA is a Japanese vision-language dataset created by translating the Idefics2 fine-tuning dataset, The Cauldron, excluding OCR, coding, and graph-related subsets to preserve data consistency.
  - Downloads: 46,341
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This repository provides a Japanese translation of the Databricks Dolly-15k dataset, licensed under CC-BY-SA-3.0.
  - Downloads: 657
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst1 dataset, flagging unsuccessful translations ("ng_translation=1" indicating identical English/Japanese text), with recent manual corrections to code-related translation errors and a chat-formatted subset available.
  - Downloads: 87
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka is a dataset of Japanese "kaidan" (ghost stories) connected to the Hyakumonogatari ritual, offering resources for exploring Japanese folklore and supernatural tales.
  - Downloads: 68
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository provides a collection of ~40 natively Japanese, non-machine-translated datasets for downstream tasks, formatted for LLM instruction finetuning.
  - Downloads: 42
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - This repository provides a 260k-sentence parallel corpus of Japanese laws, extracted from the Japanese-English Legal Parallel Corpus, for machine translation and legal NLP tasks.
  - Downloads: 41
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN is a deduplicated, Japanese-translated version of the CT-RATE dataset, offering chest CT volumes and radiology reports to support Japanese medical AI development.
  - Downloads: 33
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - This repository provides a Japanese-Korean paired text dataset from Helsinki-NLP/Tatoeba-Challenge for training translation models, excluding commercial use.
  - Downloads: 30
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset provides 69K Japanese translations of the databricks-dolly-15k dataset, licensed under CC BY SA 3.0, for ja-en translation tasks.
  - Downloads: 25
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated Japanese dataset derived from a subset of ultrachat_200k, containing 6537 training and 995 test samples, with IDs linking back to the original data.
  - Downloads: 17
### Information Retrieval
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - This dataset provides reranking scores‚Äîusing five multilingual/Japanese rerankers‚Äîfor positive and negative example documents associated with existing Japanese search and QA datasets, including average scores for each reranker.
  - Downloads: 395
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - This repository provides AutoWikiQA, a large-scale, diverse Japanese question-answering dataset generated from Wikipedia text using Swallow-MX, suitable for QA model training and Retrieval-Augmented Generation research.
  - Downloads: 270
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This repository provides the passage dataset used in the ‚ÄúAI King‚Äù competition featured in the book *Introduction to Large Language Models*, sourced from cl-tohoku/quiz-datasets and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 86
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - This dataset provides QA pairs for training document retrieval models, sourced from cl-tohoku/quiz-datasets and used in the book *Large Language Models Introduction*, with a mix of licensing including CC BY-SA 4.0, CC BY-SA 3.0, and GFDL for quiz questions and Wikipedia passages.
  - Downloads: 60
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - This dataset provides JSONL-formatted metadata for YouTube channels, labeled as either VTuber (1) or non-VTuber (0), to support text classification tasks for VTuber identification, primarily in Japanese with potential class imbalance.
  - Downloads: 56
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - This dataset provides Japanese question-answering pairs with corresponding Wikipedia article retrievals performed by human annotators, detailing both the data structure and collection process.
  - Downloads: 49
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset provides automatically translated Japanese versions of records 20k-100k from the Cosmopedia-100k dataset, excluding those with translation errors, and will be merged with/removed after the 0-20k translation is complete.
  - Downloads: 46
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - This repository provides 1 million synthetic, multi-turn chat examples generated from long-context LLMs rewriting web text, designed for continued pre-training and research into data/internet culture‚Äîbuilding upon the Refined-Anime-Text dataset.
  - Downloads: 29
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - Jawiki-20220404-c400 is a Japanese Wikipedia passage dataset (‚â§400 characters) utilized for question answering baselines, like those in the AIÁéã competition.
  - Downloads: 11
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - This dataset archives 11 years of historical comments from the now-defunct "Niconico Realtime Commentary" service, preserving a valuable record of user discussions before its transition within Niconico Live Broadcasting.
  - Downloads: 1,116,681
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100 is a 100-example Japanese instruction-tuning evaluation dataset featuring complex tasks‚Äîincluding summarization, reasoning, and creative generation‚Äîwith annotated evaluation criteria for assessing helpful and polite AI assistant outputs.
  - Downloads: 3,144
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - This dataset provides a binary sentiment classification version of the WRIME Japanese sentiment analysis dataset, labeled as positive/negative based on average reader sentiment, and intended for use with the "Large Language Model Introduction" book's sample code.
  - Downloads: 502
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - This repository provides the Sayoko TTS corpus, an 81-year-old female voice dataset with both noisy (wav_noise) and noise-reduced (wav) audio files, alongside phoneme labels for speech synthesis tasks, downloadable via Google Drive or Hugging Face Hub.
  - Downloads: 147
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - This dataset adapts the databricks-dolly-15k instruction-following dataset to emulate the emotionless speaking style of Yuki Nagato from "The Melancholy of Haruhi Suzumiya" by modifying polite Japanese speech patterns.
  - Downloads: 19
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - This repository provides a Japanese instruction dataset created by manually checking and correcting outputs from the calm2-7b-chat model, detailed in the linked Zenn article.
  - Downloads: 16
### Linguistics & Cognitive NLP
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - This repository provides refined English translations of helpful responses sourced from the `chosen` dataset within the `helpful-base` component of the anthropics/hh-rlhf project, improved by filtering and correcting outputs from the fuguMT translation model.
  - Downloads: 36
- [asahi-research/newsq](https://huggingface.co/datasets/asahi-research/newsq)
  - NewsQ is a free, Japanese question-answering benchmark for current events, distributed via Hugging Face under a terms of use agreement requiring user registration and data submission for access and usage monitoring.
  - Downloads: 32
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - NewsQ is a free Japanese question-answering benchmark for current events, available on Hugging Face under a usage agreement requiring applicant information and consent for data handling practices.
  - Downloads: 21
