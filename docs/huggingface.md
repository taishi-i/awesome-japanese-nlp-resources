# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1256 models and 467 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents üìñ

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).

Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Text Generation](#Text-Generation)
	 * [Multimodality](#Multimodality)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Reasoning](#Reasoning)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## The latest additions üéâ

**Models**
17 models have been added.

- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)


**Datasets**
4 datasets have been added.

- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)


## Models üß†

This list is sorted by downloads as of March 11, 2025.
1256 models are listed.

### Semantic Text Processing
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - A BERT base model pretrained on Japanese text using IPA dictionary word-level tokenization followed by WordPiece subword tokenization.
  - Downloads: 3,866,048
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - A fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice, CSS10, and JSUT data, sampled at 16kHz.
  - Downloads: 2,360,257
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - A fine-tuned xlm-roberta-base model for Japanese named entity recognition, trained on Stockmark Inc's Wikipedia-based dataset.
  - Downloads: 643,746
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - A DeBERTa V3 model specialized for Japanese with respect for word boundaries but avoiding multi-word tokenization in inference.
  - Downloads: 359,109
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - A refined Japanese sentence-BERT model (v2) using MultipleNegativesRankingLoss for improved accuracy compared to v1.
  - Downloads: 274,172
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - A BERT base Japanese model pretrained using Unidic-lite with whole word masking on CC-100 and jawiki-20230102 datasets.
  - Downloads: 242,596
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - A Japanese CLOOB model for image recognition, trained by rinna Co., Ltd., using ViT-B-16 architecture.
  - Downloads: 238,420
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - A BERT base Japanese model pretrained with character-level tokenization and whole word masking using jawiki-20200831 texts and Unidic 2.1.2 dictionary.
  - Downloads: 137,123
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - A BERT-based Japanese model pretrained using IPA dictionary tokenization and whole word masking.
  - Downloads: 122,962
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - A BERT-base Japanese model pretrained with word-level tokenization based on IPA dictionary followed by character-level tokenization, using cl-tohoku/bert-japanese pretraining codes.
  - Downloads: 112,047
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - A BERT base Japanese model pretrained with whole word masking on CC-100 and jawiki-20230102 using character-level tokenization and Unidic 2.1.2 dictionary.
  - Downloads: 111,071
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - A Japanese Sentence-BERT model (version 1), with version 2 offering improved precision, for sentence embedding using the transformers library.
  - Downloads: 106,740
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - A gguf-format conversion of qwen2.5-bakeneko-32b-instruct for use with ggerganov's llama.cpp, sourced from imatrix-dataset-for-japanese-llm.
  - Downloads: 77,491
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese text embedding model based on LUKE, trained on mixed data for general-purpose sentence vector similarity and semantic tasks.
  - Downloads: 50,992
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - A DistilBERT model pre-trained on Japanese web text by LINE Corporation, based on their in-house BERT-base teacher model.
  - Downloads: 49,322
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a fine-tuned Japanese language model with 8192 context length, released under Apache License 2.0.
  - Downloads: 47,518
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets, available for masked language modeling.
  - Downloads: 44,668
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is an Apache v2.0 licensed LLaMA-based 13B model pre-trained on English and Japanese data, released by Preferred Networks for text generation tasks.
  - Downloads: 43,174
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - A Japanese sentence-LUKE model trained on the same dataset as Japanese Sentence-BERT, showing improved qualitative accuracy compared to Japanese Sentence-BERT.
  - Downloads: 30,798
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B-instruct is a Japanese LLM based on Mistral-7B-v0.1 that excels in Japanese language understanding benchmarks and performs competitively in English tests compared to models like OpenCalm, Elyza, Youri, Nekomata, and Swallow.
  - Downloads: 27,163
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is an enhanced 8-billion parameter Japanese language model based on Meta-Llama-3, trained by ELYZA, Inc., for use as a Japanese-speaking assistant.
  - Downloads: 26,986
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - A fine-tuned BERT model for Japanese sentiment analysis on Amazon product reviews.
  - Downloads: 23,032
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - The Japanese version of LUKE, a pre-trained language model with knowledge-enhanced contextual embeddings, includes Wikipedia entity embeddings for specialized tasks.
  - Downloads: 22,097
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - A Japanese finetuned model based on DeepSeek-R1-Distill-Qwen-14B for causal language modeling.
  - Downloads: 21,248
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - A gguf-formatted model derived from cyberagent's DeepSeek-R1-Distill-Qwen-14B-Japanese for Japanese language processing.
  - Downloads: 18,621
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - A BERT base model pretrained on Japanese text using Unidic-lite and WordPiece tokenization with whole word masking.
  - Downloads: 17,605
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository offers a finetuned Japanese GPT-NeoX-3.6b model optimized for instruction-following in conversational agents with distinct training data splits.
  - Downloads: 17,398
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow is a series of large language models enhanced for Japanese while retaining English capabilities, built through continual pre-training on Meta Llama 3.1 with extensive Japanese corpus data.
  - Downloads: 17,149
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model fine-tuned on Databricks Dolly-15k and other datasets, designed for advanced natural language processing tasks.
  - Downloads: 16,792
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - The repository provides Japanese text embeddings using Sentence Transformers, requiring installation of specific libraries and noting the need for text prefixes during inference.
  - Downloads: 14,590
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model with added speaker diarization and punctuation features, integrated as a pipeline in the Hugging Face Transformers library.
  - Downloads: 14,066
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository contains a gguf-formatted version of the Japanese cyberagent-DeepSeek-R1-Distill-Qwen-32B model.
  - Downloads: 11,573
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - A Japanese DeBERTa V2 large model pre-trained on specific datasets with character-level tokenization, suitable for masked language modeling.
  - Downloads: 11,127
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a BERT-based multilingual sentence encoder trained for 109 languages, combining masked and translation language modeling.
  - Downloads: 10,677
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - A Japanese DeBERTa V2 tiny model pre-trained on specific datasets, available for masked language modeling.
  - Downloads: 10,588
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository offers an extra-small Japanese GPT-2 model, providing resources and instructions for integration via the `transformers` library.
  - Downloads: 10,343
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - An ELECTRA model pretrained on mC4 Japanese corpus and fine-tuned by spaCy v3 on UD_Japanese_BCCWJ, distributed as a Python package named ja_ginza_electra.
  - Downloads: 10,342
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - This repository offers a 130 million parameter ModernBERT model for Japanese, trained by SB Intuitions on a large corpus using RoPE and other modern improvements.
  - Downloads: 9,702
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - A 36-layer, 2816-hidden-size Japanese GPT-NeoX model with 3.6 billion parameters, pre-trained on 312.5B tokens from Japanese datasets to achieve a validation perplexity of 8.68.
  - Downloads: 9,635
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper v2.0 is a collection of distilled Whisper models for Japanese ASR using OpenAI's Whisper large-v3 as the teacher model and includes punctuation pipeline.
  - Downloads: 9,560
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - The ELYZA-japanese-Llama-2-7b model is a Japanese-enhanced Llama-2 7B model for conversational AI applications, with detailed instructions and usage examples provided.
  - Downloads: 8,512
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - This repository provides Japanese embeddings using the Ruri model from Sentence Transformers, requiring installation of specific libraries and including a usage example.
  - Downloads: 8,286
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - A Japanese HuBERT Base model containing 12 transformer layers, trained on about 19,000 hours of Japanese speech from the Reazon corpus.
  - Downloads: 8,228
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a continuely pre-trained model based on Meta's Llama 3, expanded with Japanese data, offering Instruct versions through SFT and Chat Vector releases scheduled for July 1, 2024.
  - Downloads: 8,102
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository offers a medium-sized Japanese GPT-2 model for text generation, accessible via transformers library.
  - Downloads: 8,065
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - A Japanese finetuned model based on DeepSeek-R1-Distill-Qwen-32B for causal language modeling.
  - Downloads: 7,854
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - A gguf-formatted version of the Moonlight-16B-A3B-Instruct model for use with ggerganov's llama.cpp, including instructions for testing and running.
  - Downloads: 7,752
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - A gguf-format conversion of the deepseek-r1-distill-qwen2.5-bakeneko-32b model for use with ggerganov's llama.cpp, based on imatrix-dataset-for-japanese-llm.
  - Downloads: 7,448
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow is a series of enhanced large language models, built through continual pre-training on Meta Llama 3.1, with improved Japanese capabilities while maintaining English proficiency.
  - Downloads: 7,278
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository contains series of Japanese Rerankers (CrossEncoders) with different model sizes and parameters for ranking text relevance.
  - Downloads: 7,246
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository offers the llm-jp-3-13b-instruct2 model from the LLM-jp-3 series, part of large language models developed by the National Institute of Informatics, with required torch and Transformers library versions.
  - Downloads: 7,069
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository provides the llm-jp-3-13b-instruct3 model, part of a series developed by the National Institute of Informatics for large language processing in Japanese.
  - Downloads: 6,973
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - A Sentence-BERT model based on Studio-oussei/Luke-Japanese-Base-Lite, trained for 1 epoch with jsnLI data, mapping Japanese sentences to 768-dimensional vectors for clustering and semantic search.
  - Downloads: 6,911
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - A Japanese RoBERTa base model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 6,878
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and new size labels "xl" and "xxl".
  - Downloads: 6,776
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitions provides Japanese autoregressive language models, including sarashina2.2-0.5B-instruct-v0.1, evaluated on Qwen tasks and MT benchmarks.
  - Downloads: 6,660
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - A BERT large Japanese model pretrained using Unidic-lite tokenization and whole word masking on CC-100 andjawiki-20230102 datasets.
  - Downloads: 6,412
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - A 12-layer wav2vec 2.0 Base model trained by rinna Co., Ltd. on about 19,000 hours of Japanese speech.
  - Downloads: 6,223
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository offers a base-sized Japanese RoBERTa model, including instructions for loading it using the transformers library.
  - Downloads: 5,983
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãVecteus-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 5,901
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - A gguf-formatted version of lightblue's DeepSeek-R1-Distill-Qwen-7B-Japanese model for use with ggerganov's llama.cpp.
  - Downloads: 5,730
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets with character-level tokenization, suitable for masked language modeling.
  - Downloads: 5,562
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository offers a small Japanese GPT-2 model for text generation, accessible via the transformers library.
  - Downloads: 5,342
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - Llama-3.1-70B-Japanese-Instruct-2407 Model Description This is a Japanese continually pre-trained model based on meta-llama/Meta-Llama-3.1-70B-Instruct.
  - Downloads: 5,149
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - A gguf format conversion of RakutenAI-2.0-mini-instruct, compatible with llama.cpp for cooking recipe generation.
  - Downloads: 5,086
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository offers a range of large language model variants, including beta and final versions up to 172B parameters, developed by the National Institute of Informatics.
  - Downloads: 4,966
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - A series of Japanese reranking models (CrossEncoders) with varying layer and hidden size configurations, including x-small, small, base, and large versions.
  - Downloads: 4,856
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository offers various large language models, including llm-jp-3-1.8b, llm-jp-3-3.7b, llm-jp-3-13b, and llm-jp-3-172b-beta1 variants, along with their instruct versions.
  - Downloads: 4,720
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a continually pre-trained model derived from Meta Llama 3, primarily augmented with Japanese data, available in Instruct versions through supervised fine-tuning.
  - Downloads: 4,713
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - The GitHub repository rinna/youri-7b contains code for continual pre-training a 32-layer, 4096-hidden-size LLaMA-2-7B model on Japanese and English datasets, enhancing performance on Japanese tasks.
  - Downloads: 4,675
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - A Japanese HuBERT Large model containing 24 transformer layers, trained on about 19,000 hours of Japanese speech from the Rea corpus.
  - Downloads: 4,388
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - A gguf-formatted version of the RakutenAI-2.0-8x7B-instruct model, created from imatrix data and usable with llama.cpp.
  - Downloads: 4,326
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow is a series of large language models enhancing Japanese language capabilities while maintaining English proficiency through continual pre-training on Meta Llama 3.1 with a dataset of about 200 billion tokens.
  - Downloads: 4,281
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - A Japanese sentence-LUKE model trained on the same dataset and configuration as JapaneseSentence-BERT, showing comparable to slightly higher quantitative accuracy and better qualitative results, utilizing studio-ousia/luke-japanese-base-lite for prettraining.
  - Downloads: 4,118
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - AXCXEPT-phi-4-deepseek-R1K-RL-EZOÁöÑggufÊ®°ÂûãÂ∫ìÔºåÁî±imatrixÊï∞ÊçÆÈõÜÊîØÊåÅÔºåÂèØÈÄöËøállama.cppÂÖãÈöÜÂíåËøêË°å„ÄÇ
  - Downloads: 4,108
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese-enhanced Llama-2 7B model for causal language tasks, with additional fine-tuning, using a provided system prompt and tokenizer.
  - Downloads: 3,831
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1 billion-parameter model pre-trained on English and Japanese data, featuring a hybrid Samba architecture with normalized layers for efficiency and performance.
  - Downloads: 3,821
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese text embedding model using RoFormer distillation for sentence encoding, capable of semantic similarity measurement and retrieval with a maximum sequence length of 1024 tokens.
  - Downloads: 3,661
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is an extended Japanese-language model derived from Llama2 through additional pre-training, with usage demonstrated via Python code.
  - Downloads: 3,221
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - The ELYZA-japanese-Llama-2-13b model is a Japanese-enhanced Llama 2 version for causal language tasks, featuring additional pre-training and ready for use with provided prompts.
  - Downloads: 3,192
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - A BERT base Japanese model pretrained with character-level and word-level tokenization and whole word masking.
  - Downloads: 3,149
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - A 7-billion-parameter Japanese language model derived from Mistral-7B-v0.1, focused on enhancing Japanese language modeling and downstream task performance.
  - Downloads: 3,135
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - A pre-trained Japanese DeBERTa V3 base model for masked language modeling.
  - Downloads: 3,131
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - A quantized Qwen2.5 Bakeneko 32B instruction-tuned language model compatible with llama.cpp apps, available in AWQ and GPTQ formats.
  - Downloads: 3,058
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - A fine-tuned BERT model for Japanese semantic similarity calculation using the cl-tohoku/bert-base-japanese-v3 and JGLUE's JSTS dataset.
  - Downloads: 2,983
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - The repository contains models for Japanese processing, including Qwen-14B and Qwen-32B, licensed under MIT.
  - Downloads: 2,938
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - A Japanese RoBERTa base model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 2,860
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a Japanese text embedding model based on Sarashina2.1-1B, trained with multi-stage contrastive learning to achieve state-of-the-art results in semantic tasks and maps texts to 1792-dimensional vectors.
  - Downloads: 2,791
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository contains large language models, including variants ranging from 1.8B to 172B parameters, developed by the National Institute of Informatics, with support from GENIAC.
  - Downloads: 2,750
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA's Japanese Llama-2 7B Fast Instruct model in GGUF format.
  - Downloads: 2,732
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - The repository provides the llm-jp-3-7.2b-instruct3 model, part of a series developed by NII for large language processing in Japanese.
  - Downloads: 2,713
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - A gguf-formatted distillation of the r1-1776 Distill LLaMA-70B model, along with instructions for using it in the llama.cpp environment.
  - Downloads: 2,705
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - The ELYZA-japanese-Llama-2-7b model is an enhanced Japanese language model derived from Llama2 through additional pre-training, enabling advanced text generation in Japanese.
  - Downloads: 2,692
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70-billion-parameter language model fine-tuned for Japanese tasks based on Llama-2-70B.
  - Downloads: 2,678
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository contains a series of large Japanese Reranker (CrossEncoder) models with various layers and hidden sizes for ranking and re-ranking text results.
  - Downloads: 2,651
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-Chat is a decoder-only language model pre-trained on large Japanese and English datasets, requiring transformers version ‚â• 4.34.1 for usage.
  - Downloads: 2,641
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese-enhanced Llama-2 model for causal language modeling, trainable via provided Python code.
  - Downloads: 2,612
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - The repository provides the llm-jp-3-1.8b-instruct3 model, part of a series developed by the National Institute of Informatics for large language processing tasks.
  - Downloads: 2,594
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - A 7-billion-parameter Japanese instruction-following language model based on Japanese Stable LM Base Gamma 7B.
  - Downloads: 2,580
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B-GGUF is an enhanced Japaneselanguage version of Meta-Llama-3-8B-Instruct, quantized for improved efficiency.
  - Downloads: 2,535
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter pretrained language model developed by Stockmark Inc., based on a large Japanese corpus, with an instruction-tuned version also available.
  - Downloads: 2,494
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri is a Japanese text embedding model usingSentence Transformers, requiring installation of specific libraries and proper prefixes for input.
  - Downloads: 2,465
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - The repository provides the OpenCALM-1B model, a Japanese-language decoder-only model from CyberAgent, Inc., along with usage examples for loading and running inference.
  - Downloads: 2,453
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instruct is a Japanese-enhanced Llama 2 model for instructional tasks, using additional pretraining and provided with a default system prompt.
  - Downloads: 2,443
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository contains a gguf-formatted version of the Fugaku-LLM-13B-instruct model, along with conversion scripts and usage instructions.
  - Downloads: 2,429
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - An experimental Japanese-speaking vision-language model that generates descriptions of images.
  - Downloads: 2,350
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow is a series of large language models enhanced for Japanese while retaining English capabilities, built through continual pre-training on Meta Llama 3.1 with a corpus of over 200 billion tokens.
  - Downloads: 2,340
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA's Japanese Llama-2 7B Fast GGUF format model, with added Japanese vocabulary for faster processing.
  - Downloads: 2,324
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This is a fine-tuned Japanese version of the DeepSeek-R1-Distill-Qwen-14B model, outputting thought processes in Japanese.
  - Downloads: 2,277
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fast is a Japanese-enhanced Llama 2 model for causal language modeling, detailed in a blog post.
  - Downloads: 2,270
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitions provides Japanese autoregressive language models, including sarashina2.2-0.5B-instruct-v0.1, evaluated on multiple tasks in both Japanese and English.
  - Downloads: 2,184
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho's gguf-formatted Japanese novel GPT-J-6B model for use with llama.cpp, subject to potential incompatibility with future updates.
  - Downloads: 2,152
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - This repository offers a 1.3B-parameter Japanese GPT model trained by rinna Co., Ltd., including instructions for model usage and integration.
  - Downloads: 2,133
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - The GitHub repository contains a quantized Japanese version of Meta's Llama 2 model (4.11GB), designed for faster execution at the cost of some performance.
  - Downloads: 2,010
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - The weblab-10b-instruction-sft-GPTQ repository houses a quantized 10-billion-parameter multilingual GPT-NeoX model tailored for Japanese, weighing in at 6.3 GB and offering faster execution at the potential cost of some inference performance.
  - Downloads: 2,004
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository offers enhanced descriptions for the Japanese-stablelm-3b-4e1t-base GGUF model from Stability AI, noting current limitations in layer support for GPU offloading.
  - Downloads: 1,995
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository offers model weights for the hubert-base, an encoder-type speech embedding model trained on JTubeSpeech, suitable for tasks like speech recognition.
  - Downloads: 1,928
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8 billion parameter, fine-tuned dialogue-oriented large language model derived fromÁ∫¶1.3T token pre-training, using_quantization techniques, and developed by a community of contributors for the GENIAC LLM project.
  - Downloads: 1,849
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository offers a 3.6 billion parameter Japanese language model from LINE Corporation, along with instructions for usage via PyTorch.
  - Downloads: 1,826
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - A RoBERTa model pre-trained on Japanese Aozora texts for UPOS tagging, derived from roberta-small-japanese-aozora.
  - Downloads: 1,825
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - AXCXEPT-phi-4-open-R1-Distill-EZOv1ÁöÑggufÊ†ºÂºèÊ®°ÂûãÔºåÂèØÈÄöËøággerganov/llama.cppÂ∑•ÂÖ∑‰ΩøÁî®„ÄÇ
  - Downloads: 1,814
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - A finely-tuned BERT model for Japanese sentiment analysis based on cl-tohoku/bert-base-japanese-v3 and MARC-ja dataset, compatible with Hugging Face's `transformers` library.
  - Downloads: 1,802
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - The repository offers a 2.7B-parameter Japanese GPT-NeoX model trained by ABEJA for text generation using transformers v4.23+.
  - Downloads: 1,750
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight transformer-based Japanese language model trained from scratch for efficient performance in resource-constrained environments, serving as the backbone for instruct models.
  - Downloads: 1,711
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - A Japanese SimCSE model based on BERT-base, trained on JSNLI, for extracting sentence embeddings, compatible with sentence-transformers and requiring fugashi and unidic-lite for tokenization.
  - Downloads: 1,697
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - A gguf-format conversion of llm-jp-3-13b-instruct3 by llm-jp, using imatrix dataset, with limitations on custom chat templates.
  - Downloads: 1,688
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - The repository offers the llm-jp-3-150m-instruct3 model, part of a series developed by the National Institute of Informatics for Japanese large language models.
  - Downloads: 1,683
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - Quantized versions of Google's gemma-2-2b-jpn-it model in GGUF format, along with conversion instructions and usage notes.
  - Downloads: 1,670
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - A gguf-formatted distilled version of DeepSeek-R1-Distill-Qwen-7B for use with ggerganov's llama.cpp, utilizing imatrix-dataset-for-japanese-llm.
  - Downloads: 1,659
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - The repository contains the 150M parameter size model from the LLM-jp-3 series developed by the National Institute of Informatics for large language processing.
  - Downloads: 1,652
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - A gguf format distillation of DeepSeek-R1-Qwen-32B, created by deepseek-ai, for use with ggerganov's llama.cpp, requiring specific configuration and commands to run.
  - Downloads: 1,650
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - A gguf conversion of the 1.7B Japanese-large-LM instruction-SFT model by line-corporation.
  - Downloads: 1,618
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese-language re-ranking model that can be used with Sentence Transformers for inference after installation.
  - Downloads: 1,599
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository offers the llm-jp-3-980m-instruct3 model from the National Institute of Informatics' Research and Development Center for Large Language Models, with required torch and transformers versions specified.
  - Downloads: 1,568
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - A Japanese continually pre-trained language model based on Mistral-Nemo-Instruct-2407, requiring updated transformers installation for usage.
  - Downloads: 1,549
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository offers a small Japanese GPT-NeoX model, updated for compatibility with HuggingFace's transformers library.
  - Downloads: 1,543
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - The repository offers the llm-jp-3-440m-instruct3 model, part of a series developed by the National Institute of Informatics for Japanese large language models, with required libraries and checkpoints in Hugging Face format.
  - Downloads: 1,535
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - A gguf-format conversion of the 1.7B Japanese large language model by line-corporation, including instructions for usage and conversion.
  - Downloads: 1,524
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - japanese-stablelm-base-alpha-7b is a 7B-parameter language model pre-trained for Japanese language and task performance.
  - Downloads: 1,502
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - A large Japanese RoBERTa model pretrained on Wikipedia and CC-100, suitable for masked language modeling.
  - Downloads: 1,497
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - A gguf-formatted version of the Llama-3.1-Swallow-8B-Instruct-v0.3 model by tokyotech-llm, created from imatrix-dataset-for-japanese-llm and compatible with llama.cpp for instruction-based use.
  - Downloads: 1,485
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - A gguf format conversion of the DeepSeek-R1-Distill-Qwen-14B model for use with ggerganov's llama.cpp, including instructions for cloning and running.
  - Downloads: 1,477
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This GitHub repository contains a quantized version of the Mistral-Nemo-Japanese-Instruct-2408 model, optimized for use with llama.cpp and applicable for Japanese natural language processing tasks.
  - Downloads: 1,472
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - Model Card For llm-jp-3-1.8b-instruct-gguf LLM-jp„Åï„Çì„ÅÆllm-jp-3-1.8b-instruct„ÇíÈáèÂ≠êÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åü„Å°„Åß„Åô„ÄÇ
  - Downloads: 1,466
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - The repository provides the llm-jp-3-980m model from the National Institute of Informatics' Research and Development Center for Large Language Models.
  - Downloads: 1,452
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a Japanese language large language model that excels in Japanese benchmarks and maintains competitive performance in English tests, leveraging the Mistral architecture.
  - Downloads: 1,432
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - The MakiArt/jp-ModernBert-base-preview is a 150M parameter BERT-like model for Japanese text processing, trained onÁ∫¶300B tokens with context length up to 8192 and vocab size of 50,368, utilizing algomatic team's resources.
  - Downloads: 1,418
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - A BERT base model pretrained in Japanese, specifically enhanced with financial corpus data.
  - Downloads: 1,382
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - The repository provides the llm-jp-3-980m-instruct2 model, part of a series developed by the National Institute of Informatics for large language processing in Japanese, with Hugging Face Transformers format and specific library requirements.
  - Downloads: 1,380
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - A 3B-parameter decoder-only language model optimized for Japanese, derived from continued pretraining of the English-based StableLM-3B-4E1T on Japanese data.
  - Downloads: 1,376
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - The GitHub repository contains continually pretrained Llama 3 Youko 8B, a model fine-tuned on Japanese and English datasets that significantly improves performance on Japanese tasks.
  - Downloads: 1,315
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository contains a GGUF-formatted version of ELYZA's Japanese-trained Llama-2-7b instruct model, with links to other variants including Fast and Codella versions.
  - Downloads: 1,310
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - LYZA's Japanese Llama-2 13B fast instruct model in gguf format, offering a highly optimized version with added Japanese vocabulary for reduced token cost and faster performance.
  - Downloads: 1,278
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - A 36-layer Japanese GPT-NeoX model fine-tuned via RLHF for instruction-following conversations.
  - Downloads: 1,276
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository contains a Japanese typo detector model based on RoBERTa that outputs the probability of each character being incorrect, categorized into various types of errors.
  - Downloads: 1,263
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - japanese-stablelm-2-instruct-1_6b-gguf stabilityai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãjapanese-stablelm-2-instruct-1_6b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,241
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - The GitHub repository hosts a lightweight Japanese version of LUKE, a pre-trained language model with knowledge-enhanced contextual embeddings, omitting Wikipedia entity embeddings.
  - Downloads: 1,214
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - A Japanese DeBERTa V2 large model pre-trained on specific datasets, usable for masked language modeling.
  - Downloads: 1,205
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievertBERT is a pre-trained Japanese Transformer Encoder based on Megatron-LM with PreNorm and bug-fixes for model parameters.
  - Downloads: 1,197
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a fine-tuned Japanese LLM optimized for instruction-following tasks with enhanced fluency and context understanding.
  - Downloads: 1,184
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-centric fine-tuned model based on Qwen/Qwen2.5-32B-Instruct with enhanced instruction-following performance through ChatVector post-training.
  - Downloads: 1,178
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B/chat is a Japanese language model based on Mistral-7B-v0.1 that excels in Japanese understanding benchmarks and performs competitively in English tests compared to models like OpenCalm, Elyza, Youri, Nekomata, and Swallow.
  - Downloads: 1,174
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA's Japanese CodeLlama-7b instruct model in gguf format.
  - Downloads: 1,170
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - A DeBERTa V2 pre-trained on Japanese texts, available for masked language modeling using the transformers library.
  - Downloads: 1,121
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - A quantized gguf version of gemma-2-2b-it with many Japanese words for improved Japanese support, optimized for faster execution using speculative decoding from the latest llama.cpp.
  - Downloads: 1,104
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - A model based on the bert-base-japanese-v3 using unsupervised SimCSE fine-tuned on jawiki-sentences for feature extraction.
  - Downloads: 1,099
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - Weighted and iMatrix quants of Mistral-Nemo-Japanese-Instruct-2408 are available in GGUF format, with notes on usage and sorting by size.
  - Downloads: 1,029
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - A 36-layer, 2816-hidden-size GPT-NeoX model fine-tuned for instruction-following conversational tasks in Japanese.
  - Downloads: 1,022
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository contains quantized GGUF format model files for Stability AI's Japanese StableLM Instruct Beta 70B, supported by an a16z grant and mass computation resources.
  - Downloads: 1,020
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA's Llama-3-ELYZA-JP-8B-AWQ is an 8-billion-parameter language model optimized for Japanese use, featuring AutoAWQ quantization while retaining Meta-Llama-3's architecture.
  - Downloads: 1,011
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - This repository offers a 3.6B parameter fine-tuned Japanese language model by LINE Corporation for instruction tuning, along with instructions and code examples for usage.
  - Downloads: 987
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository contains quantized GGUF format model files for Stability AI's Japanese StableLM Instruct Gamma 7B, supported by an a16z grant and hardware from Massed Compute.
  - Downloads: 961
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - A GGUF model description for the Japanese-stablelm-3b-4e1t-instruct model from Stability AI, noting current limitations in GPU layer support.
  - Downloads: 960
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - The repository contains models for deep learning with Japanese support, including versions for different model sizes.
  - Downloads: 956
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a continually pre-trained and instruction-tuned model based on Qwen, using 18B tokens from Japanese and English datasets to enhance performance on Japanese tasks.
  - Downloads: 928
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8b-Cosmopedia-japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 919
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - This repository offers a large Japanese GPT-2 model trained by ABEJA, Inc., requiring sentencepiece installation for text generation via the transformers library.
  - Downloads: 890
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Japanese-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 861
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - A DeBERTa V2 small Japanese pretrained model for masked language modeling, available with usage instructions and tokenization details.
  - Downloads: 850
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - A gguf format model derived from DeepSeek-R1-Distill-Qwen-1.5B for use with ggerganov/llama.cpp, based on imatrix-dataset-for-japanese-llm.
  - Downloads: 849
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - A BERT large Japanese model pretrained with whole word masking using Unidic-lite and Wikipedia text.
  - Downloads: 848
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplpl„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Instruct-Ja„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 823
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - A Japanese RoBERTa base model pre-trained on medical science articles, licensed under CC BY-NC-SA 4.0.
  - Downloads: 820
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - A gguf-formatted conversion of karakuri-lm-32b-thinking-2501-exp for use with llama.cpp, utilizing imatrix dataset.
  - Downloads: 811
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - A gguf-formatted version of the gpt-neox-japanese-1.4b model for testing with llama.cpp, requiring specific compilation commands.
  - Downloads: 779
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - ELYZA's Japanese CodeLlama-7b instruct model in GGUF format.
  - Downloads: 774
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - A gguf-format distilled LLM model from deepseek-ai, based on DeepSeek-R1-Distill-Llama-8B, optimized for use with ggerganov's llama.cpp.
  - Downloads: 694
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - This repository offers a 1.7 billion parameter Japanese language model from LINE Corporation, including instructions for import and usage via PyTorch.
  - Downloads: 688
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA's Japanese Llama-2-7b model in GGUF format with variations for instruction tuning and optimization.
  - Downloads: 675
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is an 47B-parameter LLM fine-tuned forÂØπËØù‰ªªÂä°Âπ∂ÈÄöËøá4bitÂíå8bitÈáèÂ≠êÂåñÂ§ÑÁêÜÔºåÊîØÊåÅflash attention„ÄÇ
  - Downloads: 670
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - A pre-trained Japanese ALBERT base model for fine-tuning various tasks, using Sentencepiece-tokenized Fill-Mask examples.
  - Downloads: 669
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - The repository contains continual pre-training of qwen-14b on mixed Japanese and English datasets, enhancing performance on Japanese tasks with an inclusive vocabulary exceeding 150k tokens.
  - Downloads: 662
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - A Japanese-specific 7B-parameter decoder-only language model fine-tuned on diverse public datasets, available in 70B-parameter and optimized versions.
  - Downloads: 660
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - The Japanese version of LUKE is a pre-trained, knowledge-enhanced model that provides contextualized word and entity representations, without Wikipedia entity embeddings.
  - Downloads: 654
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - This repository offers a 30 million-parameter Japanese ModernBERT model, trained by SB Intuitions on a large corpus with enhanced attention mechanisms and architectural improvements.
  - Downloads: 642
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LM is a Japanese-enhanced pretrained language model based on Llama 2, with further multilingual training, and its chat version was fine-tuned using a continual learning approach.
  - Downloads: 640
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - A fine-tuned 7B-parameter Japanese language model with an expanded vocabulary for improved text generation based on stablelm-ja_vocab-beta-7b.
  - Downloads: 638
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - A Japanese DeBERTa V2 tiny model pre-trained on specific datasets and fine-tuned for character-level tokenization and masked language modeling.
  - Downloads: 638
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA's Japanese LLaMA 2 13B Fast gguf format model, with enhanced vocabulary for faster processing.
  - Downloads: 637
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - A Japanese RoBERTa base model pre-trained on academic medical articles, released under CC BY-NC-SA 4.0.
  - Downloads: 627
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - The repository offers ModernBERT-Ja-310M, aÈ´òÊïàÊó•Êú¨ËØ≠È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÁªìÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå vocabÂ§ßÂ∞è‰∏∫102,400ÔºåÂ∫èÂàóÈïøÂ∫¶‰∏∫512„ÄÇ
  - Downloads: 622
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - GitHub repository contains a gguf format conversion of AXCXEPT's EZO-phi-4-v2_900 model, along with instructions for using it with llama.cpp.
  - Downloads: 608
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - The repository contains a pre-trained Sentence BERT base model for Japanese using the colorfulscoop/bert-base-ja model and the Japanese SNLI dataset.
  - Downloads: 607
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - This repository offers a 1.7B parameter Japanese language model fine-tuned by LINE Corporation for instruction tuning, along with instructions and example usage.
  - Downloads: 604
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - A quantized GGUF version of Aratako/calm3-22b-RP-v2, licensed under CC-BY-NC-SA 4.0 due to the inclusion of data from OpenAI's GPT-4o-mini and Anthropic's Claude 3.5 Sonnet.
  - Downloads: 600
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is an instruction-tuned 13 billion parameter Japanese language model for causal LM tasks.
  - Downloads: 583
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository offers a base-sized Japanese BART model trained by Stockmark Inc., featuring a bidirectional encoder and autoregressive decoder, pre-trained through text corruption and learning.
  - Downloads: 581
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - The repository offers a 1.4B parameter GPT-NeoX model pre-trained on Japanese text for use with PyTorch, optimized for A100 GPUs.
  - Downloads: 579
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - The repository contains the ELYZA Japanese Llama-2 13B model configured for fast instruction running with LlamaEdge v0.2.8+, using a specific prompt template and context size of 5120.
  - Downloads: 575
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - A 3.8B-parameter GPT-NeoX bilingual English-Japanese model fine-tuned via RLHF for instruction-following conversational tasks.
  - Downloads: 574
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository contains several Japanese Reranker (CrossEncoder) models with varying layers and hidden sizes for ranking text relevance.
  - Downloads: 574
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - japanese-stablelm-base-beta-7b is a 7B-parameter decoder-only Llama-2-7b fine-tuned on diverse Japanese data for high performance in Japanese language tasks.
  - Downloads: 571
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - A 3B-parameter Japanese language instruction-following model fine-tuned on instructive datasets, using the StabilityAI framework.
  - Downloads: 565
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer encoder-decoder model with GEGLU activation and no embedding-sharing, featuring improvements in the feed-forward hidden layer and pre-training dropout.
  - Downloads: 561
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM is a pretrained language model based on Llama 2 with enhanced Japanese vocabulary and multilingual pretraining, while KARAKURI LM Chat is its fine-tuned version using SteerLM and continual learning on mixed datasets.
  - Downloads: 556
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - A fine-tuned BERT model for Japanese natural language inference using the cl-tohoku/bert-base-japanese-v3 and JGLUE's MARC-ja dataset.
  - Downloads: 552
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - A 7B-parameter Japanese chat model derived from Starling-LM-7B-beta and Mistral-7B-v0.1, converted to GGUF format.
  - Downloads: 539
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - A pre-trained Japanese BART base model from Ku-NLP, using Japanese Wikipedia data, for conditional generation tasks.
  - Downloads: 537
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Rinna's nekomata-7b continuously pre-trains Qwen-7b on 30B tokens from mixed Japanese and English datasets, enhancing its performance on Japanese tasks with an inclusive vocabulary over 150k.
  - Downloads: 534
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - This repository contains a VILA 14B vision-language model from the National Institute of Informatics in Japan, requiring Python 3.10.12 and specific libraries for installation.
  - Downloads: 531
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - A 7B-parameter Japanese-language model based on Llama-2-7b, fine-tuned forJapanese tasks and enhanced with an expanded Japanese vocabulary.
  - Downloads: 531
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - A 7 billion parameter Japanese language model fine-tuned for instruction-following tasks.
  - Downloads: 525
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - The repository contains instruction models for a 13B parameter LLM from the LLM-jp project, including versions in Japanese and English, developed by various collaborating entities.
  - Downloads: 524
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a noncommercial Japanese instruction-tuned model fine-tuned on multiple public datasets, released under CC-BY-NC-4.0.
  - Downloads: 486
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - The repository contains Japanese-language Reranker (CrossEncoder) models with various configurations, including performance evaluations and documentation.
  - Downloads: 484
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100 with a 512-max sequence length, suitable for masked language modeling.
  - Downloads: 472
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - A DeBERTa(V2) model pretrained on Aozora Bunko for dependency-parsing and question-answering with support for handling ambiguous words.
  - Downloads: 467
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository offers multiple large language models developed by the National Institute of Informatics, including variants ranging from 1.8B to 172B parameters, in Hugging Face Transformers format.
  - Downloads: 463
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository contains quantized GGUF format model files for Stability AI's Japanese StableLM Instruct Beta 7B, supported by an a16z grant and hardware from Massed Compute.
  - Downloads: 461
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - A BERT small model pretrained on Japanese Wikipedia texts, with a 12-layer architecture.
  - Downloads: 446
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - A Japanese-specialized DeBERTa V3 model that omits morphological analysis during inference and partially respects word boundaries.
  - Downloads: 441
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This GitHub repository provides an XLNet-japanese model requiring Mecab and sentencepiece with NFKD normalization, losing muddles and semi-muddles, and includes usage from fugashi and transformers.
  - Downloads: 427
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository contains quantized GGUF format model files for cyberagent/Mistral-Nemo-Japanese-Instruct-2408, compatible with llama.cpp, and can be run on TensorBlock using a local machine.
  - Downloads: 399
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and size variants "xl" and "xxl".
  - Downloads: 397
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - VecTeus Ninja-v1 is a novel Mistral-7B-based LLM fine-tuned for 128k context, supporting high-quality Japanese and English generation with NSFW capabilities and excellent memory retention.
  - Downloads: 395
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - The Japanese version of LUKE is a pre-trained model that provides knowledge-enhanced contextualized representations, incorporating Wikipedia entity embeddings for specific NLP tasks.
  - Downloads: 390
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - A T5-base model fine-tuned on the LiveJournal news corpus for summary generation as described in Chapter 7 of "Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®".
  - Downloads: 388
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - A Japanese RoBERTa base model pre-trained on Wikipedia and CC-100 with character-level tokenization and whole word masking for masked language modeling.
  - Downloads: 384
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - The repository provides the llm-jp-3-3.7b-instruct3 model, part of a series of large language models developed by the National Institute of Informatics.
  - Downloads: 378
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 377
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - A Japanese character-level GPT-2 Small (90M params) model pre-trained on specific datasets, usable for text generation via provided pipeline.
  - Downloads: 375
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - A gguf-formatted version of the open-calm-7b model by cyberagent, with usage instructions for testing.
  - Downloads: 372
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - A T5 model pre-trained on Japanese web texts with a vocabulary size of 32K, using corpora from mC4 and wiki40b.
  - Downloads: 371
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - A finetuned Llama-3-ELYZA-JP-8B model developed 2x faster using Unsloth and HuggingFace's TRL, licensed under Apache-2.0.
  - Downloads: 364
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - A T5 v1.1 model card for a Japanese corpus-trained Transformer-based Encoder-Decoder model with GEGLU activation, no pre-training dropout, and "xl"/"xxl" size variations.
  - Downloads: 352
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers quantized GGUF versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model for different GPU RAM capacities, ranging from 8GB to 16GB.
  - Downloads: 345
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - A cross-encoder for Japanese natural language inference trained on JSNLI data and based on bert-base-japanese-v3.
  - Downloads: 339
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 338
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - A fine-tuned RoBERTa model for Japanese extractive question answering, trained on the JaQuAD dataset.
  - Downloads: 332
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - A DeBERTa V3 model specialized for Japanese that does not use a morphological analyzer during inference and respects word boundaries.
  - Downloads: 331
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Fine-tuned Waseda RoBERTa for evaluating generated answers on JTruthfulQA.
  - Downloads: 328
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - A fine-tuned Japanese-language version of Meta AI's Llama 3.1 model that achieved top scores in ElyzaTasks-100 among open-source models.
  - Downloads: 323
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository includes quantized GGUF format model files for Stability AI's Japanese StableLM Base Beta 70B, supported by grants from a16z and Massed Compute.
  - Downloads: 314
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - Quantum-optimized models for gemma-2-2b-jpn-it-gguf, usable with llama.cpp LM Studio and LLMFarm, created by converting npaka's LLM-jp-3 to gguf format.
  - Downloads: 303
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - A pre-trained Japanese ALBERT base model with simplified tokenization for fine-tuning various tasks using PyTorch.
  - Downloads: 294
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM is an open-source LLM for Japanese-to-Chinese translation in ACGN domains, built on large models and fine-tuned on relevant corpora, available under CC BY-NC-SA 4.0 with non-commercial usage restrictions.
  - Downloads: 294
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - The repository offers Asagi-14B, a large Japanese vision-language model trained on diverse datasets including synthesized data from other models like CALM3-22B-Chat and Phi3.5-vision-instruct.
  - Downloads: 279
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - A GPT-2 small model trained on a subset of Japanese Wikipedia data released under CC BY-SA 3.0, split into train, valid, and test sets.
  - Downloads: 279
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - A GPT2 model for generating Japanese lyrics, available on the website https://lyric.fab.moe/, implemented using PyTorch and T5Tokenizer.
  - Downloads: 275
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - A RoBERTa fine-tuned on JaQuAD for Japanese question answering.
  - Downloads: 272
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - A fine-tuned BERT model for Japanese zero-shot classification on JSNLI, achieving 92.88% accuracy, accessible via a simple pipeline from the transformers library.
  - Downloads: 270
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - A gguf-formatted version of the open-calm-3b model for use with llama.cpp, available for testing.
  - Downloads: 269
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100 with a 512-sequence length for masked language modeling.
  - Downloads: 256
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - A pre-trained Japanese BART large model for natural language processing, available via tokenizers and conditional generation from Ku-NLP.
  - Downloads: 250
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - A ggufconverted version of the rinna/japanese-gpt-neox-3.6b model, with related models and usage instructions provided.
  - Downloads: 244
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a general-purpose switch transformer model for Japanese that uses Prefix-LM and a Spout vector for context-specific fine-tuning.
  - Downloads: 244
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository contains the terms and conditions for using Fugaku-LLM, a large language model developed through collaborative research utilizing Japan's Fugaku supercomputer. Users must agree to these terms for commercial or non-commercial use, including modifications, redistribution, and service implementation based on the model.
  - Downloads: 244
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100-billion-parameter large language model focusing on Japanese, pre-trained on diverse data and enhanced with Japanese synthetic instruction-following data.
  - Downloads: 241
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - A fine-tuned Wav2Vec2-Large-Japanese model trained on over 600 hours of public Japanese data, accessible for direct use at 16kHz sampling rate. Contact nha282@gmail.com for more details.
  - Downloads: 241
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - A high-performance Japanese SPLADE v2 model for transforming text into sparse vectors, accessible via WebUI and the YASEM toolkit.
  - Downloads: 240
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - A fine-tuned Japanese Wav2Vec2 model using XLSR-53 for speech recognition at 16kHz sampling rate.
  - Downloads: 237
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - A T5 v1.1 model pre-trained on Japanese text, featuring GEGLU activation, no dropout in pre-training, and size variants "xl" and "xxl" replacing "3B".
  - Downloads: 236
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a fine-tuned model based on CodeLlama for Japanese instruction-following, with additional pre-training; instructions on how to use it via Python are provided.
  - Downloads: 235
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - A BERT model pre-trained on Japanese Wikipedia for UPOS tagging, derived from bert-base-japanese-char-extended.
  - Downloads: 234
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - The repository explores hyperparameters using Optuna for sentiment analysis with BERT, finding a cosine learning rate scheduler, learning rate of 3.91e-5, batch size of 128, and weight decay of 5.22e-5 over 100 epochs with early stopping.
  - Downloads: 231
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - A Japanese-trained Llama2 model (417.12M parameters) using script from Lightning-AI/lit-gpt for causal language modeling.
  - Downloads: 227
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - A Japanese-trained LLAMA2 model fine-tuned on instruction datasets, available at `if001/llama2_ja_small`.
  - Downloads: 226
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - A vision-language model in Japanese based on the StableLM Base 7B architecture that can converse about images, trained with Heron library.
  - Downloads: 226
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - A GPT2-based 6B parameter language model fine-tuned on 693k witty one-liners using AWS trn1 instances, pre-trained on multiple corpora including C4, CC-100, and wiki data. Licensed under Apache License 2.0.
  - Downloads: 222
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - A fine-tuned Wav2Vec2 model for recognizing Japanese accents with a test WER of 15.82%. Input should be at 16kHz sampling rate.
  - Downloads: 220
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This model is a Reasoner version of the phi-4 model using open-r1 and Deepseek-R1's Distill methodology, primarily generating Japaneseresponses with flexible integration of English.
  - Downloads: 220
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - A Japanese BERTBASE model fine-tuned on WRIME data for predicting emotion intensity scores in tweets about vaccinations.
  - Downloads: 216
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - The repository includes updates with the addition of the "oasst1-89k-ja" dataset, improving DollyJapanese-GPT-1B for dialogue systems, but shows a decrease in QA accuracy compared to previous models.
  - Downloads: 216
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - This repository offers a 70M-parameter ModernBERT model for Japanese, trained by SB Intuitions on a large corpus with modern architectural improvements.
  - Downloads: 215
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - A Japanese GPT-2 XL model withÁ∫¶1.5BÂèÇÊï∞ÔºåÈ¢ÑËÆ≠ÁªÉ‰∫éÊó•ËØ≠WikipediaÂíåCC-100ÔºåÂèØÁî®‰∫éÊñáÊú¨ÁîüÊàêÊàñÂæÆË∞É„ÄÇÈúÄË¶Å‰ΩøÁî®Juman++ÂàÜËØç„ÄÇ
  - Downloads: 212
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - A Japanese-trained LLaMA 2 model with a size of 130.78M using a specific script for tokenization and inference.
  - Downloads: 211
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 210
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This GitHub repository contains a fine-tuned BERT model for Japanese Named Entity Recognition using the dataset ner-wikipedia-dataset, based on the Kyoto University's Japanese-pretrained BERT model.
  - Downloads: 208
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - A pre-trained MobileBERT Japanese model for BERT enthusiasts looking to speed up inference.
  - Downloads: 208
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - A large Japanese RoBERTa model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 203
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This model, based on rinna/japanese-gpt-1b, is trained to perform context-based extraction QA and refine answers in new contexts using gpt-index v0.2.5, with example usage provided in the repository.
  - Downloads: 200
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - A BERT small model pretrained on Japanese Wikipedia and a financial corpus, with a comparable architecture to the original BERT small model from the ELECTRA paper.
  - Downloads: 198
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - The repository includes various pre-trained and LoRA fine-tuned large language models from the LLM-jp project, including instruction models and checkpoints.
  - Downloads: 197
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, off-dropout pre-training, and size variants "xl" and "xxl".
  - Downloads: 195
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - A finely-tuned Japanese GPT-2 model for resume writing, trained on over 20,000 resumes from interns, available in a web app at https://huranokuma-es-app-9t34vl.streamlitapp.com/.
  - Downloads: 192
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - This GitHub repo contains an evolutionary merge of four powerful Japanese language models using an algoithmic approach.
  - Downloads: 191
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - A pre-trained Japanese-language mixtral model using a 275.86M parameter dataset, enabling generation from prompts with tokenizer support.
  - Downloads: 191
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - A fine-tuned Japanese GPT-2 model for generating resumes, specifically tailored for IT industry job applications in Japan.
  - Downloads: 191
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - A fine-tuned GPT-2 model on ATOMIC data for Japanese text generation using causal language modeling.
  - Downloads: 189
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - A Japanese BERT model with up to 500M parameters that supports input sequences of 4,096 and 8,192 tokens, based on the Llama architecture.
  - Downloads: 189
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - A merged pre-trained language model, Llama-3.3 SuperSwallow 70B Instruct v0.1, incorporating Japanese-RP benchmarks for generating responses with specified parameters.
  - Downloads: 188
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - A gguf-formatted conversion of Stockmark-2-100B-Instruct-beta for use with llama.cpp, using imatrix dataset.
  - Downloads: 185
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - An AI model trained on Japanese novel data using GPT-J-6B, fine-tuned with TPU and(sentence tokenizer omitted for brevity).
  - Downloads: 184
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - A fine-tuned RoBERTa model for named entity recognition in Japanese medical text, extracting entities like symptoms, organs, tests, medications, and more using IOB2 tagging.
  - Downloads: 182
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - A BERT model pretrained on Japanese Wikipedia for dependency-parsing and question-answering, using [MASK] for ambiguous words.
  - Downloads: 181
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - A Japanese character-level GPT-2 Medium model pre-trained on Wikipedia and CC-100, accessible via a text generation pipeline.
  - Downloads: 181
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - A Japanese GPT2 lyric generation model that uses T5Tokenizer for generating lyrics given a title and prompt.
  - Downloads: 179
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pretrained on Wikipedia, suitable for text generation or fine-tuning, requiring word segmentation with Juman++.
  - Downloads: 179
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - The repository provides a Japanese BERT-base model using Juman++ + BPE tokenizer setup instructions.
  - Downloads: 178
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - The repository contains a Japanese GPT-2 small model pretrained on Wikipedia and CC-100, meant for text generation or fine-tuning, requiring Juman++-segmented input.
  - Downloads: 174
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This GitHub repository contains the terms and conditions for using Fugaku-LLM, a large language model developed as part of Fugaku's policy-oriented framework for distributed parallel learning methods.
  - Downloads: 174
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - A medium-sized Japanese GPT-2 model using BERT-like tokenizer, dependent on PyTorch, fugashi with unidic-lite, and Hugging Face Transformers.
  - Downloads: 173
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - A fine-tuned Japanese mt5-base model for error detection and correction using a reduced dataset of 20,000 pairs with a "correction:" prefix.
  - Downloads: 171
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - A model fine-tuned on the JCommonsenseQA dataset using cl-tohoku/bert-base-japanese-v3 for multiple-choice question-answering tasks, introduced in Chapter 5 of "Large Language Models Demystified."
  - Downloads: 168
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - A T5 v1.1 model card for a Japanese corpus-trained Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and non-sharing between embedding and classifier layers.
  - Downloads: 168
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - A pre-trained Japanese character-level GPT-2 Large model used for text generation.
  - Downloads: 167
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - The GitHub repository contains a BERT large Japanese model pretrained with whole word masking on CC-100 and jawiki-20230102 datasets using character-level tokenization based on Unidic 2.1.2 dictionary.
  - Downloads: 165
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository contains a GGUF conversion model for ELYZA-japanese-Llama-2-13b-fast-instruct, with instructions on usage and licensing details.
  - Downloads: 163
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - A BERT large Japanese model pretrained with character-level tokenization and whole word masking using Unidic 2.1.2 dictionary.
  - Downloads: 163
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - A Japanese ELECTRA model pretrained on 200M sentences, using SudachiTra and WordPiece tokenization.
  - Downloads: 160
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - Quantized versions of the llm-jp-3-3.7b-instruct model for use with llama.cpp LM Studio (Windows, Mac) and LLMFarm (iOS), along with conversion instructions from npaka's LLmj-3 to gguf format.
  - Downloads: 160
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - A fine-tuned T5 model on the xlsum dataset for Japanese text summarization, achieving specific loss and ROUGE scores with a learning rate of 0.0001 during training.
  - Downloads: 156
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - A gguf-formatted version of WabiSabi-V1 for local use, created from imatrix-dataset-for-japanese-llm, to be run with ggerganov's llama.cpp.
  - Downloads: 154
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - A RoBERTa model pretrained on Japanese Aozoratexts, usable for tasks like POS-tagging and fine-tuning with the Japanese-LUW-tokenizer.
  - Downloads: 152
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - The repository merges the intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b models for enhanced Japanese language processing.
  - Downloads: 151
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - This repository offers Asagi-8B, a large Japanese VLM trained on extensive datasets including synthesized data from CALM3-22B-Chat and Phi3.5-vision-instruct.
  - Downloads: 150
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„Å´chat vector„ÅßÂØæË©±ËÉΩÂäõ„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 150
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - An ELECTRA model pretrained on mC4 Japanese data and finetuned on UD_Japanese_BCCWJ r2.8, using SudachiTra for tokenization and distributed as the ja_ginza_electra package.
  - Downloads: 149
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - A fine-tuned wav2vec2-large-xlsr-53 model for Japanese hiragana inference using common_voice datasets.
  - Downloads: 147
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - A fine-tuned BERT model in Japanese for an unknown dataset, achieving a loss of 1.9164, with specific training hyperparameters including learning rate and batch size.
  - Downloads: 146
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - A BERT basal model for Japanese with character-level tokenization and whole word masking, using basic tokenization instead of MeCab.
  - Downloads: 146
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - A RoBERTa model pre-trained on Hindi texts using a character tokenizer, accessible via Transformers library.
  - Downloads: 145
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - Fine-tuned Wav2Vec2-Large-XLSR-53 on Japanese using Common Voice and JSUT datasets, suitable for 16kHz sampled speech input.
  - Downloads: 145
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - A fine-tuned Japanese MT5 model for summarizing patent claims in the pharmaceutical domain, specifically targeting antibodies against CD38.
  - Downloads: 144
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts, trained on an NVIDIA A100-SXM4-40GB for 109 hours, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 142
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - A pretrained ELECTRA small model for Japanese text,trained on the Japanese Wikipedia.
  - Downloads: 141
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - A fine-tuned Meta-Llama-3-8B-Instruct model on a Japanese conversation dataset, trained with LLaMA-Factory framework, supporting inference via transformers.
  - Downloads: 141
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 141
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and new size variants "xl" and "xxl".
  - Downloads: 140
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised pretraining model for Chinese and Japanese that uses a two-stage approach to leverage the shared linguistic knowledge in the Unihan database.
  - Downloads: 140
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - The repository focuses on the research and analysis aspects of projects related to the advanced scientific campus of Tsukuba University in Japan, including student experiments and regional investigations.
  - Downloads: 139
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - A BERT-based Japanese model fine-tuned on JaQuAD with evaluation F1 scores of 77.35 and 78.92,Exact_match scores of 61.01 and 63.38 respectively, for answering questions like "„Ç¢„É¨„ÇØ„Çµ„É≥„ÉÄ„Éº„Éª„Ç∞„É©„Éè„É†„Éª„Éô„É´„ÅØ„ÄÅ„Å©„Åì„ÅßÁîü„Åæ„Çå„Åü„ÅÆ?"
  - Downloads: 138
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - The GitHub repository contains a tokenized Japanese BART pretrained model for use with the BartJapaneseTokenizer and includes instructions for model deployment.
  - Downloads: 138
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - A BERT-based Japanese model trained using Optuna for hyperparameter tuning with cosine learning rate schedule, gradient accumulation, and regularization.
  - Downloads: 138
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - A pre-trained ELECTRA model for Japanese text, trained on the Japanese Wikipedia, with a base architecture of 12 layers, 256 hidden dimensions, and 4 attention heads.
  - Downloads: 138
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - A BERT base model trained on a Japanese Wikipedia dataset from June 20, 2021.
  - Downloads: 138
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - A pretrained ELECTRA small model for generating Japanese financial text, trained on the Japanese Wikipedia.
  - Downloads: 137
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation and no embedding-classifier layer sharing, now available in "xl" and "xxl" sizes.
  - Downloads: 137
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - A pretrained ELECTRA small model for Japanese language generation trained on the Japanese Wikipedia.
  - Downloads: 136
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - A BERT Base model for Japanese language fine-tuned on a balanced dataset for automatic cyberbullying detection, licensed under CC BY-SA 4.0.
  - Downloads: 134
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - A pre-trained ELECTRA small model for Japanese language processing, trained on the Japanese Wikipedia.
  - Downloads: 134
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - The repository offers a Japanese ELECTRA-Small model pretrained using Byte-Pair Encoding subwords from the Japanese Wikipedia, requiring proper MeCab dictionary configuration for best results.
  - Downloads: 133
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts, trained on an NVIDIA A100-SXM4-40GB in 632 hours, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 132
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - The repository contains a Japanese BART pretrained model converted to Fairseq, requiring BartJapaneseTokenizer for input and using the Simple FillMaskPipeline for inference.
  - Downloads: 132
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - A template Model Card for a pretrained T5 model on Japanese and English, aimed to serve as a base for new models.
  - Downloads: 132
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - A pre-trained Japanese RoBERTa base model for super short unit words, requiring input text to be converted to full-width characters and segmented into SSUW before use.
  - Downloads: 131
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´, compatible with transformers library for tasks like POS-tagging and dependency-parsing.
  - Downloads: 130
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - The repository offers a GGUF quantized version of the CyberAgent DeepSeek-R1-Distill-Qwen-32B-Japanese model, licensed under MIT.
  - Downloads: 129
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - A T5 fine-tuned Japanese-Ainu machine translation model, still experimental, with example inputs and outputs provided.
  - Downloads: 129
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - A pre-trained Japanese GPT-1B model fine-tuned to mask personal information in texts, including names, birthdays, phone numbers, email addresses, etc.
  - Downloads: 129
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - A XLM-RoBERTa-base model trained on Japanese mMARCO data with ANCE warmup, uploaded at 50k steps due to MRR@100 decrease after 60k steps.
  - Downloads: 127
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets, using which you can perform masked language modeling.
  - Downloads: 127
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´, compatible with transformers library for tasks like POS-tagging and fine-tuning.
  - Downloads: 127
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository aims to Japanese localize the Llama 3 8B model, providing installation instructions and usage recommendations.
  - Downloads: 127
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a novel Mistral-7B-based LLM fine-tuned with a 128k context window, capable of high-quality Japanese and English generation, NSFW content, and memory retention.
  - Downloads: 126
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - A T5 v1.1 model, pre-trained on Japanese data, featuring GEGLU activation and no dropout during pre-training.
  - Downloads: 126
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - A forked version of DistilBERT pre-trained on Japanese web text, using LINE's BERT-base teacher model and updated tokenizer for transformers 4.34+.
  - Downloads: 123
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 123
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - A RoBERTa model pre-trained on Hindi text using a character tokenizer, configured with is_decoder=False.
  - Downloads: 122
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This GitHub repository contains a BERT Base model for Japanese ironic and sarcastic Twitter detection, licensed under CC BY-SA 4.0.
  - Downloads: 122
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - The repository explores hyperparameter optimization using Optuna for a Japanese sentiment analysis model, varying learning rates and batch sizes.
  - Downloads: 121
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - An ELECTRA-based Japanese model pretrained on 200M sentences and fine-tuned for Information Triage on disaster tweets, licensed under CC BY-SA 4.0.
  - Downloads: 121
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - A fine-tuned RoBERTaJapanese base model on the JSNLI dataset, achieving a loss of 0.2039 and accuracy of 93.28%, requiring Juman++-segmented input for zero-shot classification.
  - Downloads: 121
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - A GitHub repository containing a 32B Japanese language model converted to MLX format for use with mlx-lm version 0.21.1.
  - Downloads: 121
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - A DeBERTa(V2) model pre-trained on Aozora Bunko texts for fine-tuning in tasks like POS-tagging, trained on an NVIDIA A100-SXM4-40GB in 127 hours.
  - Downloads: 120
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - A RoBERTa model pre-trained on Japanese Aozora texts using a character tokenizer, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 120
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - A pre-trained DeBERTa(V2) model on ÈùíÁ©∫ÊñáÂ∫´ texts for fine-tuning in Japanese NLP tasks.
  - Downloads: 117
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - A fine-tuned model based on luke-japanese-base for binary classification of MARC-ja data with an accuracy of 0.9.
  - Downloads: 117
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - A pre-trained Japanese BERT base model for super short unit words, suitable for masked language modeling after input text is converted to full-width characters and segmented.
  - Downloads: 116
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - A gguf conversion of the Japanese large language model 3.6B instruction-SFT by line-corporation, with branches for testing and notes on potential incompatibility.
  - Downloads: 115
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - This repository offers Asagi-2B, a large Japanese VLM trained on extensive datasets, including synthesized data from CALM3-22B-Chat and Phi3.5-vision-instruct.
  - Downloads: 115
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - A DeBERTa(V2) model pre-trained on Japanese Aozora texts, trained on an NVIDIA A100-SXM4-40GB in 6 hours 42 minutes, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 114
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository contains a fine-tuned version of luke-japanese-base for JSTS sentence similarity tasks, achieving an accuracy of 0.8971 Pearson correlation coefficient.
  - Downloads: 114
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - The repository contains weighted/imatrix quantized GGUF files for the Japanese LLaMA-3-8b-instruct-v2 model, including i1-IQ1_S, with usage instructions available in related READMEs.
  - Downloads: 114
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - Line-corporation's gguf conversion of the Japanese large language model 3.6B, with branches for trial use and instructions.
  - Downloads: 112
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - A Japanese BigBird base model pretrained on specific datasets, usable for masked language modeling.
  - Downloads: 111
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese-enhanced CodeLlama model for causal language tasks, with additional fine-tuning; includes instructions for loading and using the model via transformers.
  - Downloads: 110
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - A RoBERTa model pre-trained on Japanese Aozora texts with character tokenizer, suitable for fine-tuning in tasks like POS-tagging and dependency-parsing.
  - Downloads: 109
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
  - Downloads: 108
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - A Japanese RoBERTa base model pre-trained on medical science articles collected by JST, licensed under CC BY-NC-SA 4.0.
  - Downloads: 107
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - A fine-tuned Japanese language model for automatic defamation detection using LUKE, trained on a balanced dataset combining two existing datasets.
  - Downloads: 107
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - A RoBERTa large model pre-trained on Japanese Aozora texts with a character-level tokenizer, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 105
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - A Japanese ELECTRA-small model pretrained using subword units from Japanese Wikipedia and Byte-Pair Encoding, with guidance on MeCab dictionary setup and usage.
  - Downloads: 102
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF provides GGUF format model files and instructions for using the Japanese language AI model based on Mistral 7B Instruct.
  - Downloads: 99
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - The repository offers Asagi-4B, a large Japanese VLM trained on diverse datasets including synthesized data from CALM3 and Phi3.5 models.
  - Downloads: 95
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - A LayoutLM model pretrained on Japanese text and finetunable for token classification tasks, developed by Advanced Technology Laboratory, licensed under CC BY-SA 3.0.
  - Downloads: 93
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - A 8-bit quantized version of the 32B Qwen2.5 Bakeneko instruction-tuned model using AutoGPTQ, reducing memory usage and inference time.
  - Downloads: 91
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - A lightweight 7B parameter model fine-tuned on Japanese data, outperforming ChatGPT-3.5 on JGLUE benchmarks, using additional pre-training and custom tuning.
  - Downloads: 91
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - A fine-tuned Japanese language model based on llm-jp-1.3b-v1.0 using Cohere's aya dataset, with evaluation metrics and usage instructions provided.
  - Downloads: 86
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository contains AWQ model files for Stability AI's Japanese StableLM Instruct Gamma 7B, quantized with support from Massed Compute.
  - Downloads: 86
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - A T5 Prefix Language Model fine-tuned on Japanese corpora including Wikipedia and OSCAR for 100K steps.
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository includes quantized AWQ model files for Stability AI's Japanese StableLM Instruct Beta 7B, supported by a16z and Massed Compute.
  - Downloads: 84
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This GitHub repository contains the terms and conditions for using Fugaku-LLM, a large language model developed as part of research on distributed parallel learning methods for supercomputers. Users must agree to these terms before utilizing Fugaku-LLM for commercial or non-commercial purposes, including modifications, replication, redistribution, and service implementation.
  - Downloads: 81
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - A pretrained ELECTRA small model for Japanese language processing, trained on the Japanese Wikipedia.
  - Downloads: 78
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 ÊòØ‰∏Ä‰∏™Âü∫‰∫é LLama ÁöÑÊó•ËØ≠Ê®°ÂûãÔºåÈÄöËøáÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÂ≠¶‰π†ÔºåÈÄÇÁî®‰∫é 24GB VRAM ËÆæÂ§áÔºåÊîπËøõÂêéÂú®Áü•ËØÜË°®Ëææ‰∏äÊõ¥Âä†‰∏∞ÂØå„ÄÇ
  - Downloads: 76
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - This repository contains a Japanese-tuned LLaMA 3.1-8B-Instruct model created using Mergekit and fine-tuning, with instructions on how to use it for generating responses in Japanese.
  - Downloads: 76
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - The repository contains a Japanese+English Sentence-BERT model with improved English STS benchmark performance and slightly lower Japanese accuracy compared to monolingual Japanese versions.
  - Downloads: 74
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - A fine-tuned Japanese Whisper model based on openai/whisper-base for speech recognition using Common Voice, JVS, and JSUT, requiring 16 kHz sampled input.
  - Downloads: 72
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - A medium-sized Japanese GPT-2 model that generates text right-to-left, using a BERT-like tokenizer and dependent on PyTorch, fugashi with unidic-lite, and Hugging Face Transformers.
  - Downloads: 69
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - This repository offers large language models, specifically LLM-jp-3-7.2b-instruct, developed by the National Institute of Informatics for research and includes usage instructions and required libraries.
  - Downloads: 69
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - A fine-tuned Whisper small model for Japanese speech recognition using Common Voice, JVS, and JSUT, requiring 16kHz sampled audio input.
  - Downloads: 69
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - This repository contains the GGUF version of the rinna/nekomata-14b-instruction model, suitable for lightweight inference with llama.cpp, with recommended 4-bit quantizationsettings.
  - Downloads: 67
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model fine-tuned on 42 billion tokens from the Cultura-X dataset, adapted from Llama-2-7b via direct preference optimization.
  - Downloads: 67
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - A Japanese sentence-T5 model using sonoisa/t5-base-japanese with SentencePiece for inference.
  - Downloads: 66
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi provides GGUF format model files for the Japanese instruction-tuned Mistral 7B model,along with instructions from TheBloke.
  - Downloads: 66
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - A pretrained ELECTRA model on Japanese text using Sudachitr[wordpiece] tokenizer and MC4 dataset, enabling tokenization and model loading via specified configurations.
  - Downloads: 66
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - A pretrained ELECTRA small model for Japanese text, specifically a discriminator for finance texts, trained on the Japanese Wikipedia.
  - Downloads: 66
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small LLaMA-based Japanese language model trained entirely in Japanese, offering quick inference despite its size.
  - Downloads: 65
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - A pretrained ELECTRA small model for Japanese language processing, trained on the Japanese Wikipedia.
  - Downloads: 63
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - A pretrained ELECTRA model for Japanese text, based on the Wikipedia corpus.
  - Downloads: 62
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - A vision-language model for conversing about images, based on the Japanese StableLM Base 7B model and using the Heron GIT library.
  - Downloads: 61
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository houses a fine-tuned Reward model for evaluating the quality of Japanese novels using sbintuitions/modernbert-ja-130m, intended for applications like reinforced learning in novel generation models.
  - Downloads: 60
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - A pre-trained ELECTRA small model for Japanese finance text discrimination using Wikipedia as the training corpus.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - A Japanese RoBERTa base model pre-trained on medical science articles collected by JST, licensed under CC BY-NC-SA 4.0.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ for POS-tagging and dependency-parsing, using goeswith for subwords.
  - Downloads: 56
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - A pretrained ELECTRA model on Japanese text using SudachiTra tokenization and WordPiece subword tokenizer withÁ∫¶2‰∫øÂè•Êó•ËØ≠ÊñáÊú¨„ÄÇÂèØ‰ª•ÈÄöËøáAutoModelÂíåAutoTokenizerÂä†ËΩΩÊ®°ÂûãÂíåÂàÜËØçÂô®„ÄÇÂÖàÂÆâË£ÖSudachiTraÔºö$ pip install -U torch transformers sudachitra„ÄÇ
  - Downloads: 55
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository contains a Japanese finetuned version of the unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit model with added system prompts and outputs thoughts in Japanese.
  - Downloads: 54
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - A pre-trained GPT-Neo 1.3B model for Japanese, trained oncc100 ja, Oscar ja, and Wikipedia, enabling text generation through the transformers pipeline.
  - Downloads: 54
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained bilingual Japanese-English language model based on Llama-2-7b, trained on 42 billion tokens from the Cultura-X dataset, achieving state-of-the-art results in perplexity and FLORES-200 translation.
  - Downloads: 53
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - A BERT model pre-trained on Japanese Wikipedia for UPOS tagging of long-unit-words, derived from bert-base-japanese-char-extended.
  - Downloads: 52
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model for image conversation tasks, utilizing the heron library.
  - Downloads: 52
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - A GitHub repository containing Japanese-to-Hebrew translation models using transformer-align architecture, including preprocessed data and evaluation metrics from the OPUS corpus.
  - Downloads: 52
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - The GitHub repository hosts zenz-v2.5-small, a 91M GPT-2-based conditional language model for specialized kanji conversion tasks, with high performance in context-sensitive transformations.
  - Downloads: 52
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - A GPT-2-based model for kanji-hiragana conversion, finetuned from ku-nlp/gpt2-small-japanese-char, with 90M parameters for use in the Zenzai hiragana-to-kanji conversion system.
  - Downloads: 52
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding based on cl-tohoku/bert-base-japanese-v3, trained on limited data and datasets including JSTS, JSNLI, and MMARCO (train).
  - Downloads: 50
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - A Fine-Tuned Japanese GPT-2 Model for Writing AI-Generated ES Text, Trained on 140,000 Samples, Available via Web App http://www.eswrite.com
  - Downloads: 50
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository contains aconversion of the DeepSeek-R1-Distill-Qwen-32B-Japanese model to MLX format, usable with mlx-lm version 0.21.1.
  - Downloads: 49
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - A fine-tuned wav2vec2 model on the Japanese COMMON_VOICE dataset with evaluated loss, word error rate, and character error rate using specified training hyperparameters.
  - Downloads: 49
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - The GitHub repository provides a GGUF version of rinna/nekomata-7b for lightweight inference with llama.cpp, recommending specific quantization settings to avoid stability issues.
  - Downloads: 49
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - The GitHub repository contains a GGUF conversion of the Japanese-WizardLM2-ChatV-7B model, derived by modifying "WizardLM-2-7b" to enhance its Japanese language capabilities and performance.
  - Downloads: 49
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - A JAX/Flax-based transformer language model trained on a Japanese dataset, including updates for benchmark scores and custom model code.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a chat Japanese language model based on the Mamba state-space architecture, inspired by existing English models, and fine-tuned on 31,7k JaQuAD dataset examples.
  - Downloads: 47
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Ninja-v1 is a novel Mistral-7B-based LLM fine-tuned for high-quality Japanese and English generation with memory abilities, developed during a local AI hackathon.
  - Downloads: 46
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for head detection in dependency parsing, designed for question-answering tasks.
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository contains the MLX-format conversion of a 32B Japanese Qwen model derived from DeepSeek-R1, using tokenizer and generation utilities.
  - Downloads: 45
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - A BERT model pre-trained on Japanese Wikipedia texts with enhanced character embeddings for allÂ∏∏Áî®Êº¢Â≠ó/‰∫∫ÂêçÁî®Êº¢Â≠ó, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 44
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B-parameter finetuned Japanese GPT2 model for use with PyTorch and Rust, requiring T5Tokenizer initialization.
  - Downloads: 44
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
  - Downloads: 43
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - A model fine-tuned for long text generation using the LLCM-JP framework, based on the llm-jp/llm-jp-3-3.7b-instruct model.
  - Downloads: 42
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - A fine-tuned model based on sentence-Luke-japanese-base-lite using manually annotated toxicity evaluations from social media comments, achieving an F-score of 71.3%, with details on training conditions and evaluation metrics included.
  - Downloads: 42
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2 Japanese base model v2 uses BPE tokenizer and is trained on a subset of Japanese Wikipedia for text generation.
  - Downloads: 42
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository contains a distilled GPT-2 Japanese model trained using Rinna's Japanese-GPT2-medium as the teacher model, achieving perplexity around 40.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - Ê¶ÇË¶Å vecteus„ÅØ„ÄÅÈ´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™ûÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 41
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - A large Japanese RoBERTa model pre-trained on Wikipedia and CC-100 using character-level tokenization and whole word masking, suitable for masked language modeling.
  - Downloads: 39
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository contains GPTQ model files for Stability AI's Japanese StableLM Instruct Beta 70B with multiple parameter permutations.
  - Downloads: 39
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository contains GPTQ model files for Stability AI's Japanese StableLM Base Beta 70B with various parameter permutations.
  - Downloads: 39
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository contains a model switched from RoBERTa to Japanese BERT with Sentencepiece replaced by WordPiece, pretrained on 2023-7-1 Japanese Wikipedia data, and enhanced to handle [UNK] entities.
  - Downloads: 39
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - This repository contains the GGUF version of the rinna/nekomata-7b-instruction model, suitable for lightweight inference with llama.cpp, and requires specific quantization settings.
  - Downloads: 38
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - The repository contains a specialized GPT-2 language model for kanji-hira conversion tasks, including zenz-v2.5-small, a 91M model designed for high-performance contextual transformations in the "Zenzai" system.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - A 1.3B parameter Japanese GPT dialogue AI trained using the Alpaca_Ja and GuanacoDataset datasets, requiring 7GB VRAM/RAM for operation.
  - Downloads: 37
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz-v2.5 is a specialized GPT-2 conditional language model for kana-to-hanzi conversion tasks, available in three sizes (small, medium, x-small), and licensed under CC-BY-SA 4.0.
  - Downloads: 37
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - A Japanese-finetuned Mistral-nemo model for EPR purposes, using expanded datasets and increased epochs, with improved Japanese proficiency.
  - Downloads: 37
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese data for POS-tagging and dependency-parsing, using goeswith subwords, designed for universal dependencies with instructions for usage via Hugging Face's `transformers` library.
  - Downloads: 36
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - The repository provides a Japanese BERT-base tokenizer using MeCab and BPE, requiring the dictionary file to be downloaded and specified during initialization.
  - Downloads: 36
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - A 6.8 billion parameter pre-trained Japanese language model based on EleutherAI's Mesh Transformer JAX, using T5Tokenizer and SentencePiece.
  - Downloads: 36
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - The repository initializes SPLADE-japanese from tohoku-nlp/bert-base-japanese-v2 and encodes queries using a masked LM model trained on mMARCO Japanese dataset.
  - Downloads: 35
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - Fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice, CSS10, and JSUT data.
  - Downloads: 34
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization and trained on 42 billion Japanese tokens.
  - Downloads: 34
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - A 1.3B parameter NLLB-200 model fine-tuned to translate Japanese web novel "Ascendance of a Bookworm" into English.
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - A BERT model for nagisa, available via the Transformers library, requiring Python 3.7+ and installable via pip; includes a tokenizer for easy usage.
  - Downloads: 32
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, designed for Japanese POS-tagging and dependency-parsing.
  - Downloads: 31
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - A merged model combining Mistral-7B-Instruct and Japanese-stablelm-instruct-gamma-7b using spherical linear interpolation (slerp).
  - Downloads: 31
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - A 1.7B parameter quantized Japanese language model fine-tuned by LINE Corporation, available for use with Hugging Face's transformers library.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a Japanese language model based on the Mamba state-space architecture, developed from work by Albert Gu and Tri Dao. Pretraining code will be published soon.
  - Downloads: 31
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - A BERT-based Japanese POS-tagging model pre-trained on Wikipedia, derived from bert-base-japanese-v2, using UPOS tags for long-unit-words.
  - Downloads: 30
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - A fine-tuned T5-base model for Japanese title generation, based on sonoisa/t5-base-japanese and pretrained on a large Japanese corpus.
  - Downloads: 30
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - The repository contains the Japanese-LLaMA-2-13B model in GGUF format.
  - Downloads: 30
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - A RoBERTa model pretrained on Japanese Aozora texts for POS-tagging and dependency parsing, using goeswith for subwords.
  - Downloads: 30
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing of Japanese short-unit-words.
  - Downloads: 29
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - A RoBERTa model pre-trained on Japanese Aozora texts for UPOS tagging, specifically useful for long-unit-word part-of-speech identification.
  - Downloads: 29
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - A pre-trained ModernBERT large model on Japanese Aozora texts, available for fine-tuning in tasks like POS-tagging and dependency-parsing.
  - Downloads: 28
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - A fine-tuned 7B-parameter Japanese language model based on Gamma 7B, capable of generating fanfiction content with some knowledge of ACG media, designed for experimental cultural enhancement without compromising quality.
  - Downloads: 28
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - A pre-trained T5 model on a Japanese corpus of approximately 890GB, including Wikipedia and mC4 data, requiring fine-tuning for specific tasks and potentially subject to biased outputs.
  - Downloads: 28
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - An experimental FastText word embedding model for Japanese, including installation instructions and a usage example with Google Colaboratory.
  - Downloads: 28
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository includes GPTQ model files for Stability AI's Japanese StableLM Instruct Beta 7B with various parameter permutations.
  - Downloads: 28
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - This model, bert-base-sudachitra-v11, is a variant of SudachiTra with changes in word form type and vocab.txt structure.
  - Downloads: 28
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - A BERT large Japanese model pre-trained for POS tagging and dependency parsing, with long-unit-words tagged by UPOS.
  - Downloads: 28
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - The repository provides a tokenizer for Japanese BERT using Vaporetto and Unigram, requiring the dictionary file to be downloaded and specified for loading.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - A DeBERTa(V2) model pretrained on ÈùíÁ©∫ÊñáÂ∫´ for dependency parsing and question-answering, designed for head detection in long-unit words.
  - Downloads: 26
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - ALoRA-finetuned Japanese chat model based on rinna/japanese-gpt-neox-3.6b for playful chatting, updated with dataset enhancements.
  - Downloads: 26
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - A 8-layer trained model on 800,000 Japanese sentences derived from oshizo/japanese-e5-mistral-7b_slerp.
  - Downloads: 26
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´ for UPOS and FEATS tagging, enabling POS-tagging and dependency-parsing.
  - Downloads: 26
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing.
  - Downloads: 26
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on Japanese Aozora texts for UPOS and FEATS tagging, derived from roberta-base-japanese-aozora-char.
  - Downloads: 26
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - A BERT model pre-trained on Japanese Wikipedia text, enhanced with comprehensive character embeddings for fine-tuning in various NLP tasks.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, providing POS-tagging and dependency-parsing capabilities.
  - Downloads: 25
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - A finely-tuned Japanese-StableLM-Base-Alpha-7B model emulateing Marisa Shirai from Gensokyo, allowing conversational interaction.
  - Downloads: 25
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - A fine-tuned GPT-J 6B model allowing conversation with Marisa Kirisame from the Oriental Project, using a prompt-based approach in GoogleColab, withÈáèÂåñÈÖçÁΩÆËÆæÁΩÆ„ÄÇ
  - Downloads: 25
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - An ELECTRA model pretrained on 200M Japanese sentences and fine-tuned for UD_Japanese_BCCWJ using spaCy v3, distributed as a Python package named ja_ginza_electra.
  - Downloads: 25
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository includes GPTQ model files for Stability AI's Japanese StableLM Instruct Gamma 7B with various parameter permutations.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - Download and specify the Vaporetto + BPE dictionary file to load the tokenizer for Japanese BERT-base.
  - Downloads: 25
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - A BERT large Japanese model for universal part-of-speech tagging, derived from bert-large-japanese-char-extended and pretrained on Wikipedia texts.
  - Downloads: 25
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - A merged language model for role-playing, based on Aratako/Ninja-v1-RP-WIP, with enhanced instruction-following and expressiveness through task vectors and model merging.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - The repository hosts a 4-bit quantized version of the llm-jp-3-172b-instruct3 model from NII for reduced inference GPU/memory usage.
  - Downloads: 24
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - A DeBERTa(V2) model pretrained on Aozora Bunko for dependency-parsing and question-answering, using [MASK] to handle ambiguous words.
  - Downloads: 24
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - A RoBERTa model pretrained on Aozora texts for POS-tagging and dependency-parsing, available via Hugging Face transformers pipeline.
  - Downloads: 24
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - A model combining English chat vectors and fine-tuned Japanese language models to generate light bedtime stories.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for POS tagging, derived from deberta-base-japanese-aozora and available for use with Hugging Face Transformers.
  - Downloads: 23
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ for dependency-parsing and question-answering with specific context handling.
  - Downloads: 23
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - A 50k-step fine-tuned Hubert model for Japanese using multiple corpora, subject to specific terms of use.
  - Downloads: 23
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This GitHub repository contains AWQ model files for Stability AI's Japanese StableLM Instruct Beta 70B, quantized with support from Massed Compute.
  - Downloads: 23
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - This repository contains pre-trained and LoRA fine-tuned large language models, including instruction models in various formats from the LLM-jp project.
  - Downloads: 23
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - A pre-trained Japanese DeBERTa V2 base model for masked language modeling, trained on Japanese Wikipedia and parts of CC-100 and OSCAR.
  - Downloads: 23
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - The repository provides instructions and code for loading a Japanese BERT tokenizer using Sudachi and WordPiece dictionaries.
  - Downloads: 23
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - A RoBERTa Japanese model pretrained on 200M sentences with increased position embeddings to 1282, using Juman++ and SentencePiece tokenization.
  - Downloads: 23
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - A RoBERTa model pre-trained on Aozora texts for Japanese, using the Japanese-LUW tokenizer, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 23
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card Ëã±Êó•„ÄÅÊó•Ëã±ÁøªË®≥Áî®„É¢„Éá„É´C3TR-Adapter„ÅÆGPTQ4bitÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ for UPOS tagging, available for token classification tasks.
  - Downloads: 22
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer that uses the Nothing + Unigram approach.
  - Downloads: 22
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - A converted dataset of AIBunCho's public model for ctranslate2 with 8-bit quantization and reduced accuracy.
  - Downloads: 22
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - A merged model combining Japanese-stablelm and Mistral-7B instruct versions with a spherical linear interpolation (slerp) merge method.
  - Downloads: 22
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese texts for POS-tagging and dependency-parsing, with long-unit-word tagging by UPOS FEATS.
  - Downloads: 22
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is an enhanced GPT-2-based language model for kana-to-hanzi conversion, fine-tuned from ku-nlp/gpt2-small-japanese-char, with improved performance and additional features.
  - Downloads: 21
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - The GitHub repository megagonlabs/t5-base-japanese-web-8k contains training codes for a T5 model pre-trained on Japanese web texts with an 8K vocabulary size.
  - Downloads: 21
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - A RoBERTa model pretrained on Japanese texts for POS-tagging and dependency-parsing, using goeswith for subwords, with instructions for universal-dependencies pipeline usage.
  - Downloads: 21
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - A 7B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, based on Japanese Stable LM Base Gamma 7B and trained with notus code.
  - Downloads: 21
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - This repository offers a 3.6B parameter quantized Japanese language model fine-tuned by LINE Corporation, along with instructions for usage.
  - Downloads: 21
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository offers a 1.7BÂèÇÊï∞ÁöÑÈáèÂåñÊó•ËØ≠ËØ≠Ë®ÄÊ®°ÂûãÔºåÁî±LINE CorporationËÆ≠ÁªÉÂíåÂæÆË∞ÉÔºåÂåÖÊã¨‰ΩøÁî®ÊñπÊ≥ïËØ¥Êòé„ÄÇ
  - Downloads: 21
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - The repository provides a BERT-base tokenizer for Japanese using Sudachi and BPE, requiring the dictionary file path for loading.
  - Downloads: 21
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - A fine-tuned BPR question encoder for document retrieval using the bert-base-japanese-v3 model, introduced in Chapter 9 of "Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ," with related resources and notebooks provided.
  - Downloads: 21
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, derived from roberta-large-japanese-aozora.
  - Downloads: 21
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - A RoBERTa model pre-trained on Aozora Bunko texts for Japanese, using the Japanese-LUW tokenizer, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 21
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - A BERT large Japanese model pre-trained on Wikipedia for UPOS tagging of long-unit-words.
  - Downloads: 21
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - A BERT model fine-tuned on Japanese character-level data for classifying genres of novel titles and introductions.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - A LoRA-fine-tuned open-calm-large model for Japanese language processing using PyTorch and PEFT.
  - Downloads: 20
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - The GitHub repository rinna/nekomata-14b-gguf provides a GGUF version of the nekomata-14b model for lightweight inference with llama.cpp, recommending specific quantization settings.
  - Downloads: 20
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - A novel generation model trained on high-quality light-novel texts and other sources using QLoRA, with instruction templates to guide story direction based on 2021-era information.
  - Downloads: 20
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository contains quantized AWQ model files for Stability AI's Japanese StableLM Base Beta 70B, supported by a grant from a16z and hosted on Massed Compute hardware.
  - Downloads: 20
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - LINE Corporation's 3.6B parameter Japanese language model, fine-tuned and quantized for instruction-following tasks, supports efficient inference after tokenizer loading.
  - Downloads: 20
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - A 1.7B parameter quantized Japanese language model fine-tuned by LINE Corporation, available for use via Hugging Face transformers.
  - Downloads: 20
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - A Japanese inference model trained using the Databricks-Dolly-15K-JA dataset, based on rinna's "japanese-gpt-1b," following inu-ai/dolly-japanese-gpt-1b.
  - Downloads: 20
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS and FEATS tagging, using long-unit-word units.
  - Downloads: 20
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - A merged model extending Japanese vocabulary on the Mixtral-8x7B-Instruct-v0.1 base, evaluated as an intermediate result on ABEJA's tech blog.
  - Downloads: 20
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - This GitHub repository offers REV-Mix models for generating high-quality anime and realistic images, using recommended samplers with specific parameters.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - A RoBERTa model pretrained on Japanese AozoraBunko for dependency-parsing and question-answering, using [MASK] to handle ambiguities in multiple-used words.
  - Downloads: 19
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - This repository offers a 3.6B parameter quantized Japanese language model fine-tuned by LINE Corporation, along with instructions for loading and using the model via Hugging Face's transformers library.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - A 4-bit fine-tuned Japanese version of Llama-2-Chat 70B using the Alpaca-JA dataset, with instructions for Hugging Face usage and Meta's license agreement.
  - Downloads: 19
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - A RoBERTa model pretrained on Japanese texts for POS-tagging and dependency-parsing, using goeswith for subwords, with instructions for universal dependencies task usage.
  - Downloads: 18
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using a Nothing + BPE dictionary file.
  - Downloads: 18
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - A QLoRA-fine-tuned Llama-2-13b-chat-hf model trained on 50,000 chat and 280,000 non-chat samples for improved Chinese and Japanese performance, tested with test.py.
  - Downloads: 18
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - Download MeCab + Unigram dictionary to load Japanese BERT-base tokenizer using specified path.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - A fine-tuned LLM for transcribing audio containing Dominion board gameÊúØËØ≠ in Japanese using Whisper, based on the open-ai/large model.
  - Downloads: 18
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - A merged model of Swallow-MX-8x7b-NVE-v0.1 with Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1, optimized for natural Japanese language output and large local language models.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing of long-unit-words.
  - Downloads: 17
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - A modified KiwiMix model, credit to various contributors including lametta and spekulatius, with deformed character expression and seed-affecting variations.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using Vaporetto + WordPiece.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using a Juman++ + WordPiece dictionary file.
  - Downloads: 17
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - The repository provides a Japanese BERT-base tokenizer using MeCab and WordPiece, requiring the dictionary files for setup.
  - Downloads: 17
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Aozora texts for POS-tagging and dependency-parsing, using goeswith for subwords.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - A RoBERTa model pretrained on Japanese Aozora texts for head-detection in dependency-parsing and question-answering, using [MASK] to handle ambiguous words.
  - Downloads: 16
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This model was trained using H2O LLM Studio's cyberagent/open-calm-7b on a transformed dataset from "AIÁéã „Äú„ÇØ„Ç§„Ç∫AIÊó•Êú¨‰∏ÄÊ±∫ÂÆöÊà¶„Äú", and can be used for text generation with the transformers library on GPU-equipped machines.
  - Downloads: 16
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - A fine-tuned Japanese chat model based on abeja/gpt-neox-japanese-2.7b for personal interest and study, using ebisuke/liz-nojaloli-ja-ds dataset.
  - Downloads: 16
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero Base model and 15K instruction data from the Jaster dataset (train).
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - A Japanese vocabulary-extended pre-trained model based on Mixtral-8x7B-Instruct-v0.1 for causal language modeling.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - A Japanese vocabulary-enhanced intermediate version of the Mixtral-8x7B-Instruct model for continued pre-training, evaluated via ABEJA's tech blog.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 16
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is an instruction-following model developed on ConoHa VPS with NVIDIA H100 GPU, based on Meta-Llama-3-8B-Instruct.
  - Downloads: 15
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, derived from roberta-small-japanese-aozora-char.
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - The repository provides instructions for loading a Japanese BERT tokenizer using Sudachi and Unigram dictionaries.
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - Download the Juman++ + Unigram dictionary file to load a Japanese BERT-base tokenizer.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, derived from deberta-large-japanese-aozora.
  - Downloads: 15
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English chat model fine-tuned on 42 billion tokens and trained using direct preference optimization.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - A BERT model pretrained on Japanese Wikipedia for dependency-parsing and question-answering, featuring head detection for long-unit words.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for POS-tagging and dependency-parsing, usable via a transformers pipeline.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using a downloaded dictionary file.
  - Downloads: 14
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - The GitHub repository contains the Japanese-LLaMA-2-7B model in GGUF format.
  - Downloads: 14
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Aozora texts for POS-tagging and dependency-parsing, usable via Hugging Face transformers pipeline.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - A model trained via SFT for Q&A and context retrieval, quantized with AutoAWQ, achieving GPT3.5+ performance in a 7B parameter model. Includes learning, evaluation, and sample codes.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese-English chat model fine-tuned on 42 billion tokens and optimized for direct preference feedback.
  - Downloads: 13
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - The repository contains Rust implementations and tools for using the BERT large Japanese model from Tohoku University.
  - Downloads: 12
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository contains a fine-tuned BERT model for CommonsenseQA tasks using cl-tohoku/bert-large-japanese-v2 and the JGLUE/JCommonsenseQA dataset.
  - Downloads: 12
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - A quantized Japanese instruction-tuned version of Llama 2, compatible with Colab A100 or RTX 3000 Series due to architecture requirements.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository contains the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1, with Q4_K_M currently available and additional versions planned based on demand.
  - Downloads: 12
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository contains a GPTQ quantized model of ELYZA-japanese-CodeLlama-7b-instruct calibrated on a Japanese dataset, including 1k Wikipedia samples and additional task data.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - A pre-trained ModernBERT model for Japanese text, fine-tunable for tasks like POS-tagging and dependency-parsing.
  - Downloads: 12
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - A DeBERTa(V2) model pretrained on Japanese texts for dependency-parsing and question-answering, using UNIDIC and UD_Japanese-GSDLUW.
  - Downloads: 11
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - A fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana recognition, trained on Common Voice and JSUT data, with instructions for direct usage.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese texts for POS-tagging and dependency-parsing, using goeswith for subwords, with instructions for universal-dependencies pipeline usage.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese language model based on the Mamba state-space architecture, fine-tuned on 31,700 JaQuAD dataset examples.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - A 7B-parameter Japanese language model fine-tuned for instruction-following, built on Japanese Stable LM Base Gamma 7B.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - A Japanese transformer pipeline using BERT-base, including transformer, parser, and NER components, version 3.1.1, licensed under CC BY-SA 4.0 by Megagon Labs Tokyo.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - A pretrained TTS model from ESPnet2 using the jsut/tts1 recipe by kan-bayashi.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - A 7B-parameter Japanese instruction-following language model fine-tuned on Stable LM Base Gamma 7B, requiring Transformers 4.34.0 or newer for usage.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - A 7-billion-parameter Japanese instruction-following language model fine-tuned on instructive datasets, built on the Japanese Stable LM Base Gamma 7B model.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - A 7-billion-parameter Japanese language model fine-tuned for instruction-following, built on the Base Gamma 7B model, requiring Transformers 4.34.0 or later for usage.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - A 7-billion-parameter Japanese instruction-following language model fine-tuned on Stable LM Base Gamma 7B, requiring Transformers 4.34.0 or newer for usage.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Syntactic Text Processing
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - The repository contains OpenCALM-small, a pre-trained Japanese language model by CyberAgent, along with its tokenizer and basic usage examples.
  - Downloads: 5,604
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - The repository uses CreativeML Open RAIL-M license with an added copyright for "‰ΩêÂüéÈÉéÁîª" (sazyou_roukaku), and disclaims responsibility for any misuse, except violating usage limit A for criminal or specialized medical purposes.
  - Downloads: 4,898
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - The repository provides documentation and code for using the CyberAgent-developed OpenCALM-large model, a decoder-only language model pretrained on Japanese datasets.
  - Downloads: 4,476
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô /
  - Downloads: 3,944
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B, a Japanese decoder-only language model by CyberAgent, Inc., supports causal language modeling tasks and can be easily loaded using Hugging Face's transformers library.
  - Downloads: 3,236
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow model, derived from Llama 2 with Japanese data addition and SFT, includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 3,085
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - A gguf format conversion of the Ninja-v1-NSFW-128k model for local use, based on imatrix dataset, with instructions for running via llama.cpp.
  - Downloads: 2,642
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - The Swallow model series, derived from Llama 2 with Japanese data added, uses SFT and includes versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 2,570
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1 enhances Mistral 7B with additional Japanese pre-training to boost Japanese language efficiency in a 120k-tokenized model.
  - Downloads: 2,057
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow model series, derived from Llama 2 with Japanese data added and SFT fine-tuned, includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 1,828
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation results for various Japanese models on MIRACL and JQaRA datasets show improvements in nDCG@10, Recall@1000, and other metrics with newer versions.
  - Downloads: 1,495
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores old, obsolete, and play/use-only models, including merger materials that slightly deform models in the style of lametta for experimentation.
  - Downloads: 1,459
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-7B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,413
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - The GitHub repository contains Swallow model versions fine-tuned with Japanese data from the Llama 2 family, including instruct-tuned releases scheduled for April 26, 2024.
  - Downloads: 1,135
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,101
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-13b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,057
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow model, derived from Llama 2 with Japanese data addition and SFT, includes versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 1,008
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow model, derived from Llama 2 with Japanese data pre-training and SFT tuning, includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 997
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - The repository provides the OpenCALM-3B, a pre-trained Japanese decoder-only language model from CyberAgent, along with instructions on how to load and use it via Hugging Face's Transformers library.
  - Downloads: 944
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - The Swallow-MX-8x7b-NVE-v0.1 model is a pre-trained Japanese and English language model derived from Mixtral-8x7B-Instruct-v0.1, with added Japanese data.
  - Downloads: 942
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - Ninja-v1-NSFW-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-NSFW„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 941
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - The repository provides a pre-trained medium-sized OpenCALM decoder-only language model for Japanese text processing, including the model and tokenizer for inference.
  - Downloads: 910
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository contains the gguf format conversion of llm-jp-3-7.2b-instruct3, created by llm-jp, using imatrix-dataset-for-japanese-llm and compatible with llama.cpp except for custom chat templates.
  - Downloads: 839
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãJapanese-Chat-Umievo-itr001-7b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 830
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-8b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-70b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 732
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãc4ai-command-r-plus„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 675
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 628
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow model series, derived from Llama 2 and fine-tuned with Japanese data, includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 607
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - umiyuki-Umievo-itr012-Gleipnir-7B-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãUmievo-itr012-Gleipnir-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 607
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHonyaku-13b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 604
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - The repository features continually pre-trained Swallow models from Llama 2 with Japanese data, released as SFT-tuned versions like Swallow-7b-instruct-v0.1 on April 26, 2024.
  - Downloads: 584
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow model, derived from Llama 2 with Japanese data pre-training and SFT tuning, includes version 0.1 released on April 26, 2024.
  - Downloads: 583
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow model series, derived from Llama 2 with Japanese data incorporation and SFT tuning, includes instruct-tuned versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 572
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow model series, derived from Llama 2 with Japanese data addition and SFT tuning, includes instruct-tuned versions released on April 26, 2024.
  - Downloads: 565
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow model is an Llama 2 derivative with Japanese data added and SFT-tuned, with version 0.1 releases scheduled for April 26, 2024.
  - Downloads: 546
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - The GitHub repository features continually pre-trained Swallow models from the Llama 2 family, enhanced with Japanese data and released in versions 0.1 for instruction tuning.
  - Downloads: 535
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - A gguf-format conversion of shisa-7b-v1 for use with llama.cpp, demonstrating inference commands in English and Japanese.
  - Downloads: 527
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãQwen1.5-110B-Chat„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 518
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow model family includes Japanese-pretrained Llama 2 variants with instruction-tuned SFT versions released in April 2024.
  - Downloads: 516
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-128k„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 408
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - The GitHub repository provides a Stanza model for analyzing Japanese text, including syntactic analysis and entity recognition.
  - Downloads: 297
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI has released a gguf-formatted version of the base EvoLLM-JP-v1-7B model, along with instructions for using it with llama.cpp.
  - Downloads: 282
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - A Q4_0 quantized gguf format conversion of Deepreneur's blue-lizard 7B model for use with llama.cpp, licensed under the llama2 license.
  - Downloads: 266
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - A GitHub repository containing a Whisper-large-v2 Japanese model converted using CTranslate2 for audio transcription.
  - Downloads: 205
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-A-v1-7B-gguf is a base model in gguf format derived from EvoLLM-JP-A-v1-7B for AI language use, compatible with llama.cpp.
  - Downloads: 193
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - A fine-tuned Whisper large-v2 model on Japanese CommonVoice v11 for 5000 steps, achieving loss 0.4200 and WER 0.7449, intended for research with potentially unsatisfactory transcriptions.
  - Downloads: 183
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - The repository contains details and setup instructions for the Japanese InstructBLIP Alpha model, which generates Japanese descriptions for images and can incorporate text inputs.
  - Downloads: 174
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - This repository hosts a 12-layer ELECTRA Small model pretrained on 354 million sentences from the YACIS blog corpus using WordPiece tokenization.
  - Downloads: 161
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B-parameter Japanese chat model derived from "chatntq-ja-7b-v1.0" based on Mistral-7B-v0.1.
  - Downloads: 151
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This GitHub repository contains a beta version of a Japanese text-to-speech model derived from Parler-TTS, offering lightweight yet high-quality voice generation, but requires a unique tokenizer.
  - Downloads: 142
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmark„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãstockmark-100b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 137
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow model, based on Llama 2 with Japanese data, uses SFT; release schedule includes versions 0.1 for 7B, 13B, and 70B models on April 26, 2024.
  - Downloads: 134
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 133
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID ÊñôÁêÜ„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„ÅÆË≥™ÂïèÊñá„Åã„Çâ„ÄÅÊ§úÁ¥¢Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ„Åß„ÅÇ„ÇãÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫„Åó„Åæ„Åô Model Details Model Description ‰æã„Åà„Å∞„ÄÅ„ÄåÊù±‰∫¨„ÅÆËÇâÊñôÁêÜ„Åß„ÄÅÊò•„Å´È£ü„Åπ„Çâ„Çå„Çã„ÄÅÈ∂èËÇâ„Çí‰Ωø„Å£„ÅüÊñôÁêÜ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊñáÁ´†„ÇíÂÖ•Âäõ„Åô„Çã„Å®„ÄÅ „ÄåÊù±‰∫¨ ‚Üí ÈÉΩÈÅìÂ∫úÁúå/Âú∞Êñπ(AREA)„Äç „ÄåËÇâÊñôÁêÜ ‚Üí Á®ÆÈ°û(TYPE)„Äç „ÄåÊò• ‚Üí Â≠£ÁØÄ(SZN)
  - Downloads: 127
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - Weighted/imatrix quants for the Japanese Llama-3 8B instruct model are available, with GGUF Q2_K weighing 3GB.
  - Downloads: 111
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - The repository hosts weighted and imatrix quants for the Japanese-Starling-ChatV-7B model, including GGUF Q2_K and Q3_K_S versions, with instructions on usage and links to additional details.
  - Downloads: 104
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 99
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - A model for inferring proverbs.
  - Downloads: 96
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 93
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakuten„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãRakutenAI-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 85
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - A Japanese corpus-trained Bloom model with vocab_size=10000, hidden_sizeÊú™ÊåáÂÆö, n_head=8, and n_layer=12.
  - Downloads: 74
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 54
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps model to CTranslate2 format for use with faster-whisper and other CTranslate2-based projects.
  - Downloads: 53
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - The repository converts NTQAI/chatntq-ja-7b-v1.0, a Japanese chat model fine-tuned on StabilityAI's Japanese-StableLM-Base-Gamma-7b, to GGUF format.
  - Downloads: 44
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiÊßò„ÅÆ Japanese-Chat-Umievo-itr004-7b „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 38
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - „ÅØ„Åò„ÇÅ„Å´ „Å™„Çì„ÅãÊó•Êú¨Ë™û„ÅåË©±„Åõ„ÇãÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™AI„Åß„Åô„ÄÇ
  - Downloads: 35
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - A model for generating article text from titles.
  - Downloads: 28
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository contains a fine-tuned Reward model for evaluating the quality of Japanese novels, particularly intended for use in reinforcement learning with generative models, although user evaluations may be biased by factors like genre and style.
  - Downloads: 27
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - A q4_0 quantized format base model of Tanuki-ZeRo in gguf for natural language processing, compatible with llama.cpp.
  - Downloads: 27
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - calm3-22b-RP-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version „Åæ„Åü„ÄÅ„Åì„Å°„Çâ„ÅßÊú¨„É¢„Éá„É´„ÅÆ„Éá„É¢„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 25
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - The repository contains a Japanese-vocabulary-extended version of the Mixtral-8x7B model for causal language modeling.
  - Downloads: 23
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This GitHub repository contains a specialized model trained on SakanaAI/TinySwallow-1.5B-Instruct to generate concise, impactful text for each slide following the "È´òÊ©ã„É°„ÇΩÊñπÊ≥ï" style, creating multiple slides from user input.
  - Downloads: 22
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.2 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAE„ÅÆÂÜÖËáì„ÅØ„Å™„ÅÑ„ÅûÔºÅ„Å®Ë®Ä„Çè„Åõ„Å™„ÅÑ„ÅûÔºÅÔºÅÔºÅÔºÅ
  - Downloads: 21
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - The repository is for Japanese users who must use AutoTokenizer and AutoModelForCausalLM with Unifine format, incorporating in-context learning and instruction learning through formatted input text examples.
  - Downloads: 18
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This series shares unintended but interestingly merged models from spekulatius, based on experiments with lametta_v1921, without claiming high functionality.
  - Downloads: 18
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - A Japanese SentencePiece tokenizer trained for AI Novelist's SuperTrin and Damsel 20B models with a vocab size of 52000 (padded to 52224).
  - Downloads: 18
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - This GitHub repository contains a Japanese fine-tuned model derived from Qwen and DeepSeek models, including additional fine-tuning by Cyber Agent, optimized for causal language modeling without long inference time.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository, hosted on Civitai as AfterRealXL_beta2, includes a model licensed under CreativeML Open RAIL++-M and contains merge checkpoint lists.
  - Downloads: 17
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B „Äå„Å©„ÅÜ„Åã„ÅäÊÖàÊÇ≤„Çí „ÇÇ„ÅÜ Áñ≤„ÇåÊûú„Å¶„Åæ„Åó„Åü„Äç ÁîüÊàê‰æã [Â§™Â≠ó‰ª•Èôç„ÅåAIÁîüÊàê] „Äå„Å©„ÅÜ„Åã„Äç ‚Äù„Åù„Çå‚Äù„ÅØÊááÈ°ò„Åó„Åü„ÄÇ
  - Downloads: 17
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteus„Çí„Éô„Éº„Çπ„Å´LLava„Å´ÂØæÂøú„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - The repository challenges NER using modernBERT with custom Japanese labels for names, organizations, locations, and more.
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - A commerciallyÂèØÁî®ÁöÑÊó•Êú¨ËØ≠ÁâàÊú¨Gemma-2BÊåÅÁª≠È¢ÑËÆ≠ÁªÉÂü∫Á°ÄÊ®°ÂûãÔºåÈÄÇÂêàÁßªÂä®ËÆæÂ§áÔºåÂèØÈÄöËøáColabÂ∞ùËØï„ÄÇ
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: Âº∑ÂåñÂ≠¶Áøí„ÇíÁî®„ÅÑ„Å¶„Ç≠„É£„É©„ÇØ„Çø„Çâ„Åó„Åï„Çí‰ªò‰∏é„Åó„ÅüÈõëË´áÂøúÁ≠î„ÅÆÁîüÊàê
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - A translation model using Marian-NMT for English to Japanese translation with transformers and sentencepiece.
  - Downloads: 55,754
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - A translation model using Marian-NMT for Japanese to English translation, implemented with transformers and sentencepiece.
  - Downloads: 54,889
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - A fine-tuned Japanese version of the Shisa 7B model based on the Gamma 7B base, with favorable Ja MT-Bench performance results shared for reference.
  - Downloads: 30,373
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, English, and Japanese.
  - Downloads: 5,955
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - A 36-layer, 2816-hidden-size GPT-NeoX model trained on 524B tokens for English-Japanese translation.
  - Downloads: 5,361
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a 2B-parameter SLM optimized for Japanese-English and English-Japanese translation with near-7B-quality performance and a small 2GB file size.
  - Downloads: 5,049
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - A Japanese-to-Malay translation repository using transformer-align models, featuring pre-processing with normalization and SentencePiece, and including test set translations.
  - Downloads: 3,193
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - YOLO11 is a high-performance, versatile object detection and tracking model with advanced features for enhanced accuracy and usability.
  - Downloads: 2,292
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a bilingual Japanese and English chat model optimized for strong Japanese performance and robust English capabilities, using synthetic data and an efficient Japanese tokenizer.
  - Downloads: 2,225
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - The FINGU-AI/FinguAI-Chat-v1 model offers specialized curriculum content for English, Korean, and Japanese speakers to enhance their language proficiency in finance, investment, and legal frameworks with a global market perspective.
  - Downloads: 2,037
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - A gguf-format conversion of the Japanese Suzume-Llama-3-8B model by lightblue, using imatrix datasets.
  - Downloads: 1,992
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsuzume-llama-3-8B-multilingual„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,495
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin-inst-merge„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,273
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B series models are open-source, multilingual large Language models trained from scratch by OrionStarAI on a 2.5T multilingual corpus.
  - Downloads: 1,229
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-MS-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,038
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - It is the GGUF version of Mistral-nemo-ja-rp-v0.2. Detailed information can be found in the original model.
  - Downloads: 673
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 646
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 529
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI, based on a 2.5T multilingual corpus.
  - Downloads: 330
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - A Japanese to Korean translation model using EncoderDecoderModel based on bert-japanese and kogpt2, with inference code and dependencies provided.
  - Downloads: 311
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - The GitHub repository contains the Karasu-DPO-7B, a Japanese version of Qwen/Qwen2.5-7B-Instruct, which outperforms the base model on multilingual chat benchmarks and is recommended for general conversation AI use.
  - Downloads: 305
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source multilingual large language model by OrionStarAI, trained on a 2.5T multilingual corpus including Chinese, English, and Japanese.
  - Downloads: 294
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository contains the gguf conversion of rinna's Japanese GPT-NeoX-3.6B-instruction-ppo model, along with links to related models and notes on potential future compatibility issues.
  - Downloads: 269
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, English, and Japanese.
  - Downloads: 198
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - The repository includes a model licensed under CreativeML Open RAIL-M with an added copyright for sazyou_roukaku, noting limitations on usage and disclaiming responsibility.
  - Downloads: 188
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source multilingual large language model pretrained by OrionStarAI on a 2.5T multilingual corpus.
  - Downloads: 147
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT ElanMT-BT-ja-en is a Japanese to English translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 132
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - A Japanese-English machine translation model for Weiss Schwarz card text, accessible via a Gradio app on Hugging Face Spaces.
  - Downloads: 129
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - A pre-trained ByT5-small model fine-tuned on web-crawled bilingual datasets for translating Ainu to Japanese.
  - Downloads: 125
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
  - Downloads: 125
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - A T5 fine-tuned model trained on the friendly_JA Corpus to use Latin-derived katakana in Japanese translation for Western audiences, with examples showing both input and output.
  - Downloads: 120
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - A doc2query model based on mT5 for document expansion by generating 20-40 queries per paragraph to enhance lexical search and re-weight query terms.
  - Downloads: 118
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - A Japanese CLIP model trained on a translated dataset with 248M parameters, using OpenCLIP, for zero-shot image classification; installation and usage details provided.
  - Downloads: 106
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling 7B is a multilingual pretrained model combining Korean, English, Chinese, Japanese, and 500 other languages, based on the original Gemma-7B.
  - Downloads: 91
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - A long-context Japanese-English translation model based on tinyllama, requiring input of 500-1000 tokens and setting 'do_sample = False' or temperature to 0 for deterministic outputs.
  - Downloads: 90
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus that includes Chinese, English, and Japanese.
  - Downloads: 78
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a translation model using Marian-NMT for translating source languages (de, en, es, fr, it, ru, uk) into Japanese.
  - Downloads: 75
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP Ê¶ÇË¶Å Local-Novel-LLM-project/Ninja-v1-NSFW„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 71
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - SakuraMixSeries ËÉåÊôØ„Å®„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇØ„Ç™„É™„ÉÜ„Ç£„Éº„Çí‰∏°Á´ã„Åï„Åõ„ÅüVAEÂÜÖËîµÂûã„É¢„Éá„É´ Model with built-in VAE for both background and character quality üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá
  - Downloads: 65
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - A 3.8 billion-parameter English-Japanese GPT-NeoX model with an extended context length to 8192 tokens through fine-tuning.
  - Downloads: 59
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - A fine-tuned Llama2-13B model with additional Japanese vocabulary for joke generation, trained on AWS Trainium instances and using a customized token vocabulary of 45,046.
  - Downloads: 59
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - The GitHub repository contains a refined SD-XL 1.0 model for Japanese input, fine-tuned to match Japanese text embeddings by training on aligned English-Japanese data.
  - Downloads: 49
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - A 1.3B parameter NLLB model fine-tuned for Japanese to English translation of light and web novels, capable of processing up to 512 tokens.
  - Downloads: 45
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is an improved Japanese-English bilingual language model using the LEIA training technique, achieving enhanced performance on Japanese QA benchmarks.
  - Downloads: 43
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - LEIA-enhanced Swallow, a Japanese-English bilingual LLM, shows improved performance on Japanese question answering benchmarks.
  - Downloads: 39
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 39
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, English, and Japanese.
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - The repository contains Superswallow-70b-v0.1 with known repetition and temperature bugs, performing worse than Swallow in benchmarks due to potential issues or model limitations.
  - Downloads: 33
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - A fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en and bsd_ja_en dataset.
  - Downloads: 31
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI, based on a 2.5T multilingual corpus including Chinese, English, and Japanese.
  - Downloads: 30
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - The repository contains Japanese-Alpaca-2-13B model in GGUF format, with a provided model URL.
  - Downloads: 30
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus.
  - Downloads: 25
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - The repository contains a finetuned Japanese version of the SD-XL 1.0 base model, which uses fine-tuned OpenCLIP encoders to process Japanese text inputs.
  - Downloads: 25
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - A fine-tuned MPT-7B model evaluated on Jumtra/test_data_100QA, using mpt-7b and other variants for comparison.
  - Downloads: 23
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - The repository includes base and full Japanese-LLaMA-2-7B models, as well as a LoRA model.
  - Downloads: 20
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - A Japanese BERT-VITS2 model trained on the jvnv corpus's F2 data.
  - Downloads: 20
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba Barba is a multilingual natural language inference model for textual entailment and zero-shot text classification, based on XLM-RoBERTa and trained on English, Chinese, Japanese, Korean datasets.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository contains a quantized merged model for translating Japanese context into Chinese, with an example prompt provided.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - A merged MoE model based on the Llama-2 7B-fast and its instruction-tuned version for Japanese, developed using mergekit.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - A MoE model created using mergekit from instruction-tuned and base LLaMA-2 Japanese models for 13B parameters.
  - Downloads: 17
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - A MoE model created using mergekit from tokyotech-llm/Swallow-13b-instruct-hf and nitky/Superswallow-13b-v0.2, with the AI2 ImpACT license potentially applied.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository contains a MoE model merged from instruction-tuned and base Japanese Llama-2 7B models.
  - Downloads: 16
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - The repository includes base and full Japanese-LLaMA-2-13B models, as well as LoRA versions and instruction-following models like Japanese-Alpaca-2-13B and its LoRA variant.
  - Downloads: 16
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 16
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - A BERT-VITS2 Japanese model trained on the jvnv corpus F2 data.
  - Downloads: 15
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model based on Mistral 7B, optimized for efficient Japanese processing and extensive Japanese pre-training.
  - Downloads: 14
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model, optimized for robust Japanese performance with enhanced efficiency, pretrained on extensive Japanese data, and fine-tuned for versatility.
  - Downloads: 14
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model with enhanced Japanese performance and efficient tokenization, built on Mistral 7B and further pretrained with mainly Japanese data.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model, pretrained on extensive Japanese data and optimized with a custom tokenizer, offering strong performance in both languages.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ"chatntq-ja-7b-v1.0"„Çí„Éô„Éº„Çπ„Å´„Åó„Åü7B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 13
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - A compiled Watashiha-Llama-2-13B-Ogiri-sft model optimized for AWS inf2 instances, requiring an EC2 xlarge instance with at least 256GB storage and Deep Learning AMI Neuron PyTorch 1.13, following specific setup instructions.
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - The repository contains full and LoRA models of Japanese-Alpaca-2-13B, based on the Japanese-LLaMA-2-13B model.
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model based on Mistral 7B, optimized for strong Japanese performance with an efficiently extended tokenizer and extensive Japanese pre-training.
  - Downloads: 12
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - Details: https://spacy.io/models/ja#ja_core_news_md Japanese pipeline optimized for CPU.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - A T5 model pretrained on approximately 100GB of Japanese corpus, including Wikipedia and OSCAR datasets, requiring fine-tuning for specific tasks.
  - Downloads: 6,746
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - The repository has been updated to Version 2 as of May 21, 2023, with license and copyright changes including an additional author credit.
  - Downloads: 3,041
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,932
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - This repository hosts SB Intuitions' Japanese autoregressive language models, including sarashina2.2-0.5B-instruct-v0.1, evaluated on multiple tasks and benchmarks.
  - Downloads: 1,813
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository contains a GGUF format conversion of sarashina2.2-3b-instruct-v0.1, with usage instructions for loading and querying the model using llama.cpp.
  - Downloads: 1,769
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual is a collaborative project offering optimized Whisper models for bidirectional speech-to-text translation between Japanese and English.
  - Downloads: 1,725
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository hosts a retrained Japanese TTS model for high-quality text-to-speech, based on Parler-TTS-mini-v1, with its own tokenizer.
  - Downloads: 1,334
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - A quantized gguf version of Qwen2.5-3B-Instruct using a Japanese importance matrix for summarizing long texts up to 32K tokens, preserving significant Japanese support.
  - Downloads: 1,272
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - The repository uses neody/imatrix_dataset from fineweb-edu (English) and fineweb-2 (Japanese), containing 125 randomly selected samples each, for a more precise imatrix dataset compared to wikitext2 or cc100.
  - Downloads: 894
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - The hubert-large-asr model is a fine-tuned version for Japanese Hiragana recognition, initially trained on reazonspeech(small) and further refined on common_voice_11_0.
  - Downloads: 825
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - A fine-tuned version of Google/MT5-small for Japanese summarization using BBC news articles, where the headline serves as the summary.
  - Downloads: 743
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 725
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides aGGUF-formatted version of Rakuten/RakutenAI-2.0-mini-instruct for use with tools like llama.cpp and text-generation-webui.
  - Downloads: 710
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - A GitHub repository containing a gguf-formatted version of ABEJA-Qwen2.5-32b-Japanese-v0.1 for use with ggerganov's llama.cpp, using imatrix-dataset-for-japanese-llm.
  - Downloads: 681
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-4k-instruct-dpo„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 664
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - A pretrained T5 model for Japanese text, trained on approximately 100GB of corpora including Wikipedia and OSCAR, requiring fine-tuning for specific tasks and potentially subject to biases.
  - Downloads: 630
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 602
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - A gguf-format conversion of Qwen's QwQ-32B-Preview model, using imatrix data created with TFMC/imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 518
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - Static quantized weights and imatrix versions of Mistral-Nemo-Japanese-Instruct-2408 models are provided, with usage details and IQ quant recommendations.
  - Downloads: 517
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This GitHub repository hosts a Œ≤Áâà retrained Japanese TTS model based on parler-tts/parler-tts-mini-v1, offering light yet high-quality text-to-speech functionality, though it requires a unique tokenizer.
  - Downloads: 424
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository includes quantized versions of VNTL LLaMA 3 8B qlora with a chat mode for Japanese grammar questions, along with a translation prompt example.
  - Downloads: 283
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - A fine-tuned T5 model on ATOMIC for text-to-text generation, inspired by COMET-T5, using a pipeline with seed setting for reproducibility.
  - Downloads: 264
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - Aratako's sarashina2.1-1b-sft gguf model converted from imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 244
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository includes GGUF quantizations of the VNTL Gemma 2 27B model with chat mode for Japanese grammar questions, along with a translation prompt example.
  - Downloads: 230
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãKarasu-Mixtral-8x22B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 211
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 ja v2 is a fine-tuned GPT-2 model on the large version of ATOMIC ja for causal language modeling.
  - Downloads: 181
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - Full instruction tuning was performed on the base model of line-corporation/japanese-large-lm-1.7B via SFT.
  - Downloads: 181
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„ÇíÊó•Êú¨Ë™ûinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 139
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - A ByT5 model pretrained on a 100GB Japanese corpus including Wikipedia and OSCAR datasets, requiring fine-tuning for specific tasks and potentially subject to biased outputs.
  - Downloads: 138
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - A fused model based on multiple pre-trained language models for code generation, specifically configured for a maximum of 32768 tokens and parameterized for creating a FizzBuzz program in Python.
  - Downloads: 135
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - A model card for a Japanese audio transcription model fine-tuned from distil-whisper/distil-large-v2, specifically for visual novel audio, shared by spow12(yw_nam).
  - Downloads: 121
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - The repository describes a model trained to generate questions from answers and contexts using Japanese SQuAD data, T5 model fine-tuning, and specific hyperparameters.
  - Downloads: 118
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - Quantized GGUF version of the Aratako/c4ai-command-r-v01-japanese-instruct model. For license details, see the original model.
  - Downloads: 100
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 91
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - A fine-tuned Japanese ASR model on uniTKU dataset that predicts Hiragana, with improved WER from 0.668 to 0.337 after 600 training steps.
  - Downloads: 79
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a finetuned model on Japanese web novels based on EleutherAI's GPT-J 6B, featuring 28 layers and rotary position encodings.
  - Downloads: 75
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8b„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂öÂ≠¶Áøí„É¢„Éá„É´„Å´ChatVector„ÇíÈÅ©Áî®„Åó„ÄÅ„Åï„Çâ„Å´QLora„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - The repository uses a modified CreativeML OpenRAIL-M license allowing non-commercial use and generation of images, but restricting creditless use, commercial sale, and different permission settings for shared models.
  - Downloads: 61
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - A refined version of DeepSeek-V3 for Japanese text generation, using top 64 experts per layer based on frequency from MoE models, with improved stability and performance.
  - Downloads: 59
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "ÂΩ±" in Japanese or "Shadow" in English.
  - Downloads: 55
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - A model for generating titles from article text.
  - Downloads: 54
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - A finetuned GPT-2 xl model on the large ATOMIC ja dataset for causal language modeling, usable via a text generation pipeline with reproducible seeds.
  - Downloads: 48
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„Å´Áï∞„Å™„ÇãÊ®©Èôê„ÇíË®≠ÂÆö„Åô„Çã Have different permissions when sharing merges
  - Downloads: 45
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - A T5 model pre-trained on balanced English and Japanese corpora, including Wikipedia dumps and OSCAR datasets, requiring fine-tuning for specific tasks and potentially subject to biased outputs due to training data biases.
  - Downloads: 43
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Llama 3 Youko 70B (rinna/llama-3-youko-70b)
  - Downloads: 39
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest „Åì„ÅÆ„É¢„Éá„É´„ÅØÁîüÁâ©Â≠¶„ÉªÂåªÂ≠¶„Å´Á≤æÈÄö„Åó„ÅüOpenBioLLM-8B„Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™ûÂØæÂøú„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´Llama-3-youko-8b-instruct-chatvector„Å®„Éû„Éº„Ç∏„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - An alpha version of a writer-assisting AI fine-tuned on Cyberagent's calm2-7b-chat, which generates continuations from given text using TextGen-WebUI and trained onÁ∫¶150M tokens of novel text from ShareGPT.
  - Downloads: 23
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - A fine-tuned MPT-7B-instruct model based on the mosaicml/mpt-7b-instruct repository, evaluated on Jumtra/test_data_100QA with a 46% accuracy rate.
  - Downloads: 22
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri-sft is a largeÂñúÂà© language model trained with LLaVA to handle images, using laion/CLIP-ViT-B-32-laion2B-s34B-b79K as the Vision Encoder; it excludes certain COCO 2014 images in training and uses Japanese Visual Genome VQA data for fine-tuning.
  - Downloads: 21
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - A machine learning framework for detecting gender from Japanese names, with a ROMAJI input format and labeled output.
  - Downloads: 17
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - A QLoRA-fine-tuned LLaMA2-7B model trained on Guanaco with 49,000 chat samples, enhancing Chinese and Japanese performance, available for testing via test.py.
  - Downloads: 17
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - A 7B-parameter Japanese language model fine-tuned on preference datasets, built on the STF model and trained with notus codebase using a machine-translated Ultrafeedback dataset.
  - Downloads: 14
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 14
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - A 7B-parameter Japanese language model fine-tuned on preference datasets, trained with notus code base using a machine-translated Ultrafeedback dataset.
  - Downloads: 12
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR uses Vision Encoder Decoder for high-quality Japanese text recognition in manga, addressing unique challenges like furigana and low-quality images.
  - Downloads: 119,730
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - A gguf version of DeepSeek-V3-slice-jp64, a MoE model reconfigured withÁ≤æÈÄâÁöÑÊó•Êú¨ËØ≠ÊñáÊú¨‰∏≠ÁöÑÈ´òÈ¢ë‰∏ìÂÆ∂Â±ÇÔºåÂü∫‰∫éDeepSeek-V3„ÄÇ
  - Downloads: 62,014
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2 is a 619M parameter subword-based RNN-T speech recognition model using an improved Conformer architecture for long-form Japanese audio inference.
  - Downloads: 33,502
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - This repository contains a Japanese CLIP model trained by rinna Co., Ltd., specifically the ViT-B/16 version, along with installation and usage instructions.
  - Downloads: 28,242
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - clip-japanese-base This is a Japanese CLIP (Contrastive Language-Image Pre-training) model developed by LY Corporation.
  - Downloads: 13,024
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is an enhanced Japanese ASR model that integrates additional postprocessing for punctuation, built on kotoba-tech/kotoba-whisper-v2.0.
  - Downloads: 6,244
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - A gguf-formatted version of QwQ-32B by Qwen, created from imatrix data using TFMC/imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 1,417
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a speech foundation model offering ASR, LID, SER, and AED capabilities.
  - Downloads: 1,360
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - A gguf-formatted conversion of the Mistral-Nemo-Japanese-Instruct-2408 model by cyberagent, using imatrix dataset from TFMC/imatrix-dataset-for-japanese-llm for training.
  - Downloads: 1,253
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, with additional postprocessing stacks integrated as pipeline.
  - Downloads: 1,004
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-medium-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 669
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - A Japanese-language CLIP model derived via distillation from an English version, with sample codes and applications for multimodal processing.
  - Downloads: 656
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - A fine-tuned Donut model on a visual-novel-like synthetic dataset for recognizing text in images, exemplified by Japanese sample predictions.
  - Downloads: 636
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet TDT-CTC 0.6B is an ASR model developed by NVIDIA NeMo for transcribing Japanese speech with punctuation, based on the Hybrid FastConformer architecture.
  - Downloads: 631
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - A vision-language model for conversing about images in Japanese, based on the Heron library and available as a 7B parameter StableLM variant.
  - Downloads: 424
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - A fine-tuned wav2vec2-base model for Japanese Hiragana recognition from the common_voice_11_0 dataset, inspired by vumichien/wav2vec2-large-xlsr-japanese-hiragana, achieving reduced training and validation losses.
  - Downloads: 363
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - A fine-tuned Japanese-Hubert-base model for ASR tasks using common_voice_11_0 data, capable of predicting Hiragana only.
  - Downloads: 343
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - The SpeechT5 model is fine-tuned for Japanese speech synthesis using the JVS dataset, producing 100 speaker-specific and gender-segregated embeddings for voice-independent text-to-speech.
  - Downloads: 301
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - A fine-tuned XLSR-53 model for Japanese phone-call speaker diarization using CallHome data.
  - Downloads: 269
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - Fine-tuned Wav2Vec2-Large-XLSR-53 on Japanese using Common Voice and JSUT corpora, for 16kHz sampled speech input.
  - Downloads: 262
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - Recruit Co., Ltd. has released a pre-trained Japanese CLIP model and evaluation dataset for tasks like zero-shot image classification and text-image retrieval.
  - Downloads: 240
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Fine-tuned Wav2Vec2-Large-XLSR-53 on Japanese using Common Voice and JSUT corpora, suitable for 16kHz sampled speech input.
  - Downloads: 235
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a text-to-speech model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 193
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteus„ÅÆGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 179
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - A Japanese data2vec Audio Base model with 12 transformer layers, trained on about 19,000 hours of Japanese audio data by rinna Co., Ltd.
  - Downloads: 172
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - A fine-tuned Whisper-large-v3 model for Japanese processing using Common Voice 16.1 dataset with 4000 steps, achieving WER improvements but showing signs of overfitting.
  - Downloads: 157
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - A fine-tuned Wav2Vec2-Large-XLSR-53 model for {language} ready for direct use with 16kHz sampled speech input.
  - Downloads: 138
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - A Japanese CLIP model for contrastive language-image pre-training, capable of multimodal tasks with ViT-H/14 architecture and CC BY-NC-SA 4.0 license.
  - Downloads: 128
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - A Japanese CLIP (Contrastive Language-Image Pre-training) model using ViT-H/14 for multimodal tasks like zero-shot image classification.
  - Downloads: 126
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - A Japanese CLIP (Contrastive Language-Image Pre-training) model for multimodal tasks, developed by HAKUHODO Technologies Inc., mapping Japanese texts and images to a unified embedding space.
  - Downloads: 126
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR uses Vision Encoder Decoder to recognize Japanese text in manga, handling various text orientations, fonts, and low-quality images.
  - Downloads: 123
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - A model for optical character recognition of Japanese text, particularly targeting manga.
  - Downloads: 123
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Assistance „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 122
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - A Whisper_large_v3 model fine-tuned for Japanese speech transcription to Katakana with pitch accent, trained in two stages using Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 101
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR uses a Vision Encoder Decoder framework for high-quality Japanese text recognition in manga, including various fonts and low-quality images.
  - Downloads: 84
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - A fine-tuned wav2vec2 model for Japanese speech recognition, converting Kanji to Hiragana during training, achieving 23.64% CER on Common Voice 8.0.
  - Downloads: 76
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer This model is a Phoneme Level Speech Recognition network, originally a fine-tuned version of openai/whisper-large-v3 on a mixture of Different Japanese datasets.
  - Downloads: 75
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - Reazonspeech-espnet-next is a repository for the latest ASR models trained by the ReazonSpeech team, aimed at providing cutting-edge research results and incorporating community feedback quickly.
  - Downloads: 68
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository includes the CTranslate2 conversion of the whisper-large-v2-mix-jp model for use in projects like faster-whisper.
  - Downloads: 55
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - A fine-tuned wav2vec2 model transcribes audio into Hiragana with eval metrics of Loss 0.7751 and CER 0.2227.
  - Downloads: 55
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a vision-language model fine-tuned from llm-jp/llm-jp-1.3b-v1.0 using LLaVA method, trained with Vision Projector and Japanese datasets for conversing about images.
  - Downloads: 52
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - A demo and documentation for the Heron BLIP Japanese StableLM Base 7B model, a vision-language model that converses about images.
  - Downloads: 49
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - A vision-language model named Heron BLIP Japanese StableLM Base 7B for conversing about images, using the heron library and accessible via provided code.
  - Downloads: 48
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - A Japanese pretrained VL-T5 model for unifying vision-and-language tasks via text generation.
  - Downloads: 47
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B Transformer-based Japanese text-to-speech model supporting fluent generation and one-shot voice cloning, with inference codeadopted from MetaVoice.
  - Downloads: 40
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - A Japanese-specific Stable Diffusion model generates Pok√©mon images from text input.
  - Downloads: 35
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This GitHub repository offers free, commercial-usemissible childish and unapologetic voice generation for RikkaBotan's ASMR version, with sweet, cool, and Chinese language options for different emotional tones.
  - Downloads: 31
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - A Japanese vision-language model based on LLaVA architecture with 1.3B parameters, featuring a ConvNeXt Large vision encoder and 1024-token context length.
  - Downloads: 30
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - A finetuned VITS-TTS model using Japanese Sakura Miko voice data, developed by Lycoris52.
  - Downloads: 28
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - A voice clone model for Style Bert VITS2 capable of generating text-to-speech voices in English, Japanese, and Chinese with a young and neutral tone.
  - Downloads: 26
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - reazonspeech-espnet-v1 is an ESPnet ASR model trained on 15,000 hours of ReazonSpeech corpus for Japanese speech recognition, requiring 16kHz sampling rate for input audio.
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This fine-tuned wav2vec2-xls-r-1b model uses Public Japanese Voice datasets including Common Voice 7.0, JSUT, JSSS, and CSS10 for research, with a total training dataset of approximately 60 hours.
  - Downloads: 24
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - A fine-tuned Japanese Whisper model based on openai/whisper-tiny, evaluated with loss and WER metrics, trained using specific hyperparameters for real-time ASR in Japanese.
  - Downloads: 23
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository converts the whisper-large-v2-jp model to CTranslate2 format, enabling its use in projects like faster-whisper for audio transcription.
  - Downloads: 23
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - A fine-tuned Whisper model for Japanese using SVJ Japanese dataset and Common Voice 11.0, achieving loss 0.5596 and CER 17.7261 on the evaluation set.
  - Downloads: 21
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion is a Japanese-specific text-to-image model capable of generating realistic images from text input.
  - Downloads: 19
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - A VITS TTS Japanese model fine-tuned with free amitaro voice data, developed by Lycoris52.
  - Downloads: 19
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - A fine-tuned Japanese Whisper-tiny model for automatic speech recognition, trained on Common Voice data with a loss of 0.549100 and WER of 225.233037.
  - Downloads: 19
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - A fine-tuned Wav2Vec2-XLS-R-300M model for recognizing Japanese Hiragana characters with test CER at 9.34%, trained on multiple datasets and optimized for 16kHz audio inputs.
  - Downloads: 17
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR uses Vision Encoder Decoder for high-quality Japanese text recognition in manga, addressing unique challenges like vertical and horizontal texts, furigana, varied fonts, and low-quality images.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
### Natural Language Interfaces
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system trained on large-scale Japanese conversation data, enabling natural turn-taking in real-time.
  - Downloads: 2,950
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - The repository contains static quantized models of Qwen-7B-Japanese, with imatrix quants potentially upcoming, and provides instructions for using GGUF files.
  - Downloads: 1,875
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KUJIRA„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,111
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-RobinHood„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 672
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - The repository offers static quantized models for Qwen-14B-Japanese but warns that imatrix quants may not be available temporarily, and requests can be made via Community Discussion. Usage instructions are provided in TheBloke's READMEs.
  - Downloads: 593
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - A small Japanese DialoGPT trained on dialogue from public domain books hosted by Aozora Bunko, available for demo on Hugging Face Spaces.
  - Downloads: 183
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - A fine-tuned Luke Japanese base lite model for Question-Answering using the DDQA dataset, achieving an accuracy of 0.845933 on SQuAD tasks.
  - Downloads: 139
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository contains a fine-tuned DeBERTa-v2-tiny-japanese model for QA tasks using the DDQA dataset.
  - Downloads: 120
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - A Japanese-instruct-tuned version of the CohereForAI/c4ai-command-r-v01 model using ichikara-instruction, trained on A6000x4 GPUs for 10 epochs with specific LoRA parameters.
  - Downloads: 117
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-KUJIRA „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 80
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - This GitHub repository features Oumuamua-7b-instruct-v2, a merged language model prompting the AI to role-play as a Japanese person for improved message understanding and multi-turn conversation capabilities.
  - Downloads: 65
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - A GitHub repository containing GRPO-trained code to solve simple arithmetic problems, usingËá™Âà∂ËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ
  - Downloads: 64
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - A language model generated from fine-tuning Qwen2.5-7B-Instruct with a dataset of 1225 Chain-of-Thought pairs, combining user-created and borrowed questions, for logical reasoning from prompts to answers.
  - Downloads: 64
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-RobinHood „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 50
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - The repository contains Lightblue's QLoRA finetuned OpenOrca model for Japanese Closed Question Answering, trained on SNOW TyDiQA (Ja), XLSUM (Ja) datasets totaling 13,167 samples.
  - Downloads: 38
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This GitHub repository contains a fine-tuned DeBERTa-V2-Japanese model for QA tasks using the DDQA dataset.
  - Downloads: 33
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - A fine-tuned LukeJapaneseLargeLite model for Question-Answering tasks using the DDQA dataset, achieving an accuracy of 86.3% on strict matching.
  - Downloads: 32
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - A fine-tuned Luke Japanese base lite model for Question-Answering tasks using the JSQuAD dataset with an accuracy of EM: 0.758 and F1: 0.876.
  - Downloads: 28
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - This repository contains a QA model for answering questions about learning Japanese in English, built on the japanese-stablelm-instruct-gamma-7b base model and requiring Transformers 4.34.0 or newer.
  - Downloads: 24
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - A merged model, Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, enhancing Japanese fluency while doubling context length to 32K by integrating Mixtral instruct vectors.
  - Downloads: 20
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
  - Downloads: 16
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - A finetuned Japanese GPT-2 chatbot model based on rinna/japanese-gpt2-medium, fine-tuned on Yuyuyui scenario corpus, generating responses to contextual utterances with character-specific tokens.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 „Åì„ÅÆ„É¢„Éá„É´„ÅØtokyotech-llm/Swallow-MS-7b-instruct-v0.1„ÅÆtokenizer.chat_template„Çí‰ª•‰∏ã„Å´Â§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection„Å®„ÅØÔºü
  - Downloads: 13
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - A model trained using instruction datasets for Japanese.
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - „ÅäÁü•„Çâ„Åõ „Çà„ÇäÂõûÁ≠î„ÅåÈÅ©Âàá„Å´„Å™„Çã„Çà„ÅÜ„Å´Â≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´„ÄÅhttps://huggingface.co/hotchpotch/youri-7b-stf-qa-context-jaqket-jsquad-gptq „ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - A named entity recognition model for Japanese, fine-tuned from cl-tohoku/bert-base-japanese-v3 using the llm-book/ner-wikipedia-dataset, as described in Chapter 6 of "Large Language Model Introduction."
  - Downloads: 82,661
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - A Python model for named entity recognition in Japanese medical documents, including files for tags, attributes, prediction, and input text.
  - Downloads: 5,854
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - A BERT-based model for extracting named entities from Japanese text into 8 categories: person names, organization names, place names, facility names, product names, and event names.
  - Downloads: 1,996
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - The repository includes fastText classifiers for assessing the educational value of Japanese web pages, consisting of a wiki-based classifier trained on academic Wikipedia text and an LLM-based classifier with both licensed under CC BY-SA 4.0.
  - Downloads: 1,703
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - A CRF-fine-tuned BERT model for named entity recognition in Japanese, based on llm-book/bert-base-japanese-v3 and trained on Wikipedia data.
  - Downloads: 1,214
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - A binary classification model trained using AutoNLP with high validation metrics, accessible via API.
  - Downloads: 542
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - Access to this public repository requires accepting certain conditions before viewing its files and content.
  - Downloads: 506
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - Thisrepositorycontainsafine-tunedmodelbasedonluke-japanese-baseforNamed-EntityRecognitionusingWikipediaÊï∞ÊçÆÈõÜÔºåÊîØÊåÅNER‰ªªÂä°ÔºåÁ≤æÂ∫¶ÊåáÊ†á‰∏∫0.77„ÄÇ
  - Downloads: 353
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - Weighted and i1-IQ1_S static GGUF quants for Japanese-Starling-ChatV-7B, available for download from specified Hugging Face links.
  - Downloads: 241
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - A Python model for named entity recognition in Japanese medical documents, including files for tags, attributes, text input, and prediction.
  - Downloads: 214
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - A LLaMA 3 Youko qlora fine-tune using a new VNTL dataset version for improving Japanese visual novel to English translation accuracy and stability.
  - Downloads: 201
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - A LLaMA 3 Youko qlora fine-tune using a new VNTL dataset version for improving Japanese visual novel to English translation accuracy and stability.
  - Downloads: 157
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - A model trained using AutoNLP for binary classification with high accuracy and precision, accessible via API.
  - Downloads: 153
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - The GitHub repository contains a finetuned model for scoring whether short Japanese texts are sexual on a 0-1 scale using regression.
  - Downloads: 117
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - A fine-tuned Luke Japanese large model for Named-Entity-Recognition using a Wikipedia dataset, achieving an F1 score of 0.845.
  - Downloads: 98
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - A BERT-based text classifier fine-tuned on ~5000 labeled sentences for assigning JLPT levels to Japanese sentences, achieving good performance with macro F1 scores around 0.84.
  - Downloads: 55
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - A reward model fine-tuned for evaluating the quality of Japanese novels, intended for use in training text generation models, though it may be biased by factors such as genre and style.
  - Downloads: 43
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - A model for generating titles from article text.
  - Downloads: 40
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - The repository includes a model and predict script for named entity recognition in Japanese medical documents, trained on MedTxt-CR-JA, producing XML-tagged outputs with normalization methods.
  - Downloads: 40
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - A fine-tuned DeBERTa-v2-base-japanese model for Named Entity Recognition using a Japanese Wikipedia dataset.
  - Downloads: 36
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository contains a fine-tuned DeBERTa-v2-large-Japanese model for Named Entity Recognition (NER) using a Japanese Wikipedia dataset.
  - Downloads: 35
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - A CPU-optimized spaCy pipeline for Japanese with components including tok2vec, morphologizer, parser, attribute_ruler, and ner, based on UD Japanese GSD v2.8 data.
  - Downloads: 34
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - A Fully Convolutional Neural Network model for classifying Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - A fine-tuned BERT model for Named Entity Recognition (NER) based on cl-tohoku/bert-large-japanese-v2, trained on a Japanese Wikipedia dataset, achieving an accuracy of 0.862.
  - Downloads: 23
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - The repository includes a dataset file named wrime-ver1.tsv.
  - Downloads: 21
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This GitHub repository contains a So-vits-svc 4.0 model for generating natural, soothing female voice samples from the developer's own vocal recordings, along with notebooks and learning datasets.
  - Downloads: 17
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - A QLoRA-fine-tuned LLaMA2-7B model trained on 49,000 chat samples and 280,000 non-chat samples from the Guanaco dataset, with improved Chinese and Japanese performance, tested using test.py.
  - Downloads: 16
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This model, trained on the awesome-japanese-nlp-classification-dataset, achieves high accuracy with Precision, Recall, and F1-Score ranging from 0.74 to 0.99, and supports usage via the transformers library.
  - Downloads: 15
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - The repository includes a Japanese transformer pipeline for spaCy version 3.7.2, featuring components for transformation, morphological analysis, parsing, entity recognition, and using the cl-tohoku/BERT model.
  - Downloads: 15
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - A fine-tuned Llama-2-Chat 70B model for Japanese instruction processing using the CC-BY-SA 4.0 licensed izumi-lab/llm-japanese-dataset.
  - Downloads: 14
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - A model trained on Japanese parliament proceedings from 2022 using a national library API, showcased at the #ABCILLM hackerthon for multi-GPU training.
  - Downloads: 11
### Responsible NLP
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - A customized model for generating youthful female outputs with adjusted age manipulation requirements, though other outputs are not recommended; VAE and sampling settings noted.
  - Downloads: 9,379
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a specialized Japanese voice recognition model fine-tuned on 5300 hours of anime dialogues, excelling particularly in anime acting scenes.
  - Downloads: 2,094
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - A merged model of CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 epoch2, usable via Colab WebUI, with instructions for further modifications.
  - Downloads: 1,328
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - A GitHub repository based on the sarashina2.2-3b-instruct-v0.1-GGUF model and the TFMC/imatrix-dataset-for-japanese-llm dataset.
  - Downloads: 534
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
  - Downloads: 178
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-gguf matsuo-lab„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãweblab-10b-instruction-sft„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 59
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - The repository merges MoeDiffusion and other models to achieve SFW black-haired ponytail faces with improved control, focusing on YaguruMagiku and AbyssOrangeMix2, and notes that NSFW content is unlikely but possible.
  - Downloads: 36
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 integrates WaifuDiffusion and StableDiffusion VAEs to improve coloration, partnering with DreamShaper 3.3 for versatile generation, including high-realismÁæéË≤åÁîª.
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - A Mixture of Experts (MoE) language model created with MergeKit, combining Llama-3-Umievo-itr014-Shizuko-8b and rdefog/llama-3-sqlcoder-8b for integrated Japanese language and SQL generation abilities.
  - Downloads: 12
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 11
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - Ê¶ÇË¶Å „ÄåLOCAL AI HACKATHON„Äç„Å´„Åä„Åë„Çã„ÄÅ„ÉÅ„Éº„É†DataPilot,4„Å§„ÇÅ„ÅÆÊàêÊûúÂìÅ„Åß„Åô„ÄÇ
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By agreeing, you accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 20,748
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By clicking "Agree," users accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 299
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By agreeing, you accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 279
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository contains a private demo for a specific purpose.
  - Downloads: 242
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - A fine-tuned-electra-small-model for detecting cyberbullying in Japanese, trained on a balanced dataset from multiple sources.
  - Downloads: 166
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - The GitHub repository houses an ELECTRA Base model fine-tuned for Japanese-language cyberbullying detection, using a merged dataset from two sources, and is licensed under CC BY-SA 4.0.
  - Downloads: 144
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By agreeing, users accept the License Agreement and acknowledge the Stability AI Privacy Policy.
  - Downloads: 142
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - An ELECTRA Small model for Japanese cyberbullying detection, fine-tuned on a balanced dataset combining two datasets and licensed under CC BY-SA 4.0.
  - Downloads: 119
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - A model fine-tuned on a manually annotated dataset of social media comments using Twitter/twhin-bert-large for toxicity evaluation, achieving macro F1 scores of 64.8%.
  - Downloads: 109
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - A model fine-tuned on manually annotated offensive comments from SNS, achieving macro F1 scores of 64.0%, with key hyperparameters including 27 epochs, batch size 16, and peak learning rate 2e-5.
  - Downloads: 106
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - A model fine-tuned on hand-labeled aggression comments from social media using the Twitter/twhin-bert-base, achieving macro F1 scores of 64.7% across categories with key hyperparameters including 27 epochs and a peak learning rate of 2e-5.
  - Downloads: 106
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 20
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - A model for instruction-following tasks in Japanese.
  - Downloads: 16
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model licensed under CreativeML OpenRAIL-M, allowing users to freely distribute weights while prohibiting illegal or harmful content production, with users solely responsible for their outputs.
  - Downloads: 15
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Sentiment Analysis
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This fine-tuned Luke-japanese-large-lite model analyzes eight emotions (joy, sadness, anticipation, surprise, anger, fear, disgust, and trust) using the wrime dataset.
  - Downloads: 36,125
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - A model trained from scratch on the chABSA dataset for Japanese sentiment analysis, achieving perfect accuracy and F1 score.
  - Downloads: 17,033
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - A BERT-based Japanese model fine-tuned for sentiment analysis and irony detection in tweets, licensed under CC BY-SA 4.0.
  - Downloads: 992
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - A BERT model adapted for Japanese Twitter with pretrained on Twitter data, suitable for tasks like sentiment analysis and defamation detection.
  - Downloads: 199
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - A BERT Base model for Japanese language fine-tuned on a balanced dataset for automatic cyberbullying detection, licensed under CC BY-SA 4.0.
  - Downloads: 126
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - The repository contains a finetuned ELECTRA Base model for Japanese irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 123
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - A fine-tuned ELECTRA small Japanese model for irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 117
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - A trained sentiment analysis model for Japanese stock-related comments, categorizing them as bullish or bearish to support investors and analysts.
  - Downloads: 102
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - A model fine-tuned on the „Å§„Åè„Çà„Åø„Å°„ÇÉ„Çì dataset using calm-2-7b-chat, licensed for free use under specified terms.
  - Downloads: 41
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 37
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 36
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - A BERT-based model fine-tuned from scratch on the Japanese Sentiment Polarity Dictionary dataset for sentiment analysis.
  - Downloads: 33
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository houses a finetuned ELECTRA Base model for Japanese irony detection, licensed under CC BY-SA 4.0, based on transformers-ud-japanese-electra-ginza and trained on ironic tweet data.
  - Downloads: 31
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - A BERT-based Japanese financial news sentiment analysis model trained on translated Financial PhraseBank data, providing positive, negative, and neutral labels.
  - Downloads: 16
### Reasoning
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - A Japanese fine-tuned version of DeepSeek's R1 model for consistent bilingual English and Japanese reasoning.
  - Downloads: 3,757
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - A Mixture of Experts model combining Chinese and Japanese capabilities for multilingual math problem solving, with evaluation on GSM8K using 20GB VRAM.
  - Downloads: 1,794
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuiseki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãYuisekinAIEvol-Mistral-7B-ja-math-v0.1.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 811
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is a lightweight, risk-reduced model merged from Stable Diffusion and Wifu Diffusion with additional LoRA adjustments, offering specific strengths in anime-style illustrations of solo cute girls and certain NSFW content.
  - Downloads: 468
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - A Japanese natural language inference model trained on JGLUE-JNLI and JSICK datasets using SentenceTransformers Cross-Encoder, outputting scores for contradiction, entailment, and neutral labels.
  - Downloads: 133
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository contains a fine-tuned model based on luke-japanese-large for the JCommonsenseQA task with an accuracy of 83.82.
  - Downloads: 98
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - A model combining Japanese Llama-3.1-8B with reasoning capabilities from the DeepSeekËí∏ÁïôÊ®°Âûã„ÄÇ
  - Downloads: 57
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - A fine-tuned luke-japanese-base model for JNLI task with an accuracy of 0.8977, used for calculating natural language inference.
  - Downloads: 25
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - A fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA, using the JGLUE/JCommonsenseQA dataset, with Jumanmorphological analysis included.
  - Downloads: 21
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository contains a fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 21
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - A fine-tuned model based on luke-japanese-base for the JCommonsenseQA dataset with an accuracy of 80.07, suitable for choice-based question-answering tasks.
  - Downloads: 21
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a finetuned COMET model for Japanese, based on the TimeATOMIC dataset and trained with causal language modeling.
  - Downloads: 18
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository contains a fine-tuned DeBERTa-v2-tiny-japanese model for CommonsenseQA tasks using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 14
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performing, CPU-compatible Japanese text embedding model optimized for retrieval tasks and semantic similarity measurement.
  - Downloads: 192,159
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - Hotaru Jujo's LoRA collection is available under dual MIT or CreativeML Open RAIL-M licenses for download and use.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT is an initial release of a Japanese-only document retrieval model that outperforms previous models and nearly matches multilingual models' performance.
  - Downloads: 725
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - A fine-tuned passage encoder for the BPR document retrieval model using bert-base-japanese-v3, based on Chapter 9 of "Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®."
  - Downloads: 30
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - A 7B-parameter Japanese instruction-following model fine-tuned on original datasets, achieving a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 53
## Datasets üß†

This list is sorted by downloads as of March 11, 2025.
467 datasets are listed.

### Information Extraction & Text Mining
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - A high-quality educational Japanese dataset consisting ofÁ∫¶1.2‰∫øÊñáÊú¨ÔºàÂÖ±893‰∫øËØçÂÖÉÔºâÔºå‰ªéFineWeb2‰∏≠Á≠õÈÄâÂæóÂá∫ÔºåÂè¶Êèê‰æõÂ≠êÈõÜÊ†∑Êú¨ÂíåÂ∞èËßÑÊ®°Ê∏ÖÊ¥óÊï∞ÊçÆ„ÄÇ
  - Downloads: 2,416
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a collection of sentences and translations, loaded by language pairs specified by codes, with options to customize the version.
  - Downloads: 1,997
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - The JSICK dataset is a Japanese version of the SICK dataset, used for natural language inference and textual similarity tasks, with a stress test subset designed to challenge multilingual compositional inference models.
  - Downloads: 1,038
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a convenient ML-ready dataset from Aozora Bunko, along with the code to reproduce it.
  - Downloads: 691
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - A dataset sampled from Japanese sources for language model training, using DSIR to select documents closest to specific subsets, comprising about 5% of the corpus.
  - Downloads: 679
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository contains sharded parquet files of the Japanese subset from the cc100 dataset.
  - Downloads: 669
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - A dataset of 5,000 annotated Japanese tweets for detecting defamation, labeled with targeted individuals and types of insults.
  - Downloads: 654
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - This GitHub repository contains a dataset of sentences extracted from Japanese Wikipedia articles, including article and section titles, generated using a provided script.
  - Downloads: 524
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - A filtered Japanese subset of XL-Sum, with PaLM 2 filters applied, containing 4215 training examples, 758 validation examples, and 766 test examples.
  - Downloads: 502
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - The WRIME dataset includes both subjective and objective emotional intensity annotations for writers' social network posts, collected from 50 participants.
  - Downloads: 449
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - A cleaned RAW dataset of fineweb-2-edu-japanese's small_tokens text column, unnormalized with NFKC and filtered for noise beyond a threshold of 0.7 and 4 characters, along with noise spans indicated by start_pos and end_pos.
  - Downloads: 445
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - The repository contains cleaned Japanese news articles from Common Crawl's news subset for July to October 2024, tokenized to 612M tokens using llm-jp/llm-jp-13b-v1.0 tokenizer.
  - Downloads: 406
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository containsÊ†áÁ≠æÊï∞ÊçÆÈõÜ for Ningning's seiyu Machico, partially collected from games, with plans to complete and standardize the dataset format.
  - Downloads: 308
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository contains three parquet files with Japanese data extracted from the wiki40b dataset, generated using a specific Python script.
  - Downloads: 301
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - The Japanese-Heron-Bench dataset includes 21 images of Japan categorized into three levels of complexity with 102 questions for evaluating Japanese VLMs across seven subcategories.
  - Downloads: 275
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - Daikinrin, operated by Atsushi Nakajima, offers summaries and indexing of over a thousand mycological papers and contains manually extracted comparisons of diagnostic characters for fungal species. Last updated on 2024/9/28.
  - Downloads: 259
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - A dataset for named entity recognition in Japanese, created by Stockmark and used in the book "Introduction to Large Language Models," version 2.0.
  - Downloads: 232
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset, used in the "Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®" book, is derived from the "livedoorÊñ∞ÈóªËØ≠ÊñôÂ∫ì," cleaned of HTML tags and licensed under CC BY-ND 2.1 JP by LlonWiitt.
  - Downloads: 222
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - A dataset for extracting Japanese named entities using Wikipedia, licensed under CC-BY-SA 3.0 by Stockmark Inc.
  - Downloads: 214
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - Recruit Co., Ltd. has released a Japanese image classification dataset for four specific Japanese tasks, including 101 types of Japanese food, under CC-BY-4.0 license.
  - Downloads: 197
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - The zenz-v2.5-dataset is a JSONL dataset containing approximately 190M context-input-output triples for training high-performance kanji-conversion models, including three model sizes (large, medium, small), and an evaluation benchmark.
  - Downloads: 194
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This repository contains a named entity recognition dataset for large language models, annotated with 8 types of entities from Wikinews articles, licensed under CC BY 2.5, comprising only test set data.
  - Downloads: 190
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - This GitHub repository hosts an HF mirror of the ABEJA-CC-JA dataset and provides a link to related documentation.
  - Downloads: 184
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - The repository contains labeled GitHub repository descriptions to classify whether they are relevant to Japanese natural language processing.
  - Downloads: 179
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - A dataset card for "japanese_alpaca_data," based on masa3141's work and LORA technique, with references to additional information.
  - Downloads: 175
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - A dataset containing only September and October news from kajuma/CC-news-2024-July-October-cleaned, limited to about 1000 tokens per output for efficient learning.
  - Downloads: 160
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - A dataset containing 53,640 Japanese tweets annotated for COVID-19 relevance from January to June 2020.
  - Downloads: 160
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - A diverse collection of anime songs lyrics stored in a Parquet file named AnimeSongsLyrics.parquet, serving as a resource for enthusiasts and researchers.
  - Downloads: 159
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - The repository contains the AnswerCarefully Dataset, intended for use in enhancing LLM safety, with restrictions on commercial and misuse; permits derivative work creation but requires adherence to usage terms.
  - Downloads: 158
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - The JapaneseGoblin dataset is an unsupervised text dataset from en.touhouwiki.net primarily in English with some Japanese, structured in JSONL format for text generation tasks.
  - Downloads: 144
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - A public RLHF dataset in Japanese, where reward model construction was reformatted into a classification task, with labels 1 for chosen and 0 for rejected sentences.
  - Downloads: 144
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - A freely usable English-Japanese passage-level translation dataset created manually for machine learning purposes, with translations made considering high-performing LLMs like CALM3-22B-Chat and Qwen 2.5 32B.
  - Downloads: 144
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - The repository now includes expanded cosmopedia-japanese-20k data up to 100K, along with text generation prompts and translations from a separate Hugging Face dataset.
  - Downloads: 142
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a Japanese stopwords list based on nagisa tokenization rules, derived from the CC-100 dataset and Wikipedia, requiring installation of the Huggingface datasets library for access.
  - Downloads: 137
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - A corpus of approximately 3.5 million Japanese light novel character names extracted from "Sh≈çsetsuka ni Nar≈ç," aimed at supporting culturally sensitive NLP tasks.
  - Downloads: 128
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - A curated dataset containing inspirational quotes from various anime series, formatted as a list of dictionaries with fields for the quote text and character name.
  - Downloads: 124
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - A dataset for Japanese-Englishalignments, including scripts for downloading, parsing, and preprocessing, based on the en-ja-align data from 2003.
  - Downloads: 122
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - A JSON-based anime dataset including metadata and cross-references to MAL, ANIDB, AniList, and other sites.
  - Downloads: 119
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a benchmark for evaluating LLMs in long-context Japanese tasks, comprising documents from various websites and synthetic QA pairs from GPT-4 and Claude-3.5-Sonnet, with tasks in extractive QA and abstractive summarization.
  - Downloads: 117
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - A dataset containing excerpts from SEC filings (excluding financial statements) submitted to EDINET from June 14, 2014, to 2022, including document metadata like company name and reporting periods.
  - Downloads: 116
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - A multilabel annotated dataset for Japanese NLP research field classification using information from GitHub repository texts and screenshots.
  - Downloads: 109
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - A dataset containing only September and October news from kajuma/CC-news-2024-July-October-cleaned, with dates added before each text, optimized for continuous pre-training with about 1000 tokens per output.
  - Downloads: 105
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP contains validated Japanese linguistic minimal pairs in JSONL format for benchmarking.
  - Downloads: 104
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - A parquet file containing extracted Japanese Wikipedia data from January 1, 2023, generated using Python code.
  - Downloads: 103
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - The repository contains cleaned and parsed Japanese subtitle data from over 7,000 TV shows/movies, including text and metadata in Parquet files.
  - Downloads: 99
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - A clustering dataset for training and evaluating embedded models, containing cleaned web-scraped data from the website of the Pharmaceuticals and Medical Devices Agency, with '‰∏ÄËà¨ÁöÑÂêçÁß∞' and '‰∏ÄËà¨ÂêçÁß∞ÂÆöÁæ©' combined into text, class labels preserved, and randomly split into train and test sets.
  - Downloads: 95
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - The repository contains the dataset and code for the SLG framework described in a paper on multi-task learning for Japanese sentence classification and named entity recognition.
  - Downloads: 93
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a dataset containing 6,259 annotated Japanese instruction-response pairs extracted from CohereForAI/aya_dataset.
  - Downloads: 91
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This GitHub repository contains 8,750 unique Japanese law records with details on number, title, ID, effective date, and text, deduplicated based on the most recent versions as of August 1, 2023.
  - Downloads: 87
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - A dataset of senryu poetry collected from the marusenryu.com website, including over 5,300 samples with odes and topics in text-to-text format.
  - Downloads: 85
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - The GitHub repository contains indexed summaries of mycological taxonomy papers, provided as "Three-line Paper Summaries" on the Daikinrin website.
  - Downloads: 82
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - Automatically generated Q&A from Wikipedia Japanese Edition using Mixtral 8x22b GGUF model, processed with specific coding on TSUBAME4.0 supercomputer, beware of potential hallucinations in responses.
  - Downloads: 76
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - A dataset of furigana characters created from National Diet Library bibliographic data.
  - Downloads: 73
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - The repository contains the GIELLM dataset for Japanese information extraction, built from the livedoor news corpus.
  - Downloads: 71
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - The repository contains multilingual paragraph pairs from PubChem and Wikipedia, including English and Japanese texts labeled for same-entity or different-entity relationships.
  - Downloads: 69
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - A clustered dataset for training and evaluating embedded models, derived from customs instructional responses on commodity classification, with labels based on HS code sections.
  - Downloads: 68
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - A JSONL version of the dolly-15k-jp dataset, suitable for use with SFTTrainer's dataset_text_field property, licensed under CC BY SA 3.0.
  - Downloads: 67
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - A converted Japanese subset of the original NTX v1 dataset into the Aya instruction format, licensed under CC-BY-SA 4.0.
  - Downloads: 64
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - The JParaCrawl dataset contains cleaned English-Japanese parallel sentences, suitable for loading with `load_dataset` or Streaming from the Hoshikuzu repository.
  - Downloads: 60
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - A synthesized Q&A dataset created from multiple inference results using Nurture-intelligence/Gemma-2-108B-DPO-v0.1, licensed under the original dataset's license and Gemma Terms of Use.
  - Downloads: 59
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - A dataset containing questions for searching recipes and their corresponding search keywords, categorized into four types (AREA, TYPE, SZN, INGR), along with notebooks for creating the dataset and fine-tuning a language model, as well as application code using the fine-tuned model.
  - Downloads: 56
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains excerpts from specific sections of prospectus documents submitted to EDINET in 2024, including fields like document ID, company name, submission date, and period covered.
  - Downloads: 47
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - A converted dataset from Kyoto University's Language Media Research Lab for Japanese Wikipedia input errors, usable on HuggingFace, licensed under CC-BY-SA 3.0.
  - Downloads: 43
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset assigns 9 types of entity labels to horse names and related entities from Wikipedia articles, with a total of 600 labeled examples.
  - Downloads: 42
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - The GitHub repository hosts collaborative French-Japanese dictionary data and aligned bilingual corpus from multiple sources including Cesselin, Raguet-Martin, and JMdict, with over 154,000 Japanese-French entries.
  - Downloads: 42
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - The repository contains the AnswerCarefully Dataset, publicly available for commercial use to enhance LLM safety, but strictly forbid its misuse and require attribution for derived data while disclaiming liability.
  - Downloads: 36
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - This repository contains a dataset (J-NER) of 157 named entities for training large language models, each with 5 positive and 5 negative examples, totaling 1,570 examples.
  - Downloads: 25
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - A dataset of 221 new haiku, including translations and comments from authors and judges for approximately 200 of them, from the Oi-Ocha New Haiku Grand Prize.
  - Downloads: 24
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This repository contains an AutoTrain processed dataset for project tam_jp, in Japanese language.
  - Downloads: 15
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíUVR„Çí‰ΩøÁî®„Åó„Å¶BGM„ÇÑ„Éé„Ç§„Ç∫Èô§Âéª„Åó„Åü„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a vast, crowdsourced anime illustration dataset with over 5 million tagged images, covering detailed metadata for training various image processing tasks.
  - Downloads: 7,618
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - VOICEVOX-powered artificial voice dataset including 445,793 .wav files totaling 577 hours and 51 minutes.
  - Downloads: 6,418
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese-anime-speech is a dataset for training ASR models, featuring audio clips and transcripts from visual novels, to enhance transcription accuracy for anime dialogue.
  - Downloads: 4,634
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a large-scale, crowdsourced anime illustration dataset with over 1.2 million high-quality images and diverse tags.
  - Downloads: 4,627
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese-anime-speech-v2 is a dataset with 292,637 audio clips and transcriptions from visual novels for training ASR models.
  - Downloads: 2,759
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This non-official project aims to dataset-ize Elite Voice Project's sakura micron voice data for use in speech recognition, adhering to Hololive Production's guidelines, and welcomes contributions while emphasizing adherence to‰∫åÊ¨°Âàõ‰Ωú guidelines.
  - Downloads: 2,092
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - A diverse dataset of over 35,000 hours of natural Japanese speech from terrestrial television, formatted in FLAC at 16kHz, for ASR purposes compliant with Japanese copyright laws.
  - Downloads: 1,173
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark involving various disciplines, evaluated by native speakers, to truly assess LMM performance.
  - Downloads: 1,068
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - A diverse collection of high-quality images from various aspects of Japan, including urban landscapes and culinary experiences, captured between 2022 and 2024.
  - Downloads: 700
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - Classify KMNIST dataset images into 10 Japanese character classes.
  - Downloads: 368
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - A high-quality text dataset of Japanese academic papers, including proceedings from the annual meetings of the Japan Society for Computational Linguistics (NLP conferences), with CC-BY-4.0 licensing.
  - Downloads: 352
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This repository contains crawled data from photo haiku and haiku submission sites, including HTML files and processed datasets for image-to-text and text-to-text tasks, with 140 and 60 answers respectively.
  - Downloads: 346
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - The repository contains processed Japanese translations of MS MARCO data with hard negative mining for information retrieval models, including comparisons with mMARCO using SPLADE.
  - Downloads: 298
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - A dataset of anime-style illustrations with CC-0 licensed Japanese captions created using AI, suitable for ethical learning and reuse.
  - Downloads: 293
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This repository provides a refined dataset for evaluating vision-language models in Japanese, based on the original Japanese-Heron-Bench.
  - Downloads: 267
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - The CABank Japanese Sakura Corpus is a 31-participant audio study from Japan, available for academic use under TalkBank rules.
  - Downloads: 226
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - The CABank Japanese CallHome Corpus includes 120 audio phone calls from speakers in the United States, available for research with proper citation.
  - Downloads: 212
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - A dataset of 2735 unmodified WAV voices from Project Sekai character Emu Otori, for research use only under CC-BY-NC 4.0 license.
  - Downloads: 164
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - A collection of Whisper transcriptions for reazon_speech Japanese ASR without accompanying audio.
  - Downloads: 145
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This GitHub repository contains crawl data from HomeMate Research's photo haiku competition, includingHTML files and structured processing results, with 435 topics and 1,767 responses for use in the YANS hackathon.
  - Downloads: 139
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample subset of the Japanese Visual Genome VQA dataset used for evaluating EvoVLM-JP-v1-7B.
  - Downloads: 131
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - A dataset containing PDF pages converted to images, OCR-processed text with failed reads marked as '„Äì', and question pairs generated from the OCR results for training an image retrieval model.
  - Downloads: 124
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - A voice dataset in WAV format from Project Sekai character Emu Otori, sized 2735 samples, for research use under CC-BY-NC 4.0 license.
  - Downloads: 123
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a large-scale, crowd-sourced anime illustration dataset with over 5 million images, detailed tags, and diverse metadata for training various vision tasks.
  - Downloads: 116
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - The Lux Japanese Speech Corpus includes raw and cleaned 96kHz/16bit WAV audio files of character Lux reading text, along with transcriptions and metadata.
  - Downloads: 107
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This repository contains a synthesized dataset generated using Qwen/Qwen2-VL-7B-Instruct and Qwen/Qwen2.5-32B-Instruction-AWQ models based on photos from ThePioneer/japanese-photos, with detailed information available at https://zenn.dev/kendama/articles/cd5196a33bc46c.
  - Downloads: 106
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This repository containsÁà¨Ëô´Êï∞ÊçÆÊù•Ëá™‚ÄúÂÜôÁúüÂ∑ùÊü≥‚ÄùÂíå‚ÄúÂ∑ùÊü≥ÊäïÁ®ø„Åæ„Çã„Åõ„Çì‚ÄùÁΩëÁ´ôÔºåÁî®‰∫éËÆ≠ÁªÉÂíåÊµãËØïÂõæÂÉèËΩ¨ÊñáÊú¨ÂèäÊñáÊú¨ËΩ¨ÊñáÊú¨ÁîüÊàê‰ªªÂä°ÔºåÂÖ±ÂåÖÊã¨70‰∏™ÂõæÂÉèÈ¢òÁõÆÂíå30‰∏™ÊñáÊú¨È¢òÁõÆ„ÄÇ
  - Downloads: 103
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - A processed dataset of 3,361,443 entries from ÈùíÁ©∫ÊñáÂ∫´ÂíåËê®PieÈü≥Â£∞DAISIÊï∞ÊçÆÔºåÂåÖÂê´ÂèëÈü≥Ê≥®ÈáäÁöÑ CORPORUS„ÄÇ
  - Downloads: 101
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This repository contains voice transcription datasets for Umamusume characters with total duration sums provided for each character.
  - Downloads: 100
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - A clustered dataset for training and evaluating embedded models, derived from "DescriptionOfBusinessTextBlock" data in EDINET files, with labeled splits for train (80%) and test (20%).
  - Downloads: 96
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This repository contains a dataset from FineWeb2 Japanese web scraping data, with noise spans identified by LLMs, including 300,000 training and 30,000 test samples.
  - Downloads: 96
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 is a curated collection of over 240,000 animation clips from enthusiasts, aimed at supporting the development of generative video models and AI animations.
  - Downloads: 94
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - Key preprocessing steps for the Tanaka Corpus to create an HF Datasets version are provided.
  - Downloads: 89
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - A clustered dataset of 6,127 XML files collected from e-Gov, containing concatenated text of LawTitle and MainProvision, categorized and split into training and testing sets.
  - Downloads: 88
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - The repository includes 1024x1024 PNG images of Kanji characters with textual descriptions, adapted from KanjiVG and accessible via a load_dataset import.
  - Downloads: 84
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - A Japanese-translated trial dataset for NVIDIA's SteerLM alignment, useful for trying out alignment methods and LLM training techniques.
  - Downloads: 83
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - The repository contains analysis results of speech quality usingspeechMOS, saved in audio_analysis_results_speechMOS.json with MOS values, transcription, and filenames, and visualized as histograms.
  - Downloads: 81
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - The analysis using speechMOS of the Common Voice Corpus 17.0 resulted in an audio_analysis_results_speechMOS.json file containing SNR values, with histograms and counts for files exceeding different MOS thresholds.
  - Downloads: 75
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - A curated dataset focusing on JGLUE (JcommonsenseQA, MARC-ja, JSQuAD) from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja, licensed under Apache 2.0, CC-BY-SA-3.0, and MIT respectively.
  - Downloads: 73
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - A dataset of approximately 1000 Japanese roleplay instructions created by applying Magpie's method to nvidia/Nemotron-4-340B-Instruct, with some low-quality records possibly included.
  - Downloads: 69
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - A dataset containing only CC-BY-SA-4.0 licensed quiz data from the official distribution of AI King's quiz dataset,usable via Hugging Face Datasets library.
  - Downloads: 68
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This GitHub repository contains dataset splits for three tasks derived from Bokete, a joke submission website, including text-to-text, image-to-text, and text-image-to-text tasks, with specific counts of items.
  - Downloads: 62
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - A dataset using Music2Emotion for analyzing emotions in Japanese music, formatted as JSONL with predicted moods and emotional valence/arousal scores.
  - Downloads: 60
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - A dataset for verifying the operation of okiribito (Â§ßÂñúÂà©) generation, including text-to-text, image-to-text, and text-image-to-text tasks.
  - Downloads: 59
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This GitHub repository contains a Japanese voice-text dataset for FGO playable characters, consisting of 30800 records with 66.4 hours of audio, ideal for ASR/ASV model fine-tuning or evaluation.
  - Downloads: 59
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This corpus is based on essays generated using Phi-3 from randomly extracted Japanese text, with some computations performed on the Tokyo Tech supercomputer TSUBAME4.0.
  - Downloads: 55
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - Rakuten-Alpaca-Data-32K is an auto-generated Japanese instruction dataset created using Rakuten/RakutenAI-7B-chat, inspired by Stanford Alpaca, with seed tasks from seed_tasks_japanese.jsonl; requires filtering due to low data quality and licensed under Apache 2.0.
  - Downloads: 53
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - A public-domain dataset of Japanese places for training text-to-image models.
  - Downloads: 53
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - A copyright-free dataset of Japanese scenery images for training text-to-image models.
  - Downloads: 53
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - An improved version of Calvin-Xu/Furigana-Aozora-Speech with 2,536,041 cleaned entries out of 3,361,443 from raw data.
  - Downloads: 50
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - The repository contains Pok√©mon captions in English and Japanese generated by the BLIP model, used to train a text-to-image model, derived from images of the Few Shot Pok√©mon dataset.
  - Downloads: 48
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - A dataset converted from Japanese Visual Genome VQA and DOC CI datasets into LLaVA-Instruct format, licensed under Apache License 2.0.
  - Downloads: 46
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - A dataset of 101,702 Japanese words with hiragana pronunciations, curated by linguists, for ASR technology research.
  - Downloads: 40
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - Japanese parliament members' voice embeddings dataset created using speechbrain/spkrec-ecapa-voxceleb for tasks like speaker separation, with examples provided for analysis.
  - Downloads: 38
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - A Japanese translation of the LLaVA Visual Instruct 150K dataset, licensed under CC BY-NC-4.0, for use in Japanese language applications.
  - Downloads: 28
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - The jaCappella corpus includes musical scores and audio recordings of Japanese a cappella vocal ensembles arranged from public-domain children's songs.
  - Downloads: 14
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - A Japanese veterinary medicine dataset with categorized audio files and transcriptions for drugs, diseases, and symptoms, suitable for training purposes.
  - Downloads: 12
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - The VNTL leaderboard ranks LLMs on their performance in translating Japanese Visual Novels into English, comparing them to established translation tools.
  - Downloads: 3,907
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - A manually corrected Japanese translation of the HumanEval dataset, used as a benchmark for evaluating LLM coding abilities, retaining some original errors for consistency with the English version.
  - Downloads: 2,661
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - The multilingual Amazon Reviews Corpus, containing English, Japanese, German, French, Chinese, and Spanish reviews from 2015 to 2019, is defunct but was used for multilingual text classification.
  - Downloads: 1,152
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - A dataset translated from Japanese to English using Qwen/Qwen2.5-32B-Instruct on a subset of llm-jp-corpus-v3, created and shared as an open-source Japanese-English parallel corpus, with a distinct id column.
  - Downloads: 745
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - The repository contains a Japanese translation of the English subset from ReLAION-5B using gemma-2-9b-it and text2dataset with vLLM inference.
  - Downloads: 631
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - A repository of Japanese queries from the MMarco dataset with up to 25 hard negatives retrieved by mE5 and 10 by BM25.
  - Downloads: 536
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - A Japanese translation of openai/gsm8k questions with answers extracted, using nejumi/phi-4-GPTQ-Int4-calib-ja-1k, containing some invalid data.
  - Downloads: 465
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - The GitHub repository contains Japanese translations of the ms_marco dataset using google/mada Lad400-3b-mt, stored with the same structure as the original ms_marco, though translation quality is moderate and datasets like mMARCO may offer better alternatives.
  - Downloads: 451
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - A Japanese translation of the LIMA dataset used for training Meta's LIMA model, accessible via Hugging Face datasets.
  - Downloads: 265
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - A subset of bluemoon-fandom-1-1-rp-cleaned translated to Japanese using command-r-08-2024, with improved inference speed and VRAM efficiency.
  - Downloads: 244
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This dataset includes Japanese web novel chapters and their English translations, formatted for document translation tasks, with metadata including series titles, alignment scores, and genres.
  - Downloads: 235
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - A part of the Guanaco dataset in Japanese, comparable to datasets like inu-ai/alpaca-guanaco-japanese-gpt-1b.
  - Downloads: 167
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - A MIT-licensed dataset for RLHF experimentation, handcrafted by ebisuke.
  - Downloads: 165
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - A converted CC-BY-SA 4.0 licensed Japanese instruction dataset (~2.46M rows) for LLMs, originally v1.0.0 format now in Aya.
  - Downloads: 163
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - A filtered set of 1 million rows from the JParaCrawl v3 English-Japanese corpus, withË¥®ÈáèÈóÆÈ¢ò„ÄÇ
  - Downloads: 163
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - The repository contains Apache 2.0 licensed Japanese-English parallel texts for translation tasks, with some sources under more restrictive licenses.
  - Downloads: 160
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers a Japanese translation of the mbpp dataset created by LLM-jp, using DeepL for English to Japanese translation.
  - Downloads: 155
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - High-quality, clean 100-set Japanese CoT dataset with connected CoT and output parts in CoTangent_ja.json and separated versions in CoTangent_separated_ja.json.
  - Downloads: 136
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - A dataset containing only the Japanese-English translations from the Asian Language Treebank (ALT) Parallel Corpus.
  - Downloads: 130
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - The Nexdata/English-Japanese_Parallel_Corpus_Data repository contains 850,000 desensitized bilingual texts (23 words on average per English sentence) covering various fields, suitable for machine translation and text data analysis.
  - Downloads: 119
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - A JSON conversion of NilanE/ParallelFiction-Ja_En-100k for text-generation-webui, containing Japanese web novel chapters and their English translations.
  - Downloads: 118
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - A 20,000-item Japanese-English translation dataset created using Magpie's approach on NVIDIA Nemotron-4-340B-Instruct, with code and details provided.
  - Downloads: 115
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - Randomized, de-duplicated English-Japanese translation pairs from Tatoeba.
  - Downloads: 110
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - This repository offers an instruction tuning dataset for LLMs developed by the LLM-jp project, a collaborative effort in Japan, derived from a subset of Aratako/Synthetic-JP-EN-Coding-Dataset-801k.
  - Downloads: 107
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - A modified version of the kunishou/hh-rlhf-49k-ja dataset excluding ng_translation == 1 examples, referenced alongside the original dataset.
  - Downloads: 106
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This repository contains corrected translations andÂ°´Ë°•‰∫ÜÂéüÂßãÊï∞ÊçÆÈõÜliuhaotian/llava-bench-in-the-wild‰∏≠ÁöÑÈîôËØØÂíåÊú™ÁøªËØëÁöÑÊï∞ÊçÆ„ÄÇ
  - Downloads: 104
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - An English subset of 50,000 entries from the Synthetic-JP-EN-Coding-Dataset-801k. For details, refer to the original dataset overview.
  - Downloads: 101
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - The dataset converts Japanese content from mirACL into BeIR format for mteB compatibility.
  - Downloads: 98
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository offers a Japanese translation of an English subset from oasst2, created by LLM-jp, for instruction tuning of language models.
  - Downloads: 97
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - A dataset for long-text instruction data, utilizing Aozora Bunko clean text data, primarily aimed at prompting question-answer pairs without filtering, due to the inherent challenges this poses, and subject to performance variability based on the base model used for fine-tuning. Licensed under CC BY 4.0.
  - Downloads: 96
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - A Japanese-translated FED dataset using Google Cloud Translate API v2, with potential inconsistencies in dimensions.
  - Downloads: 95
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository contains 1000 generated Japanese responses using the DSR1D-Llama-8B model from the first 1000 items of weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked, with some formatting issues and reduced accuracy.
  - Downloads: 93
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - A processed English-Japanese parallel corpus from Wikidata for machine translation tasks, suitable for training with Hugging Face transformers.
  - Downloads: 92
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - A Japanese-language version of the LLaVA Pretrain dataset translated using DeepL API, intended for use in Japanese language applications, licensed under CC-3M and requiring acknowledgment of Google.
  - Downloads: 87
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - A collection of Japanese-Vietnamese translated sentence pairs for translation tasks and language pairing exercises.
  - Downloads: 80
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - The Tanaka-corpus dataset contains Japanese-English pairs compiled by Professor Yasuhito Tanaka and his students at Hyogo University for the Pacling2001 event.
  - Downloads: 80
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - A Chinese-Japanese parallel corpus dataset with 9.83 million sentence pairs, covering multiple fields and stored in txt format, suitable for text data analysis and machine translation.
  - Downloads: 79
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - A dataset containing machine-translated answers from the ViQuAE dataset into Japanese.
  - Downloads: 79
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - This repository offers a Japanese translation dataset of 12,000 entries from hh-rlhf, including 3,000 samples from four LLM-jp groups: harmless-base, helpful-base, helpful-online, and helpful-rejection-sampled.
  - Downloads: 79
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - The repository contains the dataset for the TaCo paper, which enhances cross-lingual transfer in LLMs through translation-assisted chain-of-thought processes. Please cite when used.
  - Downloads: 75
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - A corrected Japanese translation of MT-Bench using AI inflection, with some questions from Stability AI's Japanese MT-Bench.
  - Downloads: 71
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - The repository contains matched translations of Korean, Chinese, and Japanese versions of OpenOrca datasets, aligned based on id and embedding similarity.
  - Downloads: 69
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - A summary dataset from long texts using the globis-university/aozorabunko-clean dataset, licensed under CC BY 4.0.
  - Downloads: 68
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - The repository contains filtered training and validation sets from JSNLI Version 1.1 for the book "Large Language Models Made Simple," licensed under CC BY-SA 4.0.
  - Downloads: 67
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - A question-answer dataset generated from the wiki40b-ja repository.
  - Downloads: 65
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - A cleaned TALPCo dataset with Japanese-English translations in HuggingFace format, licensed under CC-BY 4.0.
  - Downloads: 63
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - A Japanese translation dataset of piqa using facebook/mbart-large-50-many-to-many-mmt, licensed under the same terms as the original piqa.
  - Downloads: 61
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - A chunked Alpaca-format version of NilanE/ParallelFiction-Ja_En-100k for the augmxnt/shisa-base-7b-v1 model, with each entry structured as { 'instruction' : 'Japanese chapter', 'output' : 'English translation', 'input' : 'empty' }, split into 4096-token chunks.
  - Downloads: 60
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - A vectorized dataset using the intfloat/multilingual-e5-base model with a FAISS index for Japanese Wikipedia paragraphs.
  - Downloads: 51
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - The GPT-3.5-turbo-translated Japanese version of MMLU dataset isÁî®‰∫éMultilingualSIFTÁõ∏ÂÖ≥Á†îÁ©∂„ÄÇ (Note: The translation uses brackets to indicate that "Áî®‰∫éMultilingualSIFTÁõ∏ÂÖ≥Á†îÁ©∂" is a direct translation, which can be simplified as: Used in MultilingualSIFT research.)
  - Downloads: 51
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual form understanding benchmark dataset with human-labeled key-value pairs in seven languages, including Chinese, Japanese, Spanish, French, Italian, German, and Portuguese.
  - Downloads: 50
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides a dataset in a specific format for the TaCo paper, including instructions and responses in multiple languages.
  - Downloads: 45
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - A Japanese translation dataset for "sciq," using Facebook's MBART model, licensed CC BY-NC 3.0.
  - Downloads: 44
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - The OPUS-MIT-5M dataset contains 5 million randomly sampled multilingual image translations from the OPUS corpus, ensuring balanced language representation across 20 language pairs.
  - Downloads: 40
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - A 380,000-group Japanese-English parallel corpus excluding sensitive vocabulary, suitable for machine translation and text-based data analysis.
  - Downloads: 37
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - The repository contains a function for evaluating the accuracy and quality of Japanese to English translations, rejecting those with missing, incomplete, or inaccurate parts, as well as poor grammar, spelling, or low-quality content in either language.
  - Downloads: 21
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - A Japanese to English translation project licensed under CC BY 4.0.
  - Downloads: 11
### Semantic Text Processing
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a dataset and evaluation framework for assessing Japanese biomedical LLMs, maintained by Junfeng Jiang and Jiahao Huang from Aizawa Lab.
  - Downloads: 4,424
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a benchmark for Japanese text embedding models comprising 6 tasks and 16 datasets.
  - Downloads: 3,814
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - The JGLUE dataset, developed by Yahoo Japan Corporation and Kawahara Lab at Waseda, measures general natural language understanding ability in Japanese without translation.
  - Downloads: 2,298
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - A HuggingFace Space with conversion scripts and faiss indices of Japanese Wikipedia embeddings for RAG and vector search tasks, including evaluations and demonstrations using various embeddings.
  - Downloads: 2,274
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - This repository includes synthetic Japanese and English conversation datasets derived from LMSYS-Chat-1M, used for post-training Llama models.
  - Downloads: 1,831
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - LLM-japanese-dataset is a Japanese instruction dataset for tuning LLM models like Alpaca, updated to remove outdated data and maintain compliance.
  - Downloads: 698
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - A dataset generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, with random sampling allowing duplicates, distributed under CC-BY-SA 4.0.
  - Downloads: 579
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - A refined dataset for tuning Japanese LLM models on chat instruction-response tasks, derived from izumi-lab/llm-japanese-dataset.
  - Downloads: 447
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This repository contains a dataset for the book "Large Language Models Made Simple," with BPR-generated Japanese passage embeddings added to the "llm-book/aio-passages-bert-base-japanese-v3" dataset, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 436
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - A simple dataset for testing character LLMs, containing zunda mon information from online research and internal sources.
  - Downloads: 348
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository contains a Japanese translation of the databricks-dolly-15k dataset created by LLM-jp for instruction tuning, with contributions from several authors listed alphabetically.
  - Downloads: 318
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - cc100-ja-documents is a GitHub repository containing combined, license-compliant Japanese versions of cc100 documents split into lines.
  - Downloads: 281
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - A dataset for Japanese roleplay created using gpt-4o-mini, with added system messages and increased data size to 39,600 entries, licensed under CC-BY-NC-SA 4.0.
  - Downloads: 236
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - Extracted and merged 256-character lines from the neody datasets: C4-JA-cleaned, CC100-JA-cleaned, and OSCAR-JA-cleaned.
  - Downloads: 225
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - A Japanese synthetic preference dataset of 190,854 entries created using 5 models including Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, with input quality filtered from Aratako/Magpie-Tanuki-8B-annotated-96k.
  - Downloads: 218
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - A dataset converted from Open-Platypus-Japanese-masked format to OpenAI message format.
  - Downloads: 214
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - A cleaned dataset of simple Japanese sentences using calm3-22b, incorporating various grammatical patterns including affirmative, negative, polite forms, and more.
  - Downloads: 193
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - The repository contains Windows executable files for a Japanese GPT-2 model, along with necessary ggml and SentencePiece files, but notes issues with one file's format.
  - Downloads: 186
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - The repository contains processed versions of the RyokoAI/ShareGPT52K dataset in Markdown format, with whitespace adjustments and language labels, using tools for HTML conversion, language detection, and text normalization.
  - Downloads: 179
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - A 5 million clean Japanese sentence dataset for unsupervised semantic similarity learning.
  - Downloads: 167
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - A dataset of simple Japanese sentences using calm3-22b, including various grammatical patterns such as affirmative and negative statements, polite forms, desires, continuous actions, requests, and more.
  - Downloads: 150
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - A 5-point scoring dataset of Japanese text from the fineweb-2 corpus, rated for educational content using a similar approach to FineWeb-Edu classifier, with training and test subsets.
  - Downloads: 147
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - This GitHub repository contains a dataset of short stories generated by the gpt-4o-mini model, including English and Japanese versions, for NLP research and story generation.
  - Downloads: 141
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - A dataset of Wikipedia sentences used in the "Large Language Models Primer" book, leveraging data from singletongue/wikipedia-utils, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 136
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - A filtered and modified Japanese/Chinese language pair dataset from WikiMatrix v1, including regex-based filtering, semantic similarity filtering with a threshold of 0.6, and conversion of Traditional Chinese to Simplified Chinese.
  - Downloads: 131
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - A dataset containing excerpts of outputs from the Watashiha-GPT-6B and Watashiha-Llama-2-13B-Ogiri-SFT models in response to manually created inputs, for use in the 5th session of the 2024 LLM lecture hosted by the Matsumoto-Yamazaki Lab at Tokyo University for educational and research purposes only.
  - Downloads: 124
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - A dataset of conversation texts generated using GPT-3.5-Turbo based on the Wikipedia Japanese edition (izumi-lab/wikipedia-ja-20230720), with commercial usage prohibited.
  - Downloads: 123
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - A synthesized Japanese dataset created with OpenAI GPT-4 and Azure credits, for use in fine-tuning non-English language models.
  - Downloads: 118
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - The repository databricks-dolly-15k-ja-scored adds BERTScore translation quality scores to the machine-translated databricks-dolly-15k dataset, which includes poorly translated data such as identical input and output, copied outputs, inconsistently spelled text, and failed translations of proper nouns.
  - Downloads: 117
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - The repository uses Magpie method to extract prompts on rinna/llama-3-youko-8b, employing translated system prompts from a paper, though the instruct model variant may not be appropriate for this approach.
  - Downloads: 89
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - NVIDIA has released a Japanese translation of the HelpSteer2 trial dataset for use with their SteerLM model, which also uses Nemotron-4-430B-Reward.
  - Downloads: 88
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - The dataset consists of a DSIR-sampled 90%/10% split of MADLAD-400 JA/EN tokens for training shisa-base-7b-v1.
  - Downloads: 84
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - @pokutuna ÊâÄÂàõÂª∫Ê®°ÂûãÁöÑÁº∫Èô∑‰∏∫ inspirationÔºåÁî®‰∫éËß£ÂÜ≥ ime ÂíåÊã¨Âè∑ÈÖçÂØπ‰ªªÂä°Âèò‰ΩìÈóÆÈ¢òÔºåÂá∫Ëá™‰∏ú‰∫¨Â§ßÂ≠¶ÊùæÂ∞æÁ†î2024Âπ¥Â§ßË¶èÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁ´ûËµõ„ÄÇ
  - Downloads: 79
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - A dataset of approximately 26,500 Japanese synthetic instructions generated using Magpie on Tanuki-8x8B-dpo-v1.0-GPTQ-8bit and clustered into 20000 clusters with Evol-Instruct applied.
  - Downloads: 77
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - A model fine-tuned on a dataset of 330 personality-oriented tweets after initial tuning with GPT-3.5, scored from 10 to 8 points.
  - Downloads: 76
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - A dataset recording Pok√©mon VGC Regulation F selection data collected from public YouTube streaming matches, presented at the Remote Pok√©mon Society community in May 2024.
  - Downloads: 75
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - A dataset of Wikipedia paragraphs for the book "Introduction to Large Language Models," using a dataset from singletongue/wikipedia-utils, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 71
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - The Jamp dataset provides controlled Japanese temporal inference benchmarks for evaluating language models' generalization capacity, including templates and test/training data split by tense fragment, time format, or time span.
  - Downloads: 71
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - A trial dataset for evaluation, including 50 queries generated by ChatGPT-4o from five perspectives on patent attorney introductions, with manual answers created for 10 excluded questions.
  - Downloads: 70
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - A dataset of approximately 60,000 Japanese instruction samples generated using Qwen2.5-72B-Instruct-GPTQ-Int8 through a self-instruction process.
  - Downloads: 69
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - Human-annotated responses for ELYZA-tasks-100, used to evaluate Japanese LLMs, with automatic grading by GPT-4o (3.69 avg.) and Claude 3.5 Sonnet (4.42 avg.).
  - Downloads: 66
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This GitHub repository contains the Japanese translation of the Dolly project developed by Databricks, licensed under CC BY-SA 3.0.
  - Downloads: 65
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One aimed at increasing realism and complexity in paintings, with instructions for usage included.
  - Downloads: 63
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - A dataset of approximately 69,000 Japanese-English coding dialogues created using Magpie with models like Nemotron-4-340B-Instruct and Mixtral-8x22B-Instruct.
  - Downloads: 59
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - A converted chat-formatted dataset from oasst2-135k-ja for multiturn conversation fine-tuning, in ShareGPT format, requiring substantial computational resources.
  - Downloads: 58
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - A Japanese translation of the Bluemoon_Top50MB_Sorted_Fixed dataset using GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awq, translated via 3-shot prompting and terminated at 8000 tokens, with some long dialogues truncated and repeated outputs removed.
  - Downloads: 58
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - A 10,000-item Japanese coding dialogue dataset created using Magpie on Nemotron-4-340B-Instruct, with code and some system prompts provided.
  - Downloads: 55
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - A fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice, CSS10, and JSUT datasets.
  - Downloads: 48
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - A Japanese fake news dataset converted for HuggingFace datasets, including labels for truthfulness and text lengths.
  - Downloads: 45
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - A dataset combining human-created text and LLM-generated text for evaluating LLM output detection performance.
  - Downloads: 41
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - A mirror of the LLM-jp Corpus v3 excluding Wikipedia, which is licensed under CC-BY-SA.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Natural Language Interfaces
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a corpus of about 14,000 Japanese conversations containing speakers' personas and traits, with usage guidelines to protect individual privacy.
  - Downloads: 2,402
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset for evaluating retrieval augmentation generation, designed to test and improve the performance of LLMs in providing accurate answers.
  - Downloads: 925
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - This GitHub repository contains a dataset and loading script for the Japanese Open-Domain QA task "JAQKET," which consists of Wikipedia article titles as answers, supporting tasks involving multiple-choice questions.
  - Downloads: 911
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository contains 1.56B tokens of text data from multiple datasets including Accommodation Search Dialog Corpus and Japanese Movie Recommendation Dialogue, licensed accordingly, for pre-training AKU-d_ms-0.5B-chat-v0.1.
  - Downloads: 618
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696-question-answer pair dataset in Japanese, created for machine reading comprehension, with contexts from Wikipedia articles, achieving high F1 and exact match scores when fine-tuning BERT-Japanese.
  - Downloads: 587
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - A repository containing 40 Japanese questions categorized into history, society, government, and geography for evaluating AI assistants' knowledge in Japanese.
  - Downloads: 559
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - This dataset evaluates LLM Japanese role-playing capabilities for the Japanese-RP-Bench, including genre, age division, world and scene settings, character tones, and response formats.
  - Downloads: 450
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - The repository contains a benchmark dataset for evaluating large language models in Japanese across multiple datasets, used in the book "Introduction to Large-Scale Language Models II." Licensed under Apache License 2.0.
  - Downloads: 416
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The BSD dataset includes Japanese-English conversations from various business scenarios, created by writing monolingual dialogues and then translating them.
  - Downloads: 340
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - The GitHub repository features reproducible evaluation_scores for the JEMHopQA dataset, which evaluates Japanese multi-hop question answering with derived steps representing entity relationships.
  - Downloads: 327
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - Automatically generated multi-turn dataset using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, sourced from multiple open data sources and licensed under various copyrights.
  - Downloads: 205
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - Automatically generated multi-turn dataset using Calm3-22b from open data sources, with some computations done on TSUBAME4.0 supercomputer, licensed under Apache 2.0, CC BY-SA 3.0, CC BY-4.0, and CC0.
  - Downloads: 189
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - A large-scale Japanese instruction-following dataset comprising 16 individual datasets, formatted as JSON with instructions, optional inputs, and outputs.
  - Downloads: 175
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a Japanese pharmacology Q&A dataset spanning 13 years, containing over 4,000 question-answer pairs and commentaries for the National License Examination, released under CC BY 4.0.
  - Downloads: 173
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese dataset comprising 5,000 questions and about 500,000 web page titles or summaries for evaluating information retrieval systems using natural Japanese queries.
  - Downloads: 169
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - A corpus of Japanese roleplay dialogues from forums, filtered to include only multi-participant threads with longer posts.
  - Downloads: 147
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - The repository contains Version 2.0 validation set of AIO quiz dataset, augmented with valid answers, including fields for question ID, competition name, timestamp, data split, and original question text.
  - Downloads: 145
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - A dataset of responses generated by Qwen/Qwen2.5-72B-Instruct from inputs with excellent quality, extracted from Magpie-Tanuki-Qwen2.5-72B-Answered Aratako/Magpie-Tanuki-8B-annotated-96k, licensed under Apache 2.0 with Qwen License considerations.
  - Downloads: 145
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - A processed QA dataset derived from Japanese Stack Overflow data dumps, including markdown-formatted questions and answers with adjusted code blocks and image replacements.
  - Downloads: 136
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository contains a Japanese translation of dialogue summarization datasets dialogsum and CSDS.
  - Downloads: 129
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - A dataset of conversations extracted from public-domain Japanese books using a heuristic approach to identify quoted speech.
  - Downloads: 129
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - Japanese translation of the OpenOrca dataset, about 1/5 complete, currently in progress; commercial use allowed.
  - Downloads: 109
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - A synthetic Japanese roleplay dataset with 39,600ÂØπËØùËÆ∞ÂΩïÔºåÊØèÊù°ÂåÖÂê´5Ëá≥10ËΩÆÂØπËØùÔºåÈôÑÊúâËßíËâ≤ËÆæÂÆöÂíåÂú∫ÊôØ‰ø°ÊÅØ„ÄÇ
  - Downloads: 107
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - A processed QA dataset from Japanese Stack Exchange data dumps, containing paired questions and answers with HTML formatted text cleaned up and image URLs base64 encoded.
  - Downloads: 104
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - A dataset for chatbot training generated from AI Gemini 2.0 Flash Experimental with possible Turkish and Japanese errors.
  - Downloads: 100
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - The repository contains the Japanese Vicuna QA Benchmark dataset for evaluating Japanese LLM models across 10 categories using a judge model, licensed under Apache License 2.0.
  - Downloads: 93
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - A growing handmade dataset for training Japanese chatbots.
  - Downloads: 90
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a large Japanese multi-domain dialogue dataset with 4,246 dialogues across 6 domains, annotated for tasks like dialogue state tracking.
  - Downloads: 89
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - Japanese multi-turn conversation data was generated using Qarasu14B based on Wikipedia data for non-commercial use, formatted as Human-GPT conversations suitable for training with Axolotl.
  - Downloads: 89
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - A dataset containing 11,808 multi-turn conversational instructions about Japanese photos generated using GPT-4o via Azure OpenAI API from an original dataset hosted on Hugging Face.
  - Downloads: 85
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - A multi-turn conversation dataset derived from Japanese Wikipedia using Orion14B-Chat, subject to a complex license agreement.
  - Downloads: 84
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a dataset for evaluating Japanese VLMs by adding multiple-choice questions to samples from an existing image classification dataset featuring 101 types of Japanese food.
  - Downloads: 81
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - A cleaned Japanese translation dataset of Lurunchik/WikiHowNFQA.
  - Downloads: 78
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - The repository uses aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2 to generate responses based on questions from Chatbot Arena Conversations JA (calm2), with prompts translated from lmsys/chatbot_arena_conversations.
  - Downloads: 77
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - A repository containing afiltered dataset of works from the Aozora Bunko collection, including only rows with Êñ∞Â≠óÊñ∞‰ªÆÂêç (new characters and new kana) notation.
  - Downloads: 76
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - A Japanese-translated version of the "databricks-dolly-15k" dataset with sentence-ending changes to "„Å´„ÇÉ„ÇìÔºÅ" using ArrowPro-7B-KUJIRA, preserving the original license.
  - Downloads: 73
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - A dataset of 3,000 usable multi-turn conversations generated from the Japanese Wikipedia using llama2Pro8B, available for commercial use.
  - Downloads: 70
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - A dataset of over 80,000 multi-turn conversations derived from Wikipedia, generated using llama2Pro8B and available for commercial use.
  - Downloads: 69
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - A commercially usable, automatically screened dataset of 60,000 multi-turn conversations derived from Japanese Wikipedia using llama2Pro8B.
  - Downloads: 69
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - A multi-turn conversation dataset generated from Japanese Wikipedia using Orion14B-Chat, licensed under the Orion-14B Series Models Community License Agreement.
  - Downloads: 68
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - A dataset of 3,000 usable multi-turn conversations generated from the Japanese Wikipedia using llama2Pro8B, suitable for commercial services.
  - Downloads: 68
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - A Magpie-influenced Japanese instruction tuning dataset with about 10,000 synthetic conversations created using DeepInfra, including unfiltered low-quality records.
  - Downloads: 68
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - A dataset generated by Swallow-MX from instructions manually checked and corrected for open-source LLM outputs, created during the "LOCAL AI HACKATHON #000."
  - Downloads: 56
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - The dataset includes about 1000 speakers recording natural face-to-face conversations on various topics, suitable for speech recognition tasks.
  - Downloads: 54
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - A dataset of multilingual corpora from the Multilingual Common Crawl project hosted on Hugging Face.
  - Downloads: 50
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - A dataset with data points excluding those matching prompts from chatbot-arena-ja-calm2-7b-chat.
  - Downloads: 49
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - A synthetic dataset of approximately 10,000 Japanese roleplay dialogues with each sequence consisting of about 10 turns, created using Magpie and Nemotron-4.
  - Downloads: 48
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository contains machine-translated Japanese versions of the everyday-conversations-llama3.1-2k dataset using DeepL, topic-wise dialog pairs, with some manual corrections and licensed under Apache 2.0.
  - Downloads: 45
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - A dataset of simple Japanese sentences created using Elementray_m calm3-22b, including various grammatical patterns such as affirmative, negative, polite forms, wishes, progressive, requests, permission, obligations, speculation, preferences, opinions, reasons, completions, simultaneity, conditionals, preparations, efforts, and appearances.
  - Downloads: 41
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - A Japanese translation dataset of Lurunchik/WikiHowNFQA.
  - Downloads: 41
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - A dataset containing Japanese-human and AI-assistant conversations from OASST1, structured as pairs of ### Human: and ### Assistant: blocks per line.
  - Downloads: 39
### Text Generation
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese-language benchmark comprising both translated MMLU questions and culturally specific Japanese questions to evaluate large language models.
  - Downloads: 124,464
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This GitHub repository contains Bokete dataset for three tasks: text-to-text, image-to-text, and text-image-to-text, derived from CLoT-Oogiri-Go. It includes 500 images and 1155 text responses as of August 30th.
  - Downloads: 2,781
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - The Allganize RAG Leaderboard evaluates and publishes Japanese RAG performance across five industry domains for parser, retrieval, and generation components.
  - Downloads: 653
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This GitHub repository contains datasets for three tasks related to Okuribito (Bokete) humor: text-to-text, image-to-text, and text-image-to-text, derived from a crawling dataset of the Bokete humor posting site.
  - Downloads: 507
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - A cleaned Japanese corpus clustered into about 10,000 text pieces using unsupervised learning models, based on web corpora like mc4-ja, with some files parquetized.
  - Downloads: 489
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - A dataset generated by using LLMs to automatically create questions from Japanese Wikipedia and then answering them with cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, licensed under CC-BY-SA 4.0.
  - Downloads: 464
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX offers 945 samples in four languages for NL-to-Code generation, with support for English, Spanish, Japanese, and Russian.
  - Downloads: 360
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - A synthetic Japanese-English coding dataset of 801,262 instances, expanded using the Evol-Instruct method from an original dataset of 69k instances, including models such as Nemotron-4-340B-Instruct and Phi-3-medium-4k-instruct.
  - Downloads: 356
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - The repository contains an updated instructional dataset for coding, including 180 new JaxTon and Java code records, with details on licensing and content sources.
  - Downloads: 228
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - A Japanese preference dataset created by re_generating responses with Aratako/Llama-Gemma-2-27b-SFT-trial1 and judging them against the original Qwen/Qwen2.5-32B-Instruct responses, licensing it under META LLAMA 3.1 COMMUNITY LICENSE and Gemma Terms of Use.
  - Downloads: 221
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This repository contains a filtered dataset of Japanese Wikipedia input errors, specifically for kanji conversion mistakes, split into pre_text and post_text segments.
  - Downloads: 172
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends CommonCatalog CC-BY by adding English captions from Dense Captioning with Phi-3 Vision, Japanese translations of those captions, and maintains photoid as the primary key, licensed under CC BY for commercial use. Sample code provided to load the dataset.
  - Downloads: 163
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - A dataset converted from llm-jp-corpus-v3's kaken subset to HF format, with titles of original articles obtained from URLs and licensed under CC-BY 4.0.
  - Downloads: 152
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - The SNOW T15 dataset includes 50,000 manually simplified Japanese sentences, their English translations, and a restricted 2,000-word vocabulary for text simplification and translation tasks.
  - Downloads: 133
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - A formatted dataset of 3-line summaries for the Livedoor News corpus, suitably shaped for Llama v2 with suggested special tokens [R_START] and [R_END].
  - Downloads: 116
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - Generated from deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, this repository contains the first 1000 responses of weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked with max_new_tokens=3060, using bnb's 8-bit transformation, which slightly reduces accuracy and frequently fails to generate <think>.
  - Downloads: 115
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - Extracted items from neody/oscar-ja-cleaned with less than 256 characters.
  - Downloads: 114
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - A Japanese Preference Dataset created by generating 5 responses each for selected Aratako/Magpie-Tanuki-Instruction data using Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter1, then scoring and filtering the responses with Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8.
  - Downloads: 108
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - Auto-generated Q&A data using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, based on Common Crawl and other team-created data sources.
  - Downloads: 103
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - A Japanese dataset for evaluating AI-generated text detection methods, derived from Hugging Face‚Äôs GPT-4-Self-Instruct-Japanese and processed with elyz√†/ELYZA-japanese-Llama-2-13b-instruct.
  - Downloads: 95
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository contains crawled data from the mobile pun show "Keitai Oogiri" including all topics and responses, structured into columns like `odai_id`, `episode_id`, `type`, `odai`, and `responses`.
  - Downloads: 94
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - A Q4_K_M tokenizer dataset for generating instruction-following responses using CALM3-22B, including some hallucinations, with datasets from Kendamarron/jimba-instruction-1k-beta and Wikipedia Japanese.
  - Downloads: 91
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This repository contains a Japanese translation of the Open_o1_sft_Pro dataset using Qwen2.5-14B-Instruct, including conversational templates and retaining the original license and acknowledgments.
  - Downloads: 85
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - Automatically generated Q&A from Common Crawl using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, with random text snippets to reduce similarity to original text, butÂê´Ëã±Êñá:Automatically generated Q&A from Common Crawl using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF with randomized text snippets to decrease similarity to original content, though some unnatural text may be present; cleaning recommended.
  - Downloads: 85
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This repository contains a dataset for testing humor generation tasks, including text-based and image-based responses.
  - Downloads: 84
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository contains a Japanese preference dataset created by generating responses with Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter2 and scoring them with Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, excluding low-scored answers.
  - Downloads: 76
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - A dataset of approximately 3000 simplified Japanese children's stories generated by GPT-4o-mini, created using a method from arXiv:2305.07759.
  - Downloads: 75
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - A dataset of cleaned questions used to train the Qwen model, with instructions licensed under Apache 2.0 and uncleaned answers.
  - Downloads: 75
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - A dataset of level2-filtered data from llm-jp-corpus-v3 converted to HF format, with article titles obtained from associated URLs, under CC-BY 4.0 licensing.
  - Downloads: 69
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset, synthesized using null-instruct-ja and DeepSeek-v2.5 q4, was created with ollama and 7 A5000 GPUs in 2 hours and 7 minutes, under the DeepSeek license.
  - Downloads: 67
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - A Japanese dataset generated with Qwen/Qwen1.5-14B for evaluating AI-generated text detection methods and suitable for self-instruct approaches.
  - Downloads: 63
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset contains Japanese translations of English instruction texts about investing and Berkshire Hathaway using KUJIRA, sourced from glaive-ai.
  - Downloads: 62
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - A Japanese translation dataset of English quotes using the llm-jp-3-3.7b-instruct model, licensed under CC BY 4.0.
  - Downloads: 58
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset aimed at advancing research for more sophisticated models.
  - Downloads: 58
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - A templated Japanese open-source dataset for instruction tuning LLMs, containing up to 20,000 samples per task split into 0-shot and few-shot examples.
  - Downloads: 48
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - A dataset generated by machine-translating NLVR into Japanese.
  - Downloads: 48
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - A public RLHF dataset in Japanese, reformatted as a classification task where labels 1 and 0 represent chosen and rejected sentences, respectively, combining synthetic text and machine translation.
  - Downloads: 48
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - The repository contains analysis of audio quality using WADA SNR, saved as reazonspeech-all-wada-snr.json with file name, SNR value, and transcription, including 1,208,360 data points with WAND SNR values above 100.
  - Downloads: 47
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - A collection of Japanese-English AIÊúØËØ≠Ôºå‰ªÖ‰æõÂèÇËÄÉÁøªËØë‰ΩøÁî®„ÄÇ
  - Downloads: 38
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
### Syntactic Text Processing
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - A transformed Japanese dataset for easy training with SentenceTransformers, structured in (anchor, positive), (anchor, positive, negative) formats, filtered for contrastive learning using specific Hugging Face datasets.
  - Downloads: 1,586
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - A multilingual dataset licensed under MIT.
  - Downloads: 702
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - A fun sticker collection by„Çã„Çä.
  - Downloads: 457
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - A text dataset extracted from the Jan. 1, 2024, Wikipedia HTML dump, maintaining document structure and removing markup for various NLP tasks.
  - Downloads: 411
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - A dataset of automatically generated Q&A using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, sourced from CC-BY or Apache-2.0 licensed data with random text snippets to reduce similarity to originals; licenses included.
  - Downloads: 347
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - This repository contains cleaned synthetic Japanese dataset created using the Stanford Alpaca method, curated by HachiML and licensed under Apache 2.0.
  - Downloads: 198
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - A dataset of about 2800 high-beauty-score images, primarily featuring beauty scores 87 to 90+, particularly focusing on versions with up to 1000+ score 90+ images.
  - Downloads: 158
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - A refined dataset of pronunciation aids derived from national bibliography data, with 5064 instance mismatches removed.
  - Downloads: 137
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - A dataset annotated with difficulty, quality, and category using cyberagent/calm3-22b-chat for Magpie's method applied to weblab-GENIAC/Tanuki-8B-dpo-v1.0.
  - Downloads: 132
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - The repository is named after "hachiwari/„ÅØ„Å°„Çè„Çå" and "chiikawa/„Å°„ÅÑ„Åã„Çè."
  - Downloads: 131
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - A Japanese dataset of approximately 1,300 question-answer pairs about Databricks, sourced from the company's Japanese blog and Q&A posts.
  - Downloads: 128
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A modified parsing and chunking method for Wikipedia data crawled between December 5 and 8, 2023.
  - Downloads: 109
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - A dataset with mismatches identified and corrected from Braille data of literary works by authors including ‰∏≠Â≥∂Êï¶ and Â†ÄËæ∞ÈõÑ.
  - Downloads: 107
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - A 3-turn mult Turner instruction dataset generated by Qwen, containing mixed English and Chinese records, based on Aratako/Magpie-Tanuki-8B-annotated-96k.
  - Downloads: 107
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - A dataset of approximately 150,000 paired Danbooru and Japanese tags created on 2024/10/15, with improved filtering for high-confidence Japanese tags.
  - Downloads: 105
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - A 97,269-record Japanese dialogue dataset created by applying Magpie's method to weblab-GENIAC/Tanuki-8B-dpo-v1.0 with no post-processing.
  - Downloads: 104
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data.
  - Downloads: 89
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - A processed embedding dataset from 50,000 posts by @t_w for Delight, not licensed for redistribution.
  - Downloads: 87
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese dataset created using the Evol-Instruction method from mistralai/Mixtral-8x22B-Instruct-v0.1, based on Stanford Alpaca seed tasks, curated by HachiML, licensed under Apache 2.0.
  - Downloads: 86
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - A collection of 20,000 auto-generated Japanese instructions and responses using Qwen2.5-32B-Instruct for training or evaluating instruction-following tasks, formatted in JSON under Apache-2.0 license.
  - Downloads: 84
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - The repository contains gathered question-answer pairs and human evaluations from the LLMChat system, which evaluated responses from 13 models between August 19 and 25, 2024.
  - Downloads: 80
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - A 525k instruction tuning dataset of ApolloCorpus-ja, auto-translated from English, aims to provide quality medical translation data for language models.
  - Downloads: 80
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - The GitHub repository hosts the code for the JSEC website.
  - Downloads: 80
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - Code for extracting data from the CommonCrawl PDFs in the Japanese domain.
  - Downloads: 79
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This GitHub repository houses a curated dataset of fungal traits, manually processed and standardized by Atsushi Nakajima for research convenience, last updated on December 29, 2023.
  - Downloads: 73
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - A Japanese-language version of part of the LLaVA v1.5 Visual Instruct 655K dataset, translated using DeepL API, for use in Japanese-language applications.
  - Downloads: 64
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - A dataset containing quiz questions with a "free" reuse permission level, sourced from ËÆØÊÅØÊù•Ê∫êÁöÑÂÖ∑‰ΩìÂêçÁß∞ as of August 5, 2024, suitable for use in RAG, document search systems, and other applications.
  - Downloads: 52
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - A dataset of adult-oriented NHentai Japanese manga in CBZ format, suitable for image analysis and text recognition research.
  - Downloads: 50
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - A dataset of 280 female illustrations generated with nijijourney v5, including some copyrighted characters, for model transparency; text files with tags are freely usable but not for improper or criminal use. LoRA for SDHKv3.0 is also available.
  - Downloads: 47
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - A freely usable dataset containing quizzes from Quiz Works, suitable for Searchable Retrieval-Augmented Generation (RAG) and document search system construction.
  - Downloads: 47
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - Metadata scraped from dengekibunko.jp/novecomi/novel.
  - Downloads: 46
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - A subset of 26,728 annotated Japanese instruction data for fine-tuning small-scale LLMs, derived from Aratako/Magpie-Tanuki-8B-annotated-96k and filtered for coding-excluded chat tasks.
  - Downloads: 45
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - A cleaned-up embedding dataset from 50,000 posts published by t_w on Deelight, with fixes to missing text and altered data structure, under Japanese usage terms.
  - Downloads: 43
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - Seiso Kazakiri's voice data from "Tenchi Portable."
  - Downloads: 40
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository contains a dataset of Japanese web pages extracted from CommonCrawler, using cc-downloader-rs, and is intended for research purposes only.
  - Downloads: 20
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
### Responsible NLP
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository contains a dataset of Japanese web pages extracted from CommonCrawler, using cc-downloader-rs, for research purposes only; other permissions are required for commercial use.
  - Downloads: 8,097
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository contains a Japanese dataset extracted from CommonCrawler using cc-downloader-rs, provided by the IPA's ICSCoE for research purposes only.
  - Downloads: 2,822
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository contains processed Japanese web corpus data from 2009, automatically punctuated with morphological analysis, along with conversion scripts for use in information analysis.
  - Downloads: 2,075
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - A dataset of 20,000 auto-generated Japanese instructions and responses for large-scale language model instruction-giving task training and evaluation using the LLM-JP 3.13B Instruct model.
  - Downloads: 512
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - A cleaned and de-duplicated mqa dataset with preprocessed noisy text and normalized IDs for query-passage pairs.
  - Downloads: 412
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - Randomly extracted text from sources including Wikibooks, Wikipedia, and legal data is used to regenerate texts with phi3, where dataset parquet files exceed several GBs and require git lfs for full download, with some computations done on TSUBAME4.0 supercomputer.
  - Downloads: 400
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - A dataset generated by creating replacements from Japanese Wikipedia text, then forming queries and responses for LLMs, licensed under CC-BY-SA 4.0.
  - Downloads: 344
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset consists of hand-crafted FAQ questions and answers from Japanese government agency websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models.
  - Downloads: 228
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - Japanese translations of the GuanacoDataset using langdetect.
  - Downloads: 133
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - Download and use the publicly shared models and datasets while agreeing not to exploit them for profit, understanding no guarantees are provided regarding legality or quality.
  - Downloads: 124
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - A converted chat-format dataset for mult-turn conversation fine-tuning, in ShareGPT format, derived from oasst1-89k-ja. Requires significant computational resources.
  - Downloads: 114
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository contains a Japanese dataset extracted from the CommonCrawler using cc-downloader-rs, provided by IPA's ICSCoE for research purposes only.
  - Downloads: 106
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - A corpus of Japanese text randomly extracted from sources like Wikibooks and Wikipedia, re-generated with Phi-3, automatically translated to English, and stored in parquet files.
  - Downloads: 100
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - A dataset translated from English Wikipedia to Japanese using cyberagent/DeepSeek-R1-Distill-Qwen-32B, processed with A6000 GPUs and vLLM, including original English text, preprocessed Japanese translation, inputs, and outputs.
  - Downloads: 90
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - Automatically generated Japanese Q&A in RAG format from randomly extracted text sources including Wikibooks, Wikipedia, and case laws for pre-training purposes, with some computations using TSUBAME4.0 supercomputer.
  - Downloads: 86
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - A Japanese translation dataset of "OpenAssistant/oasst2" converted using DeepL, available for fine-tuning after running aconversion script.
  - Downloads: 77
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - Automatic dialogue data generated from randomly excerpted text from the Youth Air Literature Archive using Calm3-22B-chat, with a cleaned version limited to "I am a Cat."
  - Downloads: 76
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - A dataset generated using Phi-3 to create Japanese sentences based on ConceptNet 5.7 triples, following a specific prompt template.
  - Downloads: 71
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This repository contains 1,243 curate tweets from the author's tweets between May 16, 2022, and May 24, 2024, defined as those that articulate difficult concepts or have unique worlds, intended to enhance base model expression.
  - Downloads: 69
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - Based on Japanese Post's international mail contentÁâ©ÂìÅÁöÑÊó•Ëã±„Éª‰∏≠Ëã±ÁøªËØëÂèäHSÁºñÁ†ÅÔºåÊõ¥Êñ∞Êó•Êúü‰∏∫2024Âπ¥5Êúà9Êó•„ÄÇËØ¶ÊÉÖËØ∑ËÆøÈóÆÁõ∏ÂÖ≥ÁΩëÈ°µ„ÄÇ
  - Downloads: 67
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - The repository includes a Japanese subset of the Open Assistant dataset, available at https://huggingface.co/datasets/timdettmers/openassistant-guanaco.
  - Downloads: 58
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - A cleaned dataset with unique queries and preprocessed for encoding errors, where query IDs correspond to collection subset indices for easy data access.
  - Downloads: 54
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset contains questions and answers about characters from the Tokamak Club in Eastern Project, structured in CSV files for use in chatbots, QA systems, and machine learning models.
  - Downloads: 47
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This GitHub repository contains a collection of automatically generated Japanese instructions and corresponding inferences, initial responses, and refined answers for large-scale language model training and evaluation, created using a specific process involving Qwen2.5 72B Instruct models.
  - Downloads: 45
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset includes slightly cleaned lines from the anime "My Favorite is Evil Lady," containing mainly Lay's dialogue with Claire's responses, though the user does not own the rights to the anime.
  - Downloads: 43
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - Users must agree not to use the publicly downloadable models and datasets for profit or to distribute them without legal compliance, and must accept no warranties are provided for the contents' legality or quality.
  - Downloads: 29
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - Download and agree to use the publicly shared models, datasets, and other content while respecting copyright and legal compliance.
  - Downloads: 25
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - Users must agree to terms for downloading publicly available models, data sets, and other content while understanding no guarantees are made regarding compliance, quality, or liability, and must ensure legal adherence and non-commercial use.
  - Downloads: 19
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - Users must agree not to exploit the content for profit and acknowledge downloading publicly shared models, data sets, and agreeing to legal compliance and quality disclaimers.
  - Downloads: 18
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - Users must agree to terms for downloading public models and datasets, understand no guarantees are provided, andÊâøËØ∫ÈÅµÂÆàÊ≥ïÂæãËßÑÂÆöÂπ∂Âú®ÂêëÁ¨¨‰∏âÊñπÂàÜ‰∫´Êó∂‰πüË¶ÅÊ±ÇÂØπÊñπÈÅµÂÆàÊ≥ïÂæã„ÄÇ
  - Downloads: 14
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository contains a Japanese-language dataset extracted from CommonCrawler using cc-downloader-rs, provided by the IPA's ICSCoE for research purposes only.
  - Downloads: 13
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - Users must download the publicly available models, datasets, and other content while agreeing not to use them for commercial or derivative purposes and to comply with applicable laws.
  - Downloads: 13
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - Users must agree to not derive commercial benefit from the content and understand that the repository providers do not guarantee legal compliance or quality,ÂÖçË¥£‰∏î‰∏çÂæóÁî®‰∫éËé∑Âà©ÁõÆÁöÑ„ÄÇÁî®Êà∑È°ªÈÅµÂÆàÊ≥ïÂæãÊ≥ïËßÑÂπ∂Âú®ÂêëÁ¨¨‰∏âÊñπÂàÜ‰∫´ÂÜÖÂÆπÊó∂‰πüÁ°Æ‰øùÂÖ∂ÈÅµÂÆàÊ≥ïÂæãÊ≥ïËßÑ„ÄÇ
  - Downloads: 13
### Reasoning
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a handcrafted Japanese dataset for logical reasoning tasks, suitable for both pre-training and post-training.
  - Downloads: 413
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - The JSNLI dataset is a Japanese translation of the SNLI benchmark for natural language inference, structured in TSV format and preprocessed with JUMAN++.
  - Downloads: 319
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - This repository includes code for ensuring evaluative score reproducibility and contains a clone of the SB Intuitions‰øÆÊ≠£Áâà for the JSQuAD dataset, a Japanese reading comprehension dataset derived from SQuAD 1.1.
  - Downloads: 225
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - A commercially usable, Japanese-translated dataset of 180K mathematical instruction-tuning pairs derived from OpenMathInstruct-1, licensed by NVIDIA for commercial use.
  - Downloads: 219
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - The repository contains a Japanese version of CommonsenseQA (JCommonsenseQA) for multiple-choice question answering, along with evaluation scripts and SB Intuitions fixes.
  - Downloads: 218
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - The repository contains a high-quality, commercially usable, ultra-small Japanese dataset including commonsense_qa, Calc-ape210k, and Japanese-commonsense-openqa, licensed under DbCL v1.0.
  - Downloads: 176
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - The abc-multiple-choice dataset consists of multiple-choice questions used in the "abc" quiz competition, accompanied by evaluation scripts and licensing information.
  - Downloads: 158
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - The repository offers a reasoning-enhanced Japanese math dataset with 50k examples to bolster reflective problem-solving skills for large language models.
  - Downloads: 106
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - A collection of 125,000 Japanese instruction-response pairs generated using the Qwen2.5-32B-Instruct model for task learning and evaluation, formatted in JSONL.
  - Downloads: 103
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - A Japanese-translated subset of 100k samples from the original NuminaMath CoT dataset, containing math problems with Chain of Thought solutions.
  - Downloads: 92
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench evaluates advanced Japanese reasoning capabilities using mathematics entrance exam questions from Kyoto University, targeting LLMs.
  - Downloads: 92
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - The repository contains a Japanese semantic test suite (JSeM) based on the FraCaS framework, including entailment tests for various linguistic phenomena to validate natural language processing and semantic reasoning systems.
  - Downloads: 89
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - A dataset of 1800 high-quality Japanese instruction, reasoning, and answer triples generated using Qwen2.5-32B-Instruct model from SkunkworksAI/reasoning-0.01 instructions, excluding problematic data.
  - Downloads: 86
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository contains high-information-dialogue multi-turn conversation data derived from the cosmopedia Japanese dataset, focusing on interactions about making mathematics more accessible and engaging.
  - Downloads: 83
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - Filtered dataset from DeL-TaiseiOzaki/magpie-reasoning-llama-nemotron-70b-100k excluding rows with "ÊîπËâØ" in refined_answer, formatted as OpenAI messages.
  - Downloads: 81
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - The JGLUE dataset is a Japanese Natural Language Inference dataset including entailment, contradiction, and neutral relations between premise and hypothesis sentences.
  - Downloads: 77
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - A Japanese-translated version of the OpenO1-SFT dataset (77,685 samples) containing Chain of Thought reasoning examples for fine-tuning language models.
  - Downloads: 67
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - The LogicJa dataset includes 105 multi-turn tasks covering various domains such as math, writing, and coding, with 210 questions in total to evaluate Japanese language models' reasoning abilities.
  - Downloads: 57
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - A dataset of 200 simplified tasks derived from Kendamarron/jimba-instruction-1k-beta, created for reproducing the in-depth evolving of Wizard LM, with plans to increase record numbers. Developed during a hackathon hosted by Discord server "Èù¢‰∏¥ÁöÑÂ∞èÁªÑ" and MetaData Lab.
  - Downloads: 46
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Responsible & Trustworthy NLP
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - The JaNLI dataset includes Japanese language instances aimed at testing model understanding of linguistic phenomena and identifying vulnerabilities.
  - Downloads: 338
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - The repository contains a dataset of 42k Japanese-English pairs for instruction tuning, part of Swallow-Magpie-Ultra-v0.1, used to train specific LLaMA models.
  - Downloads: 250
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - A dataset of Japanese medical licensing examination questions from 2018 to 2023, intended for model evaluation and tasks like evolutionary model merging, licensed under CC-BY-NC-ND 4.0.
  - Downloads: 186
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - The Gendec framework uses machine learning to detect gender from Japanese names, as presented in a paper accepted at ISDA'23.
  - Downloads: 177
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset provides a Japanese-language dataset for identifying harmful content.
  - Downloads: 126
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - The AttaQ-JA dataset comprises 1402 translated questions in Japanese to evaluate LLMs for their tendency to generate harmful responses, containing offensive content.
  - Downloads: 122
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - The repository contains pairwise evaluation data for two models' responses, used to verify consistency between human and open-source AI evaluations.
  - Downloads: 81
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - A Japanese unsupervised speech dataset covering 28 domains, validated for real-world interaction simulation and privacy compliance.
  - Downloads: 65
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository contains a dataset with keys for identifying game state, units, and their positions, including team-specific unit classes and statuses.
  - Downloads: 50
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - The dataset is not publicly distributed to prevent leakage into LLM training data.
  - Downloads: 34
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - You agree to the terms of the LICENSE when using this dataset.
  - Downloads: 32
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - The Cauldron-JA is a dataset translating 50 vision-language training sets from 'The Cauldron' into Japanese using DeepL, excluding certain types of data.
  - Downloads: 11,823
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - A Japanese translation of the "databricks-dolly-15k" dataset, licensed under CC-BY-SA-3.0, last updated on 2023-05-11.
  - Downloads: 1,083
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - The repository contains Japanese-Korean paired text for training translation models, sourced from Helsinki-NLP/Tatoeba-Challenge, with usage restricted to non-commercial purposes.
  - Downloads: 326
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - The repository contains a dataset of automatically translated "OpenAssistant/oasst1" into Japanese, including failed translations marked with "ng_translation."
  - Downloads: 285
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - A dataset of 69K Japanese-English translations automatically created from the "databricks-dolly-15k" dataset, licensed under CC BY SA 3.0.
  - Downloads: 156
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - The CT-RATE-JPN dataset offers deduplicated Japanese translations of radiology reports from the original CT-RATE dataset to support Japanese medical AI model development.
  - Downloads: 143
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated subset of ultrachat_200k for Japanese, with 6,537 training and 995 testing examples, using the cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese model.
  - Downloads: 110
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - A curated collection of roughly 40 high-quality Japanese open-source downstream task datasets for instruction fine-tuning LLMs, not machine-translated to or from Japanese.
  - Downloads: 105
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - The Kaidan Nihonbunka dataset collects 100 Japanese ghost stories (kaidan) related to the Hyakumonogatari tradition, embodying aspects of Japanese culture.
  - Downloads: 95
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - A dataset containing approximately 260k Japanese-English sentence pairs extracted from Japanese laws, available for loading via a Python script.
  - Downloads: 71
### Information Retrieval
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - A large Japanese Q&A dataset generated from Wikipedia text using Swallow-MX, suitable for knowledge-base training and retrieval-augmented generation models.
  - Downloads: 867
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - A dataset of 1 million retrieval-based multi-turn chat synthetic entries, generated using advanced LLMs, for continued pre-training and multi-scenario training exploration.
  - Downloads: 428
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - Yusuke Oda defined a dataset for Japanese question answering by having workers retrieve information from Wikipedia, with Baobab, Inc. handling data collection and formatting.
  - Downloads: 297
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - A JSONL dataset containing metadata of YouTube channels, including VTubers and non-VTubers, for text classification tasks.
  - Downloads: 185
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This dataset, used in the book "Introduction to Large Language Models," comprises passages from the "AI King" competition, licensed under CC BY-SA 3.0 and GFDL, based on data from cl-tohoku/quiz-datasets.
  - Downloads: 124
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - A QA dataset for document retrieval models used in the book "Large Language Models Made Simple," sourced from the cl-tohoku/quiz-datasets repository, with licensed content including Wikipedia passages under CC BY-SA 3.0 and CC BY-SA 4.0 licenses.
  - Downloads: 99
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - Automatically translated text from cosmopedia-100k index 20k to 100k into Japanese, excluding records with translation errors due to lengthy text.
  - Downloads: 97
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - The repository contains up-to-April-2022 passages from Japanese Wikipedia, each ‚â§400 characters, used in AIÁéã QA competition baseline systems.
  - Downloads: 37
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - A dataset of archived comments from over 11 years of NicoNico Live streaming history, rescued by community effort after the service's official discontinuation.
  - Downloads: 1,167,893
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - A dataset of 100 complex Japanese instructions for evaluating instruction-tuned language models, with annotated evaluation criteria covering various tasks.
  - Downloads: 3,066
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - A binary-labeled Japanese sentiment analysis dataset derived from WRIME, categorized as positive or negative based on Avg. Readers_Sentiment values, intended for use with the "Â§ßË¶èÊ®°Ë®ÄËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®ÂâçÊ≤øÊäÄÊúØ" book's sample code.
  - Downloads: 577
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - A corpus of audio recordings from an 81-year-old woman, including noisy and cleaned wav files, with phonemic and tonal labels.
  - Downloads: 181
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - A dataset edited from Dolly-15k-en to mimic Yuki Nagato's emotionless speaking style, with "„Åß„Åô„ÄÅ„Åæ„Åô" and "„Å†„ÄÅ„Åß„ÅÇ„Çã" replaced.
  - Downloads: 102
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - A manually checked and corrected Japanese Instruction dataset created from the output of cyberagent/calm2-7b-chat.
  - Downloads: 86
### Linguistics & Cognitive NLP
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - A curated collection of translations using fuguMT of helpful-base's chosen English text from https://github.com/anthropics/hh-rlhf, excluding and correcting poor translations.
  - Downloads: 89
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - The Japanese QA benchmark for news information, NewsQ, is freely available on Hugging Face but requires submitting a form with personal details and agreeing to terms and privacy policies before access is granted.
  - Downloads: 21
