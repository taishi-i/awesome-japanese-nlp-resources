# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 616 models and 120 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [æ—¥æœ¬èª (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ç¹é«”ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ç®€ä½“ä¸­æ–‡ (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

# Contents

 * [Models](#models)
 * [Datasets](#datasets)

## Models

This list is sorted by downloads as of May 30, 2024.
616 models are listed.

- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - BERT base Japanese (IPA dictionary)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 3,909,579
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 2,701,696
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - xlm-roberta-ner-japanese(Japanese caption : æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification.
  - Downloads: 1,080,186
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This is a Japanese sentence-BERT model.
  - Downloads: 1,063,216
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 332,689
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - BERT base Japanese (IPA dictionary, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 303,441
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - shisa-gamma-7b-v1For more information see our main Shisa 7B modelWe applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
  - Downloads: 193,784
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 122,699
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 122,537
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - BERT base Japanese (character tokenization)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 118,223
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
  - Downloads: 94,766
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - BERT base Japanese (character tokenization, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 83,045
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This is a Japanese sentence-BERT model.
  - Downloads: 65,397
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - Sentence BERT base Japanese modelThis repository contains a Sentence BERT base model for Japanese.
  - Downloads: 62,275
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 53,774
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japaneseæ—¥æœ¬èªã®README/Japanese READMEGLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
  - Downloads: 46,790
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 46,547
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 41,144
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  - Downloads: 41,098
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - Model Card for Japanese DeBERTa V2 baseModel
  - Downloads: 38,936
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSEModel descriptionLanguage-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
  - Downloads: 30,515
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 18,287
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B (rinna/llama-3-youko-8b)
  - Downloads: 17,803
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
  - Downloads: 15,991
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - bert-finetuned-japanese-sentimentThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
  - Downloads: 13,164
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - ã€å‘ŠçŸ¥ã€‘chilled_remixåŠã³reversemixã¯2023å¹´5æœˆ21æ—¥ã«Versionå¤‰æ›´ã‚’è¡Œã„ã€v2ã¸ç§»è¡Œã„ãŸã—ã¾ã—ãŸã€‚
  - Downloads: 12,189
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - japanese-sentiment-analysisThis model was trained from scratch on the chABSA dataset.
  - Downloads: 12,028
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-ggufLocal-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Vecteus-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 11,488
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - japanese-roberta-baseThis repository provides a base-sized Japanese RoBERTa model.
  - Downloads: 11,285
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 11,176
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 11,035
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - rinna/japanese-clip-vit-b-16This is a Japanese CLIP (Contrastive Language-Image Pre-Training) model trained by rinna Co.
  - Downloads: 9,886
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - Ninja-v1-NSFW-128k-ggufLocal-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFW-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 9,862
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 8,609
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-ggufDataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-RobinHoodã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 8,602
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-ggufDataPilotã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ArrowPro-7B-KUJIRAã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 8,409
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 7,990
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
  - Downloads: 7,741
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-RobinHood ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 7,537
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - bert-base-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-char-extended.
  - Downloads: 7,509
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Japanese Stable LM Instruct Gamma 7BModel
  - Downloads: 7,441
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - japanese-gpt-neox-3.6bOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 7,246
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - gpt-neox-japanese-2.7bThe open PR is merged on 2022/9/14.You can use this model with v4.23 and higher versions of transformers as follows,pip install transformersThis repository provides a 2.7B-parameter Japanese GPT-NeoX-based model.
  - Downloads: 7,207
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-MediumModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 7,017
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-ggufLocal-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-128kã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,863
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-ggufLocal-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,773
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguftokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MS-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,703
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - Ninja-v1-NSFW-ggufLocal-Novel-LLM-projectã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ninja-v1-NSFWã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,697
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7BModel
  - Downloads: 6,653
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguftokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-7b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,592
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 6,530
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
  - Downloads: 6,426
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-ggufyuisekiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,412
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 6,351
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instructModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 6,240
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-ggufryota39ã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-4k-instruct-dpoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,209
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2reazonspeech-nemo-v2 is an automatic speech recognition model trainedon ReazonSpeech v2.0 corpus.
  - Downloads: 6,195
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ï¼Ÿ
  - Downloads: 6,155
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - ğŸˆ FlexDreamHKFlexDreamHKã¯ãƒªãƒ¼ã‚¯ã•ã‚ŒãŸNovelAIãƒ¢ãƒ‡ãƒ«ã®å…¥ã£ã¦ã„ãªã„ã€ã‚ã‚‹ã„ã¯ãã®ãƒªã‚¹ã‚¯ã‚’å¯èƒ½ãªé™ã‚Šä½ãã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 6,141
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-ggufrinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llama-3-youko-8bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 6,105
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotæ§˜ã® ArrowPro-7B-KUJIRA ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 5,686
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-ggufmistralaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Mistral-7B-Instruct-v0.3ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 5,653
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufaixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8b-Cosmopedia-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 5,587
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-multilingualã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 5,570
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 5,468
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - sbert-jsnli-luke-japanese-base-liteThis is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
  - Downloads: 5,459
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 5,432
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
  - Downloads: 5,428
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-ggufmeta-llamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Meta-Llama-3-8B-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 5,167
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 5,166
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - rinna/youri-7bOverviewWe conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
  - Downloads: 5,156
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
  - Downloads: 5,151
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
  - Downloads: 5,119
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-WhisperKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteenAsahi Ushio and Kotoba Technologies.
  - Downloads: 5,094
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fastModel DescriptionELYZA-japanese-Llama-2-13b ã¯ã€ Llama 2ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 4,852
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - japanese-gpt2-xsmallThis repository provides an extra-small-sized Japanese GPT-2 model.
  - Downloads: 4,835
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT
  - Downloads: 4,626
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - bilingual-gpt-neox-4bOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 4,557
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM-LargeModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 4,485
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-ggufmicrosoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-mini-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 4,329
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)This is an ELECTRA model pretrained on approximately 200M Japanese sentences extracted from the mC4 and finetuned by spaCy v3 on UD_Japanese_BCCWJ r2.8.The base pretrain model is megagonlabs/transformers-ud-japanese-electra-base-discrimininator.
  - Downloads: 4,289
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FINGU-AI/FinguAI-Chat-v1OverviewThe FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
  - Downloads: 4,251
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B (CALM2-7B)
  - Downloads: 4,234
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 4,093
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-base-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese-with-auto-jumanpp")
  - Downloads: 3,967
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,917
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 3,895
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-ggufmicrosoftã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Phi-3-medium-128k-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,880
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - IntroductionWho am I: Qishen Ha
  - Downloads: 3,762
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
  - Downloads: 3,728
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - japanese-gpt-neox-3.6b-instruction-sft-v2OverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 3,692
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - japanese-stablelm-2-instruct-1_6b-ggufstabilityaiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-stablelm-2-instruct-1_6bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,633
- [Helsinki-NLP/opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja)
  - en-jasource group: Englishtarget group: JapaneseOPUS readme: eng-jpnmodel: transformer-alignsource language(s): engtarget language(s): jpnmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 3,541
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 3,532
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7BShisa 7B (shisa-7b-v1)
  - Downloads: 3,512
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 3,436
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - bilingual-gpt-neox-4b-8kOverviewNotice: This model requires transformers&gt;=4.31.0 to work properly.
  - Downloads: 3,318
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This is a Japanese sentence-LUKE model.
  - Downloads: 3,283
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,208
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguftokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-13b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 3,128
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - japanese-gpt2-mediumThis repository provides a medium-sized Japanese GPT-2 model.
  - Downloads: 3,097
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - Japanese SimCSE (BERT-base)
  - Downloads: 3,042
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, withadditional postprocessing stacks integrated as pipeline.
  - Downloads: 2,991
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - rinna/japanese-hubert-baseOverviewThis is a Japanese HuBERT Base model trained by rinna Co.
  - Downloads: 2,976
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - c4ai-command-r-v01-japanese-instructGGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF versionæ¦‚è¦CohereForAI/c4ai-command-r-v01ã‚’ã€ichikara-instructionã‚’ä½¿ã£ã¦è¿½åŠ ã§æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 2,941
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 2,895
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 2,894
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - hotchpotch/japanese-reranker-cross-encoder-large-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 2,862
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - japanese-gpt2-smallThis repository provides a small-sized Japanese GPT-2 model.
  - Downloads: 2,843
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - japanese-gpt-neox-3.6b-instruction-ppoOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 2,843
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªç‰ˆã¯ã¾ã ä½œæˆä¸­ã§ã™ã€‚
  - Downloads: 2,674
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM-SmallModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
  - Downloads: 2,670
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-novel-gpt-j-6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,635
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - ELYZA-japanese-Llama-2-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,614
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - Model Card for Japanese DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')
  - Downloads: 2,571
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - rinna/nekomata-14bOverviewWe conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
  - Downloads: 2,545
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
  - Downloads: 2,536
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectæ§˜ã® Assistance ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 2,535
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiæ§˜ã® Japanese-Chat-Umievo-itr004-7b ã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 2,477
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
  - Downloads: 2,382
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLM-13B-instruct-ggufFugaku-LLMã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Fugaku-LLM-13B-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,373
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 2,333
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model IDå®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ã§ã™ /
  - Downloads: 2,326
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - yuyuyui-chatbotThis model is based on rinna/japanese-gpt2-medium and finetuned on Yuyuyui scenario corpus.
  - Downloads: 2,316
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-ggufaixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Honyaku-13bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 2,256
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 2,238
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 2,118
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - japanese-gpt-1bThis repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 2,102
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
  - Downloads: 2,053
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - stockmark/stockmark-13bStockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
  - Downloads: 2,014
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,952
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for VecTeus-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the f
  - Downloads: 1,938
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUFæ¦‚è¦Aratako/Ninja-v1-RP-expressiveã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 1,931
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 1,884
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - japanese-large-lm-3.6b-instruction-sftThis repository provides a 3.6B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
  - Downloads: 1,867
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - hubert-base-asrThis model is a fine-tuned version of rinna/japanese-hubert-base on the common_voice_11_0 dataset for ASR tasks.
  - Downloads: 1,775
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - oldï¼Ÿ
  - Downloads: 1,761
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mixæ¦‚è¦ / OverviewYaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ 
  - Downloads: 1,672
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUFOriginal Modelhaqishen/Llama-3-8B-Japanese-InstructRun with LlamaEdgeLlamaEdge version: v0.10.1 and abovePrompt templatePrompt type: llama-3-chatPrompt string&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;{{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
  - Downloads: 1,664
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - bert-base-japanese-wikipedia-ud-headModel
  - Downloads: 1,651
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 1,635
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-ggufhaqishenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Japanese-Instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,627
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 1,582
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - weblab-10b-instruction-sft-GPTQOriginal model weblab-10b-instruction-sft which is a Japanese-centric multilingual GPT-NeoX model of 10 billion parameters created by matsuo-labTakeshi Kojima.
  - Downloads: 1,576
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 1,536
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,518
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - stockmark-gpt-neox-japanese-1.4b-ggufstockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹gpt-neox-japanese-1.4bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,509
- [KBlueLeaf/guanaco-7b-leh-v2](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2)
  - Guanaco-leh-V2: A Multilingual Instruction-Following Language Model Based on LLaMA
  - Downloads: 1,493
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUFæ¦‚è¦Aratako/Ninja-v1-RPã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 1,489
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-ggufCohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹c4ai-command-r-plusã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,483
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-ggufumiyukiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Japanese-Chat-Umievo-itr001-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,481
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
  - Downloads: 1,466
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯Luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 1,410
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - rinna/japanese-cloob-vit-b-16This is a Japanese CLOOB (Contrastive Leave One Out Boost) model trained by rinna Co.
  - Downloads: 1,380
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - uniTKU-hubert-japanese-asrThis model was fine-tuned on a dataset provided by uniTKU, and it has maintained the original performance metrics on the common_voice_11_0 dataset.
  - Downloads: 1,318
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - Japanese-Starling-ChatV-7B-GGUFGGUF conversion of "Japanese-Starling-ChatV-7B""Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.I applied the chat vector acquired by subtracting the weights of Mistral-7B-v0.1 from the weights of "Starling-LM-7B-beta" to this model.
  - Downloads: 1,301
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 1,298
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 1,258
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - albert-base-japanese-v1æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«Sentencepieceã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ãã®ã¾ã¾ã§ã¯[MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚ã¨ã«ä½™è¨ˆãªãƒˆãƒ¼ã‚¯ãƒ³ãŒæ··å…¥ã™ã‚‹å•é¡ŒãŒã‚ã‚‹ã®ã§ã€åˆ©ç”¨ã™ã‚‹éš›ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™for PyTorchfrom transformers import (AlbertForMaskedLM, AlbertTokenizerFast)import torchtokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")
  - Downloads: 1,250
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 1,250
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - wav2vec2-base-asrThis model is a fine-tuned version of rinna/japanese-wav2vec2-base on the common_voice_11_0 dataset for ASR tasks.
  - Downloads: 1,237
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUFæ¦‚è¦Aratako/Ninja-v1-RP-expressive-v2ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 1,205
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - hubert-large-asrThis model is a fine-tuned version of rinna/japanese-hubert-large ASR.
  - Downloads: 1,175
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2"A parrot able to speak Japanese, ukiyoe, edo period" â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
  - Downloads: 1,157
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - rinna/japanese-wav2vec2-baseOverviewThis is a Japanese wav2vec 2.0 Base model trained by rinna Co.
  - Downloads: 1,155
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
  - Downloads: 1,130
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - clip-japanese-baseThis is a Japanese CLIP (Contrastive Language-Image Pre-training) model developed by LY Corporation.
  - Downloads: 1,078
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - lightblue-suzume-llama-3-8B-japanese-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹suzume-llama-3-8B-japaneseã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 1,063
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - Fugaku-LLMåˆ©ç”¨è¦ç´„ã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
  - Downloads: 1,039
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 1,032
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-InstructModel DescriptionPLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 1,026
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 1,025
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - Japanese StableLM-3B-4E1T InstructModel DescriptionThis is a 3B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese StableLM-3B-4E1T Base.
  - Downloads: 1,009
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - japanese-large-lm-3.6bThis repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation.
  - Downloads: 1,003
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 992
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation on MIRACL japaneseThese models don't train on the MIRACL training data.
  - Downloads: 989
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)CoolJapanDiffusion 2.1.1ã¨WaifuDiffusion 1.4 anime epoch2ã®ãƒãƒ¼ã‚¸ã€‚
  - Downloads: 980
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - hotchpotch/japanese-bge-reranker-v2-m3-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 966
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - bert-base-japanese-v3-marc_jaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„Ÿæƒ…åˆ†æ)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 939
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - bilingual-gpt-neox-4b-instruction-ppoOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
  - Downloads: 936
- [mmnga/aixsatoshi-Ex-karakuri-8x12B-chat-v1-gguf](https://huggingface.co/mmnga/aixsatoshi-Ex-karakuri-8x12B-chat-v1-gguf)
  - aixsatoshi-Ex-karakuri-8x12B-chat-v1-ggufaixsatoshiã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Ex-karakuri-8x12B-chat-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 919
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 891
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - gpt2-large-japaneseThis repository provides a large sized Japanese GPT-2 model.
  - Downloads: 889
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - Model Card for Tanrei/GPTSAN-japaneseGeneral-purpose Swich transformer based Japanese language modelGPTSAN has some unique features.
  - Downloads: 883
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 883
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA-japanese-Llama-2-7b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 883
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - Model Card for Japanese DeBERTa V3 baseModel
  - Downloads: 875
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - rinna/nekomata-7bOverviewWe conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
  - Downloads: 860
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUFOriginal Modelhaqishen/Llama-3-8B-Japanese-InstructRun with GaianetPrompt template:prompt template: llama-3-chatContext size:chat_ctx_size: 4096Run with GaiaNet:Quick start: https://docs.gaianet.ai/node-guide/quick-startCustomize your node: https://docs.gaianet.ai/node-guide/customizeQuantized GGUF ModelsNameQuant methodBitsSizeUse caseLlama-3-8B-Japanese-Instruct-Q2_K.ggufQ2_K23.18 GBsmallest, significant quality loss - not recommended for most purposesLlama-3-8B-Japanese-I
  - Downloads: 852
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - luke-japaneseluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
  - Downloads: 852
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - hotchpotch/japanese-reranker-cross-encoder-small-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 850
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-ggufQwenã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Qwen1.5-110B-Chatã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 842
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - japanese-gpt-neox-smallThis repository provides a small-sized Japanese GPT-NeoX model.
  - Downloads: 842
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 837
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - japanese-gpt-neox-3.6b-instruction-sftOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
  - Downloads: 834
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguftokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-70b-instruct-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 822
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 815
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 814
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - roberta-small-japanese-luw-uposModel
  - Downloads: 796
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-ggufpfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfin-inst-mergeã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 788
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NCModel DescriptionPLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
  - Downloads: 786
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - stockmark/gpt-neox-japanese-1.4bThis repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
  - Downloads: 784
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-datasetã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬6ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 782
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteusã®GGUFç‰ˆã§ã™ã€‚
  - Downloads: 780
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€CreativeML Open RAIL-Mã€ã§Licenseãã®ã‚‚ã®ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  - Downloads: 757
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - BERTã«ã‚ˆã‚‹æ—¥æœ¬èªå›ºæœ‰è¡¨ç¾æŠ½å‡ºã®ãƒ¢ãƒ‡ãƒ«BertForTokenClassificationã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã®æ–‡ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
  - Downloads: 756
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - Japanese StableLM-3B-4E1T BaseModel DescriptionThis is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
  - Downloads: 728
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - nlp-waseda/bigbird-base-japaneseModel descriptionThis is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/bigbird-base-japanese")
  - Downloads: 723
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 722
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 722
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 721
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-ggufpfnetã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹nekomata-14b-pfn-qfinã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 713
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA-japanese-CodeLlama-7b-instruct-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 688
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
  - Downloads: 665
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 653
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-ggufalfredplplã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Llama-3-8B-Instruct-Jaã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 645
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - XLNet-japaneseModel descriptionThis model require Mecab and senetencepiece with XLNetTokenizer.
  - Downloads: 629
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fastã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 624
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - rinna/japanese-gpt-neox-3.6brinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 608
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA-japanese-Llama-2-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-Llama-2-7bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 608
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - rinna/japanese-gpt-neox-3.6b-instruction-pporinnaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-gpt-neox-3.6b-instruction-ppoã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 599
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - Model Card for Japanese character-level DeBERTa V2 baseModel
  - Downloads: 596
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 592
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 582
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - hotchpotch/japanese-reranker-cross-encoder-base-v1æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸ Reranker (CrossEncoder) ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 581
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - æ—¥æœ¬èªå‘ã‘ Llama 3 8Bã¯ã˜ã‚ã«ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯Llama 3ã‚’æ—¥æœ¬èªåŒ–ã—ã‚ˆã†ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚
  - Downloads: 572
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - Model Card for Japanese DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese')
  - Downloads: 566
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - japanese-large-lm-1.7b-instruction-sftThis repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
  - Downloads: 566
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This is a Japanese sentence-T5 model.
  - Downloads: 564
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - nlp-waseda/roberta-large-japanese-seq512Model descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")
  - Downloads: 556
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - japanese-large-lm-1.7bThis repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
  - Downloads: 550
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0Model
  - Downloads: 550
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-1.7b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 532
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - alabnii/jmedroberta-base-sentencepiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 531
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - c4ai-command-r-v01-japanese-instruct-GGUFæ¦‚è¦Aratako/c4ai-command-r-v01-japanese-instructã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 521
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - rinna/japanese-hubert-largeOverviewThis is a Japanese HuBERT Large model trained by rinna Co.
  - Downloads: 518
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - luke-japanese-largeluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
  - Downloads: 515
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - line-corporation/japanese-large-lm-1.7bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-1.7bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 515
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet TDT-CTC 0.6B (ja)||parakeet-tdt_ctc-0.6b-ja is an ASR model that transcribes Japanese speech with Punctuations.
  - Downloads: 514
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-ggufstockmarkã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹stockmark-100bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 506
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7BKage is "å½±" in Japanese or "Shadow" in English.
  - Downloads: 505
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - bert-base-japanese-v3-jnliã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(è‡ªç„¶è¨€èªæ¨è«–)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 496
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifierText classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 484
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - nlp-waseda/roberta-base-japaneseModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese")
  - Downloads: 474
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - llm-book/t5-base-long-livedoor-news-corpusã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬7ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹è¦ç´„ç”Ÿæˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 474
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 455
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - mt5_summarize_japanese(Japanese caption : æ—¥æœ¬èªã®è¦ç´„ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization.
  - Downloads: 451
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - luke-japaneseluke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
  - Downloads: 446
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - bert-base-japanese-v3-unsup-simcse-jawikiã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬8ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 431
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-instruct - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-instructStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
  - Downloads: 428
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-A-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-A-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 417
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUFOriginal Modelelyza/ELYZA-japanese-Llama-2-13b-fast-instructRun with LlamaEdgeLlamaEdge version: v0.2.8 and abovePrompt templatePrompt type: llama-2-chatPrompt string&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{{ system_prompt }}&lt;&lt;/SYS&gt;&gt;{{ user_msg_1 }}
  - Downloads: 415
- [mmnga/tokyotech-llm-Swallow-7b-plus-hf-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-plus-hf-gguf)
  - tokyotech-llm-Swallow-7b-plus-hf-gguftokyotech-llmã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-7b-plus-hfã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 406
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7Bã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 394
- [webbigdata/C3TR-Adapter](https://huggingface.co/webbigdata/C3TR-Adapter)
  - ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰(Model Card for Model ID)C3TR-Adapterã¯GoogleãŒç™ºè¡¨ã—ãŸLLMã§ã‚ã‚‹gemma-7bã®æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹QLoRA Adapterã§ã™ã€‚
  - Downloads: 391
- [sin2piusc/whisper-medium-5k-jp](https://huggingface.co/sin2piusc/whisper-medium-5k-jp)
  - whisper-medium-5kThis model is a fine-tuned version of openai/whisper-medium on the None dataset.
  - Downloads: 384
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-ggufCohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-35Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 382
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 380
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 380
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Karasu-Mixtral-8x22B-v0.1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 370
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - Wav2Vec2-Large-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
  - Downloads: 369
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 361
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - Mixtral-8x7B-v0.1-japaneseMixtral-8x7B-v0.1-japaneseã¯Mixtral-8x7B-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 356
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - ELYZA-japanese-CodeLlama-7b-ggufELYZAã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 354
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - bert-base-japanese-v3-jstsã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(æ„å‘³é¡ä¼¼åº¦è¨ˆç®—)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 353
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-base - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-baseStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
  - Downloads: 351
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-ggufmatsuo-labã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹weblab-10b-instruction-sftã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 349
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-v1-7B-ggufSakanaAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹EvoLLM-JP-v1-7Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 345
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hackathon.
  - Downloads: 341
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 340
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - Model Card for Japanese BART baseModel
  - Downloads: 320
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - å›ç­”ã¨å›ç­”ãŒå‡ºã¦ãã‚‹ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’ä¸ãˆã‚‹ã¨è³ªå•æ–‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://github.com/sonoisa/deep-question-generationæœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚¹ãƒ†ãƒƒãƒ—æ¦‚è¦SQuAD 1.1ã‚’æ—¥æœ¬èªã«æ©Ÿæ¢°ç¿»è¨³ã—ã€ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯ç´„åŠåˆ†ï¼‰ã€‚
  - Downloads: 319
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 316
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B-GGUFGGUF conversion of "Japanese-WizardLM2-ChatV-7B"This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.We aimed to add the high performance of WizardLM-2 to the Japanese language capability of ChatNTQ.
  - Downloads: 314
- [rinna/nue-asr](https://huggingface.co/rinna/nue-asr)
  - rinna/nue-asrOverview[Paper][GitHub]We propose a novel end-to-end speech recognition model, Nue ASR, which integrates pre-trained speech and language models.
  - Downloads: 302
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM
  - Downloads: 295
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 295
- [mmnga/lightblue-ao-karasu-72B-gguf](https://huggingface.co/mmnga/lightblue-ao-karasu-72B-gguf)
  - lightblue-ao-karasu-72B-gguflightblueã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹ao-karasu-72Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 294
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1æ—¥æœ¬èªç‰ˆã¯è¿‘æ—¥å…¬é–‹äºˆå®šã§ã™ï¼ˆæ—¥æœ¬èªã‚’å‹‰å¼·ä¸­ãªã®ã§ã€é–“é•ã„ã¯ã”å®¹èµ¦ãã ã•ã„ï¼
  - Downloads: 281
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦AWSã®trn1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦é–‹ç™ºã—ãŸå¤§å–œåˆ©è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 278
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - Japanese Natural Language Inference ModelThis model was trained using SentenceTransformers Cross-Encoder class, gradient accumulation PR, and the code from CyberAgentAILab/japanese-nli-model.
  - Downloads: 274
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - t5-base-japanese-web (with Byte-fallback, 32K)Descriptionmegagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 270
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - Additional pretrained BERT base Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 269
- [webbigdata/C3TR-Adapter_gguf](https://huggingface.co/webbigdata/C3TR-Adapter_gguf)
  - Gemmaãƒ™ãƒ¼ã‚¹ã®æ—¥è‹±ã€è‹±æ—¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹webbigdata/C3TR-Adapterã‚’GPUãŒãªã„PCã§ã‚‚å‹•ã‹ã›ã‚‹ã‚ˆã†ã«ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 269
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - deberta-base-japanese-aozora-ud-headModel
  - Downloads: 268
- [votepurchase/Yaki-Dofu-Mix](https://huggingface.co/votepurchase/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mixæ¦‚è¦ / OverviewYaki-Dofu-Mixã¯ã€ã‚¢ãƒ‹ãƒ¡é¢¨ã®ç”»é¢¨ã«ç‰¹åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ 
  - Downloads: 265
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - BERT Base Japanese for IronyThis is a BERT Base model for sentiment analysis in Japanese additionally finetuned for automatic irony detection.
  - Downloads: 261
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 259
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 256
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 254
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - Model Card for Japanese character-level GPT-2 SmallModel descriptionThis is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.
  - Downloads: 251
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - Deepreneur-blue-lizard-ggufDeepreneurã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹blue-lizardã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 251
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - shisa-7b-v1-ggufaugmxntã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹shisa-7b-v1ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 248
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - bert-japanese-nerã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºã‚¿ã‚¹ã‚¯ã‚’ç›®çš„ã¨ã—ã¦ã€äº¬éƒ½å¤§å­¦ é»’æ©‹ãƒ»è¤šãƒ»æ‘è„‡ç ”ç©¶å®¤ãŒå…¬é–‹ã—ã¦ã„ã‚‹BERTæ—¥æœ¬èªPretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ãŒå…¬é–‹ã—ã¦ã„ã‚‹ner-wikipedia-datasetã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 236
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new format introduced
  - Downloads: 236
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - line-corporation/japanese-large-lm-3.6b-instruction-sftline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6b-instruction-sftã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 235
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - The English document is here.
  - Downloads: 225
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - Tanuki-ZeRo-ggufkanhatakeyamaã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹Tanuki-ZeRoã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 224
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - RoBERTa base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model.
  - Downloads: 215
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - Stanza model for Japanese (ja)Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages.
  - Downloads: 212
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-baseWe have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 212
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 204
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics.
  - Downloads: 199
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - Model Card for Japanese character-level GPT-2 LargeModel descriptionThis is a Japanese character-level GPT-2 Large (717M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.
  - Downloads: 196
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Licenseä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M licenseã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹Use the model without crediting the creatorã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹Sell images they generateã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹Run on services that generate images for moneyã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹Share merges using this modelã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹Sell this model or merges using this modelã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹Have different permissions when sharing merges
  - Downloads: 195
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7bThis is a merge of pre-trained language models created using mergekit.
  - Downloads: 190
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - nlp-waseda/roberta-large-japaneseModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese")
  - Downloads: 190
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
  - Downloads: 188
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 187
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - Original ModelOptical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 186
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Licenseä¿®æ­£ CreativeML OpenRAIL-M ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / Modified CreativeML OpenRAIL-M licenseã“ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å…¥ã‚Œãšã«ä½¿ç”¨ã™ã‚‹Use the model without crediting the creatorã“ã®ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸç”»åƒã‚’å•†ç”¨åˆ©ç”¨ã™ã‚‹Sell images they generateã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å•†ç”¨ã®ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã§åˆ©ç”¨ã™ã‚‹Run on services that generate images for moneyã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹Share merges using this modelã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã¾ãŸã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è²©å£²ã™ã‚‹Sell this model or merges using this modelã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹æ¨©é™ã‚’è¨­å®šã™ã‚‹Have different permissions when sharing mergesğŸ–¼ï¸ ä¾‹ / Examples(â€»ä»–ã®äººãŒç”Ÿæˆã—ãŸç‰©ã‚’è¡¨ç¤ºã—ã¦ã„ã‚‹å ´åˆã¯æœ¬äººã®è¨±è«¾ã‚’å¾—ã¦è¡¨ç¤ºã—ã¦ã„ã¾ã™)ã‚‚ã¡Pã•ã‚“ä½œ
  - Downloads: 182
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new f
  - Downloads: 175
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - æ—¥æœ¬èªç‰ˆCLIPãƒ¢ãƒ‡ãƒ«This is a CLIP text/image encoder model for Japanese.
  - Downloads: 175
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - recruit-jp/japanese-clip-vit-b-32-roberta-baseOverviewDeveloped by: Recruit Co.
  - Downloads: 165
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ã‚„Instaç³»ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒãƒ»Instaç³»ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
  - Downloads: 165
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b ã¯ã€ Code Llamaã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«è¿½åŠ äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 153
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - reazonspeech-espnet-nextReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team.
  - Downloads: 152
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - japanese-gpt-1b-PII-maskingModel Descriptionjapanese-gpt-1b-PII-masking ã¯ã€ æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿1B GPTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ—¥æœ¬èªã®æ–‡ç« ã‹ã‚‰å€‹äººæƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 149
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
  - Downloads: 149
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 148
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1-128kThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja-128k has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hack
  - Downloads: 145
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush â€” Stable Diffusion XLModel Descriptionjapanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
  - Downloads: 145
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")
  - Downloads: 144
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - line-corporation/japanese-large-lm-3.6bline-corporationã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹japanese-large-lm-3.6bã®ggufå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 140
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-ggufCohereForAIã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹aya-23-8Bã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆã§ã™ã€‚
  - Downloads: 139
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model IDæ–™ç†ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®è³ªå•æ–‡ã‹ã‚‰ã€æ¤œç´¢æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚ã‚‹å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™Model DetailsModel Descriptionä¾‹ãˆã°ã€ã€Œæ±äº¬ã®è‚‰æ–™ç†ã§ã€æ˜¥ã«é£Ÿã¹ã‚‰ã‚Œã‚‹ã€é¶è‚‰ã‚’ä½¿ã£ãŸæ–™ç†ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã†æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã€Œæ±äº¬ã€€â†’ã€€éƒ½é“åºœçœŒ/åœ°æ–¹(AREA)ã€ã€€ã€Œè‚‰æ–™ç†ã€€â†’ã€€ç¨®é¡(TYPE)ã€ã€€ã€Œæ˜¥ã€€â†’ã€€å­£ç¯€(SZN)ã€ã€€ã€Œé¶è‚‰ã€€â†’ã€€é£Ÿæ(INGR)ã€ã®ã‚ˆã†ã«ã€å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã—ã¾ã™æŠ½å‡ºå¯¾è±¡ã¯ã€AREAã€TYPEã€SZNã€INGRã®ï¼”ã¤ã§ã™Language(s) (NLP):
  - Downloads: 139
- [Local-Novel-LLM-project/Ninja-v1-GGUF](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-GGUF)
  - Ninja-v1 ã®GGUFç‰ˆOur Models for GGUFVecteus-GGUFNinja-v1-GGUFNinja-v1-NSFW-GGUFNinja-v1-128k-GGUFNinja-v1-NSFW-128k-GGUF
  - Downloads: 138
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - SpeechT5 (TTS task) for JapaneseSpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech)
  - Downloads: 137
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 135
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - whisper-large-v2-japanese-5k-stepsThis model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
  - Downloads: 133
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - DeBERTa V2 small JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 133
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 131
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 131
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 129
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - â—†REV-Mix"ãƒ¬ãƒœãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 128
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - æ—¥æœ¬èªT5 Prefix Language ModelThis is a T5 (Text-to-Text Transfer Transformer)
  - Downloads: 127
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 126
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP Model CardModel detailModel type:LLaVA-JP is a vision-language model that can converse about input images.
  - Downloads: 125
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 124
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instructStockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
  - Downloads: 124
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM
  - Downloads: 124
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 124
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - transformer-lm-japanese-0.1bThis is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 123
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
  - Downloads: 122
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - roberta_qa_japanese(Japanese caption : æ—¥æœ¬èªã® (æŠ½å‡ºå‹) è³ªå•å¿œç­”ã®ãƒ¢ãƒ‡ãƒ«)This model is a fine-tuned version of rinna/japanese-roberta-base (pre-trained RoBERTa model provided by rinna Co.
  - Downloads: 120
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
  - Downloads: 117
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - æ›´æ–°å±¥æ­´2023å¹´5æœˆ7æ—¥ã€Œoasst1-89k-jaã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ã¾ã—ãŸã€‚
  - Downloads: 116
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - ã“ã¡ã‚‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã€civitaiã«ã¦å…ˆã«å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 115
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collectionã¨ã¯ï¼Ÿ
  - Downloads: 114
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - bert-large-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-large-japanese-char-extended.
  - Downloads: 113
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - Japanese-LLaMA-2-13B-GGUFJapanese-LLaMA-2-13B-GGUFã¯Japanese-LLaMA-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 111
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - è‹±èª+æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
  - Downloads: 110
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - Kokuwalamettaã®æ”¹è‰¯ã§ãƒãƒ¼ã‚¸ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«æ¢ã—ã‚’ã—ã¦ã„ãŸã‚‰KiwiMixã¨ã„ã†é¢ç™½ãã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚
  - Downloads: 110
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - ã¯ã˜ã‚ã«Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 110
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - rinna/nekomata-14b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b-instruction.
  - Downloads: 107
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - Model Card for Japanese character-level
  - Downloads: 106
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech-v0.1Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
  - Downloads: 105
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - MobileBERT æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çˆ†èª•ï¼ï¼
  - Downloads: 105
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - Japanese to Korean translatorJapanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)
  - Downloads: 99
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 jaFinetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
  - Downloads: 99
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
  - Downloads: 99
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - japanese-soseki-gpt2-1bThis repository provides a 1.3B-parameter finetuned Japanese GPT2 model.
  - Downloads: 97
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This is a Japanese+English sentence-BERT model.
  - Downloads: 96
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2Model Details: Built with Meta Llama 3This is a model that has been fine-tuned (using QLora) on a very small dataset (around 1k) based on Meta's llama-3-8b-instruct.
  - Downloads: 95
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chatkarasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 95
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59363Validation MetricsLoss: 0.12651239335536957Accuracy: 0.9532079853817648Precision: 0.9729688278823665Recall: 0.9744633462616643AUC: 0.9717333684823413F1: 0.9737155136027014UsageYou can use cURL to access this model:$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}'
  - Downloads: 94
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - spekulatiusãƒãƒ¼ã‚¸ã—ã¦ã„ã‚‹ã¨ãŸã¾ã«å‡ºã¦ãã‚‹ã€Œç›®çš„ã®æ„å›³ã¨ã¯é•ã†ã®ã ã‘ã©ãªã‚“ã ã‹æ¶ˆã™ã«ã¯ã‚‚ã£ãŸã„ãªã„ãƒ¢ãƒ‡ãƒ«ã€ã‚’ãŠã™ãåˆ†ã‘ã™ã‚‹ã‚·ãƒªãƒ¼ã‚ºã§ã™ã€‚
  - Downloads: 89
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 85
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - DeBERTa V2 base JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
  - Downloads: 85
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - bert-base-japanese-unidic-luw-uposModel
  - Downloads: 85
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3ã€‚
  - Downloads: 84
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - ãƒ¢ãƒ‡ãƒ«èª¬æ˜ (model explanation)YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4ãƒãƒ¼ã‚¸å…ƒã®ãƒ«ãƒ¼ãƒ„ã«NAIãƒªãƒ¼ã‚¯ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†å™‚ãŒã‚ã‚‹ã®ã§ã€NAIãƒªãƒ¼ã‚¯ã‚¢ãƒ³ãƒã«ã¯éæ¨å¥¨ç†æƒ³ã®é»’é«ªãƒãƒ‹ãƒ†é¡”ãŒå‡ºã›ã‚‹YaguruMagikuã‚’ã€ã‚ã‚‹ç¨‹åº¦é¡”ãŒè¿‘ãã¦åˆ¶å¾¡ã—ã‚„ã™ã„AbyssOrangeMix2ã¨æ··ãœã¦ã¿ãŸã€‚
  - Downloads: 84
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP AlphaModel DetailsJapanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
  - Downloads: 81
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 80
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - zenz-v1zenz-v1ã¯GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãã‹ãªæ¼¢å­—å¤‰æ›ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 77
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 76
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - rinna/nekomata-7b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b-instruction.
  - Downloads: 75
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - Japanese-Alpaca-2-13B-GGUFJapanese-Alpaca-2-13B-GGUFã¯Japanese-Alpaca-2-13Bã®GGUFå½¢å¼ã§ã™ã€‚
  - Downloads: 73
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizardModel DescriptionDeepreneur-blue-lizardã¯ã€Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 73
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - bert-base-japanese-char-extendedModel
  - Downloads: 73
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - Model overviewThis model is the baseline model for awesome-japanese-nlp-classification-dataset.
  - Downloads: 71
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - bert-japanese_finetuned-sentiment-analysisThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
  - Downloads: 69
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - WS TCG Card Text TranslatorA Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
  - Downloads: 68
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - AIBunCho/japanese-novel-gpt-j-6bAI BunChoã§åˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 67
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 67
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - ELECTRA base Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 66
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-base-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 65
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This is for (private) DEMO only.
  - Downloads: 65
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - Model Card for Japanese BART largeModel descriptionThis is a Japanese BART large model pre-trained on Japanese Wikipedia.
  - Downloads: 64
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM
  - Downloads: 64
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.Usage:Install pip install faster-whisper (Check faster-whisper for detailed instructions.
  - Downloads: 64
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - recruit-jp/japanese-typo-detector-roberta-baseãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦æ—¥æœ¬èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨å„æ–‡å­—ã”ã¨ã«èª¤å­—è„±å­—ã§ã‚ã‚‹ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¾ã™å„ãƒ©ãƒ™ãƒ«ã®æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™idlabelmeaning0OKèª¤å­—ãªã—1deletion1æ–‡å­—ã®æŠœã‘2insertion_aä½™åˆ†ãª1æ–‡å­—ã®æŒ¿å…¥3insertion_bç›´å‰ã®æ–‡å­—åˆ—ã¨ä¸€è‡´ã™ã‚‹ï¼’æ–‡å­—ä»¥ä¸Šã®ä½™åˆ†ãªæ–‡å­—ã®æŒ¿å…¥4kanji-conversion_aåŒä¸€ã®èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰5kanji-conversion_bè¿‘ã„èª­ã¿ã‚’æŒã¤æ¼¢å­—ã®å…¥ã‚Œæ›¿ãˆï¼ˆèª¤å¤‰æ›ï¼‰6substitution1æ–‡å­—ã®å…¥ã‚Œæ›¿ãˆ7transpositionéš£æ¥ã™ã‚‹ï¼’æ–‡å­—é–“ã®è»¢ç½®8othersãã®ä»–ã®å…¥åŠ›èª¤ã‚Šèª¤ã‚Šç¨®é¡ã®è©³ç´°ã«ã¤ã„ã¦ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…ƒè«–æ–‡ã‚’ã”å‚ç…§ãã ã•ã„æ—¥æœ¬èª Wikipedia ã®ç·¨é›†å±¥æ­´ã«åŸºã¥ã å…¥åŠ›èª¤ã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ãã®ä»–ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯å½“ç¤¾ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„èª¤å­—è„±å­—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’Hugging Face Hubã«å…¬é–‹ã—ã¾ã—ãŸ (Recruit Data Blog)å­¦ç¿’ãƒ‡ãƒ¼ã‚¿äº¬éƒ½å¤§å­¦å¤§å­¦é™¢æƒ…å ±å­¦ç ”ç©¶ç§‘çŸ¥èƒ½æƒ…
  - Downloads: 63
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 61
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - rinna/nekomata-7b-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b.
  - Downloads: 61
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - deberta-base-japanese-wikipediaModel DescriptionThis is a DeBERTa(V2) model pre-trained on Japanese Wikipedia and é’ç©ºæ–‡åº« texts.
  - Downloads: 61
- [mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis)
  - Sentiment Analysis in Japanese - PhÃ¢n tÃ­ch cáº£m xÃºc trong tiáº¿ng Nháº­tBert phÃ¢n tÃ­ch cáº£m xÃºcModel descriptionMÃ´ hÃ¬nh cÃ³ tÃ¡c dá»¥ng xÃ¡c Ä‘á»‹nh cáº£m xÃºc cá»§a Ä‘oáº¡n vÄƒn.
  - Downloads: 59
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - rinna/nekomata-14b-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b.
  - Downloads: 57
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 57
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - Mixtral-8x7B-Instruct-v0.1-japaneseMixtral-8x7B-Instruct-v0.1-japaneseã¯Mixtral-8x7B-Instruct-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªã®èªå½™æ‹¡å¼µç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 56
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - WRIME-fine-tuned BERT base JapaneseThis model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
  - Downloads: 56
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-base-gamma-7bğŸ§© Configurationslices:- sources:-
  - Downloads: 56
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-instruct-gamma-7bğŸ§© Configurationslices:- sources:-
  - Downloads: 56
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 54
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion Model CardSFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
  - Downloads: 53
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - One more step before getting this model.
  - Downloads: 52
- [shinyice/chatvector-llava-v1.6-mistral-7b-ja](https://huggingface.co/shinyice/chatvector-llava-v1.6-mistral-7b-ja)
  - ChatVector-llava-v1.6-mistral-7b-JA Model CardModel Detailschatvector-llava-v1.6-mistral-7b-jaã¯æ—¥æœ¬èªã§ç”»åƒã‚’èª¬æ˜ã™ã‚‹ã“ã¨ãŒå¯èƒ½ãªVLMã§ã™ã€‚
  - Downloads: 50
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - alabnii/jmedroberta-base-sentencepieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 50
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59362Validation MetricsLoss: 0.13092292845249176Accuracy: 0.9527127414314258Precision: 0.9634070704982427Recall: 0.9842171959602166AUC: 0.9667289746092403F1:
  - Downloads: 49
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM
  - Downloads: 47
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - nlp-waseda/gpt2-small-japaneseThis model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
  - Downloads: 47
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - bert-base-japanese-jsnliThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
  - Downloads: 46
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Finetuned Waseda RoBERTa to evaluate the generated answers on JTruthfulQA.
  - Downloads: 46
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - japanese-sexual-moderation-v2ã¯ã€studio-ousia/luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 46
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸTokara-0.5B-v0.1ã«chat vectorã§å¯¾è©±èƒ½åŠ›ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 46
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 46
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2äº‹å‰å­¦ç¿’ã‹ã‚‰å…¨éƒ¨æ—¥æœ¬èªã§å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã§ã™ã€‚
  - Downloads: 45
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - Cross-Encoder for Natural Language Inference(NLI) for JapaneseConsidering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
  - Downloads: 44
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - Finance Sentiment JA (base)Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
  - Downloads: 43
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3Model Details: Built with Meta Llama 3llama-3-8bã®æ—¥æœ¬èªç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ChatVectorã‚’é©ç”¨ã—ã€ã•ã‚‰ã«QLoraã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 43
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - luke-japanese-large-liteluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
  - Downloads: 43
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦Qwen/Qwen1.5-0.5Bã‚’æ—¥è‹±ãƒ‡ãƒ¼ã‚¿5Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 42
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 42
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
  - Downloads: 42
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - albert-base-japanese-v1-with-japaneseæ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ALBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯Tokenizerã«BertJapaneseTokenizerã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™albert-base-japanese-v1ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒæ¥½ã«ãªã£ã¦ã„ã¾ã™How to useãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯PreTrainedãƒ¢ãƒ‡ãƒ«ã§ã™åŸºæœ¬çš„ã«ã¯å„ç¨®ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™Fill-Maskfor PyTorchfrom transformers import (AutoModelForMaskedLM, AutoTokenizer)tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")
  - Downloads: 41
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - sonoisa/t5-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 41
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - (English part follows Japanese one.
  - Downloads: 41
- [Local-Novel-LLM-project/Assistance](https://huggingface.co/Local-Novel-LLM-project/Assistance)
  - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kTHIS IS WIP MODELã“ã‚Œã¯ Ninja ã‚’ å°èª¬èƒ½åŠ›ã§ã¯ãªãã‚³ãƒ¼ãƒ‰ã‚„æ•°å­¦ç³»ã®çŸ¥è­˜ã‚’æŒãŸã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™
  - Downloads: 41
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 41
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 40
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦QAã‚¿ã‚¹ã‚¯ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 39
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B pre-trained model for JapaneseModel DescriptionGPT2/GPT3 like model trained on Japanese.corpus.
  - Downloads: 39
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - æ—¥æœ¬èªByT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 39
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 38
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressiveGGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF versionæ¦‚è¦This is a merge of pre-trained language models created using mergekit.
  - Downloads: 38
- [Helsinki-NLP/opus-mt-ja-nl](https://huggingface.co/Helsinki-NLP/opus-mt-ja-nl)
  - jpn-nldsource group: Japanesetarget group: DutchOPUS readme: jpn-nldmodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latntarget language(s): nldmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 37
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - bert-base-japanese-v3-bpr-question-aioã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®è³ªå•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
  - Downloads: 37
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ calm-2-7b-chat ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 37
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - (English part follows Japanese one.
  - Downloads: 36
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model cardè‹±æ—¥ã€æ—¥è‹±ç¿»è¨³ç”¨ãƒ¢ãƒ‡ãƒ«C3TR-Adapterã®GPTQ4bité‡å­åŒ–ç‰ˆã§ã™ã€‚
  - Downloads: 36
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset.
  - Downloads: 35
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using the Common Voice and JSUT.The sentence outputs do not contain word boundaries.
  - Downloads: 35
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - BERT base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer BERT base Japanese for details about the pre-training model.
  - Downloads: 34
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - IntroductionWho am I: Qishen Ha
  - Downloads: 34
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 1.7B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation.
  - Downloads: 34
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - Ninja-v1-RPGGUFç‰ˆã¯ã“ã¡ã‚‰/Click here for the GGUF versionæ¦‚è¦This is a merge of pre-trained language models created using mergekit.Aratako/Ninja-v1-RP-WIPã‚’ãƒ™ãƒ¼ã‚¹ã«ã€Task Vectorã®åŠ ç®—ãƒ»Model Stockã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ã‚’è¡Œã„æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã¨è¡¨ç¾åŠ›ã‚’å¼·åŒ–ã—ãŸãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 33
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - nlp-waseda/gpt2-xl-japaneseThis is Japanese GPT2 with approximatelyã€€1.5B parameters pretrained on Japanese Wikipedia and CC-100The model architecture of the model are based on Radford+ 2019.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
  - Downloads: 33
- [espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804](https://huggingface.co/espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804)
  - ESPnet2 TTS pretrained modelkan-bayashi/jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjtalk_accent_with_pause_latestâ™»
  - Downloads: 33
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JSTS(æ–‡ç« ã®é¡ä¼¼åº¦è¨ˆç®—)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 32
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
  - Downloads: 32
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª |ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 32
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - æ›´æ–°æƒ…å ±æ—¥æœ¬èªæ©Ÿèƒ½ã¨instructãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã—ãŸver.2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ãƒ¢ãƒ‡ãƒ«æ¦‚è¦Swallow-MX-8x7b-NVE-v0.1ã«å¯¾ã—ã€Mixtral-8x7B-Instruct-v0.1ã¨Mixtral-8x7B-v0.1ã®å·®åˆ†ã‚’ãƒãƒ¼ã‚¸ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 32
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-BaseSambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
  - Downloads: 32
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - doc2query/msmarco-japanese-mt5-base-v1This is a doc2query model based on mT5 (also known as docT5query).
  - Downloads: 32
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - GPT-2 small Japanese modelThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
  - Downloads: 32
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - æ—¥æœ¬èªåŒ»ç™‚å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«æ¦‚è¦ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶å®¤ã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹MedTxt-CRã‚’ç”¨ã„ã¦ã€alabniiã•ã¾ã‚ˆã‚Šå…¬é–‹ã•ã‚Œã¦ã„ã‚‹RoBERTaã‚’fine-tuningã—ãŸå›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 32
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - bart-base-japanese-news(base-sized model)This repository provides a Japanese BART model.
  - Downloads: 31
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteusã‚’ãƒ™ãƒ¼ã‚¹ã«LLavaã«å¯¾å¿œã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 30
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUFæ¦‚è¦Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1ã®é‡å­åŒ–æ¸ˆã¿GGUFç‰ˆã§ã™ã€‚
  - Downloads: 29
- [alfredplpl/gemma-2b-it-ja-poc-3](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-3)
  - ã¯ã˜ã‚ã«ãªã‚“ã‹æ—¥æœ¬èªãŒè©±ã›ã‚‹å•†ç”¨åˆ©ç”¨å¯èƒ½ãªAIã§ã™ã€‚
  - Downloads: 29
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 29
- [Helsinki-NLP/opus-mt-ja-it](https://huggingface.co/Helsinki-NLP/opus-mt-ja-it)
  - jpn-itasource group: Japanesetarget group: ItalianOPUS readme: jpn-itamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): itamodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 28
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - bart-base-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 28
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - bert-base-japanese-v3-jcommonsenseqaã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬5ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹(å¤šè‚¢é¸æŠå¼è³ªå•å¿œç­”)ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 28
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€MARC-ja(positive or negativeã®äºŒå€¤åˆ†é¡)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 28
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
  - Downloads: 28
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - Model Card for Model IDThis is a small T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese and English corpus.
  - Downloads: 28
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - luke-large-defamation-detection-japaneseæ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºå™¨This model is a fine-tuned version of studio-ousia/luke-japanese-large for the Japanese language finetuned for automatic defamation detection.
  - Downloads: 27
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZA-japanese-Llama-2-13b-fast-instructã® GGUF
  - Downloads: 27
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 27
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 27
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This is a model for named entity recognition of Japanese medical documents.
  - Downloads: 26
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
  - Downloads: 26
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - BERT base Japanese modelThis repository contains a BERT base model trained on Japanese Wikipedia dataset.
  - Downloads: 26
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - ESã‚’æ›¸ãAIJapanese GPT-2 modelã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€å†…å®šè€…ã®äºŒä¸‡ä»¶ä»¥ä¸Šã®ESã‚’ç”¨ã„ã¾ã—ãŸã€‚
  - Downloads: 25
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for JapaneseThis model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 25
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - whisper-large-v3-japanese-4k-stepsThis model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
  - Downloads: 25
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - Model Card for Model IDFine tunned ASR model from distil-whisper/distil-large-v2.This model aimed to transcribe japanese audio especially visual novel.
  - Downloads: 24
- [sin2piusc/whisper-large-v2-anime](https://huggingface.co/sin2piusc/whisper-large-v2-anime)
  - whisper-large-v2-animeThis model is a fine-tuned version of clu-ling/whisper-large-v2-japanese-5k-steps on joujiboi/japanese-anime-speech (https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Downloads: 24
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - Japanese CLIP ViT-H/14 (Wider)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
  - Downloads: 24
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 3.6B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation.
  - Downloads: 22
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUFThis model was converted to GGUF format from inu-ai/alpaca-guanaco-japanese-gpt-1b using llama.cpp via the ggml.ai's GGUF-my-repo space.
  - Downloads: 21
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp")
  - Downloads: 21
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - roberta-base-japanese-aozora-charModel DescriptionThis is a RoBERTa model pre-trained on é’ç©ºæ–‡åº« texts with character tokenizer.
  - Downloads: 21
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - æ—¥æœ¬èª gpt2 è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt2-meduimã‚’æ•™å¸«ã¨ã—ã¦è’¸ç•™ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 20
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - NLLB 1.3B fine-tuned on Japanese to English Light Novel translationThis model was fine-tuned on light and web novel for Japanese to English translation.
  - Downloads: 20
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
  - Downloads: 20
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This pretrained model is almost the same as cl-tohoku/bert-base-japanese-char-v2 but do not need fugashi or unidic_lite.
  - Downloads: 20
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - Japanese DialoGPT trained with Aozora(ja) é’ç©ºæ–‡åº«ã®ã‚»ãƒªãƒ•ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®DialoGPT Smallã§ã™(en) Japanese DialoGPT Small trained on Aozora Bunko.
  - Downloads: 20
- [ryota39/llm-jp-1b-sft-100k-LoRA-dpo-12k](https://huggingface.co/ryota39/llm-jp-1b-sft-100k-LoRA-dpo-12k)
  - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šryota39/llm-jp-1b-sft-100k-LoRAå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šllm-jp/hh-rlhf-12k-jaå­¦ç¿’æ–¹å¼ï¼šãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«import torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer =
  - Downloads: 19
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7Bã€Œã©ã†ã‹ãŠæ…ˆæ‚²ã‚’ ã‚‚ã† ç–²ã‚Œæœã¦ã¾ã—ãŸã€ç”Ÿæˆä¾‹[å¤ªå­—ä»¥é™ãŒAIç”Ÿæˆ]ã€Œã©ã†ã‹ã€â€ãã‚Œâ€ã¯æ‡‡é¡˜ã—ãŸã€‚
  - Downloads: 19
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14BğŸŒEnglish | ğŸ‡¨ğŸ‡³ä¸­æ–‡ | ğŸ‡¯ğŸ‡µæ—¥æœ¬èª | ğŸ‡°ğŸ‡·í•œêµ­ì–´ğŸ¤—
  - Downloads: 19
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - roberta-base-japanese-aozora-ud-headModel
  - Downloads: 19
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - æ—¥æœ¬èªT5äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
  - Downloads: 19
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - bert-large-japanese-unidic-luw-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-large-japanese.
  - Downloads: 19
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€€ãœã²éŠã³ã«ãã¦ã­ã€‚
  - Downloads: 19
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - hubert-base-jtubeThis repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.Scroll down for the model usageFAQQ. 
  - Downloads: 19
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM
  - Downloads: 19
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - roberta-large-japanese-luw-uposModel
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - wav2vec2-live-japanesehttps://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using thecommon_voiceJSUTCSS10TEDxJP-10KJVSJSSSInference#usageimport torchimport torchaudiofrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processormodel = Wav2Vec2ForCTC.from_pretrained("ttop324/wav2vec2-live-japanese")
  - Downloads: 18
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - 275.86Mã®mixtralã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§pretrainingã—ãŸã‚‚ã®ã§ã™samplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 18
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - roberta-large-japanese-aozora-ud-headModel
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - deberta-large-japanese-unidic-ud-headModel
  - Downloads: 18
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU-Modelã€€(T5 fine-tuned model)JAINU is a Japanese - Ainu language machine translation model.
  - Downloads: 18
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«SEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
  - Downloads: 18
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS modelkan-bayashi/jsut_transformer_accent_with_pauseâ™»
  - Downloads: 18
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - åè¨€æ¨è«–ãƒ¢ãƒ‡ãƒ«
  - Downloads: 17
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - alabnii/jmedroberta-base-manbyo-wordpieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).
  - Downloads: 17
- [Local-Novel-LLM-project/Vecteus-Constant](https://huggingface.co/Local-Novel-LLM-project/Vecteus-Constant)
  - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kThis is a prototype of Vecteus-v1Model Card for VecTeus-ConstantThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the firs
  - Downloads: 17
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-smallFine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_vits_prosody)
  - ESPnet2 TTS pretrained modelkan-bayashi/jsut_vits_prosodyâ™»
  - Downloads: 17
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - bert-large-japanese-char-extendedModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts, derived from bert-large-japanese-char.
  - Downloads: 17
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - Japanese CLIP ViT-H/14 (Base)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
  - Downloads: 17
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset )ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒãƒ™ãƒ«é¢¨ç”»åƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§naver-clova-ix/donut-baseã‚’è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 17
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - yacis-electra-small-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 16
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - roberta-base-japanese-jsnliThis model is a fine-tuned version of nlp-waseda/roberta-base-japanese on the JSNLI dataset.
  - Downloads: 16
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - Fine-tuned Japanese Whisper model for speech recognition using whisper-baseFine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 16
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetestã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿç‰©å­¦ãƒ»åŒ»å­¦ã«ç²¾é€šã—ãŸOpenBioLLM-8Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ—¥æœ¬èªå¯¾å¿œã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«Llama-3-youko-8b-instruct-chatvectorã¨ãƒãƒ¼ã‚¸ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 16
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - deberta-base-japanese-luw-uposModel
  - Downloads: 16
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - roberta-large-japanese-aozora-charModel DescriptionThis is a RoBERTa model pre-trained on é’ç©ºæ–‡åº« texts with character tokenizer.
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - deberta-large-japanese-aozoraModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts.
  - Downloads: 16
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - deberta-small-japanese-luw-uposModel
  - Downloads: 16
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - ELECTRA small Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 16
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - whisper-large-v2-mix-jp model for CTranslate2This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
  - Downloads: 15
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - In-progess long-context Japanese-English translation model based on tinyllama.
  - Downloads: 15
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
  - Downloads: 15
- [haih2/open-calm-7b-summarizer-lora](https://huggingface.co/haih2/open-calm-7b-summarizer-lora)
  - Fine-tuned OpenCALM-7B Adapters for Meeting SummarizationDescriptionThese are weights for LoRA adapters fine-tuned on the OpenCALM-7B (Andonian et al.
  - Downloads: 15
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - studio-ousia/luke-japanese-baseã«å¯¾ã—ã¦æ¬¡ã®å¤‰æ›´ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 15
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - bert-base-japanese-luw-uposModel
  - Downloads: 15
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
  - Downloads: 15
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - roberta-small-japanese-aozoraModel DescriptionThis is a RoBERTa model pre-trained on é’ç©ºæ–‡åº« texts with Japanese-LUW-Tokenizer.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - deberta-large-japanese-unidicModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts with BertJapaneseTokenizer.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - deberta-base-japanese-wikipedia-luw-uposModel
  - Downloads: 15
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - Japanese BERT-base (Juman++ + Unigram)How to load the tokenizerPlease download the dictionary file for Juman++ + Unigram from our GitHub repository.
  - Downloads: 15
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 15
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 jaFinetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
  - Downloads: 15
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This model is a fine-tuned version of facebook/wav2vec2-xls-r-1b on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
  - Downloads: 14
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - friendly_JA-Modelã€€(T5 fine-tuned model)MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexiconExamplesinputoutputæœ€é©åŒ–ã‚’å¿œç”¨ã—ãŸæ©Ÿæ¢°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾åº¦ã ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿œç”¨ã—ãŸãƒã‚·ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ã å½¼ã¯æ¶ç©ºã®ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹å½¼ã¯ã‚¤ãƒã‚¸ãƒŠãƒªãƒ¼ä¸–ç•Œã«ä½ã‚“ã§ã„ã‚‹æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«æ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã«ã‹ã‹ã£ã¦ã—ã¾ã£ãŸæ·±å±¤å­¦ç¿’ã¯é›£ã—ã„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚€ãšã‹ã—ã„æ–°ãŸãªæ¦‚å¿µã‚’ç´¹ä»‹ã™ã‚‹æ–°ã—ã„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ç´¹ä»‹ã™ã‚‹æ´¥æ³¢ã®è­¦å ±ãŒæµã‚ŒãŸãƒ„ãƒŠãƒŸã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒæµã‚ŒãŸå—æµ·ãƒˆãƒ©ãƒ•ã®ç½å®³ã¯éœ‡æºåœ°ã«ã‚ˆã‚‹å—æµ·ãƒˆãƒ©ãƒ•ã®ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒ¼ã¯ã‚¨ãƒ”ã‚»ãƒ³ã‚¿ãƒ¼ã«ã‚ˆã‚‹æ¯å­ã¯éš›ã©ã„å†…å®¹ã®æœ¬ã‚’
  - Downloads: 14
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6BPlease check our blog post for more details, samples, evaluations and more:BlogpostModel DescriptionGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
  - Downloads: 14
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - GPT2 Japanese base model version 2Prerequisitestransformers==4.19.2Model
  - Downloads: 14
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics.
  - Downloads: 14
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - NLLB-200 1.3B fine-tuned on Ascendance of a BookwormThis model was fine-tuned on Ascendance of a Bookworm to translate the web novel in Japanese to English.
  - Downloads: 14
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - Chat &amp; support: TheBloke's Discord serverWant to contribute?
  - Downloads: 14
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 14
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - Japanese Stock Comment Sentiment ModelThis model is a sentiment analysis tool specifically trained to analyze comments and discussions related to Japanese stocks.
  - Downloads: 14
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€€ãœã²éŠã³ã«ãã¦ã­ã€‚
  - Downloads: 14
- [DataPilot/ArrowSmart-mistral-7B-KEMURI](https://huggingface.co/DataPilot/ArrowSmart-mistral-7B-KEMURI)
  - æ¦‚è¦ArrowSmart-mistral-7B-KEMURIã¯é«˜åº¦ãªæ—¥æœ¬èªèƒ½åŠ›ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°èƒ½åŠ›ã®åŒæ™‚ç²å¾—ã‚’ç›®æŒ‡ã—ã¦chat vectorã‚’ç”¨ã„ã¦ä½œã‚‰ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 14
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7BModel
  - Downloads: 14
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-tiny-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 14
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - roberta-small-japanese-aozora-charModel DescriptionThis is a RoBERTa model pre-trained on é’ç©ºæ–‡åº« texts with character tokenizer.
  - Downloads: 13
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - Japanese_Fine_Tuned_Whisper_ModelThis model is a fine-tuned version of openai/whisper-tiny on the Common Voice dataset.
  - Downloads: 13
- [jovyan/Swallow-MS-7b-v0.1-ChatVector](https://huggingface.co/jovyan/Swallow-MS-7b-v0.1-ChatVector)
  - Swallow-MS-7b-v0.1-ChatVectorJapanese "instruction tuned" model made by the technique of Chat VectorThe weights of this model are obtained not by any instruction tuning but by the following arithmetic:Swallow-MS-7b-v0.1 + Mistral-7B-Instruct-v0.2 - Mistral-7B-v0.1Chat Vectorã®æ‰‹æ³•ã‚’ä½¿ã£ã¦ã€å­¦ç¿’æ¸ˆã¿é‡ã¿ã®è¶³ã—å¼•ãã®ã¿ã§Swallow-MS-7b-v0.1ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å¯¾è©±èƒ½åŠ›ã‚’ä¸ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 13
- [wietsedv/xlm-roberta-base-ft-udpos28-ja](https://huggingface.co/wietsedv/xlm-roberta-base-ft-udpos28-ja)
  - XLM-RoBERTa base Universal Dependencies v2.8 POS tagging:
  - Downloads: 13
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - bert-large-japanese-luw-uposModel
  - Downloads: 13
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
  - Downloads: 13
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - LINE DistilBERT Japanese (forked by liwii)This is a forked version of DistilBERT model pre-trained on 131 GB of Japanese web text.
  - Downloads: 13
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 largeFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
  - Downloads: 13
- [Helsinki-NLP/opus-mt-ja-pl](https://huggingface.co/Helsinki-NLP/opus-mt-ja-pl)
  - jpn-polsource group: Japanesetarget group: PolishOPUS readme: jpn-polmodel: transformer-alignsource language(s): jpn jpn_Bopo jpn_Hani jpn_Hira jpn_Kana jpn_Latntarget language(s): polmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 12
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS modelkan-bayashi/jsut_conformer_fastspeech2â™»
  - Downloads: 12
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g. 
  - Downloads: 12
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT
  - Downloads: 12
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - bart-large-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University.
  - Downloads: 12
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-baseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€JNLI(æ–‡ç« ã®é–¢ä¿‚æ€§åˆ¤åˆ¥)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - bert-base-japanese-v3-bpr-passage-aioã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ã®ç¬¬9ç« ã§ç´¹ä»‹ã—ã¦ã„ã‚‹æ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«BPRã®ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã™ã€‚
  - Downloads: 12
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ Twitter/twhin-bert-base ã‚’SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆã«äººæ‰‹ã§æ”»æ’ƒæ€§è©•ä¾¡ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Fine-tuningã™ã‚‹ã“ã¨ã§ä½œæˆã—ã¾ã—ãŸã€‚
  - Downloads: 12
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7Bã“ã®ãƒ¢ãƒ‡ãƒ«ã¯"chatntq-ja-7b-v1.0"ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
  - Downloads: 12
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - deberta-large-japanese-juman-ud-goeswithModel DescriptionThis is a DeBERTa(V2) model pretrained on Japanese Wikipedia, CC-100, and OSCAR texts for POS-tagging and dependency-parsing (using goeswith for subwords), derived from deberta-v2-large-japanese.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - deberta-base-japanese-juman-ud-goeswithModel DescriptionThis is a DeBERTa(V2) model pretrained on Japanese Wikipedia, CC-100, and OSCAR texts for POS-tagging and dependency-parsing (using goeswith for subwords), derived from deberta-v2-base-japanese.
  - Downloads: 12
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - electra-base-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 12
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)This is the longer input version of RoBERTa Japanese model pretrained on approximately 200
  - Downloads: 12
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - ELECTRA base Japanese generatorThis is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 12
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - roberta-large-japanese-char-luw-uposModel
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - roberta-base-japanese-luw-uposModel
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - roberta-base-japanese-char-luw-uposModel
  - Downloads: 12
- [Helsinki-NLP/opus-mt-ja-bg](https://huggingface.co/Helsinki-NLP/opus-mt-ja-bg)
  - jpn-bulsource group: Japanesetarget group: BulgarianOPUS readme: jpn-bulmodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kanatarget language(s): bulmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 12
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - jpn-msasource group: Japanesetarget group: Malay (macrolanguage)OPUS readme: jpn-msamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kanatarget language(s): ind zlm_Latn
  - Downloads: 12
- [Helsinki-NLP/opus-mt-ja-hu](https://huggingface.co/Helsinki-NLP/opus-mt-ja-hu)
  - jpn-hunsource group: Japanesetarget group: HungarianOPUS readme: jpn-hunmodel: transformer-alignsource language(s): jpn_Bopo jpn_Hani jpn_Hira jpn_Kana jpn_Yiiitarget language(s): hunmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - Japanese BERT-base (Nothing + Unigram)How to load the tokenizerPlease download the dictionary file for Nothing + Unigram from our GitHub repository.
  - Downloads: 12
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯cl-tohoku/bert-large-japanese-v2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 12
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - Model Card for Model IDã“ã®ãƒ¢ãƒ‡ãƒ«ã¯rinna/japanese-gpt-1bã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®æŠ½å‡ºå‹QAã¨ã€è§£ç­”ã‚’æ–°ãŸãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒªãƒ•ã‚¡ã‚¤ãƒ³ã™ã‚‹ãŸã‚ã®å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [Helsinki-NLP/opus-mt-ja-da](https://huggingface.co/Helsinki-NLP/opus-mt-ja-da)
  - jpn-dansource group: Japanesetarget group: DanishOPUS readme: jpn-danmodel: transformer-alignsource language(s): jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): danmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 12
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - MPT-7B-baseã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - MPT-7B-instã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€MosaicMLã®llm-foundryãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦mosaicml/mpt-7b-instructã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 12
- [atsuki-yamaguchi/Mistral-7B-v0.1-random-ja](https://huggingface.co/atsuki-yamaguchi/Mistral-7B-v0.1-random-ja)
  - Mistral-7B Japanese
  - Downloads: 12
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - nagisa_bertA BERT model for nagisa.
  - Downloads: 12
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-ZeroBase model: llm-jp/llm-jp-13b-v1.0Instruction data: Randomly sampled, 15k Jaster dataset (train)Code is here.
  - Downloads: 12
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS modelkan-bayashi/jsut_tacotron2_accentâ™»
  - Downloads: 12
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
  - Downloads: 12
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - Model Card for Japanese DeBERTa V2 baseModel
  - Downloads: 12
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - yacis-electra-smallThis is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.
  - Downloads: 12
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - roberta-small-japanese-char-luw-uposModel
  - Downloads: 11
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - Byt5-small-ain-jpn-mt is a machine translation model pretrained with Google's ByT5-small and fine-tuned on bilingual datasets crawled from the Web.
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - ELECTRA small Japanese generatorThis is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - ELECTRA small Japanese finance generatorThis is a ELECTRA model pretrained on texts in the Japanese language.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - deberta-large-japanese-unidic-luw-uposModel
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - deberta-base-japanese-unidic-luw-uposModel
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - deberta-large-japanese-luw-uposModel
  - Downloads: 11
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - bert-base-ironyThis is a BERT Base model for the Japanese language finetuned for automatic irony detection.
  - Downloads: 11
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯luke-japanese-large-liteã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€Question-Answeringã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - æ—¥æœ¬èªã§trainingã—ãŸllama2model size:  130.78Mtrainingã¯ä»¥ä¸‹ã®scriptå‚ç…§https://github.com/Lightning-AI/lit-gpt/tree/mainusefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")
  - Downloads: 11
- [AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer](https://huggingface.co/AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer)
  - Adapter bert-base-multilingual-cased-ja-wiki_pfeiffer for bert-base-multilingual-casedPfeiffer Adapter trained with Masked Language Modelling on Japanese Wikipedia Articles for 250k steps and a batch size of 64.This adapter was created for usage with the Adapters library.
  - Downloads: 11
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - electra-base-cyberbullyingThis is an ELECTRA Base model for the Japanese language finetuned for automatic cyberbullying detection.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - deberta-base-japanese-wikipedia-ud-headModel
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - deberta-base-japanese-unidic-ud-headModel
  - Downloads: 11
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - deberta-small-japanese-uposModel DescriptionThis is a DeBERTa(V2) model pre-trained on é’ç©ºæ–‡åº« texts for POS-tagging and dependency-parsing, derived from deberta-small-japanese-aozora.
  - Downloads: 11
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - Japanese transformer pipeline (bert-base).
  - Downloads: 11
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - jpn-hebsource group: Japanesetarget group: HebrewOPUS readme: jpn-hebmodel: transformer-alignsource language(s): jpn_Hani jpn_Hira jpn_Kanatarget language(s): hebmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)
  - Downloads: 11
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-large-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - rinna/japanese-data2vec-audio-baseOverviewThis is a Japanese data2vec Audio Base model trained by rinna Co.
  - Downloads: 11
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - outputç­‘æ³¢ 2.0035860538482666ã¤ãã° 1.6586617231369019ç ”ç©¶ 1.6227693557739258å¤§å­¦ 1.3798155784606934å®Ÿé¨“ 0.5522942543029785å­¦ç”Ÿ 0.42351895570755005åˆ†æ 0.37844282388687134å›½ç«‹ 0.3685397505760193ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ 0.36495038866996765èŒ¨åŸ 0.3056415021419525ç§‘å­¦ 0.2876652181148529é–¢æ± 0.24301066994667053åœ°åŸŸ 0.21340851485729218å®Ÿæ–½ 0.1976248174905777å…ˆç«¯ 0.192025288939476ã‚µã‚¤ãƒˆ 0.11629197001457214èª¿æŸ» 0.09159307181835175ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ 0.08552580326795578è­°è«– 0.07484486699104309æ¤œè¨ 0.007034890353679657
  - Downloads: 11
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯deberta-v2-base-japaneseã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦CommonsenseQA(é¸æŠå¼ã®è³ªå•)ã«ç”¨ã„ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 11
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - swallow-hermes-st-v1ç‰©èªä½œæˆã«å¼·ã‚ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºæ¥ãªã„ã‹ã¨è€ƒãˆã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
  - Downloads: 11
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - t5-base-japanese-web-8k (with Byte-fallback, 8K)Descriptionmegagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
  - Downloads: 11
## Datasets

This list is sorted by downloads as of May 30, 2024.
120 datasets are listed.

- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - Please feel free to open an issue or pull request.
  - Downloads: 34,264
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100: æ—¥æœ¬èªinstructionãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆData Descriptionæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯instruction-tuningã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 5,607
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB:
  - Downloads: 5,525
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLUJapanese Massive Multitask Language Understanding BenchmarkJMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
  - Downloads: 5,211
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
  - Downloads: 3,043
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-jaThis repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 1,874
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
  - Downloads: 1,804
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - databricks-dolly-15k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 1,782
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - AutoWikiQAæ±å·¥å¤§ãŒå…¬é–‹ã—ã¦ã„ã‚‹Swallow-MXã‚’ç”¨ã„ã¦ã€Wikipediaä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ã€Œè³ªå•(query)ã€ã¨ã€Œå›ç­”(answer)ã€ã‚’ç”Ÿæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã¨å›ç­”ã«ã¤ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 1,619
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - range3/wiki40b-jaThis dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
  - Downloads: 1,601
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª ids-cv/wrime ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 1,417
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ï¼Œæ—¢å­˜ç ”ç©¶ [7] ã«å€£ã„ï¼ŒWikipedia2 ã®è¨˜äº‹åã‚’ç­”ãˆã¨ã—ãŸï¼Œæ—¥æœ¬èªã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³ QA ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹.
  - Downloads: 1,295
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Dataset DetailsDataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
  - Downloads: 1,088
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
  - Downloads: 764
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miraclThis dataset represents a conversion of the Japanese (Ja) section from the miracl dataset into the BeIR format, making it compatible for use with mteb.
  - Downloads: 661
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - llm-japanese-datasetLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³(ãƒãƒ£ãƒƒãƒˆ)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸»ã«ï¼Œè‹±èªã§æ§‹ç¯‰ã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãªã©ã«å¯¾ã—ã¦ï¼Œãƒãƒ£ãƒƒãƒˆ(Instruction)å¿œç­”ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦LoRAãªã©ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ï¼
  - Downloads: 655
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - è‡ªå‹•ç”ŸæˆQ&amp;Aç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡CC-BYç³»ã¾ãŸã¯Apatch-2.0ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ”¹å¤‰ã—ã¦ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 563
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - LLM ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆèƒ½åŠ›ã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ HumanEval ã®æ—¥æœ¬èªç¿»è¨³ç‰ˆã§ã™ã€‚
  - Downloads: 546
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
  - Downloads: 546
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µã‚¤ãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 534
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - Githubãƒªãƒã‚¸ãƒˆãƒªstockmarkteam/ner-wikipedia-datasetã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 426
- [llm-jp/oasst1-21k-ja](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja)
  - oasst1-21k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 416
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA : Japanese Question Answering with Retrieval Augmentation - æ¤œç´¢æ‹¡å¼µ(RAG)è©•ä¾¡ã®ãŸã‚ã®æ—¥æœ¬èª Q&amp;A ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜æ€§èƒ½ãª LLM ã®å°é ­ã«ä¼´ã„ã€LLM ã‚’ç”¨ã„ãŸè³ªç–‘å¿œç­”ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 409
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - Update:2023/12/25oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸoasst2-chat-68k-jaã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
  - Downloads: 383
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - OverviewThis dataset provides a convenient and user-friendly format of data from Aozora Bunko (é’ç©ºæ–‡åº«), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.
  - Downloads: 372
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - è©³ç´°ã¯ GitHub ã‚’ã”è¦§ãã ã•ã„ï¼
  - Downloads: 311
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
  - Downloads: 308
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 265
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda - Questions for Japanese modelsRepository:
  - Downloads: 248
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - Japanese stopwords for nagisaThis is a stopword list of frequently used words in the Japanese language, created according to the tokenization rules of the Japanese text analysis library, nagisa.
  - Downloads: 243
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - oasst2-33k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 242
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯kunishouæ°ãŒå…¬é–‹ã—ã¦ã„ã‚‹"databricks-dolly-15k"ã‚’æ—¥æœ¬èªè¨³ã—ãŸkunishou/databricks-dolly-15k-jaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èªå°¾ã‚’ArrowPro-7B-KUJIRAã‚’ç”¨ã„ã¦ã€Œã«ã‚ƒã‚“ï¼
  - Downloads: 232
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - Dataset overviewThis dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).
  - Downloads: 228
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - chatbot-arena-ja-calm2-7b-chatã‹ã‚‰promptãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 228
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - [github].
  - Downloads: 210
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["æ–‡å­—é£ã„ç¨®åˆ¥"] == "æ–°å­—æ–°ä»®å"
  - Downloads: 207
- [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en)
  - The dataset was constructed in 3 steps:selecting business scenes,writing monolingual conversation scenarios according to the selected scenes, andtranslating the scenarios into the other language.
  - Downloads: 202
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - range3/wikipedia-ja-20230101This dataset consists of a parquet file from the wikipedia dataset with only Japanese data extracted.
  - Downloads: 198
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - JSNLI Version 1.1 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆ (train_w_filtering)
  - Downloads: 190
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500Dataset DescriptionJA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
  - Downloads: 172
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - Dataset detailsEach entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
  - Downloads: 170
- [yongtae-jp/orca_dpo_pairs_ja](https://huggingface.co/datasets/yongtae-jp/orca_dpo_pairs_ja)
  - About this datasetThis dataset is a machine translation of the Intel/orca_dpo_pairs dataset with Palm 2 (prompt for translation is pasted below).
  - Downloads: 167
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹2010ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’huggingfaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®ã§ã™ï½¡2009 å¹´åº¦ã«ãŠã‘ã‚‹è‘—ä½œæ¨©æ³•ã®æ”¹æ­£ï¼ˆå¹³æˆ21å¹´é€šå¸¸å›½ä¼šã€€è‘—ä½œæ¨©æ³•æ”¹æ­£ç­‰ã«ã¤ã„ã¦ | æ–‡åŒ–åºï¼‰ã«åŸºã¥ãï¼Œæƒ…å ±è§£æç ”ç©¶ã¸ã®åˆ©ç”¨ã«é™ã£ã¦åˆ©ç”¨å¯èƒ½ã§ã™ï½¡å½¢æ…‹ç´ è§£æã‚’ç”¨ã„ã¦ï½¤è‡ªå‹•ã§å¥ç‚¹ã‚’ã¤ã‘ã¾ã—ãŸï½¡å¤‰æ›ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆå½¢æ…‹ç´ è§£æãªã©
  - Downloads: 154
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial)LLMã®ãŸã‚ã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ å…¬é–‹ãƒšãƒ¼ã‚¸å…¬é–‹ãƒšãƒ¼ã‚¸ã‚ˆã‚Šã€æœ¬ãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ï¼“ï¼å›å¹´æ¬¡å¤§ä¼šã«ãŠã„ã¦ç™ºè¡¨ã‚’è¡Œã„ã¾ã™ã€‚
  - Downloads: 150
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500Evol-Alpaca-gen3-500ã¯ã€Stanford Alpacaã®seed tasksã‚’æ—¥æœ¬èªåŒ–Evol-Instructionã®æ‰‹æ³•mistralai/Mixtral-8x22B-Instruct-v0.1ã§ä½œã£ãŸåˆæˆãƒ‡ãƒ¼ã‚¿(Synthetic data)ã§ã™ã€‚
  - Downloads: 143
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - Wikipediaã‚’ç”¨ã„ãŸæ—¥æœ¬èªã®å›ºæœ‰è¡¨ç¾æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆGitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/LICENSE: CC-BY-SA 3.0Developed by Stockmark Inc.
  - Downloads: 142
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 142
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ãŸã‚‚ã®ã§ã™ã€‚
  - Downloads: 136
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - llm-japanese-dataset-vanillaLLMæ§‹ç¯‰ç”¨ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆizumi-lab/llm-japanese-dataset ã‹ã‚‰ï¼Œæ—¥è‹±ç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç­‰ã‚’æŠœã„ãŸã‚‚ã®ã§ã™ï¼
  - Downloads: 135
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - Dataset PreprocessingSupported Tasks and LeaderboardsLanguagesæ³¨é‡ˆã¯ã™ã¹ã¦æ—¥æœ¬èªã‚’ä¸»è¦è¨€èªã¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 120
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023:
  - Downloads: 118
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - alpaca_jp_pythonalpaca_jp_pythonã¯ã€Stanford Alpacaã®æ‰‹æ³•mistralai/Mixtral-8x22B-Instruct-v0.1ã§ä½œã£ãŸåˆæˆãƒ‡ãƒ¼ã‚¿(Synthetic data)ã§ã™ã€‚
  - Downloads: 116
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - cyberagent/calm2-7b-chatã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã™ã‚‹ã“ã¨ã§ä½œæˆã—ãŸæ—¥æœ¬èªInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 111
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-InstructUpdate:2023/12/27ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« JaxTon , ãƒ—ãƒ­ã«ãªã‚‹Java ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ 180 ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
  - Downloads: 102
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - IntroductionThis is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
  - Downloads: 102
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_jaã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯CohereForAI/aya_datasetã®æ—¥æœ¬èªã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 100
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - en-ja-alignæ—¥è‹±å¯¾è¨³æ–‡å¯¾å¿œä»˜ã‘ãƒ‡ãƒ¼ã‚¿(å†…å±±ã‚‰, 2003)ã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥è‹±å¯¾è¨³æ–‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 99
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - Japanese Wikipedia Human Retrieval datasetThis is a Japanese question answereing dataset with retrieval on Wikipedia articlesby trained human workers.
  - Downloads: 90
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - ja-stackoverflowæ—¥æœ¬èªç‰ˆ Stack Overflow ã® ã‚¹ã‚¿ãƒƒã‚¯ãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ— ã‚’ã‚‚ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€è³ªå•æ–‡ã¨å›ç­”æ–‡ã®ãƒšã‚¢ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ãŸ QA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 87
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_datasetå•†ç”¨åˆ©ç”¨å¯èƒ½ãªè¶…å°è¦æ¨¡é«˜å“è³ªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
  - Downloads: 87
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª singletongue/wikipedia-utils ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 75
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - è‡ªå‹•ç”ŸæˆQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡Common Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚ 
  - Downloads: 73
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦æ‰‹å‹•ã§ä½œæˆã—ãŸDatabricksã«é–¢ã™ã‚‹è³ªå•ã¨å›ç­”ãƒšã‚¢ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 73
- [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus)
  - The corpus has 50,000 manually simplified and aligned sentences.
  - Downloads: 71
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - abc-multiple-choice Datasetabc-multiple-choice ã¯ã€ç«¶æŠ€ã‚¯ã‚¤ã‚ºã®å¤§ä¼šã€Œabcã€ã§ä½¿ç”¨ã•ã‚ŒãŸ4æŠå•é¡Œã‚’å…ƒã«ä½œæˆã•ã‚ŒãŸã€å¤šè‚¢é¸æŠå¼ã®è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 70
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - è‡ªå‹•ç”ŸæˆQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡ãƒãƒ¼ãƒ ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãŠã‚ˆã³ã€ŒCommon Crawlã‚’ã‚‚ã¨ã«ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚ 
  - Downloads: 68
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - è‡ªå‹•ç”Ÿæˆã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ï½¤MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFã‚’ä½¿ã£ã¦Q&amp;Aã‚’è‡ªå‹•ç”Ÿæˆã—ãŸã‚‚ã®ã§ã™ï½¡é–¢é€£ã‚³ãƒ¼ãƒ‰ä¸€éƒ¨ã®è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ã˜ã‚ã®è³ªå•(q1)ã‚’ï½¤ç¨®ã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¾ã—ãŸï½¡ãã®å¾Œã®ã‚„ã‚Šã¨ã‚Šã¯ã™ã¹ã¦ï½¤MixtralãŒç”Ÿæˆã—ã¾ã—ãŸï½¡è³ªå•æ–‡ã«ã¤ã„ã¦ã¯ï½¤å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã—ã¾ã™ï½¡oasst2-33k-jaapache 2.0databricks-dolly-15k-jacc-by-sa-3.0minnadeCC0cyberagent/chatbot-arena-ja-calm2-7b-chat-experimentalcc-by-4.0
  - Downloads: 67
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - Filtered and modified version of Japanese/Chinese language pair data from WikiMatrix v1.Process steps:1.
  - Downloads: 67
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese Anime Speech Datasetæ—¥æœ¬èªã¯ã“ã¡ã‚‰japanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
  - Downloads: 65
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - Wikipediaæ—¥æœ¬èªç‰ˆã‹ã‚‰ã®Q&amp;Aã®è‡ªå‹•ç”ŸæˆMixtral 8x22bã®GGUF(5bit)ã‚’ãƒ™ãƒ¼ã‚¹ã«ï½¤Wikipediaæ—¥æœ¬èªç‰ˆã®è¨˜äº‹ã‹ã‚‰ï½¤è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰1è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰2ã‚’ä½¿ã£ã¦Q&amp;Aã‚’ä½œæˆã—ã¾ã—ãŸï½¡è¨ˆç®—ã«ã¯æ±äº¬å·¥æ¥­å¤§å­¦ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿TSUBAME4.0ã‚’åˆ©ç”¨ã—ã¾ã—ãŸï½¡æ³¨æ„å›ç­”ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç­‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ï½¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‹ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï½¡
  - Downloads: 64
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - Wikidata parallel descriptions en-jaParallel corpus for machine translation generated from wikidata dump (2024-05-06).
  - Downloads: 61
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangentã¯äººæ‰‹ã§ä½œæˆã•ã‚ŒãŸé«˜å“è³ªã§ã‚¯ãƒªãƒ¼ãƒ³ãª100ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªCoTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 57
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
  - Downloads: 54
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - J-ResearchCorpusUpdate:2024/3/16è¨€èªå‡¦ç†å­¦ä¼šç¬¬30å›å¹´æ¬¡å¤§ä¼š(NLP2024)ã‚’å«ã‚€ã€è«–æ–‡ 1,343 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 2024/2/25è¨€èªå‡¦ç†å­¦ä¼šèªŒã€Œè‡ªç„¶è¨€èªå‡¦ç†ã€ã®ã†ã¡ CC-BY-4.0 ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ 360 æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ æ¦‚è¦CC-BY-* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè«–æ–‡ã‚„å­¦ä¼šèªŒç­‰ã‹ã‚‰æŠœç²‹ã—ãŸé«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 50
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - oasst2-135k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 49
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - defamation_japanese_twitterTwitteræ—¥æœ¬èªèª¹è¬—ä¸­å‚·æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDataset SummarySNSã«ãŠã‘ã‚‹èª¹è¬—ä¸­å‚·æ¤œå‡ºã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ï¼
  - Downloads: 49
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR: Japanese Casual Web IR - æ—¥æœ¬èªæƒ…å ±æ¤œç´¢è©•ä¾¡ã®ãŸã‚ã®å°è¦æ¨¡ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªWebã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿‘å¹´ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å°é ­ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚’ç”¨ã„ãŸè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§è³ªå•ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã¦ã„ã¾ã™ã€‚
  - Downloads: 48
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - Lurunchik/WikiHowNFQAã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€äººæ‰‹ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 47
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - OpenMathInstruct-1 ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãª180ä¸‡ä»¶ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 46
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - JaWikiWikipediaã®HTMLå½¢å¼ã®ãƒ€ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 45
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - AbstructThis is a multi-turn conversation dataset generated from the Japanese Wikipedia dataset using Orion14B-Chat.
  - Downloads: 41
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - Dataset used to train PokÃ©mon text to image model, add a Japanese Column of PokÃ©mon BLIP captionsBLIP generated captions for PokÃ©mon images from Few Shot PokÃ©mon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
  - Downloads: 40
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja ã®ä¸­ã‹ã‚‰ JGLUEï¼ˆ JcommonsenseQA , MARC-ja , JSQuAD ï¼‰ã®è¦³ç‚¹ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 40
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-BenchDataset DescriptionJapanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
  - Downloads: 40
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - range3/cc100-jaThis dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
  - Downloads: 38
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - [Under Construction]This is a repository containing all the queries from the Japanese part of the MMarco dataset, the multilingual version of the MSMarco dataset.
  - Downloads: 36
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - Japanese-Vietnamese Translated Sentence Pairs.
  - Downloads: 36
- [Nexdata/multi_language](https://huggingface.co/datasets/Nexdata/multi_language)
  - SummaryThe dataset contains 25,000 hours of multi-language reading speech data.
  - Downloads: 26
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - fungi_trait_circus_databaseå¤§èŒè¼ªã€ŒTrait Circusã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆçµ±åˆ¶å½¢è³ªï¼‰æœ€çµ‚æ›´æ–°æ—¥ï¼š2023/12/29LanguagesJapanese and EnglishPlease do not use this dataset for academic purposes for the time being. 
  - Downloads: 26
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - llm-book/aio-passages ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€llm-book/bert-base-japanese-v3-bpr-passage-encoder ã«ã‚ˆã‚‹ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«ãŒ embeddings ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚
  - Downloads: 25
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - This pre-training dataset was created for shisa-base-7b-v1.It is primarily composed of a DSIR sampling of MADLAD-400 JA/EN tokens in a 90%/10% ratio.
  - Downloads: 25
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - fungi_diagnostic_chars_comparison_japaneseå¤§èŒè¼ªã€Œè­˜åˆ¥å½¢è³ªã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰LanguagesJapaneseThis dataset is available in Japanese only.
  - Downloads: 25
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - fungi_indexed_mycological_papers_japaneseå¤§èŒè¼ªã€Œè«–æ–‡3è¡Œã¾ã¨ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€çµ‚æ›´æ–°æ—¥ï¼š2024/2/23ï¼ˆR3-11457ã¾ã§ï¼‰LanguagesJapaneseThis dataset is available in Japanese only.
  - Downloads: 24
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - Dataset.
  - Downloads: 23
- [RyokoAI/Syosetu711K](https://huggingface.co/datasets/RyokoAI/Syosetu711K)
  - Not all information here may be accurate or accessible.
  - Downloads: 22
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - recruit-jp/japanese-image-classification-evaluation-datasetOverviewDeveloped by: Recruit Co.
  - Downloads: 21
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã®å‡ºåŠ›ã‚’äººæ‰‹ã§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ã—ãŸinstructionã«Swallow-MXã§outputã‚’ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 19
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - mqaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 17
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã¯llm-book/ner-wikipedia-datasetã¨åŒæ§˜ã®ã‚‚ã®ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å…¨éƒ¨ã§8ç¨®é¡ (äººåã€æ³•äººåã€åœ°åã€è£½å“åã€æ”¿æ²»çš„çµ„ç¹”åã€æ–½è¨­åã€ãã®ä»–ã®çµ„ç¹”åã€ã‚¤ãƒ™ãƒ³ãƒˆå)ã‚ã‚Šã¾ã™ã€‚
  - Downloads: 16
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - mmarcoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®query--passageã®ãƒšã‚¢ã«ã¤ã„ã¦ã€queryã‚’keyã¨ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
  - Downloads: 16
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
  - Downloads: 15
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
  - Downloads: 15
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - For the English version, please click here.
  - Downloads: 15
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - mbpp-jaThis repository provides a mbpp dataset translated from English into Japanese by LLM-jp, a collaborative project launched in Japan.
  - Downloads: 14
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - oasst1-89k-jaã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 14
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - Sorry, it's no longer available on Hugging Face.
  - Downloads: 14
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-jaæ¦‚è¦å¤šè¨€èªåŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® ApolloCorpus ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸ 525k ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚
  - Downloads: 14
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMPThis is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
  - Downloads: 13
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - cosmopedia-100k ã®index 20k ï½ 100k ã‚’æ—¥æœ¬èªã«è‡ªå‹•ç¿»è¨³ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ãªã‚Šã¾ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¦ç¿»è¨³ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã¯é™¤å¤–ã—ã¦ã„ã¾ã™ï¼‰ã€‚
  - Downloads: 13
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This is a Japanese portion of the Guanaco dataset.
  - Downloads: 12
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - OverviewThis dataset is of conversations extracted from Aozora Bunko (é’ç©ºæ–‡åº«), which collects public-domain books in Japan, using a simple heuristic approach.
  - Downloads: 12
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - It covers multiple fields such as tourism, medical treatment, daily life, news, etc. 
  - Downloads: 12
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
  - Downloads: 12
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - Japanese LawsThis dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. 
  - Downloads: 12
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - cosmopedia-japanese-20kã®ãƒ‡ãƒ¼ã‚¿ã«ã€kunishouæ§˜ã‹ã‚‰20k-100kã‚’ã”æä¾›ã„ãŸã ã‘ã‚‹ã“ã¨ã«ãªã‚Š100kã¾ã§æ‹¡å¤§ã—ã¾ã—ãŸã€‚
  - Downloads: 12
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - Chatbot Arena Conversationsã®è³ªå•æ–‡ã‹ã‚‰ã€aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2ã‚’ä½¿ç”¨ã—ã¦å¿œç­”æ–‡ã‚’ä½œæˆã—ã¾ã—ãŸè³ªå•æ–‡ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®Promptéƒ¨åˆ†ã‚’ä½¿ç”¨ã—ã¾ã—ãŸChatbot Arena Conversations JA (calm2)ä»¥ä¸‹å¼•ç”¨ã§ã™ã€‚
  - Downloads: 12
- [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese)
  - The annotation is by majority decision by 5 - 10 crowd workers.
  - Downloads: 11
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - CABank Japanese CallHome CorpusParticipants:   120Type of Study:  phone callLocation:   United StatesMedia type: audioDOI:    doi:10.21415/T5H59VWeb: https://ca.talkbank.org/access/CallHome/jpn.htmlCitation informationSome citation here.
  - Downloads: 11
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - GitHub ãƒªãƒã‚¸ãƒˆãƒª cl-tohoku/quiz-datasets ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - Downloads: 11
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - Asian Language Treebank (ALT) ProjectALT
  - Downloads: 11
