# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1274 models and 477 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents üìñ

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).

Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Text Generation](#Text-Generation)
	 * [Multimodality](#Multimodality)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Reasoning](#Reasoning)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## The latest additions üéâ

**Models**
5 models have been added.

- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)


**Datasets**
5 datasets have been added.

- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)


## Models üß†

This list is sorted by downloads as of March 25, 2025.
1274 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - A fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice, CSS10, and JSUT data, sampled at 16kHz.
  - Downloads: 6,045,219
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - A BERT base model for Japanese, pretrained with IPA dictionary-based word-level tokenization and WordPiece subword tokenization, using the architecture from cl-tohoku/bert-japanese.
  - Downloads: 3,443,591
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - A fine-tuned xlm-roberta-base model for Japanese named entity recognition, trained on Stockmark-incÊèê‰æõÁöÑWikipediaÊï∞ÊçÆÈõÜÔºåÁî®‰∫é‰∫∫ÂêçÂíåÁªÑÁªáÂêçËØÜÂà´„ÄÇ
  - Downloads: 637,459
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - A BERT base Japanese model pretrained with whole word masking on CC-100 and jawiki-20230102 datasets, using Unidic-lite tokenization.
  - Downloads: 265,643
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - This repository provides Japanese text embeddings using Sentence Transformers, requiring installation of specific libraries and noting the need to prepend certain prefixes to input text.
  - Downloads: 231,313
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - This repository contains a Japanese CLOOB model for image embedding, trained by rinna Co., Ltd., with installation and usage instructions provided.
  - Downloads: 224,369
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - A BERT base Japanese model pretrained with whole word masking using texts from jawiki-20200831 and character-level tokenization based on Unidic 2.1.2.
  - Downloads: 143,045
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - A DistilBERT model pre-trained on Japanese web text by LINE Corporation, based on their in-house BERT-base model.
  - Downloads: 139,042
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - A DeBERTa V3 model specialized for Japanese that omits morphological analysis during inference and respects word boundaries.
  - Downloads: 133,475
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - A BERT base Japanese model pretrained with IPA dictionary-based tokenization and whole word masking.
  - Downloads: 111,547
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - A BERT base Japanese model pretrained with word-level IPA dictionary tokenization followed by character-level tokenization, based on the cl-tohoku/bert-japanese repository.
  - Downloads: 110,868
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - A BERT base Japanese model pretrained using character-level tokenization and whole word masking on CC-100 and jawiki-20230102 datasets.
  - Downloads: 110,755
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - A Japanese Sentence-BERT model (version 1), with a version 2 that has improved accuracy, along with usage instructions and code details.
  - Downloads: 102,500
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - A refined Japanese Sentence-BERT model (v2) using MultipleNegativesRankingLoss for improved accuracy over its predecessor.
  - Downloads: 81,868
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - A gguf-format conversion of qwen2.5-bakeneko-32b-instruct for use with ggerganov's llama.cpp, created from imatrix-dataset-for-japanese-llm.
  - Downloads: 57,889
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese text embedding model based on LUKE, trained on diverse datasets for general-purpose sentence vector tasks.
  - Downloads: 51,061
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets, usable for masked language modeling.
  - Downloads: 47,025
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - A 36-layer, 2816-hidden-size GPT-NeoX model with 3.6 billion parameters trained on Japanese data for language modeling.
  - Downloads: 34,742
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B-instruct is a Japanese language model using the Mistral architecture that excels in Japanese benchmarks and performs competitively on English tests compared to models like OpenCalm, Elyza, Youri, Nekomata, and Swallow.
  - Downloads: 33,619
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - A BERT base Japanese model pretrained with whole word masking using Unidic-lite and Wikipedia data.
  - Downloads: 27,920
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - A fine-tuned BERT model for Japanese sentiment analysis on Amazon product reviews.
  - Downloads: 22,922
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - A Japanese DeBERTa V2 tiny model pre-trained on specific datasets, available for masked language modeling.
  - Downloads: 22,856
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is an LLaMA-based 13 billion parameter model pre-trained on English and Japanese data, released under Apache v2.0 license for text generation tasks using transformers pipeline.
  - Downloads: 22,175
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - A Japanese DeBERTa V2 large model pre-trained on specific Japanese text corpora and trained with character-level tokenization and whole word masking.
  - Downloads: 21,051
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a Japanese-language fine-tuned model for instruction-following tasks based on PLaMo-13B, released under Apache License 2.0.
  - Downloads: 18,735
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper v2.0 includes distilled Whisper models for Japanese ASR using faster-whisper weights and stable-ts punctuation, developed by Asahi Ushio and Kotoba Technologies.
  - Downloads: 17,061
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow is an enhanced series of large language models (8B, 70B) built via continual pre-training on Meta Llama 3.1, with improved Japanese capabilities while maintaining English proficiency.
  - Downloads: 15,988
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - This repository contains Japanese autoregressive language models trained by SB Intuitions, including sarashina2.2-0.5B-instruct-v0.1, evaluated on multiple tasks and benchmarks.
  - Downloads: 15,814
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is an enhanced 8-billion-parameter Japanese-language model based on Meta-Llama-3, trained for proficient Japanese conversation and instruction following.
  - Downloads: 15,320
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a BERT-based multilingual sentence encoder trained for 109 languages, useful for sentence embeddings and bi-text retrieval.
  - Downloads: 13,600
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository contains various Japanese Reranking models (CrossEncoders) with different model sizes and configurations.
  - Downloads: 13,442
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - A Japanese finetuned model based on DeepSeek-R1-Distill-Qwen-14B for causal language modeling.
  - Downloads: 12,724
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - A gguf-formatted version of the Moonlight-16B-A3B-Instruct model for use with ggerganov's llama.cpp.
  - Downloads: 11,260
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is an enhanced Japanese ASR model with added speaker diarization and punctuation functionalities, supported by Hugging Face Transformers from v4.39.
  - Downloads: 10,825
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository offers an extra-small Japanese GPT-2 model for text generation, accessible via Transformers.
  - Downloads: 10,313
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository offers a base-sized Japanese RoBERTa model, complete with instructions on how to load it using transformers.
  - Downloads: 10,098
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - A gguf-format model derived from cyberagent's DeepSeek-R1 Distill Qwen-14B Japanese model for use with llama.cpp.
  - Downloads: 10,034
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - A pre-trained ELECTRA model on Japanese mC4 data, fine-tuned for UD_Japanese_BCCWJ using spaCy v3, distributed as the ja_ginza_electra Python package.
  - Downloads: 9,660
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - A Japanese DeBERTa V2 large model pre-trained on specific datasets, usable for masked language modeling.
  - Downloads: 8,967
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - The Japanese version of LUKE, a pre-trained knowledge-enhanced contextual representation model, includes Wikipedia entity embeddings not typically used in general NLP tasks.
  - Downloads: 8,503
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - A BERT large Japanese model pretrained using Unidic-lite and WordPiece tokenization with whole word masking on CC-100 and Wikipedia data.
  - Downloads: 8,014
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - A Japanese RoBERTa base model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 7,971
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow, built from Meta Llama 3 with Japanese data, includes pre-trained models like Llama-3-Swallow-8B-v0.1 and Llama-3-Swallow-70B-v0.1, with scheduled releases starting July 1, 2024.
  - Downloads: 7,832
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository offers a medium-sized Japanese GPT-2 model for text generation, including instructions on how to load and use it via transformers.
  - Downloads: 7,664
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri: Japanese general text embeddings require installing Sentence Transformers, fugashi, and unidic-lite to load and run queries and passages with specific prefixes.
  - Downloads: 7,236
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository contains a Japanese sentence-LUKE model trained on the same dataset and configuration as Japanese Sentence-BERT, with higher quantitative accuracy and better qualitative results on private datasets.
  - Downloads: 6,830
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - A gguf-format conversion of the cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese model for Japanese language processing.
  - Downloads: 6,812
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - The ELYZA-japanese-Llama-2-7b model is a Japanese-language extension of Llama-2 through additional pretraining, designed for instructional tasks with a default system prompt in Japanese.
  - Downloads: 6,807
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - A gguf-format conversion of the deepseek-r1-distill-qwen2.5-bakeneko-32b model for use with ggerganov's llama.cpp, including instructions for cloning and running.
  - Downloads: 6,642
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - A Sentence-BERT model based on Luke Japanese Base Lite for mapping sentences and paragraphs into a 768-dimensional space, suitable for clustering and semantic search tasks.
  - Downloads: 6,473
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - A Japanese wav2vec 2.0 Base model trained by rinna Co., Ltd., using 19,000 hours of Japanese speech, with 12 transformer layers and 12 attention heads.
  - Downloads: 6,223
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãVecteus-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 5,901
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - This repository offers a Japanese ModernBERT model (ModernBERT-Ja-130M) with 130 million parameters, trained on a large corpus for efficient handling of long sequences.
  - Downloads: 5,896
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - A gguf-formatted version of the lightblue DeepSeek-R1-Distill-Qwen-7B-Japanese model for use with ggerganov's llama.cpp, created from imatrix-dataset-for-japanese-llm.
  - Downloads: 5,731
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow is a series of enhanced large language models that incorporate Japanese language capabilities through continual pre-training on Meta Llama 3.1 with a focus on diverse Japanese content sources.
  - Downloads: 5,298
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets, trained with character-level tokenization, for masked language modeling.
  - Downloads: 5,089
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository contains series of Japanese Rerankers (CrossEncoders) including details like model layers and hidden size, ranging from small to large models.
  - Downloads: 4,856
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - The repository contains continual pre-training code for a 32-layer, 4096-hidden-size Transformer model (llama2-7B) on Japanese and English datasets, improving performance on Japanese tasks.
  - Downloads: 4,662
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - A Japanese HuBERT Large model with 24 transformer layers trained on about 19,000 hours of Japanese speech corpus.
  - Downloads: 4,388
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository offers a small Japanese GPT-2 model for text generation, obtainable via Hugging Face Transformers.
  - Downloads: 4,240
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - The repository provides Japanese text embeddings using Sentence Transformers, requiring installation of specific libraries and including model usage examples.
  - Downloads: 4,126
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - A Japanese fine-tuned model based on DeepSeek-R1-Distill-Qwen-32B for causal language tasks.
  - Downloads: 4,119
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - A Japanese HuBERT Base model with 12 transformer layers, trained on about 19,000 hours of Japanese speech from the Reazon corpus.
  - Downloads: 4,119
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - AXCXEPT-phi-4-deepseek-R1K-RL-EZOÁöÑggufÊ†ºÂºèÊ®°ÂûãÔºåÂèØÈÄöËøállama.cppÂÖãÈöÜÂíåËøêË°å‰ª•Êó•ËØ≠ÁÉπÈ•™‰∏ªÈ¢òËøõË°å‰∫§‰∫í„ÄÇ
  - Downloads: 4,108
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - A gguf-formatted distillation of the r1-1776 Distill LLaMA-70B model, compatible with ggerganov's llama.cpp for Japanese language processing.
  - Downloads: 4,054
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - A gguf-formatted mini version of RakutenAI-2.0 instruct model, derived from mmnga/RakutenAI-2.0-mini-instruct and usable with ggerganov's llama.cpp.
  - Downloads: 3,916
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - This repository includes autoregressive Japanese language models trained by SB Intuitions, evaluated on multiple tasks.
  - Downloads: 3,846
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository contains Japanese Reranking models (CrossEncoders) with varying layers and hidden sizes, including versions up to large and BGE variant.
  - Downloads: 3,756
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - A gguf-formatted version of RakutenAI-2.0-8x7B-instruct, built from mmnga/RakutenAI-2.0-8x7B-instruct-gguf, for use with llama.cpp.
  - Downloads: 3,626
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese-fine-tuned Llama-2 7B model for causal language tasks, documented in a blog post.
  - Downloads: 3,576
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - A gguf-format conversion of the Fugaku-LLM-13B-instruct model with instructions for usage and a clone link provided.
  - Downloads: 3,456
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository offers various large language models, including llm-jp-3-1.8b, llm-jp-3-3.7b, llm-jp-3-13b, and llm-jp-3-172b-beta1, with instruction variants, in Hugging Face Transformers format.
  - Downloads: 3,238
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - This repository offers ModernBERT-Ja-310M, ahighly efficient BERT variant for Japanese text processing with 310 million parameters, trained on a large corpus of text.
  - Downloads: 3,217
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - A 7B-parameter Japanese language model designed for Japanese language and downstream task performance, derived from continued pretraining on an English Mistral-7B model.
  - Downloads: 3,212
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository includes various large language model variants developed by the National Institute of Informatics, including llm-jp-3-172b-instruct3.
  - Downloads: 3,208
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - The ELYZA-japanese-Llama-2-7b model is an enhanced Japanese-language Llama-2 7B model with additional pre-training, using a default system prompt for instruct-style text generation.
  - Downloads: 3,160
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - A fine-tuned BERT model for Japanese semantic similarity computation using the cl-tohoku/bert-base-japanese-v3 and JGLUE's JSTS dataset.
  - Downloads: 3,121
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - ELYZA's Japanese Llama-2 7B Fast Instruct model in gguf format, with enhanced vocabulary for faster processing.
  - Downloads: 3,034
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - A quantized 32B instruction-tuned model for Qwen2.5 Bakeneko, compatible with llama.cpp and available in AWQ, GGUF, GPTQ int8, and GPTQ int4 formats.
  - Downloads: 3,003
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b-fast-instruct is a fine-tuned Japanese-language model based on Llama 2, designed for instruction-following tasks with specific system and instruction templates.
  - Downloads: 2,929
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - A BERT base Japanese model pretrained with character tokenization and whole word masking.
  - Downloads: 2,893
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This GitHub repository contains a Japanese Sentence-LUKE model trained on the same dataset and settings as the Japanese Sentence-BERT model, showing higher accuracy in certain aspects with pre-trained studio-ousia/luke-japanese-base-lite.
  - Downloads: 2,866
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese text embedding model based on RoFormer and Distillation, excelling in semantic similarity measurement and retrieval tasks with a maximum sequence length of 1024 tokens.
  - Downloads: 2,831
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a Japanese text embedding model trained with multi-stage contrastive learning, mapping texts to 1792-dimensional vectors for use in semantic tasks.
  - Downloads: 2,825
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - A Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0 for masked language modeling.
  - Downloads: 2,808
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese-enhanced Llama 2 model for causal language tasks, with additional pretraining and a default system prompt.
  - Downloads: 2,786
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow is a series of large language models enhanced for Japanese while maintaining English capabilities, trained on diverse datasets including a Japanese web corpus, Wikipedia, and mathematical/coding content.
  - Downloads: 2,782
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - The ELYZA-japanese-Llama-2-7b model is an extended Japanese-language version of Llama-2 through additional pre-training, with usage demonstrated via Python code for text generation.
  - Downloads: 2,779
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - This repository offers a 1.3 billion-parameter Japanese GPT model from rinna Co., Ltd., with instructions for model installation and usage.
  - Downloads: 2,763
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - A gguf converted version of the 1.7B Japanese large language model for instruction fine-tuning from Line Corporation.
  - Downloads: 2,747
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter decoder-only language model fine-tuned on diverse Japanese data for high performance in Japanese language tasks.
  - Downloads: 2,704
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - A 70B-parameter Japanese language model fine-tuned on various datasets, available in smaller configurations for different use cases.
  - Downloads: 2,694
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - A 7B-parameter Japanese instruction-following language model fine-tuned on StabilityAI's Japanese Stable LM Base Gamma 7B.
  - Downloads: 2,620
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository contains a gguf conversion of the 1.7B Japanese language model by line-corporation, along with conversion scripts and usage instructions.
  - Downloads: 2,616
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-Chat is a decoder-only language model pre-trained on 1.3T tokens, requiring transformers >= 4.34.1 for usage.
  - Downloads: 2,534
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - ELYZA Japanese Llama 2 7B Fast GGUF format model, a fast version of the ELYZA Japanese language model with reduced token cost and enhanced speed.
  - Downloads: 2,468
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - The GitHub repository provides the llm-jp-3-13b-instruct3 model, part of a series developed by the National Institute of Informatics for large language models.
  - Downloads: 2,468
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese-enhanced Llama 2 model using additional pre-training, with detailed usage and import instructions provided.
  - Downloads: 2,417
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - AIBunCho's gguf conversion of the Japanese novel GPT-J-6B model for use with llama.cpp, intended for testing purposes.
  - Downloads: 2,404
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - An experimental Vision-Language model in Japanese capable of generatinganswers to image-related questions using Llava-CALM2-SigLip.
  - Downloads: 2,402
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow is a series of large language models enhanced for Japanese while retaining English capabilities, built through continual pre-training on Meta Llama 3.1 with a corpus of 200 billion tokens.
  - Downloads: 2,340
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository contains a fine-tuned Japanese version of DeepSeek-R1-Distill-Qwen-14B that outputs thought processes in Japanese.
  - Downloads: 2,277
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b, a 13B-parameter LLM pretrained on a Japanese corpus, is accompanied by an instruction-tuned variant and supported by AWS; provide usage instructions via PyTorch.
  - Downloads: 2,246
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository offers various large language models in Japanese, including different variants from 1.8B to 172B parameters, with instruction-tuned versions available.
  - Downloads: 2,212
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - Weighted and IQ quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model are available as GGUF files, with usage details provided in another README.
  - Downloads: 2,197
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - Llama-3-ELYZA-JP-8B-GGUF is an enhanced Japanese-language large language model based on Meta-Llama-3, quantized for efficiency and available in GGUF format.
  - Downloads: 2,164
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b-fast is a Japanese-enhanced Llama 2 model for causal language modeling, detailed in a blog post.
  - Downloads: 2,157
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - A gguf-formatted conversion of the Japanese-stablelm-2-instruct-1_6b model by stabilityai, using imatrix-dataset-for-japanese-llm, with associated licensing and commercial use requirements.
  - Downloads: 2,017
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - A quantized Japanese Llama 2-7b instruction-tuned model with reduced size and improved speed, but slightly diminished performance.
  - Downloads: 2,016
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a pre-trained Japanese decoder-only language model from CyberAgent, Inc., enabling AI text generation with specific import and usage instructions provided.
  - Downloads: 2,014
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - A Japanese continually pre-trained model based on Mistral-Nemo, requiring an updated transformers installation and using AutoModelForCausalLM and AutoTokenizer for inference.
  - Downloads: 1,994
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - Matsuo-lab's Japanese-centric multilingual weblab-10b Instruction Fine-Tuned (INinstruction-sft) GPT-NeoX model quantized to 6.3 GB for improved speed at slightly reduced inference performance.
  - Downloads: 1,978
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - The repository offers ModernBERT-Ja-30M, a highly efficient BERT variant trained on large datasets for handling long sequences in Japanese text.
  - Downloads: 1,953
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository contains enhanced descriptions for the Japanese StableLM-3b-4e1t-base GGUF model by Stability AI, noting current limitations in layer support for GPU offloading.
  - Downloads: 1,915
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository offers a 3.6B parameter Japanese language model from LINE Corporation, along with instructions for usage and setup.
  - Downloads: 1,852
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a Japanese language large language model that excels in Japanese benchmarks and performs competitively in English tests, built on the Mistral-7B-v0.1 architecture.
  - Downloads: 1,770
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - A RoBERTa model pre-trained on Japanese Aozora texts for UPOS tagging, derived from roberta-small-japanese-aozora.
  - Downloads: 1,765
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - The repository contains the llm-jp-3-13b-instruct2 model, part of a series of large language models developed by the National Institute of Informatics.
  - Downloads: 1,715
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - A GitHub repository containing the DeepSeek-R1 distillation model for Qwen-14B Japanese-language fine-tuning, licensed under MIT.
  - Downloads: 1,696
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA's Llama-3-ELYZA-JP-8B-AWQ is an 8-billion-parameter language model fine-tuned for Japanese, built on Meta-Llama-3, with AutoAWQ quantized versions available.
  - Downloads: 1,637
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - A 3-billion-parameter decoder-only language model optimized for Japanese, derived from StableLM-3B-4E1T through continued pretraining on Japanese data.
  - Downloads: 1,575
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - A fine-tuned Japanese instruction-tuned reasoning model based on Qwen2.5 Bakeneko 32B, optimized with ORPO for superior performance in Japanese language tasks.
  - Downloads: 1,566
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - The repository offers a 2.7B-parameter Japanese GPT-NeoX model for text generation, compatible with transformers v4.23+, trained by ABEJA.
  - Downloads: 1,551
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese text reranking model using Sentence Transformers that can be loaded and used for inference after installing the necessary library.
  - Downloads: 1,520
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - The repository contains a model resulting from merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b, requiring specific loading and saving steps.
  - Downloads: 1,516
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repo offers model weights for the hubert-base, an audio embedding model trained on JTubeSpeech, suitable for speech recognition tasks and not for audio generation.
  - Downloads: 1,487
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - A gguf-formatted version of the Llama-3.1-Swallow-8B-Instruct-v0.3 model by tokyotech-llm, created from imatrix-dataset-for-japanese-llm, ready for use with llama.cpp.
  - Downloads: 1,475
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - A 36-layer, 2816-hidden-size Japanese GPT-NeoX model fine-tuned for instruction-following conversational tasks using translated Anthropic, HH, RLHF, and FLAN datasets.
  - Downloads: 1,472
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - The GitHub repository offers the llm-jp-3-1.8b-instruct3 model, part of a series developed by the National Institute of Informatics for large language processing.
  - Downloads: 1,469
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - ELYZA's Japanese CodeLlama-7b instruct model in GGUF format.
  - Downloads: 1,448
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - ELYZA's Japanese-trained Llama-2-7b instruct model in GGUF format, including fast and Codellama versions.
  - Downloads: 1,440
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing Japanese language model based on Mistral architecture that excels in Japanese benchmarks and maintains competitive performance in English tests.
  - Downloads: 1,432
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese general reranking model using Sentence Transformers that can be loaded and used for inference after installing the necessary library.
  - Downloads: 1,417
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is aContinuously pre-trained model from the Llama 3 family, including Japanese data, with Instruct versions released in July 2024.
  - Downloads: 1,411
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - The repository contains a Japanese typo detection model using RoBERTa that outputs the probability of each character being misspelled, categorizing errors into 8 types plus "OK".
  - Downloads: 1,406
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - A fine-tuned BERT model for sentiment analysis in Japanese, based on cl-tohoku/bert-base-japanese-v3 and the JGLUE MARC-ja dataset.
  - Downloads: 1,402
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7-billion-parameter language model pre-trained for Japanese language and task performance, with instructions available under different terms in Japanese-StableLM-Instruct-Alpha-7B.
  - Downloads: 1,394
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - This repository provides Japanese text embeddings using the Ruri large model, requiring installation of Sentence Transformers and related libraries before inference can be performed.
  - Downloads: 1,361
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - Abeja/Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused post-training continual learning model based on Qwen2.5-32B-Instruct, with enhanced instruction-following performance through ChatVector tuning.
  - Downloads: 1,361
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - ELYZA's Japanese Llama-2 13B Fast Instruct model in GGUF format.
  - Downloads: 1,346
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - A gguf-formatted version of llm-jp-3-13b-instruct3, created by llm-jp, using imatrix-dataset-for-japanese-llm, with limitations noted for custom chat templates.
  - Downloads: 1,314
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - A quantized version of a Japanese-trained Mistral model created using llama.cpp for inference.
  - Downloads: 1,276
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - AX CXEPT's phi-4-open-R1-Distill-EZOv1 model in gguf format for use with imatrix dataset and llama.cpp.
  - Downloads: 1,248
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight transformer-based Japanese language model for efficient performance in resource-constrained environments, serving as the backbone for instruct models.
  - Downloads: 1,239
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8 billion parameter large-language model fine-tuned for dialogue using SFT and DPO, with options for 4-bit and 8-bit quantization.
  - Downloads: 1,171
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - The repository offers the llm-jp-3-7.2b-instruct3 model, part of a series of large language models developed by the National Institute of Informatics for Japanese instruction-tuned applications.
  - Downloads: 1,170
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - The repository contains a GGUF-format version of the ELYZA-japanese-CodeLlama-7b-instruct model, alongside other variations and versions.
  - Downloads: 1,087
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - The GitHub repository contains the continually pretrained 8B Llama 3 model (Youko) fine-tuned on 22B tokens from Japanese and English datasets, enhancing performance on Japanese tasks.
  - Downloads: 1,072
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1B English-Japanese pre-trained model using the hybrid Samba architecture with added normalization layers.
  - Downloads: 1,051
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - A teacher-free SimCSE model for Japanese text, finetuned on jawiki-sentences using bert-base-japanese-v3 from cl-tohoku.
  - Downloads: 1,045
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - A fine-tuned Japanese GPT-NeoX 3.6B parameter model designed for instruction-following conversations, distinct from the previous version through a different data split.
  - Downloads: 1,043
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - A 36-layer, 2816-hidden-size Japanese GPT-NeoX model fine-tuned for instruction-following through RLHF.
  - Downloads: 1,035
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository includes enhanced descriptions for the Japanese-stablelm-3b-4e1t-instruct GGUF model from Stability AI, noting limitations in current Llama.cpp GPU support.
  - Downloads: 1,007
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - The luke-japanese-large-lite repository contains a lightweight Japanese version of LUKE, a pre-trained language model, without Wikipedia entity embeddings.
  - Downloads: 996
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - The Stockmark-2-100B-Instruct-beta is a 100-billion-parameter LLM pre-trained on diverse text and code, with post-training in Japanese, focusing on instruction-following capabilities.
  - Downloads: 996
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - A gguf-formatted version of the gpt-neox-japanese-1.4b model for use with llama.cpp, to be used on the mmnga-dev branch until GPT-NeoX support is implemented in the main repository.
  - Downloads: 990
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B parameter LLM enhanced for Japanese while maintaining English capabilities through continual pre-training on diverse datasets.
  - Downloads: 976
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - A Japanese SimCSE model based on BERT-base and trained on JSNLI, usable withsentence-transformers for extracting sentence embeddings, requiring fugashi and unidic-lite for tokenization.
  - Downloads: 974
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a fine-tuned Japanese LLM designed for advanced instruction-following tasks with enhanced fluency and coherence.
  - Downloads: 957
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - The GitHub repository contains the continually pre-trained Qwen2.5 Bakeneko 32B model, enhanced for Japanese tasks using 18B tokens from mixed Japanese and English datasets.
  - Downloads: 920
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8b-Cosmopedia-japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 919
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - A Japanese-supportive, quantized gguf version of gemma-2-2b-it with speculative decoding for faster execution.
  - Downloads: 895
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository includes various large language models developed by the National Institute of Informatics, including variants ranging from 1.8B to 172B parameters, along with their necessary Hugging Face library versions.
  - Downloads: 881
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - A T5 v1.1 model card for a Japanese-pretrained Transformer Encoder-Decoder model with GEGLU activation and no embedding-classifier parameter sharing, now available in xl and xxl sizes.
  - Downloads: 861
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Japanese-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 861
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - A gguf-formatted Distilled DeepSeek-R1 model based on Qwen-32B for use with ggerganov's llama.cpp, including instructions for loading and prompting the model.
  - Downloads: 851
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository includes quantized GGUF format model files for Stability AI's Japanese StableLM Instruct Gamma 7B, supported by an a16z grant and mass-produced with hardware from Massed Compute.
  - Downloads: 843
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - This repository offers a 1.7B parameter Japanese language model trained by LINE Corporation, along with instructions for importing and using it via PyTorch.
  - Downloads: 839
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository offers a small Japanese GPT-NeoX model, updated for loading via Hugging Face's implementation.
  - Downloads: 838
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - ELYZA's Japanese Llama-2-7b model in gguf format, available as the usual and fast versions, and corresponding Codellama versions with GPTQ calibration.
  - Downloads: 832
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplpl„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Instruct-Ja„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 823
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - A gguf-format conversion of rinna's qwq-bakeneko-32b model for use with ggerganov's llama.cpp.
  - Downloads: 809
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository includes quantized GGUF format model files for Stability AI's Japanese StableLM Instruct Beta 7B, supported by an a16z grant and Massed Compute hardware.
  - Downloads: 801
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - This repository offers a large Japanese GPT-2 model trained by ABEJA, Inc., requiring installation of sentencepiece for text generation via the Hugging Face transformers library.
  - Downloads: 795
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - ELYZA's Japanese Llama-2 13B Fast gguf model, with enhanced Japanese vocabulary for faster processing.
  - Downloads: 776
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository includes quantized GGUF format model files for the Japanese StableLM Instruct Beta 70B created by Stability AI, supported by a16z and Massed Compute.
  - Downloads: 774
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - A 3.6B parameter Japanese language model fine-tuned by LINE Corporation for instruction tuning, including details and usage instructions.
  - Downloads: 771
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - A gguf format distillation of the DeepSeek-R1 Qwen-14B model for use with ggerganov's llama.cpp, built from imatrix dataset.
  - Downloads: 768
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - The GitHub repository rinna/nekomata-14b contains continual pre-trained qwen-14b on 66B tokens, enhancing performance on Japanese tasks with an inclusive vocabulary larger than 150k.
  - Downloads: 737
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - A high-performance Japanese SPLADE v2 model for sparse vector transformation from text, accessible via a Web UI and implemented with YASEM for inference and token inspection.
  - Downloads: 725
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository provides the 3.7B parameter llm-jp-3 instruct3 model for large language processing, with guidance on model overviews and required library versions.
  - Downloads: 718
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - A gguf-formatted distillation of DeepSeek-R1 Qwen-7B model for use with ggerganov's llama.cpp framework.
  - Downloads: 694
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - A gguf-formatted conversion of karakuri-lm-32b-thinking-2501-exp for use with llama.cpp.
  - Downloads: 686
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - japanese-stablelm-instruct-beta-7b is a fine-tuned 7B-parameter language model based on Japanese-StableLM-base-beta-7b, offering calligraphy-style writing capabilities and available in larger or faster versions.
  - Downloads: 673
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - This GitHub repository provides Japanese text embeddings using the Ruri model, which requires installing Sentence Transformers and specific libraries, and includes instructions for loading and running inference.
  - Downloads: 666
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - Luke-japanese is a lightweight Japanese-language version of LUKE, a pre-trained language model that provides knowledge-enhanced contextual representations of words and entities.
  - Downloads: 662
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - A fine-tuned 7B-parameter Japanese language model specifically enhanced for Japanese vocabulary through additional training data.
  - Downloads: 659
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - A BERT large Japanese model pretrained using Unidic-lite and whole word masking on Wikipedia data.
  - Downloads: 637
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - KARAKURI LM is a Japanese-enhanced pretrained language model based on Llama 2, further fine-tuned using SteerLM and continual learning techniques.
  - Downloads: 637
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - A BERT small model pretrained on Japanese Wikipedia texts.
  - Downloads: 630
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - This repository offers a 1.7B parameter Japanese language model fine-tuned by LINE Corporation for instruction tuning, along with instructions and usage examples.
  - Downloads: 628
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is an instruction-tuned 13 billion parameter Japanese LLM developed by Stockmark Inc., using 2023/11/03 Project of Development of Japanese Instruction data for training.
  - Downloads: 628
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - A 3.8B-parameter GPT-NeoX bilingual English-Japanese model fine-tuned via RLHF for instruction-following conversational tasks.
  - Downloads: 614
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a pre-trained Japanese Transformer Encoder based on Megatron-LM with PreNorm and recent bug fixes.
  - Downloads: 613
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - Quantized versions of Google's gemma-2-2b-jpn-it model in gguf format with instructions for use and conversion.
  - Downloads: 604
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - KARAKURI LM is a Japanese-enhanced pretrained language model based on Llama 2, further fine-tuned with SteerLM for continuous learning.
  - Downloads: 595
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - A 7B-parameter Japanese language model fine-tuned for‰∏ãÊ∏∏‰ªªÂä°ÔºåÂü∫‰∫éLlama-2-7b„ÄÇ
  - Downloads: 578
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - A DeBERTa(V2) model pretrained on Aozora Bunko for dependency parsing and question-answering, using [MASK] to handle ambiguous words.
  - Downloads: 578
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - The repository offers instruction models for large language models developed by LLM-jp, including variants like dpo-lora, instruct-full, and jaster versions in both Japanese and English.
  - Downloads: 577
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - A series of Japanese rerankers (CrossEncoders) with varying model sizes, including base and large models.
  - Downloads: 574
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - The repository offers Asagi-14B, a large Japanese VLM trained on diverse datasets including synthesized data from CALM3 and Phi3.5, without restrictions on output usage.
  - Downloads: 567
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - A Japanese RoBERTa base model pretrained on Wikipedia and CC-100, suitable for masked language modeling tasks.
  - Downloads: 565
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - A GitHub repository containing the DeepSeek-R1 Distillation model for Qwen-32B in Japanese, licensed under MIT.
  - Downloads: 561
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - A 3-billion parameter Japanese instruction-following model fine-tuned on decoder-only architecture, based on Japanese StableLM-3B-4E1T Base.
  - Downloads: 555
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - This repository offers a 1.4B parameter GPT-NeoX model pre-trained on Japanese text for use with PyTorch.
  - Downloads: 554
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - japanese-stablelm-instruct-alpha-7b-v2 is a 7B-parameter language model fine-tuned for Japanese instruction following, built on top of the Japanese-StableLM-Base-Alpha-7B.
  - Downloads: 553
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - The GitHub repository rinna/nekomata-7b features continual pre-training of qwen-7b on 30B tokens from mixed Japanese and English datasets, enhancing performance on Japanese tasks with an inclusive vocabulary.
  - Downloads: 549
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - A gguf-formatted version of the Stockmark-2-100B-Instruct-beta model for use with the llama.cpp framework.
  - Downloads: 539
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - A Japanese DeBERTa V2 tiny model pre-trained on specific datasets, using character-level tokenization and whole word masking.
  - Downloads: 533
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - A pre-trained Japanese ALBERT base model for fine-tuning on various tasks, using Sentencepiece Tokenizer.
  - Downloads: 527
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - A 7B-parameter Japanese-language model fine-tuned for downstream tasks, using an expanded Japanese vocabulary.
  - Downloads: 522
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - This repository offers a 70M-parameter ModernBERT model for Japanese, trained by SB Intuitions on a large corpus using RoPE and other modern architectures.
  - Downloads: 520
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - A gguf-formatted distillation model of DeepSeek-R1-Distill-Qwen-1.5B for use with ggerganov's llama.cpp, trained on imatrix-dataset-for-japanese-llm.
  - Downloads: 510
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - A fine-tuned BERT model for Japanese natural language inference based on cl-tohoku/bert-base-japanese-v3 and Colab notebooks for training and inference.
  - Downloads: 499
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is an 47B parameter, 8x8B active model fine-tuned for dialogue using SFT and DPO, with quantumized versions available.
  - Downloads: 482
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a noncommercial, instruct-fine-tuned model for Japanese text generation based on PLaMo-13B, licensed under CC-BY-NC-4.0.
  - Downloads: 482
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - A T5 v1.1 model card featuring a Japanese-pretrained Transformer Encoder-Decoder with GEGLU activation, no pre-training dropout, and new size labels "xl" and "xxl".
  - Downloads: 481
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - This repository offers a Japanese large vision language model (VILA 14B) developed by the National Institute of Informatics, including setup instructions for Python 3.10.12 and necessary library installation.
  - Downloads: 480
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - A quantized Japanese instruction-tuning model based on Qwen2.5 Bakeneko 32B, compatible with llama.cpp apps and using GPTQ integer compression techniques.
  - Downloads: 461
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - A quantized GGUF version of Aratako/calm3-22b-RP-v2, licensed under CC-BY-NC-SA 4.0 due to the inclusion of OpenAI's GPT-4o-mini and Anthropic's Claude 3.5 Sonnet outputs.
  - Downloads: 454
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - A Japanese RoBERTa base model pre-trained on medical science articles, released under CC BY-NC-SA 4.0.
  - Downloads: 427
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - A GGUF conversion of a 7B-parameter Japanese chat model derived from "Starling-LM-7B-beta" and chatntq-ja-7b-v1.0, using weights obtained by subtracting Mistral-7B-v0.1.
  - Downloads: 425
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository contains a pre-trained Sentence BERT base model for Japanese using the colorfulscoop/bert-base-ja and Japanese SNLI dataset.
  - Downloads: 416
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - The megagonlabs/t5-base-japanese-web is a T5 model pre-trained on 32K vocab sized Japanese web texts and Wikipedia, offering byte-fallback support.
  - Downloads: 404
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - A T5-base model fine-tuned on the large-scale LiveJournal news corpus for text summarization as described in Chapter 7 of "Large Language Model Primer."
  - Downloads: 381
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - Finetuned Waseda RoBERTa for evaluating generated answers on JTruthfulQA.
  - Downloads: 379
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - AXXCXEPT-EZO-phi-4-v2_900 gguf model for Japanese LLM, converted by AXCXEPT and using imatrix dataset.
  - Downloads: 375
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository contains a gguf-formatted version of the open-calm-7b model by cyberagent, along with build instructions and usage examples.
  - Downloads: 372
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - The repository contains series of Japanese Reranker (CrossEncoder) models with varying layers and hidden sizes, including the highest performing Japanese Reranker.
  - Downloads: 369
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - A model fine-tuned from Llama-3-ELYZA-JP-8B using UnSloth and TRL, licensed under Apache-2.0.
  - Downloads: 364
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - VecTeus-v1.0 is a novel Mistral-7B-based LLM fine-tuned for 128k context window, supporting high-quality Japanese and English generation, NSFW content, and memory retention.
  - Downloads: 361
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - A DeBERTa V2 base model pretrained on Japanese text, available for masked language modeling with provided code and instructions.
  - Downloads: 354
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - A gguf format distillation of the DeepSeek-R1 Llama-8B model by deepseek-ai, utilizing imatrix-dataset-for-japanese-llm, for use with ggerganov's llama.cpp.
  - Downloads: 352
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - This repository offers Asagi-8B, a large Japanese vision-and-language model trained on diverse datasets including synthesized content from other models.
  - Downloads: 346
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - A BERT base Japanese finance model pretrained on general Japanese texts and additional financial data from Tohoku University.
  - Downloads: 339
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - A cross-encoder for Japanese natural language inference trained on JSNLI data and based on BERT.
  - Downloads: 339
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 338
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - A Japanese-specialized DeBERTa V3 model that omits morphological analysis during inference and respects word boundaries.
  - Downloads: 337
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - A Japanese continually pre-trained 70B-parameter LLM based on Meta-Llama-3.1, with instructions in Japanese, using the transformers library for deployment.
  - Downloads: 337
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository contains quantized GGUF format model files for Stability AI's Japanese StableLM Base Beta 70B, supported by a16z and Massed Compute.
  - Downloads: 336
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - LUKE Japanese is a pre-trained language model that enhances contextual representation with Wikipedia entity embeddings for natural language processing tasks.
  - Downloads: 327
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri is a Japanese text embedding model that requires installing Sentence Transformers and can be used after loading the model with specific prefixes for queries and passages.
  - Downloads: 322
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100 with a 512 sequence length, usable for masked language modeling.
  - Downloads: 316
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This GitHub repository contains an XLNet-japanese model that requires Mecab and sentencepiece with XLNetTokenizer, using NFKD normalization and omits Japanese muddles and semi-muddles, accessible via a custom XLNet class.
  - Downloads: 310
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - rinna/japanese-gpt-neox-3.6b provides a gguf conversion of the Japanese GPT-NeoX model, with related models available, and instructions for usage.
  - Downloads: 307
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M-parameter text embedding model trained by Retrieva, Inc., primarily for Japanese with English support, based on the modernbert-ja-310m architecture.
  - Downloads: 306
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - A pre-trained Japanese BART large model for natural language processing tasks, available for use via the transformers library.
  - Downloads: 286
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - A pre-trained Japanese BART base model for natural language processing tasks, accessible via Transformer libraries.
  - Downloads: 273
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer encoder-decoder model with GEGLU activation, no pre-training dropout, and size variants "xl" and "xxl".
  - Downloads: 271
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - A gguf-formatted version of the open-calm-3b model for use with llama.cpp, available for testing.
  - Downloads: 269
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - The repository contains the ELYZA Japanese Llama 2 13B fast instruct model configured to run with LlamaEdge v0.2.8+, using a specific prompt template and context size of 5120.
  - Downloads: 256
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese-pretrained model derived from CodeLlama, supporting causal language modeling tasks with an additional instruction-tuning stage.
  - Downloads: 256
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - Quantized versions of rinna's gemma-2-baku-2b-it model forgemma-2-2b-jpn-it-gguf, usable with tools like llama.cpp and LM Studio. Conversion instructions provided.
  - Downloads: 252
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - A T5 v1.1 model card for a Japanese corpus-trained Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and new size variants "xl" and "xxl".
  - Downloads: 251
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - A Japanese-trained LLaMA2 model (417.12M parameters) using a specific script and tokenizer from the given GitHub link.
  - Downloads: 242
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - A Japanese-trained Llama2 model of size 130.78M, using scripts from Lightning-AI/lit-gpt for tokenization and inference.
  - Downloads: 238
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides quantized GGUF format model files for cyberagent/Mistral-Nemo-Japanese-Instruct-2408, compatible with llama.cpp, and includes instructions for running on TensorBlock.
  - Downloads: 233
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - A GPT-2 small model trained on a subset of Japanese Wikipedia as of Aug20, 2021.
  - Downloads: 227
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a vision-language model for conversing about images, trained with the heron library and available via provided code and installation guide.
  - Downloads: 226
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository contains the gguf conversion of the Japanese-large-LM-3.6b-instruction-sft model by Line Corporation, along with related models and usage instructions.
  - Downloads: 225
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This model is trained to perform Extractive QA from context and refine answers in new contexts using gpt-index v0.2.5, focused on two prompt templates and intended for use with the provided examples.
  - Downloads: 217
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository offers quantized GGUF versions of the CyberAgent/DeepSeek-R1-Distill-Qwen-14B-Japanese model for different VRAM capacities.
  - Downloads: 214
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - This repository contains an evolutionary merge of four strong Japanese language models: Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004-7b.
  - Downloads: 212
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - This repository merges pre-trained language models to create a Japanese-language instruct model named Llama-3.3-SuperSwallow-70B-Instruct-v0.1, incorporating Gemini 2.0 for enhanced functionality.
  - Downloads: 211
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 210
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - A RoBERTa base Japanese model fine-tuned on JaQuAD for Japanese question answering tasks.
  - Downloads: 209
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - The repository contains the llm-jp-3-150m-instruct3 model from the National Institute of Informatics' R&D Center for Large Language Models, with requirements for specific versions of PyTorch and Transformers.
  - Downloads: 200
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - Sakuramoto has created a pre-trained Japanese MobileBERT model for fast BERT inference, adaptable with existing transformers libraries.
  - Downloads: 199
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - A gguf conversion of the Japanese large language model line-corporation/japanese-large-lm-3.6b for use with llama.cpp.
  - Downloads: 198
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - A vision-language model named Heron GIT Japanese StableLM Base 7B that can converse about input images, using the heron library for training and inference.
  - Downloads: 197
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - Quantized versions of LLM-jp-3-1.8b-instruct models for use with llama.cpp LM Studio (Windows/Mac) and LLMFarm (iOS), including conversion instructions from npaka's LLM-jp-3 to gguf format.
  - Downloads: 187
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - A BERT model pretrained on Japanese Wikipedia for dependency-parsing and question-answering, using [MASK] to handle ambiguity in multiple-used words.
  - Downloads: 186
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - The repository offers Asagi-2B, a large Japanese VLM trained on diverse datasets including synthesized data from CALM3 and Phi models.
  - Downloads: 185
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - A fine-tuned RoBERTa model for named entity extraction in Japanese medical text using MedTxt-CR, tagging entities like symptoms, organs, tests, drugs, and more.
  - Downloads: 182
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - A fine-tuned Japanese Whisper model for speech recognition from openai/whisper-base using Common Voice, JVS, and JSUT datasets.
  - Downloads: 179
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - Key tokenizer instructions for loading Japanese BERT-base (Juman++ + BPE) from a dictionary file.
  - Downloads: 178
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - A BERT small model pretrained on Japanese Wikipedia and a financial corpus, with details on its architecture and training data.
  - Downloads: 177
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - A Japanese RoBERTa base model pre-trained on Wikipedia and CC-100 with character-level tokenization and whole word masking.
  - Downloads: 172
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This model, based on the phi-4 framework and using open-r1, mimics Deepseek-R1's Distill methodology to provide Japanese responses with English flexibility.
  - Downloads: 171
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - A fine-tuned RoBERTa model for extractive question answering on Japanese, trained on the JaQuAD dataset.
  - Downloads: 171
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM is an open-source SFT and RLHF model for Japanese to Chinese translation in ACGN domain, built onÂºÄÊ∫êÂ§ßÊ®°ÂûãÂπ∂Âú®ÈÄöÁî®Êó•ÊñáÂèäËΩªÂ∞èËØ¥/GalgameËØ≠Êñô‰∏äÂæÆË∞ÉÔºåÈááÁî®CC BY-NC-SA 4.0ÂçèËÆÆÔºå‰ªÖÈôêÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®„ÄÇ
  - Downloads: 171
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - A GPT2-based 6B parameter language model fine-tuned on 693 million punchline datasets for humor, using pre-trained on extensive corpora including C4, CC-100, OSCAR, and Wikipedia. Licensed under Apache 2. 0.
  - Downloads: 168
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - A fine-tuned BERT model for Japanese zero-shot classification on JSNLI, achieving 92.88% accuracy, available via a simple pipeline from the transformers library.
  - Downloads: 163
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - The GitHub repository contains a Model Card for Tanrei/GPTSAN-japanese, a general-purpose transformer model for Japanese that uses a Prefix-LM structure and includes a Spout vector for flexible fine-tuning.
  - Downloads: 163
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - The repository provides the llm-jp-3-980m-instruct3 model for large language processing, part of a series developed by the National Institute of Informatics' R&D Center for Large Language Models.
  - Downloads: 162
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - Quantized versions of LLM-jp-3-3.7b-instruct models for use with llama.cpp LM Studio (Windows, Mac) and LLMFarm (iOS), along with conversion instructions from npaka‚Äôs LLM-jp-3 to gguf format.
  - Downloads: 160
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - A Stockmark Inc.-trained Japanese BART model, providing a base-sized transformer seq2seq model pre-trained through text corruption and auto-regressive learning.
  - Downloads: 158
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts, trained on an NVIDIA A100-SXM4-40GB for 109 hours, suitable for fine-tuning tasks like POS-tagging.
  - Downloads: 155
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - A fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese, usable at 16kHz sampling rate.
  - Downloads: 151
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - A pre-trained Japanese ELECTRA model using SudachiTra tokenization and WordPiece subword handling for approximately 200 million sentences.
  - Downloads: 150
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„Å´chat vector„ÅßÂØæË©±ËÉΩÂäõ„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 150
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - An ELECTRA model pretrained on 200M mC4 Japanese sentences and fine-tuned on UD_Japanese_BCCWJ r2.8 using spaCy v3, requiring SudachiTra for tokenization.
  - Downloads: 145
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - A Japanese novel-specific language model based on GPT-J-6B trained for 4 weeks using TPU and fine-tuned on novel data.
  - Downloads: 145
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised pretraining model for Chinese and Japanese that merges morphological similarities using a two-stage coarse-to-fine approach from the Unihan database.
  - Downloads: 144
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 141
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository focuses on research and analysis at the National Institute of Science and Technology in Tsukuba, Japan, with an emphasis on advanced experiments and student projects in the Kanto region.
  - Downloads: 139
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - A DeBERTa(V2) model fine-tuned on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´, suitable for tasks like POS-tagging and dependency-parsing.
  - Downloads: 137
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - A BERT-based Japanese model fine-tuned on JaQuAD with evaluation F1 scores of 78.92 and 77.35 on test and development sets, respectively, achieving exact match rates of 63.38 and 61.01.
  - Downloads: 136
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - The repository includes updates to a dialogue system model trained with Japanese datasets, improving the conversation history length but reducing response accuracy on certain questions.
  - Downloads: 136
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - The repository offers the llm-jp-3-150m model from the LLM-jp-3 series developed by the National Institute of Informatics, with requirements for checkpoints and dependencies specified.
  - Downloads: 136
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - A BERT large Japanese model pretrained using character-level tokenization and whole word masking on Wikipedia data from 2020.
  - Downloads: 134
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - A fine-tuned wav2vec2-large-xlsr-53 model for Japanese hiragana inference using JSUT, CSS10, TEDxJP-10K, and JVS datasets.
  - Downloads: 134
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - A BERT base model trained on a Japanese Wikipedia dataset from June 20, 2021.
  - Downloads: 133
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - A fine-tuned BERT model for an unknown dataset achieving a loss of 1.9164, trained with specific hyperparameters.
  - Downloads: 132
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B parameter language model built through continual pre-training on Meta Llama 3.3, enhanced for Japanese while preserving English capabilities using a large Japanese corpus and additional content.
  - Downloads: 131
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and sizes "xl" and "xxl" replacing "3B".
  - Downloads: 131
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - A Japanese ELECTRA-Small model pre-trained using Byte-Pair Encoding subwords from Wikipedia, built on mecab-ipadic-NEologd tokenization.
  - Downloads: 131
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - The repository offers the llm-jp-3-440m-instruct3 model from the National Institute of Informatics, part of a series of large language models for Japanese instruction tasks.
  - Downloads: 130
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - A BERT-based Japanese model using Optuna for hyperparameter tuning with a cosine learning rate schedule, gradient accumulation steps of 1, and weight decay of 0.00017.
  - Downloads: 129
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - A pretrained Japanese GPT-1B model trained to mask PII in texts, replacing specific types of personal information with placeholders.
  - Downloads: 129
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 128
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - A Japanese character-level GPT-2 Medium (310M parameters) model pre-trained on specific Japanese corpora, available for direct text generation.
  - Downloads: 127
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - A gguf-formatted version of matsuo-lab's weblab-10b for use with llama.cpp examples and development via Git cloning.
  - Downloads: 127
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and size variations from "xl" to "xxl."
  - Downloads: 127
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - A pre-trained Japanese ALBERT model using BertJapaneseTokenizer for easier tokenization, intended for fine-tuning on various tasks.
  - Downloads: 127
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - A template Model Card for a Japanese and English pretrained T5 model, intended as a base for new models.
  - Downloads: 126
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - A BERT model pre-trained on Japanese Wikipedia for universal part-of-speech tagging, derived from bert-base-japanese-char-extended.
  - Downloads: 125
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - A GitHub repository hosting the Llama-3.3 Swallow 70B Instruct v0.4 GGUF base model and the imatrix dataset for Japanese language training.
  - Downloads: 124
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 123
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - A BERT base Japanese model with character-level tokenization and whole word masking, using basic tokenization instead of MeCab, based on jawiki-20200831.
  - Downloads: 120
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - The repository contains the llm-jp-3-980m-instruct2 model, part of a series developed by the National Institute of Informatics for large language processing in Japanese, with requirements for specific versions of torch and transformers.
  - Downloads: 120
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository contains a GGUF conversion model for ELYZA-japanese-Llama-2-13b-fast-instruct, including instructions and licensing information.
  - Downloads: 119
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - A DeBERTa(V2) model pre-trained on Aozora Bunko texts, trained on an NVIDIA A100-SXM4-40GB for 127 hours 8 minutes, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 118
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - A DeBERTa V3 model specialized for Japanese that omits morphological analysis during inference and respects word boundaries.
  - Downloads: 116
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - A Japanese RoBERTa base model pre-trained on academic medical articles, released under CC BY-NC-SA 4.0.
  - Downloads: 116
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - A forked version of DistilBERT pre-trained on Japanese web text, adapted for use with transformers>=4.34 by LINE Corporation.
  - Downloads: 116
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - A DeBERTa(V2) model trained on Japanese Aozora texts for tasks like POS-tagging and dependency-parsing.
  - Downloads: 115
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - The repository offers the llm-jp-3-980m model from the LLM-jp-3 series, developed by the National Institute of Informatics, with dependencies including torch 2.3.0+, transformers 4.40.1+, and tokenizers 0+.
  - Downloads: 113
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - A 32B instruction-tuned, 8-bit quantized model of Qwen2.5-Bakeneko for fast inference using AutoGPTQ.
  - Downloads: 113
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - A fast 7B parameter Heron GIT Japanese ELYZA Llama 2 vision-language model for conversing about images, accessible via the heron library.
  - Downloads: 112
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100 with a 512 sequence length, usable for masked language modeling.
  - Downloads: 111
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - A 1.5B-parameter Japanese GPT2 pretrained on Wikipedia and CC-100, suitable for text generation or fine-tuning, requiring Juman++ word segmentation.
  - Downloads: 110
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
  - Downloads: 108
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a novel Mistral-7B-based LLM fine-tuned for 128k context window, high-quality Japanese and English generation, andÁöÑËÆ∞ÂøÜ‰∏≠Êñ≠‰∫ÜÔºåËØ∑ÈóÆÊÇ®ÊòØË¶ÅÊàëÁªßÁª≠Ê¶ÇÊã¨ËøôÊÆµÊèèËø∞ÂêóÔºüÊ†πÊçÆ‰πãÂâçÁöÑÂÜÖÂÆπÔºåÊàëÂèØ‰ª•ËøôÊ†∑ÊÄªÁªìÔºöWabisabi-v1.0 ÊòØÂü∫‰∫é Mistral-7B ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖ∑Êúâ 128k ‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÅÈ´òË¥®ÈáèÁöÑÊó•Ëã±ÁîüÊàêËÉΩÂäõÂíåÈïø‰πÖËÆ∞ÂøÜÁâπÊÄß„ÄÇ
  - Downloads: 105
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - The repository provides weighted/imatrix quants and GGUF files for the Japanese LLaMA-3B instruct model, with IQ-quants recommended when size is a priority.
  - Downloads: 105
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - A RoBERTa model pre-trained on Japanese Aozora texts using a character tokenizer, available for fine-tuning in tasks like POS-tagging and dependency-parsing.
  - Downloads: 105
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository contains a fine-tuned model based on luke-japanese-base for binary positive/negative classification of MARC-ja, with a reported accuracy of 0.9.
  - Downloads: 104
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - A GPT2 model for generating Japanese lyrics, accessible via a website and available for custom use with provided code.
  - Downloads: 103
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - The makiart/jp-ModernBert-base-preview model is a 150M parameter BERT-based Japanese language model with 8192 context length and 50,368 vocabulary size, fine-tuned on Japanese web data.
  - Downloads: 101
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - A RoBERTa large model pre-trained on Japanese Aozora texts, using a character tokenizer, for tasks like POS-tagging and dependency-parsing.
  - Downloads: 100
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - A Japanese ELECTRA-Small model pre-trained using subword units from Japanese Wikipedia and Byte-Pair Encoding, with instructions for setting up MeCab and using the discriminator from the transformers library.
  - Downloads: 99
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - A fine-tuned Wav2Vec2-Large-Japanese model trained on over 600 hours of public Japanese data, accessible upon contact, for direct speech recognition at 16kHz.
  - Downloads: 99
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - This repository offers Asagi-4B, a large-scale Japanese VLM trained on extensive datasets including synthesized data from CALM3-22B-Chat and Phi3.5-vision-instruct.
  - Downloads: 99
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - A BERT model fine-tuned on the JCommonsenseQA dataset for multiple-choice question-answering tasks, based on cl-tohoku/bert-base-japanese-v3.
  - Downloads: 98
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - A Meta-Llama-3-8B-Instruct model fine-tuned on Japanese conversation data, available for use via transformers or the original llama3 codebase.
  - Downloads: 94
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - This repository offers large language models, including the LLM-jp-3-3.7.2b-instruct model, developed by the National Institute of Informatics, with required libraries and usage instructions provided.
  - Downloads: 94
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - A fine-tuned BERT model for Japanese named entity recognition using the ner-wikipedia-dataset, based on the Kyoto University pretrained model and requiring additional Tokenizer and Juman++ installation.
  - Downloads: 93
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - A LayoutLM pretrained model in Japanese developed by Advanced Technology Laboratory, licensed under CC BY-SA 3.0, primarily for token classification tasks and available for fineduning.
  - Downloads: 93
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - A fine-tuned Llama3.1-8B-instruct model for enhanced Japanese fluency using Mergekit, with a default system prompt in Japanese and instructions on how to import and use the model.
  - Downloads: 92
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - A gguf-formatted version of WabiSabi-V1 for local novel LLM projects, created from imatrix-dataset-for-japanese-llm, using llama.cpp for inference.
  - Downloads: 90
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - A fine-tuned GPT-2 Japanese model for resume writing, trained on 20,000 resumes, with a web app available at https://huranokuma-es-app-9t34vl.streamlitapp.com/.
  - Downloads: 89
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - The GGUF version of rinna/nekomata-14b-instruction can be used with llama.cpp for lightweight inference, with recommendations for specific quantization settings.
  - Downloads: 89
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - A Japanese-specific T5 prefix language model fine-tuned on a large corpus including Wikipedia and OSCAR data for token sequence prediction tasks.
  - Downloads: 89
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - A Japanese GPT-2 model pretrained on Wikipedia and CC-100, usable for text generation or fine-tuning, requiring word segmentation with Juman++.
  - Downloads: 88
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository contains a Japanese version of the Llama 3 8B model and provides instructions for installation and usage, recommending Colab or local execution via specific steps.
  - Downloads: 87
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - A fine-tuned llm-jp-1.3b-v1.0 model on Cohere's aya Japanese dataset, evaluated with AVG score of 0.0698, for natural language processing tasks.
  - Downloads: 86
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - A DeBERTa V2 small Japanese pretrained model for masked language modeling, available with usage instructions and tokenizer information.
  - Downloads: 84
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer Encoder-Decoder model with GEGLU activation, no pre-training dropout, and size variations "xl" and "xxl".
  - Downloads: 83
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - A fine-tuned Japanese language model for automated defamation detection using LUKE, trained on a balanced dataset combining two datasets.
  - Downloads: 83
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - A Japanese BigBird base model pretrained on specific datasets, available for masked language modeling.
  - Downloads: 83
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - A Japanese character-level GPT-2 Small language model pre-trained on specific datasets, available for text generation via a provided pipeline.
  - Downloads: 82
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - The Deepreneur-blue-lizard model is a 7B parameter fine-tuned version of Meta's Llama-2-7b with additional Japanese preprocessing and tuning, achieving superior performance on JGLUE benchmarks compared to ChatGPT-3.5.
  - Downloads: 82
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This GitHub repository includes AWQ model files for Stability AI's Japanese StableLM Instruct Gamma 7B, quantized with support from Massed Compute.
  - Downloads: 80
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository includes AWQ model files for Stability AI's Japanese StableLM Instruct Beta 7B, quantized with support from Massed Compute.
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - A Japanese BERT model with up to 500M parameters supporting input sequences of 4,096 or 8,192 tokens, based on the Llama architecture and compatible with Transformers version 4.46.2.
  - Downloads: 79
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository contains a fine-tuned Reward model for evaluating the quality of Japanese novels using ModernBERT-ja-130m, intended for text generation model reinforcement learning.
  - Downloads: 77
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - A GPT2 model for generating Japanese lyrics, using T5Tokenizer and optimized for CPU or GPU.
  - Downloads: 74
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - The repository offers a quantized GGUF version of the CyberAgent DeepSeek-R1-Distill-Qwen-32B-Japanese model with an MIT License.
  - Downloads: 74
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - A pretrained ELECTRA small model for Japanese, fine-tuned on the Japanese Wikipedia.
  - Downloads: 70
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - A T5 v1.1 model card for a Japanese corpus-based Transformer encoder-decoder model with GEGLU activation, no pre-training dropout, and size variants "xl" and "xxl".
  - Downloads: 69
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - A fine-tuned AI GPT-2 model for writing job resumes, specifically tailored for IT industry resumes in Japanese.
  - Downloads: 68
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - The makiart/jp-modernbert-large-preview model, created by Algomatic team with ABCI resources, supports 8192-context length and 396M parameters, trained on Japanese data, suitable for efficient inference with FlashAttention.
  - Downloads: 68
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - A pre-trained Japanese character-level GPT-2 Large model with 717M parameters, usable for text generation via a provided pipeline.
  - Downloads: 66
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi provides GGUF format model files for the Japanese Mistral 7B Instruct v0.1 model.
  - Downloads: 66
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a fine-tuned model for Japanese instruction-following tasks based on CodeLlama, with additional pre-training. It includes instructions for usage with Hugging Face's transformers library.
  - Downloads: 66
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - A pretraining repository for a 12-layer Japanese ELECTRA small model using the Japanese Wikipedia.
  - Downloads: 65
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This GitHub repository provides a Japanese GPT-2 model pretrained on Wikipedia for text generation or fine-tuning, requiring word-level segmentation with Juman++.
  - Downloads: 64
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This GitHub repository contains the terms and conditions for using Fugaku-LLM, a large language model developed for distributed parallel learning on supercomputer Fugaku, by multiple collaborating institutions.
  - Downloads: 63
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 63
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - A finetuned GPT-2 model on ATOMIC data for Japanese text generation using causal language modeling.
  - Downloads: 61
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - A Japanese RoBERTa base model pre-trained on academic medical articles, licensed under CC BY-NC-SA 4.0.
  - Downloads: 60
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository hosts a medium-sized Japanese GPT-2 model using BERT-like tokenizer, requiring PyTorch, fugashi with unidic-lite, and Hugging Face Transformers for implementation.
  - Downloads: 60
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - The GitHub repository contains code for pretraining a 12-layer ELECTRA small model on Japanese Wikipedia texts.
  - Downloads: 60
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - A pretraining repository for an ELECTRA base model on Japanese-language Wikipedia text.
  - Downloads: 60
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - A Japanese-trained Llama2 model fine-tuned on instruction datasets, using a specific script.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding based on cl-tohoku/bert-base-japanese-v3, trained on limited data, and utilizes datasets like JSTS, JSNLI, and MMARCO for similarity, entailment, and retrieval tasks.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - A Japanese BERT base model fine-tuned on WRIME data for predicting emotion intensity scores in tweets about vaccinations.
  - Downloads: 57
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository contains a fine-tuned Reward model for evaluating the quality of Japanese novels using modernbert-ja-310m, primarily for applications in reinforcement learning with generative models.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - A Google MT5-base model fine-tuned in Japanese for error detection and correction using a dataset of 20,000 text pairs.
  - Downloads: 56
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - A fine-tuned Japanese BPR question encoder for document retrieval based on bert-base-japanese-v3, accompanied by training and inference Colab notebooks and available under Apache License 2.0.
  - Downloads: 56
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - A pre-trained ELECTRA small model for Japanese finance text discrimination, using Wikipedia as the training corpus.
  - Downloads: 56
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - A PRETRAINED ELECTRA BASED MODEL FOR JAPANESE TEXT PROCESSING, TRAINED ON THE JAPANESE WIKIPEDIA.
  - Downloads: 54
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository contains a Japanese fine-tuned version of the unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit model, with a system prompt added and further tuned using Japanese data.
  - Downloads: 53
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained bilingual Japanese and English language model adapted from Llama-2-7b using 42 billion tokens from the Cultura-X dataset, achieving state-of-the-art results in perplexity and FLORES-200 translation.
  - Downloads: 53
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz-v2.5-small is a 91M-parameter GPT-2 model for specialized kanji conversion tasks, part of the Zenzai system, licensed under CC-BY-SA 4.0.
  - Downloads: 52
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - A Japanese-pretrained version of the 275.86M mixtral model using Japanese datasets, with example code for generation.
  - Downloads: 51
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - A 1.3B parameter NLLB-200 model fine-tuned for translating Japanese text from "Ascendance of a Bookworm" into English.
  - Downloads: 50
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - A fine-tuned Japanese language model based on Meta AI's Llama 3.1 that achieved top scores in ElyzaTasks-100 among open-source models.
  - Downloads: 50
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - The GitHub repository contains a GGUF conversion of the Japanese-WizardLM2-ChatV-7B model, which combines the Japanese language capability of chatntq-ja-7b-v1.0 with the performance of WizardLM-2-7b.
  - Downloads: 49
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ for POS-tagging and dependency-parsing, using goeswith for subwords.
  - Downloads: 48
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - MaziyarPanahi provides GGUF format model files for the Japanese-instruct gamma 7B and Mistral 7B instruct v0.1 model.
  - Downloads: 48
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - A fine-tuned Japanese Whisper model using whisper-small on Common Voice, JVS, and JSUT, suitable for speech input sampled at 16kHz.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Afinetuned GPT-2-based model for kana-to-hanzi conversion tasks, using a 90M parameter model and BPE tokenizer, developed for the "Zenzai" system under CC-BY-SA 4.0 license.
  - Downloads: 45
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - The GitHub repository contains the GGUF version of rinna/nekomata-7b, optimized for lightweight inference with llama.cpp, and recommends specific quantization settings.
  - Downloads: 45
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2ÊòØÂü∫‰∫éLLaMAÁöÑÁ¨¨‰∫åÁâàÊó•ËØ≠Ê®°ÂûãÔºå‰ΩøÁî®Flash AttentionÂπ∂Âú®WikipediaÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºåÈÄÇÂêàÂú®24GB VRAMËÆæÂ§á‰∏äËøêË°å„ÄÇ
  - Downloads: 45
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - A fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53, suitable for 16kHz sampled input.
  - Downloads: 44
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - A GPU-friendly MLX-formatted 32B Japanese language model derived from DeepSeek-R1, optimized for inference with 4-bit quantization.
  - Downloads: 44
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
  - Downloads: 43
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - A MLX-formatted version of the DeepSeek-R1-Distill-Qwen-32B-Japanese model, converted from a source repository and usable with mlx-lm.
  - Downloads: 42
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - A Japanese RoBERTa base model pre-trained on medical science articles, released under CC BY-NC-SA 4.0.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts, trained on an NVIDIA A100-SXM4-40GB in 632 hours, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 41
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - Ê¶ÇË¶Å vecteus„ÅØ„ÄÅÈ´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™ûÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 41
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - A BERT model pre-trained on Japanese Wikipedia texts, extended with enhanced character embeddings for fine-tuning in tasks like POS-tagging and dependency-parsing.
  - Downloads: 40
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - The repository contains a BERTÂ§ßÂûãÊ®°ÂûãÔºåÂü∫‰∫éUnidic 2.1.2Â≠óÂÖ∏ËøõË°åÂ≠óÁ¨¶Á∫ßÂàÜËØçÂπ∂ÂêØÁî®Êï¥ËØçÊé©Á†ÅËÆ≠ÁªÉÔºåÁî®‰∫éÊó•ËØ≠ÊñáÊú¨È¢ÑËÆ≠ÁªÉ„ÄÇ
  - Downloads: 40
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - This repository provides a GGUF version of rinna/nekomata-7b-instruction for lightweight inference with llama.cpp, recommending specific quantization settings.
  - Downloads: 40
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - A pretrained ELECTRA small model for Japanese language processing, trained on the Japanese Wikipedia.
  - Downloads: 40
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese and English language model fine-tuned on 42 billion tokens from the Cultura-X dataset, trained using direct preference optimization to align with human preferences.
  - Downloads: 37
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for dependency parsing and question-answering, optimized for handling multiple-used words in context.
  - Downloads: 37
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - A pretrained ELECTRA small model for Japanese language processing, trained on the Japanese Wikipedia.
  - Downloads: 37
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - A fine-tuned Japanese MT5-base model for summing up patent claims in the pharmaceutical domain.
  - Downloads: 36
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This GitHub repository hosts a Japanese+English Sentence-BERT model pre-trained on cl-tohoku/bert-base-japanese-whole-word-masking, showing improved English STSbenchmark performance while slightly lower Japanese accuracy.
  - Downloads: 36
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - A GitHub repository for initializing and using the SPLADE-Japanese model on the mMARCO Japanese dataset for query encoding.
  - Downloads: 35
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - A Japanese RoBERTa large model pretrained on Wikipedia and CC-100, usable for masked language modeling.
  - Downloads: 34
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - A fine-tuned T5-base model on the xlsum dataset for Japanese summarization, achieving specified loss and Rouge scores with given training hyperparameters.
  - Downloads: 34
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - A GPT2 Japanese base model version 2 using BPE tokenizer and trained on wiki40b/ja subset, enabling text generation.
  - Downloads: 34
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a human-aligned chat model trained in Japanese and English using direct preference optimization on top of Llama-2-7b fined-tuned with 42 billion Japanese tokens.
  - Downloads: 34
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - A T5 v1.1 model card for a Japanese corpus-based encoder-decoder model featuring GEGLU activation, no pre-training dropout, and different size variants "xl" and "xxl".
  - Downloads: 33
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository contains a medium-sized Japanese reversed GPT-2 model using BERT-like tokenizer, requiring PyTorch, fugashi with unidic-lite, and Hugging Face Transformers for usage.
  - Downloads: 32
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository contains a fine-tuned BERT-largeJapanese-v2 model for CommonsenseQA tasks using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 32
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - An ELECTRA model pretrained on 200M Japanese sentences using SudachiTra tokenization, compatible with mC4 Japanese dataset.
  - Downloads: 32
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - A model fine-tuned for long text generation from the llm-jp/llm-jp-3-3.7b-instruct repository, using SFT and a Japanese dataset with 3k samples.
  - Downloads: 32
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - The repository provides a Japanese BERT-base tokenizer using MeCab and Byte-Pair Encoding (BPE), requiring the dictionary file for installation.
  - Downloads: 31
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B-parameter fine-tuned Japanese GPT2 model for use with T5Tokenizer, available in PyTorch and Rust.
  - Downloads: 31
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - A novel dataset fine-tuned version of Mistral-7B for high-quality Japanese and English generation with memory abilities, developed during a LocalAI hackathon.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a chat Japanese language model based on the Mamba state-space model architecture, inspired by the work of Albert Gu and Tri Dao.
  - Downloads: 31
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository contains a distilled GPT-2 Japanese model trained from rinna/japanese-gpt2-medium, using Google GPU instances and Hugging Face Transformers.
  - Downloads: 30
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing of Japanese short-unit-words.
  - Downloads: 30
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - A BERT model for nagisa,available in Transformers and requiring Python 3.7+, with a provided tokenizer and usage via the pipeline method.
  - Downloads: 29
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - A 1.3B parameter Japanese GPT dialogue AI trained on Alpaca_Ja and GuanacoDataset, requiring 7GB VRAM or RAM for operation.
  - Downloads: 29
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - A fine-tuned Japanese GPT-2 model for writing AI-generated ES texts, trained on over 140,000 examples from various fields, available via web app http://www.eswrite.com.
  - Downloads: 28
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - A pretrained ELECTRA model for Japanese language processing, based on Wikipedia text.
  - Downloads: 28
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - A fine-tuned wav2vec2 model achieving low loss and error rates on the Japanese Common Voice dataset.
  - Downloads: 27
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - A DeBERTa(V2) model pretrained on the Aozora Bunko corpus for dependency-parsing and question-answering, with support for handling multiple-used words.
  - Downloads: 27
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - A pre-trained ModernBERT large model on Japanese Aozora texts, usable for tasks like POS-tagging and dependency-parsing.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´ for UPOS and FEATS tagging, derived from deberta-base-japanese-unidic.
  - Downloads: 26
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - A pre-trained GPT-Neo 1.3B model for Japanese, trained on cc100 ja, oscar ja, and wikipedia, enabling text generation via Hugging Face transformers.
  - Downloads: 26
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - A 8-layer-trained Japanese language model derived from oshizo/japanese-e5-mistral-7b_slerp, trained on 800,000 sentences.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - The repository contains a Japanese Qwen-32B language model converted to MLX format, along with instructions for usage.
  - Downloads: 25
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - This repository includes various instruction-tuned and LoRAfine pre-trained language models from the LLM-jp project, formatted for Hugging Face.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - The repository provides a Vaporetto + BPE tokenizer for Japanese BERT-base, requiring users to download the dictionary file and specify its path.
  - Downloads: 25
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - The repository provides instructions and code for loading the Sudachi + BPE tokenizer used with Japanese BERT-base models.
  - Downloads: 25
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - A DeBERTa(V2) model pretrained on ÈùíÁ©∫ÊñáÂ∫´ for dependency-parsing and question-answering, using [MASK] to handle ambiguous words.
  - Downloads: 25
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository includes GPTQ model files for Stability AI's Japanese StableLM Instruct Gamma 7B with various parameter permutations.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - A Japanese sentence-T5 model using sonoisa/t5-base-japanese for inference with sentencepiece.
  - Downloads: 24
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - An experimental FastText word embedding model for Japanese, includes setup instructions for Google Colaboratory and dependencies like MeCab.
  - Downloads: 24
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - A XLM-RoBERTa-base model trained on Japanese mMARCO data with ANCE warmup, checkpoint saved at 50k steps due to MRR decline at 60k.
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This GitHub repository contains JAX/Flax-based transformer language models trained on Japanese data, including updates and benchmark scores.
  - Downloads: 24
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - A BERT Base model for Japanese language fine-tuned on a balanced dataset for automatic cyberbullying detection, licensed under CC BY-SA 4.0.
  - Downloads: 24
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - A pre-trained ELECTRA small model for generating Japanese financial texts, trained on the Japanese Wikipedia.
  - Downloads: 24
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - A Japanese-finetuned Mistral-Nemo model for EPR purposes, using multiple datasets to enhance Japanese proficiency and with adjusted training epochs and parameters.
  - Downloads: 24
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - A RoBERTa model pretrained on Japanese Aozora texts for POS-tagging and dependency-parsing, using goeswith for subwords, suitable for universal dependencies tasks.
  - Downloads: 24
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - A model combining English GPT-4 chat vector and Japanese language patterns to generate light bedtime stories.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging of long-unit-words, derived from deberta-base-japanese-aozora.
  - Downloads: 23
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - The repository provides a Japanese BERT-base tokenizer using Juman++ + Unigram, including instructions on how to load it by specifying the dictionary file path.
  - Downloads: 23
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - The GitHub repository contains the GGUF version of rinna/nekomata-14b, suitable for lightweight inference with llama.cpp, and recommends specific quantization settings.
  - Downloads: 23
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - The repository provides instructions for loading a Japanese BERT tokenizer derived from Vaporetto and Unigram, requiring the dictionary file's path.
  - Downloads: 23
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small LLaMA-based Japanese-language model trained from scratch, offering quick inference but limited knowledge and generating humorous responses.
  - Downloads: 23
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - A BERT model for Japanese POS-tagging and dependency-parsing, derived from bert-large-japanese-char-extended and tagged with UPOS.
  - Downloads: 23
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - A BERT model for Japanese POS-tagging and dependency-parsing, derived from bert-largeJapanese, with UPOS tagging of long-unit-words.
  - Downloads: 23
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for dependency-parsing and question-answering with masked input handling.
  - Downloads: 23
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card Ëã±Êó•„ÄÅÊó•Ëã±ÁøªË®≥Áî®„É¢„Éá„É´C3TR-Adapter„ÅÆGPTQ4bitÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - A merged role-playing language model enhanced with Task Vector and Model Stock techniques, based on Aratako/Ninja-v1-RP-WIP, using Vicuna prompt format.
  - Downloads: 22
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - A BERT model pre-trained on Japanese Wikipedia texts with enhanced character embeddings forÂ∏∏Áî®Êº¢Â≠ó/‰∫∫ÂêçÁî®Êº¢Â≠ó, suitable for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 22
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - A RoBERTa model pre-trained on Hindi texts with a character-level tokenizer, available for masked language modeling.
  - Downloads: 22
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging of long-unit-words, derived from roberta-large-japanese-aozora.
  - Downloads: 22
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - Provides instructions for loading a Japanese BERT-base tokenizer using MeCab and WordPiece dictionaries.
  - Downloads: 21
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - An ELECTRA model pretrained on mC4 Japanese sentences and fine-tuned on UD_Japanese_BCCWJ using spaCy v3, distributed as the ja_ginza_electra Python package.
  - Downloads: 21
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using Sudachi and WordPiece dictionaries.
  - Downloads: 21
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - This repository contains a modified version of SudachiTra for BERT, with changes to word form type and vocabulary handling.
  - Downloads: 21
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - A fine-tuned 7B-parameter Japanese language model capable of generating fanfics based on ACG content, built on Japanese Stable LM Base Gamma 7B.
  - Downloads: 21
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - A BERT model fine-tuned on Japanese literary genres using titles and descriptions from kakuyomu.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This GitHub repository contains the terms and conditions for using Fugaku-LLM, a large language model developed as part of research on distributed parallel learning methods for supercomputers, with commercial and non-commercial applications allowed under specified conditions.
  - Downloads: 20
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This GitHub repository houses the Fugaku-LLM, a large language model developed for distributed parallel learning on supercomputer Fugaku, with usage terms and conditions outlined by its developers.
  - Downloads: 20
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - A BERT-based Japanese model for POS tagging and dependency parsing, derived from bert-base-japanese-v2 and tagged with UPOS.
  - Downloads: 20
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - A RoBERTa model pre-trained on Hindi texts with character-level tokenizer, configured as `is_decoder=False`.
  - Downloads: 20
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - The repository provides instructions for loading a Japanese BERT tokenizer using MeCab and Unigram dictionaries.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - A dataset converted from AIBunCho's public model for ctranslate2, with 8-bit quantization and potential loss in accuracy.
  - Downloads: 20
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - The repository provides a Japanese BERT-base tokenizer using Juman++ + WordPiece, requiring users to download the dictionary file and specify its path for loading.
  - Downloads: 20
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - A fine-tuned Luke-Japanese-base model for JSTS sentence similarity calculation with an accuracy of 0.8971 using Yahoo Japan's JGLUE dataset.
  - Downloads: 20
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - A fine-tuned Wav2Vec2 model for Japanese accent detection with a WER of 15.82%.
  - Downloads: 20
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - A novel generation model trained on high-quality light-novel texts and other sources using QLoRA, guided by instruction templates covering various genres and topics up to 2021.
  - Downloads: 20
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository includes AWQ model files for the Japanese StableLM Base Beta 70B created by Stability AI, quantized with support from Massed Compute.
  - Downloads: 20
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - A merged model of the Japanese-vocabulary-enhanced Mixtral-8x7B-Instruct-v0.1, evaluated as an intermediate result on ABEJA's tech blog.
  - Downloads: 20
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - The repository features REV-Mix models, including REV-I and REV-R samplers for anime and real-life images, using specific settings and recommending embeddings for improved results.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - The GitHub repository contains a specialized GPT-2 conditional language model, zenz-v2.5-small, for kana-to-hanzi conversion tasks, with three model sizes and a focus on context-aware performance.
  - Downloads: 19
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - A BERT model pre-trained on Japanese Wikipedia for UPOS and FEATS tagging, derived from bert-base-japanese-char-extended.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - A DeBERTa(V2) model pre-trained on Aozora Bunko texts for fine-tuning in tasks like POS-tagging, with training details and usage instructions provided.
  - Downloads: 19
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - A chat model flavored with a lolicon theme fine-tuned on rinna/japanese-gpt-neox-3.6b for personal interest and development.
  - Downloads: 19
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese texts from ÈùíÁ©∫ÊñáÂ∫´ for UPOS tagging, supporting long-unit-word POS classification.
  - Downloads: 19
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - A RoBERTa model pre-trained on Japanese-Aozora texts, compatible with the Japanese-LUW tokenizer, for tasks like POS-tagging and dependency-parsing.
  - Downloads: 19
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - A RoBERTa model pre-trained on Aozora Bunko texts for Japanese UPOS tagging, derived from roberta-base-japanese-aozora.
  - Downloads: 19
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - A BERT model pre-trained on Japanese Wikipedia for UPOS and FEATS tagging of long-unit-words, using the transformers library for token classification.
  - Downloads: 19
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - A transformer-align model for Japanese to Hebrew translation using normalization and SentencePiece preprocessing, with evaluation metrics and test set translations from OPUS-2020.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing of Japanese.
  - Downloads: 19
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository includes GPTQ model files for Stability AI's Japanese StableLM Base Beta 70B with various parameter permutations.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - A 4-bit fine-tuned Llama-2-Chat 70B model for Japanese chat tasks, based on the Alpaca-JA dataset. Users must adhere to Meta's license.
  - Downloads: 19
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - A MoE-based language model combining 8 specialized experts from sbintuitions/sarashina2.2-3b-instruct-v0.1, offering high-quality Japanese response generation and easy Hugging Face integration.
  - Downloads: 18
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - A RoBERTa model pretrained on Japanese Aozora literature for dependency parsing and question-answering with support for handling ambiguous words.
  - Downloads: 18
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository contains GPTQ model files for Stability AI's Japanese StableLM Instruct Beta 70B with multiple parameter permutations.
  - Downloads: 18
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - A Japanese DeBERTa V2 base model pre-trained on specific datasets, usable for masked language modeling.
  - Downloads: 18
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - A Japanese inference model trained on the "databricks-dolly-15k-ja" dataset, inspired by and similar to inu-ai/dolly-japanese-gpt-1b.
  - Downloads: 18
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - A pre-trained Japanese RoBERTa base model for super short unit words, suitable for masked language modeling after input text is converted to full-width characters and segmented using KyTea.
  - Downloads: 18
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This GitHub repository contains a model converted from Kyoto University's Japanese BART Pretrained model, requiring BartJapaneseTokenizer for input and supporting tasks like FillMask.
  - Downloads: 18
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU-Model is an experimental fine-tuned T5 machine translation model for Japanese-Ainu language pairs, currently in need of refinement.
  - Downloads: 18
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - The repository hosts a 4-bit quantized derivative model of the llm-jp-3-172b-instruct3 provided by NII, aimed at reducing GPU/memoroy usage during inference.
  - Downloads: 18
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository contains a Japanese BERT-based model named Luke-Japanese-Base, using WordPiece tokenizer, fine-tuned on 2023 Wikipedia data, capable of handling [UNK] entities.
  - Downloads: 18
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - The repository contains the GGUF-formatted Japanese-LLaMA-2-13B model, available at https://huggingface.co/owner203/japanese-llama-2-13b.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - A fine-tuned LLM for Whisper targeting Japanese Dominion game audio transcription.
  - Downloads: 18
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - A merged model of Swallow-MX-8x7b-NVE-v0.1 with a weighted combination of Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1 for more natural Japanese output.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - A work-in-progress, 6.8 billion parameter pre-trained Japanese language model based on EleutherAI's Mesh Transformer JAX, using T5Tokenizer and SentencePiece.
  - Downloads: 17
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - A T5 model pretrained on a large Japanese corpus, requiring fine-tuning for specific tasks, with potential biases to consider.
  - Downloads: 17
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - A fine-tuned RoBERTa Japanese model for JSNLI with evaluation results of 0.2039 loss and 0.9328 accuracy, requiring Juman++-segmented input for zero-shot classification using the transformers pipeline.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - A RoBERTa model pretrained on Japanese texts for POS-tagging and dependency-parsing, using goeswith for subwords, with instructions for universal-dependencies pipeline usage and fugashi requirement.
  - Downloads: 17
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - A fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice, CSS10, and JSUT datasets, trained at 16kHz sampling rate.
  - Downloads: 17
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - A T5 v1.1 model pre-trained on Japanese, featuring GEGLU activation, no dropout in pre-training, and size variations "xl" and "xxl".
  - Downloads: 17
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - A merged model combining elements of Mistral-7B-Instruct and Japanese-stablelm-base-gamma-7b using spherical linear interpolation.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - The GitHub repository contains a specialized GPT-2 model, zenz-v2.5-small, for accurate kanji conversion tasks in Japanese, with context-aware transformations and three varying model sizes.
  - Downloads: 16
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for POS-tagging Japanese long-unit-words, derived from deberta-small-japanese-aozora.
  - Downloads: 16
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - A fine-tuned T5-base Japanese model for title generation, inputting text to generate a summary title.
  - Downloads: 16
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - A Japanese DeBERTa V2 base model pre-trained on specific corpora, usable for masked language modeling.
  - Downloads: 16
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporation's 3.6B parameter Japanese language model, fine-tuned for instruction-following, available in 4-bit quantization for lightweight deployment.
  - Downloads: 16
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository offers a 1.7B parameter quantized Japanese language model fine-tuned by LINE Corporation, along with instructions for usage.
  - Downloads: 16
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - The repository contains models trained for Q&A with context using SFT and quantization techniques (AutoAWQ, GPTQ), based on the youri-7b-instruction model.
  - Downloads: 16
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - A repository using Optuna to explore hyperparameters for sentiment analysis with BERT, including learning rates, batch sizes, and regularization strengths.
  - Downloads: 16
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - The repository provides instructions and code for loading a Japanese BERT tokenizer based on Vaporetto + WordPiece.
  - Downloads: 16
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This model, trained using H2O LLM Studio on a base cyberagent/open-calm-7b with transformed AIÁéã dataset, requires transformers, accelerate, and torch libraries for GPU-powered text generation.
  - Downloads: 16
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - A RoBERTa Japanese model pre-trained on 200M sentences with increased position embeddings to 1282, using Juman++ and SentencePiece tokenization.
  - Downloads: 16
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository includes GPTQ model files for Stability AI's Japanese StableLM Instruct Beta 7B with various parameter permutations.
  - Downloads: 16
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero Base model llm-jp-13b-v1 includes 15k randomly sampled instruction data from the Jaster dataset for training.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - A Japanese vocabulary-extended pre-trained model based on Mixtral-8x7B-Instruct-v0.1 for causal language modeling.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - A JapaneseË™û fine-tuned version of Mixtral-8x7B-Instruct-v0.1 intermediate model, evaluated in ABEJA's tech blog.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 16
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS and FEATS tagging, derived from roberta-base-japanese-aozora-char.
  - Downloads: 15
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - The GitHub repository megagonlabs/t5-base-japanese-web-8k contains training codes for a T5 model pre-trained on Japanese web texts with an 8K vocabulary size.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, available for token classification tasks.
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - A BERT model pretrained on Japanese Wikipedia for dependency-parsing and question-answering, with support for handling ambiguous words.
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Aozora texts for POS-tagging and dependency-parsing, using goeswith subwords.
  - Downloads: 15
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This GitHub repository contains a BERT Base model for Japanese irony detection, fine-tuned on ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 15
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - A DeBERTa(V2) model pretrained for Japanese POS-tagging and dependency-parsing, using goeswith for subwords, which can be used via a transformers pipeline for universal dependencies.
  - Downloads: 15
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - A fine-tuned Llama-2-13b-chat-hf model using QLoRA on a dataset of 50,000 chat samples and 280,000 non-chat samples, with improved performance in Chinese and Japanese.
  - Downloads: 15
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - This repository includes various pre-trained and LoRA fine-tuned large language models from the LLM-jp project, including instruction models and checkpoints for different variants.
  - Downloads: 15
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - A merged model combining Mistral-7B-Instruct and Japanese-stablelm-instruct-gamma-7b using SLERP method for configuration slicing.
  - Downloads: 15
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - An ELECTRA-based Japanese model pretrained on 200M sentences and finetuned for Information Triage on disaster tweets, licensed under CC BY-SA 4.0.
  - Downloads: 15
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - A RoBERTa model pretrained on Japanese Aozora texts for POS-tagging and dependency-parsing, using goeswith for subwords, with instructions provided for universal-dependency parsing via Hugging Face's transformers library.
  - Downloads: 15
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, derived from roberta-small-japanese-aozora-char.
  - Downloads: 15
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - The GitHub repository contains the Japanese-LLaMA-2-7B model in GGUF format, with a provided model URL.
  - Downloads: 15
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is an enhanced GPT-2-based language model for kana-to-hanzi conversion tasks, finetuned from ku-nlp/gpt2-small-japanese-char and optimized for high performance in Japanese NLP.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - The repository contains continual pre-training and instruction-tuning of a 70B parameter Llama model on Japanese and English datasets, resulting in improved performance for Japanese tasks, called "Llama 3 Youko."
  - Downloads: 15
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a human-aligned chat model trained in Japanese and English, fine-tuned from the Llama-2-7b base model on 42 billion Japanese tokens.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - A DeBERTa(V2) model pre-trained on ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging and dependency parsing of long-unit-words.
  - Downloads: 14
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - A 50k-step fine-tuned Hubert model for Japanese using multiple corpora, subject to specific terms of use.
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - This repository offers a 3.6B parameter quantized Japanese language model fine-tuned by LINE Corporation, along with instructions for‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑ tokenizer ÂíåÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository offers a 3.6 billion parameter quantized Japanese language model fine-tuned by LINE Corporation, available for use via Hugging Face transformers.
  - Downloads: 14
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - The repository uses Optuna to explore hyperparameters for a BERT-based Japanese sentiment analysis model, with the best configuration found being cosine learning rate schedule, a learning rate of approximately 3.91e-5, a batch size of 128, and weight decay of approximately 5.22e-5 over 100 epochs with early stopping.
  - Downloads: 14
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository contains a fine-tuned model for predicting the offensiveness of social media comments using a dataset with manual aggression evaluations, achieving an F1 score of 71.3%.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - The repository provides a Japanese BERT-base tokenizer for Nothing + Unigram, requiring users to download the dictionary file and specify its path.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - The repository provides instructions for loading a Japanese BERT-base tokenizer using Sudachi and Unigram dictionaries.
  - Downloads: 14
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - A fine-tuned Japanese chat model based on abeja/gpt-neox-japanese-2.7b for personal interest and study, using ebisuke/liz-nojaloli-ja-datasets.
  - Downloads: 14
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - A pre-trained Japanese BERT base model for super short unit words, requiring input text to be converted to full-width characters and segmented into SSUW before use.
  - Downloads: 14
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - AJapanese BART pretrained model converted to match the Fairseq architecture, requiring BartJapaneseTokenizer for input and using Simple FillMaskPipeline from transformers.
  - Downloads: 14
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - A RoBERTa model pre-trained on Aozora Bunko texts with a character tokenizer, available for fine-tuning tasks like POS-tagging and dependency-parsing.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - A RoBERTa large model pre-trained on Japanese Aozora texts, using the Japanese-LUW tokenizer, for tasks like POS-tagging and dependency-parsing.
  - Downloads: 14
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is an instruction-following model developed and tested on ConoHa VPS with NVIDIA H100 GPU.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - A DeBERTa(V2) model pre-trained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for UPOS tagging, available for token classification tasks.
  - Downloads: 13
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - A Japanese RoBERTa large model pre-trained on Wikipedia and CC-100 with character-level tokenization and whole word masking for masked language modeling.
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - A RoBERTa model pretrained on Japanese text for POS-tagging and dependency-parsing, which can be used via a transformers pipeline requiring fugashi.
  - Downloads: 13
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - A 1.7 billion parameter quantized Japanese language model fine-tuned by LINE Corporation, available for use via Hugging Face transformers library.
  - Downloads: 13
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporation's 1.7B parameter Japanese language model, fine-tuned for instruction-following and quantized to 4-bit, allows efficient text generation.
  - Downloads: 13
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - A merged model, KiwiMix_v10, with Lora enhancements and deformed character styling, potentially for version updates.
  - Downloads: 13
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - A reproduction of a 7B-parameter decoder-only fine-tuned Japanese language model trained on instruction-following datasets using the notus codebase.
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository contains quantized AWQ model files for Stability AI's Japanese StableLM Instruct Beta 70B, supported by a16z grants and Massed Compute hardware.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a human-aligned chat model trained in Japanese and English using direct preference optimization on a base model fine-tuned from Llama-2-7b with 42 billion tokens from the Cultura-X dataset.
  - Downloads: 13
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - A Rust implementation of the BERT large Japanese model from Tohoku University, including setup instructions and usage examples.
  - Downloads: 12
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - A Fine-Tuned Japanese StableLM Base Alpha 7B model mimicking Reimu Hakurei's speech pattern for conversation, with usage examples andÊ≥®ÊÑè‰∫ãÈ°π„ÄÇ
  - Downloads: 12
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - A fine-tuned Japanese GPT-j-6b model for conversing with the character Marisa Kirisame from the Eastern Project, including a GoogleColab example andÊ≥®ÊÑè‰∫ãÈ°π„ÄÇ
  - Downloads: 12
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - This repository contains the LoRA fine-tuned OpenCALM-LARGE model for Japanese language processing using PyTorch and PEFT, based on CyberAgent's original decoder-only models.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - Provide the path to the downloaded Nothing + BPE dictionary file to load the Japanese BERT-base tokenizer.
  - Downloads: 12
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - A RoBERTa model pre-trained on Aozora Bunko texts for UPOS and FEATS tagging, derived from roberta-large-japanese-aozora-char.
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - A RoBERTa model pre-trained on Japanese Aozora texts, compatible with the Japanese-LUW tokenizer, for tasks like POS-tagging and fine-tuning.
  - Downloads: 12
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a chat-oriented Japanese language model based on the Mamba state-space architecture,ÁªÜË∞ÉËá™Mamba-chatÂπ∂ÈíàÂØπSkelterLabsInc/JaQuADÊï∞ÊçÆÈõÜ„ÄÇ
  - Downloads: 12
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - A quantized Japanese instruction-tuned version of Llama 2 (7B parameters), optimized for Colab A100 or RTX 3000 Series GPUs.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This GitHub repository contains the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1, currently featuring Q4_K_M with other versions potentially available based on demand.
  - Downloads: 12
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository contains a GPTQ quantized model of the ELYZA-japanese-CodeLlama-7b-instruct model, calibrated with 1k random samples from Japanese Wikipedia and additional task data.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - A pre-trained ModernBERT model for Japanese text, fine-tunable for tasks like POS-tagging and dependency-parsing.
  - Downloads: 12
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - A RoBERTa model pretrained on Japanese Aozora corpus for head detection in dependency parsing and question-answering using [MASK] for disambiguation.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Aozora Bunko texts for POS-tagging and dependency-parsing, using goeswith subwords.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - A DeBERTa(V2) model pretrained on Japanese Wikipedia and ÈùíÁ©∫ÊñáÂ∫´ texts for POS-tagging and dependency-parsing, using goeswith for subwords.
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - A model trained via SFT for Q&A and contextual responses, quantized with AutoGPTQ, achieving performance comparable to GPT3.5 in a 7B parameter model. Includes learning, evaluation, and sample codes.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - The repository provides instructions and code for loading a Japanese BERT-base tokenizer using the Nothing + WordPiece vocabulary.
  - Downloads: 11
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - A pretrained ELECTRA model using approximately 200M Japanese sentences, tokenized with Sudachitra and WordPiece, for which you need to install SudachiTra before use.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - A DeBERTa(V2) model pretrained on ÈùíÁ©∫ÊñáÂ∫´ for dependency-parsing and question-answering, using [MASK] to handle ambiguity in multiple-used words.
  - Downloads: 11
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - A fine-tuned Wav2Vec2-XLS-R-300M model for recognizing Japanese Hiragana from 16kHz audio inputs.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - A DeBERTa(V2) model for Japanese POS-tagging and dependency-parsing, pretrained on multiple datasets, using goeswith for subwords.Reusable via Hugging Face transformers pipeline.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a chat-based Japanese language model derived from Mamba architecture, fine-tuned on 31,7k JaQuAD dataset examples.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - A 7-billion parameter Japanese language model fine-tuned for instruction-following tasks, based on the Japanese Stable LM Base Gamma 7B model.
  - Downloads: 11
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - A Japanese transformer pipeline using BERT for spaCy, including transformer, parser, and NER components, based on UD_Japanese-GSD r2.8+NESudachiDict_corecl-tohoku, with 45 labels and licensed under CC BY-SA 4.0.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - A pre-trained ESPnet2 TTS model based on the jsut/tts1 recipe by kan-bayashi.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - A 7-billion-parameter Japanese instruction-following language model fine-tuned on specific datasets, built upon the Japanese Stable LM Base Gamma 7B.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - A 7-billion-parameter Japanese instruction-following language model fine-tuned on Stable LM Base Gamma 7B, requiring Transformers 4.34.0 or later for usage.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - A 7 billion-parameter Japanese instruction-following language model fine-tuned on the base model Japanese Stable LM Base Gamma 7B, requiring Transformers 4.34.0 or later for usage.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - A 7B-parameter Japanese instruction-following language model fine-tuned on instructional datasets, built onÁ®≥ÂÆöÊÄßAIÁöÑJapanese Stable LM Base Gamma 7B„ÄÇËØ∑Á°Æ‰øù‰ΩøÁî®Transformers 4.34.0ÊàñÊõ¥È´òÁâàÊú¨ÔºåÂπ∂Êèê‰æõÁõ∏Â∫îÁöÑ tokernizer ÂíåÊ®°ÂûãÂä†ËΩΩ‰ª£Á†ÅÁ§∫‰æã„ÄÇ
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Syntactic Text Processing
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - The repository provides a pre-trained small model of the OpenCALM decoder-only language model for Japanese text processing.
  - Downloads: 5,595
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - The repository follows CreativeML Open RAIL-M licensing, with an additional copyright credited to @sazyou_roukaku; usage limitations apply, especially for criminal or specialized purposes.
  - Downloads: 4,315
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô /
  - Downloads: 3,944
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - A converted gguf format model of Ninja-v1-NSFW for local novel LLM project, based on imatrix dataset and including instructions for usage with llama.cpp.
  - Downloads: 3,728
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - The repository provides the OpenCALM-7B model for Japanese language processing, including its pre-trained weights and usage examples with tokenizers.
  - Downloads: 3,674
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - This repository provides a large OpenCALM decoder-only language model for Japanese, including the pretrained model and tokenizer from CyberAgent.
  - Downloads: 3,666
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow model is a Japanese-language-enhanced version of Llama 2 with instruction-tuned SFT, released in March 2024 as v0.1 previews.
  - Downloads: 3,073
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - A gguf-format conversion of the Ninja-v1-NSFW-128k model, created using imatrix data, for use with llama.cpp.
  - Downloads: 2,938
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow model is an Llama 2 derivative with Japanese data added and SFT-tuned; version 0.1 releases include Swallow-7b-instruct-v0.1, Swallow-13b-instruct-v0.1, and Swallow-70b-instruct-v0.1.
  - Downloads: 2,754
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow model series, derived from Llama 2 with Japanese data added, uses SFT; v0.1 releases include Swallow-7b-instruct-v0.1, Swallow-13b-instruct-v0.1, and Swallow-70b-instruct-v0.1.
  - Downloads: 2,121
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - shisa-base-7b-v1 enhances Mistral 7B with additional Japanese pre-training and an extended tokenizer for improved Japanese efficiency.
  - Downloads: 2,036
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores old, deprecated, and experimental models for fun merged with a specific material, suitable for playful adjustments and deformations.
  - Downloads: 1,914
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - Evaluation results for Japanese models on MIRACL and JQaRA datasets include metrics like nDCG@10, Recall@1000, and others, with varying performance across different model versions.
  - Downloads: 1,495
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-7B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,413
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,101
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-13b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,057
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow model series is an enhancement of Llama 2 with Japanese data, using SFT, and includes versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 1,024
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - The Swallow-MX-8x7b-NVE-v0.1 model is a pre-trained language model based on Mixtral with added Japanese data, maintaining the original architecture and tokenizer.
  - Downloads: 942
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow model, derived from Llama 2 with Japanese data, uses SFT; releases include Swallow-7b-instruct-v0.1, Swallow-13b-instruct-v0.1, and Swallow-70b-instruct-v0.1 on April 26, 2024.
  - Downloads: 933
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - The repository contains the OpenCALM-3B decoder-only language model, pre-trained on Japanese datasets, along with examples for loading and using it in PyTorch.
  - Downloads: 911
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - A gguf format conversion of shisa-7b-v1 for use with llama.cpp, demonstrating text generation commands.
  - Downloads: 891
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãJapanese-Chat-Umievo-itr001-7b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 830
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-8b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow model, derived from Llama 2 with Japanese data integration and SFT tuning, includes versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 784
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-70b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 732
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãc4ai-command-r-plus„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 675
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - The repository contains the OpenCALM-Medium model, a pre-trained Japanese decoder-only language model from CyberAgent, Inc., along with usage instructions.
  - Downloads: 664
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow model family, derived from Llama 2 with Japanese data added, features instruction-tuned versions released in April 2024.
  - Downloads: 647
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 628
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - The repository features continually pre-trained Swallow models from the Llama 2 family, enhanced with Japanese data and released in versions 0.1 on April 26, 2024, through supervised fine-tuning.
  - Downloads: 607
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHonyaku-13b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 604
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow model series, derived from Llama 2 with Japanese data added, uses SFT and includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 589
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow model, derived from Llama 2 with Japanese data addition and SFT, includes versions 7B, 13B, and 70B released on April 26, 2024.
  - Downloads: 584
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow model is a Japanese-language-focused variant of Llama 2, fine-tuned with supervised fine-tuning, including versions Swallow-7b-instruct-v0.1, Swallow-13b-instruct-v0.1, and Swallow-70b-instruct-v0.1 released on April 26, 2024.
  - Downloads: 567
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow model series is a Japanese-language tuned version from Llama 2 with SFT, including instruct-tuned 7B, 13B, and 70B models released on April 26, 2024.
  - Downloads: 565
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - A gguf-format conversion of llm-jp-3-7.2b-instruct3 by llm-jp, using imatrix data from TFMC/imatrix-dataset-for-japanese-llm. Custom chat template support is not available via llama.cpp.
  - Downloads: 548
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow model series, derived from Llama 2 with Japanese data added and SFT-tuned, includes releases like Swallow-7b-instruct-v0.1 on April 26, 2024.
  - Downloads: 536
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow model is a Japanese-language-enhanced version of Llama 2 with instruction-tuning, releasing new versions in April 2024.
  - Downloads: 535
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãQwen1.5-110B-Chat„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 518
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-128k„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 408
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - A Stanza model for Japanese that enables syntactic analysis and entity recognition using state-of-the-art NLP tools.
  - Downloads: 309
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - SakanaAI-EvoLLM-JP-v1-7B-gguf is a base model in gguf format converted from EvoLLM-JP-v1, licensed under terms of its merge sources.
  - Downloads: 282
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - This repository contains details and setup instructions for the Japanese InstructBLIP Alpha model, enabling image description generation in Japanese with optional text input.
  - Downloads: 277
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - A Q4_0 quantized gguf format conversion of Deepreneur's blue-lizard 7B model for use with llama.cpp, licensed under the llama2 license.
  - Downloads: 266
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - A fine-tuned Whisper large-v2 model on Japanese CommonVoice v11 for 5000 steps, achieving Loss: 0.4200 and Wer: 0.7449.
  - Downloads: 213
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - A GitHub repository containing a Japanese Whisper model converted from clu-ling/whisper-large-v2-japanese-5k-steps for audio transcription using CTranslate2.
  - Downloads: 213
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - SakanaAI's EvoLLM-JP-A-v1-7B gguf base model for AI linguistic processing, compatible with llama.cpp.
  - Downloads: 193
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - This GitHub repository contains a 12-layer ELECTRA Small model pretrained on 354 million sentences from the YACIS blog corpus using MeCab tokenization and WordPiece subword tokenization.
  - Downloads: 142
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmark„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãstockmark-100b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 137
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow model, derived from Llama 2 and fine-tuned with Japanese data, includes instruction-tuned versions released on April 26, 2024.
  - Downloads: 134
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 133
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID ÊñôÁêÜ„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„ÅÆË≥™ÂïèÊñá„Åã„Çâ„ÄÅÊ§úÁ¥¢Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ„Åß„ÅÇ„ÇãÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫„Åó„Åæ„Åô Model Details Model Description ‰æã„Åà„Å∞„ÄÅ„ÄåÊù±‰∫¨„ÅÆËÇâÊñôÁêÜ„Åß„ÄÅÊò•„Å´È£ü„Åπ„Çâ„Çå„Çã„ÄÅÈ∂èËÇâ„Çí‰Ωø„Å£„ÅüÊñôÁêÜ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊñáÁ´†„ÇíÂÖ•Âäõ„Åô„Çã„Å®„ÄÅ „ÄåÊù±‰∫¨ ‚Üí ÈÉΩÈÅìÂ∫úÁúå/Âú∞Êñπ(AREA)„Äç „ÄåËÇâÊñôÁêÜ ‚Üí Á®ÆÈ°û(TYPE)„Äç „ÄåÊò• ‚Üí Â≠£ÁØÄ(SZN)
  - Downloads: 127
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B-parameter Japanese chat model derived from "chatntq-ja-7b-v1.0" and Mistral-7B-v0.1.
  - Downloads: 125
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository hosts a retrained Japanese TTS model based on parler-tts/parler-tts-large-v1, offering high-quality audio with a lightweight architecture, but notes that it requires a custom tokenizer and is currently in beta.
  - Downloads: 123
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 99
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 93
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakuten„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãRakutenAI-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 85
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - Weighted/imatrix quantized models and GGUF files for Japanese Starling-ChatV-7B, including Q2_K and Q3_K_S variants.
  - Downloads: 76
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - A fine-tuned Reward model for evaluating the quality of Japanese novels using regression on user evaluations, intended for applications like novel generation but noting potential biases.
  - Downloads: 73
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - A 12B parameter Mixture of Experts model combining four sarashina2.2-3Bx4 models for advanced text generation tasks.
  - Downloads: 57
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps model to CTranslate2 format for use with faster-whisper or other CTranslate2-based projects.
  - Downloads: 54
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 54
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - The repository contains GGUF conversions for the ChatNTQ-JA-7b-v1.0 Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b.
  - Downloads: 44
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - A Bloom model trained on a Japanese corpus with vocab_size=10000, hidden_size=hidden_size, n_head=8, and n_layer=12.
  - Downloads: 43
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - Weighted/imatrix quantized static models of Japanese Llama-3 8B instruct-v2 are available in GGUF format, with Q2_K quantization providing the smallest file size at 3 GB.
  - Downloads: 40
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiÊßò„ÅÆ Japanese-Chat-Umievo-itr004-7b „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 38
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - „ÅØ„Åò„ÇÅ„Å´ „Å™„Çì„ÅãÊó•Êú¨Ë™û„ÅåË©±„Åõ„ÇãÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™AI„Åß„Åô„ÄÇ
  - Downloads: 35
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This repository contains a specialized model trained on SakanaAI/TinySwallow-1.5B-Instruct to generate highÊ°•ÊñπÊ≥ïÈ£éÊ†ºÁöÑÂπªÁÅØÁâáÊñáÊú¨ÔºåÊØèÂº†ÂπªÁÅØÁâáÂåÖÂê´ÁÆÄÊ¥ÅÁöÑÂÖ≥ÈîÆÁü≠ËØ≠ÔºåÈÄÇÂêà‰ΩúÂè£Â§¥ËØ¥ÊòéÊëòË¶Å„ÄÇ
  - Downloads: 31
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - A q4_0 quantized format conversion of Tanuki-ZeRo's gguf base model for language processing, compatible with llama.cpp.
  - Downloads: 27
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - calm3-22b-RP-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version „Åæ„Åü„ÄÅ„Åì„Å°„Çâ„ÅßÊú¨„É¢„Éá„É´„ÅÆ„Éá„É¢„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 25
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - The repository contains a Japanese-extended Mixtral-8x7B model for continued pretrained learning, along with instructions and code snippet for usage.
  - Downloads: 23
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.2 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAE„ÅÆÂÜÖËáì„ÅØ„Å™„ÅÑ„ÅûÔºÅ„Å®Ë®Ä„Çè„Åõ„Å™„ÅÑ„ÅûÔºÅÔºÅÔºÅÔºÅ
  - Downloads: 21
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - The repository challenges NER using modernBERT with Japanese labels for names, organizations, locations, and more.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B „Äå„Å©„ÅÜ„Åã„ÅäÊÖàÊÇ≤„Çí „ÇÇ„ÅÜ Áñ≤„ÇåÊûú„Å¶„Åæ„Åó„Åü„Äç ÁîüÊàê‰æã [Â§™Â≠ó‰ª•Èôç„ÅåAIÁîüÊàê] „Äå„Å©„ÅÜ„Åã„Äç ‚Äù„Åù„Çå‚Äù„ÅØÊááÈ°ò„Åó„Åü„ÄÇ
  - Downloads: 17
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - This repository contains a customized 32B Japanese language model with unique fine-tuning, derived from Qwen and further refined by Cyber Agent and Data Pilot.
  - Downloads: 16
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - A model for generating article bodies from titles.
  - Downloads: 15
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares occasionally merged models that deviate from intended purposes, inspired by other models like lametta_old, lametta_v1921, and others, without being direct copies.
  - Downloads: 15
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteus„Çí„Éô„Éº„Çπ„Å´LLava„Å´ÂØæÂøú„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - The repository provides Japanese content using AutoTokenizer, AutoModelForCausalLM with Unifine format, including in-context and instruction learning examples for musical album evaluations.
  - Downloads: 13
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - The repository hosts a model variant of AfterRealXL with beta2 improvements, licensed under CreativeML Open RAIL++-M, and includes merge checkpoint references.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - A Japanese SentencePiece tokenizer trained for AI Novelist's SuperTrin and Damsel 20B models with a vocabulary size of 52000.
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - A commercially usable, lightweight Japanese version of Google's Gemma-2B model derived through continued pre-training, suitable for mobile devices but with limitations in instruction tuning.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - A model for reasoning with famous quotes.
  - Downloads: 12
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: Âº∑ÂåñÂ≠¶Áøí„ÇíÁî®„ÅÑ„Å¶„Ç≠„É£„É©„ÇØ„Çø„Çâ„Åó„Åï„Çí‰ªò‰∏é„Åó„ÅüÈõëË´áÂøúÁ≠î„ÅÆÁîüÊàê
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - A translation model using Marian-NMT for Japanese to English translation, utilizing transformers and sentencepiece.
  - Downloads: 54,305
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is a translation model using Marian-NMT for English to Japanese translation, utilizing transformers and sentencepiece.
  - Downloads: 54,282
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - A fine-tuned Japanese version of the Gamma 7B model achieved good results on JA MT-Bench.
  - Downloads: 11,479
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a compact 2B-parameter SLM optimized for Japanese-English and English-Japanese translation with near-7B-parameter model quality.
  - Downloads: 5,898
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source, multilinguallarge language model trained by OrionStarAI, based on a 2.5T multilingual corpus.
  - Downloads: 5,326
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - A 36-layer, 2816-hidden-size GPT-NeoX model trained on 524B tokens for English-Japanese translation, available on Hugging Face.
  - Downloads: 5,216
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - A gguf-formatted conversion of the Suzume-Llama-3-8B Japanese model by lightblue, using imatrix dataset.
  - Downloads: 3,005
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - YOLO11 is a fast, accurate, and flexible SOTA model for object detection, tracking, instance segmentation, image classification, and pose estimation.
  - Downloads: 2,393
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a bilingual Japanese and English chat model with enhanced Japanese performance and robust English capabilities, trained on synthetic data and featuring a highly efficient tokenizer.
  - Downloads: 2,162
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - The FINGU-AI/FinguAI-Chat-v1 model offers specialized curriculum for English, Korean, and Japanese speakers focusing on finance, investment, and legal frameworks to enhance language proficiency and provide insights into global financial markets and regulations.
  - Downloads: 2,037
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsuzume-llama-3-8B-multilingual„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,495
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin-inst-merge„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,273
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-MS-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,038
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI, based on a 2.5T multilingual corpus.
  - Downloads: 859
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 646
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - A GGUF version of the ascktgcc/Mistral-nemo-ja-rp-v0.2 model.
  - Downloads: 529
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 529
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus.
  - Downloads: 315
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - A gguf conversion of rinna's Japanese GPT-Neox-3.6B instruction-PPO model for use with llama.cpp, part of a series of related models.
  - Downloads: 308
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - A Japanese to Korean translation model using EncoderDecoderModel based on bert-japanese and kogpt2, with inference code provided.
  - Downloads: 277
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, Japanese, and English.
  - Downloads: 243
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a DPO-trained Japanese version of Qwen2.5-7B-Instruct that outperforms the base model on multilingual chat benchmarks and is suitable for general conversational AI applications.
  - Downloads: 237
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source, multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, English, and Japanese.
  - Downloads: 162
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus, including Chinese, English, and Japanese.
  - Downloads: 153
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - The repository includes a model licensed under CreativeML Open RAIL-M with an added copyright for sazyou_roukaku, while emphasizing strict usage limitations and the provider's disclaimers.
  - Downloads: 133
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT ElanMT-BT-ja-en is a Japanese to English translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 132
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - A Japanese-English machine translation model fine-tuned for Weiss Schwarz card text, with a Gradio app available on Hugging Face Spaces.
  - Downloads: 129
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
  - Downloads: 125
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - A pretrained ByT5-small model fine-tuned for translating Ainu to Japanese.
  - Downloads: 116
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - A doc2query model based on mT5 for generating 20-40 queries per paragraph to improve lexical search through synonym expansion and word weighting.
  - Downloads: 115
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - A Japanese CLIP model trained on a translated subset of ReLAION-5B with 248M parameters, using OpenCLIP, and can be installed via pip for zero-shot image classification.
  - Downloads: 101
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling 7B is a multilingual language model pretrained on Korean, English, Chinese, Japanese, and additional 500 languages.
  - Downloads: 91
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - A long-context Japanese-English translation model based on tinyllama, requiring inputs of 500-1000 tokens and setting 'do_sample = False' or temperature to 0 for deterministic outputs.
  - Downloads: 90
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP Ê¶ÇË¶Å Local-Novel-LLM-project/Ninja-v1-NSFW„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 71
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides Japanese-to-Malay translation models using TransformerAlign, including pre-processing with normalization and SentencePiece, and offers original weights and test set translations.
  - Downloads: 70
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - SakuraMixSeries ËÉåÊôØ„Å®„Ç≠„É£„É©„ÇØ„Çø„Éº„ÇØ„Ç™„É™„ÉÜ„Ç£„Éº„Çí‰∏°Á´ã„Åï„Åõ„ÅüVAEÂÜÖËîµÂûã„É¢„Éá„É´ Model with built-in VAE for both background and character quality üìÑ „É©„Ç§„Çª„É≥„Çπ / License ‰øÆÊ≠£ CreativeML OpenRAIL-M „É©„Ç§„Çª„É≥„Çπ / Modified CreativeML OpenRAIL-M license „Åì„ÅÆ„É¢„Éá„É´„ÅÆ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂÖ•„Çå„Åö„Å´‰ΩøÁî®„Åô„Çã Use the model without crediting the creator „Åì„ÅÆ„É¢„Éá„É´„ÅßÁîüÊàê„Åó„ÅüÁîªÂÉè„ÇíÂïÜÁî®Âà©Áî®„Åô„Çã Sell images they generate „Åì„ÅÆ„É¢„Éá„É´„ÇíÂïÜÁî®„ÅÆÁîªÂÉèÁîüÊàê„Çµ„Éº„Éì„Çπ„ÅßÂà©Áî®„Åô„Çã Run on services that generate images for money „Åì„ÅÆ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åô„Çã Share merges using this model „Åì„ÅÆ„É¢„Éá„É´„ÄÅ„Åæ„Åü„ÅØ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éû„Éº„Ç∏„Åó„Åü„É¢„Éá„É´„ÇíË≤©Â£≤„Åô„Çã Sell this model or merges using this model „Åì„ÅÆ„É¢„Éá
  - Downloads: 65
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - A 3.8BÂèÇÊï∞ÁöÑEnglish-JapaneseÂèåËØ≠GPT-NeoXÊ®°ÂûãÔºå‰∏ä‰∏ãÊñáÈïøÂ∫¶Êâ©Â±ïËá≥8192ÔºåÂÖºÂÆπtransformers>=4.31.0„ÄÇ
  - Downloads: 51
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - A 1.3B parameter NLLB model fine-tuned for Japanese to English light novel translation, capable of processing up to 512 tokens.
  - Downloads: 47
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a translation model using Marian-NMT for translating source languages de, en, es, fr, it, ru, uk into Japanese.
  - Downloads: 47
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - A fine-tuned Japanese-vocabulary-added Llama2-13b model for limerick generation, pre-trained with Continual Pre-Training on AWS Trainium instances and fine-tuned with limeric data, licensed under LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 44
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - LEIA-Swallow-7B enhances cross-lingual knowledge transfer, improving a Japanese-English LLM's performance on six Japanese QA benchmarks.
  - Downloads: 43
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - A Japanese-refined version of the SD-XL 1.0 model, fine-tuned to integrate OpenCLIP-ViT/G and CLIP-ViT/L text encoders for Japanese input, using aligned English-Japanese translation data.
  - Downloads: 42
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - The Leia-Swallow-13B model improves non-English language performance through LEIA training applied to a Japanese-English bilingual LLM based on LLaMA 2.
  - Downloads: 39
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 39
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - The GitHub repository for Orion-14B includes model introductions, downloads, benchmarks, and inference tools for open-source multilingual large language models trained on a 2.5T corpus.
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - A fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en using bsd_ja_en data.
  - Downloads: 31
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B series models are open-source, multilingual large language models trained by OrionStarAI from a 2.5T multilingual corpus.
  - Downloads: 28
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - A BERT-VITS2 Japanese model trained on the jvnv corpus F2 data.
  - Downloads: 20
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - The repository contains the Japanese-Alpaca-2-13B model in GGUF format, accessible via the provided URL.
  - Downloads: 19
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - The repository contains Superswallow-70b-v0.1 with two known bugs affecting performance, which may explain its lower benchmark scores compared to Swallow.
  - Downloads: 19
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba Barba is a multilingual natural language inference model based on XLM-RoBERTa, trained on GLUE, CLUE, JGLUE, KLUE, and private datasets, serving textual entailment and zero-shot text classification through TensorFlow Serving.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This GitHub repository contains a quantized model combining qwen-14b-vndl and Qwen1.5-14B-Chat for translating Japanese text into Chinese.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - A merged MoE model using Llama-2 as the base for instruction-tuned Japanese language processing, combining elyza/ELYZA-japanese-Llama-2-7b-fast and elyza/ELYZA-japanese-Llama-2-7b-fast-instruct.
  - Downloads: 18
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source multilingual large language model trained by OrionStarAI from a 2.5T multilingual corpus including Chinese, English, Japanese.
  - Downloads: 17
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - A MoE model created using mergekit from instruction-tuned and base Japanese Llama-2 models, licensed under the Llama 2 Community License.
  - Downloads: 17
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - A T5 fine-tuned model trained on the friendly_JA Corpus to simplify Japanese for occidental speakers, replacing standard lexicon with Latin/English-derived katakana terms.
  - Downloads: 16
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - A fine-tuned MPT-7B-base model evaluated on Jumtra/test_data_100QA, using the llm-foundry from mosaicml, with evaluation results and usageÊ≥®ÊÑè‰∫ãÈ°π provided.
  - Downloads: 16
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - A MoE model created using mergekit from Llama-2-based tokyotech-llm/Swallow-13b-instruct-hf and nitky/Superswallow-13b-v0.2 models, with relevant licensing information included.
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - A MoE merged model based on instruction-tuned Llama-2 Japanese models, created using mergekit.
  - Downloads: 16
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - The repository includes base and LoRA models for Japanese LLaMA 2 and Japanese Alpaca 2, all in the 13B parameter size.
  - Downloads: 16
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 16
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English general-purpose chat model with enhanced Japanese performance and robust English capabilities, trained on synthetic data and featuring a more efficient Japanese tokenizer.
  - Downloads: 14
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - A Japanese-finetuned version of the SD-XL 1.0 base model, using text encoder fine-tuning with English-to-Japanese aligned data.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model based on Mistral 7B, featuring enhanced Japanese performance and robust English capabilities through synthetic data and a customized tokenizer.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model, optimized for strong Japanese performance and retaining robust English capabilities, using synthetic data and an enhanced Japanese tokenizer.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model with enhanced Japanese performance and retained English capabilities, pre-trained on 8B Japanese tokens and optimized with a more efficient tokenizer.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ"chatntq-ja-7b-v1.0"„Çí„Éô„Éº„Çπ„Å´„Åó„Åü7B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 13
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - A compiled Watashiha-Llama-2-13B-Ogiri-sft model optimized for AWS inf2 instances, requiring at least 256GB storage and Deep Learning AMI Neuron PyTorch 1.13 (Ubuntu 20.04) for deployment.
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - The repository contains full and LoRA models of the Japanese-Alpaca-2-13B instruction-following model based on Japanese-LLaMA-2-13B.
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese and English chat model optimized for strong Japanese performance and robust English capabilities, built on Mistral 7B with a custom tokenizer.
  - Downloads: 12
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source, multilingual large language model trained by OrionStarAI, based on a 2.5T multilingual corpus including Chinese, English, and Japanese.
  - Downloads: 11
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - The repository includes base and full models of Japanese-LLaMA-2-7B, as well as a LoRA model.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - A BERT-VITS2 Japanese model trained on the jvnv corpus F2 data.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - A T5 model pretrained on a 100GB Japanese corpus including Wikipedia, OSCAR, and CC-100 data, requiring fine-tuning for specific tasks.
  - Downloads: 7,120
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - A gguf-format conversion of sarashina2.2-3b-instruct-v0.1 for use with ggerganov's llama.cpp toolkit.
  - Downloads: 3,503
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitions' Japanese language model sarashina2.2-0.5B-instruct-v0.1, evaluated in Japanese and English tasks, outperforms several other models in terms of performance metrics.
  - Downloads: 3,214
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - The repository includes a model license update to CreativeML Open RAIL-M with an additional copyright for sazyou_roukaku (now ‰ΩêÂüéÈÉéÁîª), and notes that the creator disclaims liability for generated content except as specified in the license. Version v2Âèñ‰ª£‰∫Üv1Âπ∂Â∑≤Âà†Èô§ÊóßÁâàÊú¨„ÄÇ
  - Downloads: 2,739
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - The repository uses neody/imatrix_dataset from fineweb-edu (English) and fineweb-2 (Japanese), expecting higher precision than traditional datasets like wikitext2 or cc100, specifically for higher bits.
  - Downloads: 1,948
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - aya-23-8B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,932
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual includes distilled Whisper models for Japanese and English speech-to-text translation, developed through collaboration between Asahi Ushio and Kotoba Technologies.
  - Downloads: 1,081
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository hosts a retrained Japanese Parler-TTS Mini model for text-to-speech conversion, providing high-quality audio output with compatibility notes.
  - Downloads: 996
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This fine-tuned Japanese Hubert large ASR model predicts Hiragana after initial tuning on reazonspeech(small) and subsequent tuning on common_voice_11_0, inspired by vumichien/wav2vec2-large-xlsr-japanese-hiragana.
  - Downloads: 825
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - A gguf-formatted version of the qwen2.5-bakeneko-32b-instruct-v2 model, created from imatrix data using the TFMC/imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 809
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - A gguf version of Qwen/Qwen2.5-3B-Instruct quantized with a Japanese iMatrix for summarizing long texts beyond 32K tokens while maintaining substantial Japanese support.
  - Downloads: 797
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - Static quantized weights and IMatrix quants for the Mistral-Nemo-Japanese-Instruct-2408 model are provided, with usage instructions for GGUF files included.
  - Downloads: 795
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 725
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - A T5 v1.1 model pretrained on approximately 100GB of Japanese corpora including Wikipedia and OSCAR, requiring fine-tuning for specific tasks and potentially biased outputs.
  - Downloads: 721
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-4k-instruct-dpo„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 664
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 602
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This GitHub repository hosts a Œ≤Áâà retrained Japanese TTS model based on parole-tts/parler-tts-mini-v1, offering high-quality lightweight text-to-speech functionality with a unique tokenizer.
  - Downloads: 561
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - A fine-tuned MT5-small model for Japanese summarization trained on BBC news articles, using the headline sentence as summary and the rest of the article as input.
  - Downloads: 559
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - A gguf-formatted conversion of ABEJA-Qwen2.5-32b-Japanese-v0.1 for‰ΩøÁî®Êåá‰ª§Âä©ÊâãÁöÑËã±ÊñáÊÄªÁªìËÉΩÂäõÔºå‰∏äËø∞‰ªìÂ∫ìÊòØ‰∏Ä‰∏™Â∞ÜABEJA-Qwen2.5-32b-Japanese-v0.1ËΩ¨Êç¢‰∏∫ggufÊ†ºÂºèÁöÑÁâàÊú¨ÔºåÂπ∂ÂåÖÂê´Âú®ÂÖ∂‰∏äËøêË°åÊâÄÈúÄÁöÑÂëΩ‰ª§„ÄÇ
  - Downloads: 479
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - A converted gguf format model based on sarashina2.1-1b-sft, created using imatrix data from TFMC/imatrix-dataset-for-japanese-llm, for use with ggerganov's llama.cpp.
  - Downloads: 464
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides a GGUF-formatted version of Rakuten/RakutenAI-2.0-mini-instruct for use with tools like llama.cpp and text-generation-webui.
  - Downloads: 414
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - A gguf-formatted version of Qwen's QwQ-32B-Preview model, created from imatrix data using the Japanese LLM dataset, for use with llama.cpp.
  - Downloads: 378
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository includes quantized GGUF versions of the merged VNTL LLaMA 3 8B qlora model, featuring a chat mode for Japanese grammar questions and translation prompts.
  - Downloads: 330
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - The repository includes GGUF quantizations of the VNTL Gemma 2 27B model, featuring a chat mode for Japanese grammar questions, and provides translation prompts.
  - Downloads: 227
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãKarasu-Mixtral-8x22B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 211
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„ÇíÊó•Êú¨Ë™ûinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 139
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - A ByT5 model pretrained on approximately 100GB of Japanese corpora, including Wikipedia and OSCAR datasets, requiring fine-tuning for specific tasks and potentially subject to biased outputs.
  - Downloads: 125
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - Quantized GGUF version of the Aratako/c4ai-command-r-v01-japanese-instruct model.
  - Downloads: 100
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 91
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - The repository contains a fine-tuned GPT-2 model on the ATOMIC ja dataset using causal language modeling objective, enabling text generation with reproducibility through a set seed.
  - Downloads: 87
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - A fine-tuned Japanese ASR model on uniTKU data that predicts Hiragana, achieving improved WER with training, though limited to Hiragana prediction.
  - Downloads: 79
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository contains a fused language model based on FusO1-DeepSeekR1-QwQ-SkyT1-Flash for code generation, specifically including parameters for generating a Python FizzBuzz program with a temperature of 0.7 and a maximum of 32768 tokens.
  - Downloads: 76
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - A model for generating questions from answers and paragraphs, trained on cleaned Japanese translations of SQuAD 1.1 data using a T5 model with specific hyperparameters.
  - Downloads: 73
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - A model for generating titles from article text. See: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
  - Downloads: 68
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8b„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂öÂ≠¶Áøí„É¢„Éá„É´„Å´ChatVector„ÇíÈÅ©Áî®„Åó„ÄÅ„Åï„Çâ„Å´QLora„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - A fine-tuned DeepSeek-V3 model for Japanese text generation using top 64 experts per layer selected from the original 256.
  - Downloads: 61
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - The repository contains a model licensed under a modified CreativeML OpenRAIL-M license, prohibiting non-credited use, commercial sale of generated images or services using the model, and sharing merged models with altered permissions.
  - Downloads: 61
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "ÂΩ±" in Japanese or "Shadow" in English.
  - Downloads: 55
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a finetuned Japanese storytelling model based on EleutherAI's GPT-J 6B, trained on web novels with specific hyperparameters.
  - Downloads: 54
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - Full instruction tuning was performed on the base model of line-corporation/japanese-large-lm-1.7B via sft.
  - Downloads: 50
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - A seq2seq Japanese ASR model fine-tuned from distil-whisper/distil-large-v2 for transcribing visual novel audio.
  - Downloads: 45
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - A finetuned T5 model on ATOMIC data for text-to-text generation, compatible with the Hugging Face pipeline and reproducible with a seed.
  - Downloads: 34
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest „Åì„ÅÆ„É¢„Éá„É´„ÅØÁîüÁâ©Â≠¶„ÉªÂåªÂ≠¶„Å´Á≤æÈÄö„Åó„ÅüOpenBioLLM-8B„Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™ûÂØæÂøú„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´Llama-3-youko-8b-instruct-chatvector„Å®„Éû„Éº„Ç∏„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - COMET-GPT2 ja v2 is a finetuned GPT-2 xl model on the large ATOMIC ja dataset for causal language modeling.
  - Downloads: 26
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - A T5 model pretrained on balanced English and Japanese corpora, including Wikipedia dumps and OSCAR datasets, requiring fine-tuning for specific tasks and potentially biased outputs.
  - Downloads: 22
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri-sft is a LLaVA-trained dialogue model with vision encoder CLIP-ViT-B-32, fine-tuned on STAIR Captions and Japanese Visual Genome VQA data, licensed under LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 21
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository contains a fine-tuned version of mosaicml/mpt-7b-instruct using the MosaicML llm-foundry, evaluated on Jumtra/test_data_100QA with a 46/100 accuracy rate.
  - Downloads: 20
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - An alpha version of a writer's assistant AI fine-tuned on Cyberagent's calm2-7b-chat, trained on approximately 150 million tokens of novel text, generating continuations from given prompts in Japanese.
  - Downloads: 16
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - A machine learning framework for detecting gender from Japanese names, complete with ROMAJI input and labeled outputs.
  - Downloads: 16
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - The repository uses a modified CreativeML OpenRAIL-M license allowing non-commercial use and generation of images, but restricting credit-free, commercial, service-based, and shared/market models.
  - Downloads: 14
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - A 7B-parameter Japanese language model fine-tuned on preference datasets using the notus codebase, reproduced from Japanese Stable LM Instruct Gamma 7B.
  - Downloads: 14
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 14
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - A QLoRA fine-tuned LLaMA2-7B model trained on the Guanaco dataset with 49,000 chat samples, improved for Chinese and Japanese, using specific parameters for testing.
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - A 7B-parameter Japanese language model fine-tuned on preference datasets using STF, trained with a machine-translated Ultrafeedback dataset and DPO training method.
  - Downloads: 12
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR uses a Vision Encoder Decoder framework for high-quality Japanese text recognition in mangas, including various fonts and low-quality images.
  - Downloads: 96,246
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - A gguf version of DeepSeek-V3-slice-jp64, a model derived from DeepSeek-V3 with selected Japanese MoE experts, licensed under terms consistent with the original model.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - This repository provides a Japanese CLIP (Contrastive Language-Image Pre-Training) model for image and text matching tasks, including installation instructions and a usage example.
  - Downloads: 31,868
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - reazonspeech-nemo-v2 is an improved Conformer-based speech recognition model trained on ReazonSpeech v2.0 for long-form Japanese audio inference.
  - Downloads: 24,411
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - LY Corporation's clip-japanese-base is a 1B-web-collected image-text paired CLIP model for tasks like zero-shot classification and multimedia retrieval.
  - Downloads: 11,063
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is an enhanced Japanese ASR model that integrates additional postprocessing for punctuation, built on kotoba-tech/kotoba-whisper-v2.0.
  - Downloads: 4,020
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - A gguf-formatted version of Qwen's QwQ-32B model, created from imatrix data using the TFMC/imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 3,575
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - A gguf-formatted conversion of umiyuki's Umievo-itr012-Gleipnir-7B model, created from imatrix-dataset-for-japanese-llm, usable with llama.cpp.
  - Downloads: 3,079
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a speech foundation model offering ASR, LID, SER, and AED capabilities.
  - Downloads: 1,136
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - A gguf-format conversion of the Mistral-Nemo-Japanese-Instruct-2408 model by cyberagent, using imatrix data from TFMC/imatrix-dataset-for-japanese-llm.
  - Downloads: 1,062
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-scoring Japanese vision-language model trained by SB Intuitions, achieving top rankings in four benchmarks.
  - Downloads: 866
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-medium-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 669
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - parakeet-tdt_ctc-0.6b-ja is an XL Hybrid FastConformer ASR model for Japanese speech transcription with punctuation developed by NVIDIA NeMo team.
  - Downloads: 631
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - A Japanese-language version of a CLIP text/image encoder model derived via distillation from the English version, accompanied by samples and documentation for its use in multimodal processing tasks.
  - Downloads: 613
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is an enhanced Japanese ASR model that integrates punctuation postprocessing, merging additional features from kotoba-tech/kotoba-whisper-v1.0 into a seamless pipeline.
  - Downloads: 598
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a top-performing Japanese vision language model, achieving highest scores in four benchmarks, based on Sarashina2-13B and Qwen2-VL-7B's image encoder, with detailed installation and inference instructions provided.
  - Downloads: 444
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a vision-language model for conversing about images, trained with the heron library.
  - Downloads: 424
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - A fine-tuned Japanese-Hubert-base model for ASR tasks that predicts only Hiragana, trained on common_voice_11_0, achieving improved WER with training iteration.
  - Downloads: 343
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - A fine-tuned wav2vec2-base model for Japanese Hiragana recognition, trained on Common Voice dataset, achieving improved performance metrics.
  - Downloads: 307
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR uses Vision Encoder Decoder to recognize Japanese text in manga, addressing unique challenges like furigana and low-quality images.
  - Downloads: 271
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - A fine-tuned Donut base model on a visual novel-like synthetic dataset for recognizing text in images from oshizo/donut-base-japanese-visual-novel.
  - Downloads: 211
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - A model for optical character recognition specifically tailored for Japanese manga text.
  - Downloads: 201
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - The GitHub repository contains a fine-tuned SpeechT5 model for Japanese text-to-speech synthesis using the JVS dataset, featuring 100 speakers and 16-dimensional speaker embeddings.
  - Downloads: 198
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - A fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese, using Common Voice and JSUT corpora, requiring 16kHz sampled speech input.
  - Downloads: 192
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR utilizes the Vision Encoder Decoder framework for high-quality Japanese text recognition in manga, including various text orientations and low-quality images.
  - Downloads: 189
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteus„ÅÆGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 179
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - A Japanese data2vec Audio Base model with 12 transformer layers, trained on about 19,000 hours of Japanese audio data by rinna Co., Ltd.
  - Downloads: 172
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a vision-language model trained by fine-tuning llm-jp/llm-jp-1.3b-v1.0 with LLaVA method, using Vision Projector and Japanese Visual Genome for conversing about input images.
  - Downloads: 166
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - Recruit Co., Ltd. has released a pre-trained Japanese CLIP model and evaluation dataset for tasks like zero-shot image classification and text-image retrieval.
  - Downloads: 152
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - A fine-tuned Wav2Vec2-large-XLSR-53 model for {language} on Common Voice and other datasets, suitable for speech input sampled at 16kHz.
  - Downloads: 137
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - A fine-tuned Whisper-large-v3 model for Japanese on Common Voice 16.1, trained for 4000 steps, resulting in overfitting and poorer WER despite lower loss.
  - Downloads: 133
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - A Japanese CLIP model for contrastive language-image pre-training, capable of multimodal tasks with ViT-H/14 architecture.
  - Downloads: 128
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a text-to-speech model trained on 300k hours of multilingual audio data, with demos available at Fish Audio.
  - Downloads: 126
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - A fine-tuned wav2vec2 model for Japanese speech recognition, converting Kanji to Hiragana during training and evaluating with CER, achieving 23.64% CER on the Common Voice dataset.
  - Downloads: 126
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - A Contrastive Language-Image Pre-trained Model for Japanese text and image embedding with capabilities in zero-shot image classification.
  - Downloads: 126
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - A Japanese CLIP model for contrastive language-image pre-training, suitable for multimodal tasks likeÈõ∂Ê†∑Êú¨ÂõæÂÉèÂàÜÁ±ªÔºåÈááÁî®ViT-H/14Êû∂ÊûÑ„ÄÇ
  - Downloads: 126
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Assistance „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 122
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - A demo and documentation for the Heron BLIP Japanese StableLM Base 7B model, a vision-language model that can converse about images.
  - Downloads: 110
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - A fine-tuned wav2vec2 model for transcribing audio into Hiragana, achieving a loss of 0.7751 and CER of 0.2227 on the evaluation set.
  - Downloads: 110
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - A Whisper model for transcribing Japanese speech to Katakana with pitch accents, fine-tuned on a subset of Galgame-Speech and jsut-5000 datasets.
  - Downloads: 106
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - Fine-tuned Wav2Vec2-XLS-R-300M on Japanese Hiragana characters using multiple datasets with 16kHz sampled audio, achieving a CER of 9.34%.
  - Downloads: 89
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - A fine-tuned XLSR-53 model for Japanese phone-call speaker diarization, usable via Python with audio processing libraries.
  - Downloads: 81
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - Fine-tuned Wav2Vec2-Large-XLSR-53 model on Japanese using Common Voice and JSUT corpora, intended for 16kHz speech input.
  - Downloads: 77
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR Phonemizer This model is a Phoneme Level Speech Recognition network, originally a fine-tuned version of openai/whisper-large-v3 on a mixture of Different Japanese datasets.
  - Downloads: 75
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository provides the whisper-large-v2-mix-jp model converted to CTranslate2 format for use in projects like faster-whisper.
  - Downloads: 64
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - reazonspeech-espnet-next is a repository for the latest ASR models trained by the ReazonSpeech team, designed to rapidly share cutting-edge research and incorporate community feedback.
  - Downloads: 50
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a vision-language model trained with the Heron library for image captioning and conversation, accessible via PyTorch imports.
  - Downloads: 48
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - A Japanese-specific stable diffusion model for generating Pokemon images from text input.
  - Downloads: 35
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository offers free, unapologetic, childish voice generation for both commercial and non-commercial use, with versions including sweet, English, cool, and Chinese.
  - Downloads: 31
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - A fine-tuned wav2vec2 model on Japanese datasets including Common Voice 7.0, JSUT, JSSS, and CSS10 with a total ofÁ∫¶60Â∞èÊó∂ËÆ≠ÁªÉÊï∞ÊçÆÔºåWERÊïàÊûúÂæÖË°•ÂÖÖ„ÄÇ
  - Downloads: 30
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B Transformer-based model supporting fluent Japanese text-to-speech and one-shot voice cloning, with inference code derived from MetaVoice.
  - Downloads: 27
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - A Japanese-pretrained VL-T5 model for unifying vision-and-language tasks via text generation.
  - Downloads: 24
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - A Japanese vision-language model based on LLaVA architecture with 1.3B parameters and a context length of 1024 tokens, using a ConvNeXt Large vision encoder.
  - Downloads: 21
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository converts the whisper-large-v2-jp model to CTranslate2 format for use in projects like faster-whisper.
  - Downloads: 19
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - A VITS-TTS model fine-tuned with Japanese voice data from Elite35P-Server for Sakura Miko, developed by Lycoris52.
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO Stable Diffusion is a Japanese-specific diffusion model for generating photo-realistic images from text input.
  - Downloads: 19
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - A fine-tuned Japanese Whisper-tiny model trained on Common Voice data for real-timeAutomatic Speech Recognition with evaluation results showing loss of 0.780524 and WER of 301.625840.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - A voice cloning model for Style Bert VITS2 that supports English, Japanese, and Chinese text-to-speech with a young, neutral voice suitable for various applications.
  - Downloads: 16
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - A fine-tuned Japanese Whisper-tiny model trained on Common Voice data for real-time JapaneseAutomatic Speech Recognition with a loss of 0.549100 and WER of 225.233037.
  - Downloads: 14
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - reazonspeech-espnet-v1 is an ESPnet ASR model trained on 15,000 hours of ReazonSpeech corpus for Japanese speech recognition, requiring 16kHz sampling rate for input audio.
  - Downloads: 14
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - A finetuned VITS TTS model for Japanese using free voice data from amitaro, developed by Lycoris52 and based on Plachtaa's fine-tuning code.
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR uses a Vision Encoder Decoder framework for high-quality printed Japanese text recognition, especially suited for manga with various fonts and low-quality images.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - A fine-tuned Whisper model for Japanese using the SVJ dataset and Common Voice 11.0, achieving specific loss and CER metrics.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
### Natural Language Interfaces
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - The repository contains static quantized models for Japanese language processing but may lack weighted/imatrix quants, which can be requested through Community Discussion if needed. Usage instructions for GGUF files are available in TheBloke's READMEs.
  - Downloads: 1,384
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KUJIRA„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,111
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-RobinHood„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 672
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built on the Moshi model, trained with additionalJapanese dialogue data for natural turn-taking in real-time conversations.
  - Downloads: 601
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - Key quants for the DeepSeek-R1-Distill-Qwen-14B-Japanese model are static and available, with imatrix versions pending; users can request them or find usage details in TheBloke's READMEs.
  - Downloads: 183
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - A GitHub repository containing GRPO-trained code to solve simple arithmetic problems, using self-created datasets.
  - Downloads: 119
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - A Japanese-tuned CohereForAI model using ichikara-instruction, trained on A6000x4 GPUs for 10 epochs with specific LoRA parameters, evaluated on jsquad and jcommonsenseqa datasets.
  - Downloads: 117
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-KUJIRA „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 80
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - A fine-tuned Luke Japanese base lite model for Question-Answering tasks using the DDQA dataset with an accuracy of 0.845933 in strict matching.
  - Downloads: 72
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - A Japanese DialoGPT Small model trained on dialogue extracted from Aozora Bunko public domain books, optimized for limited GPU memory.
  - Downloads: 68
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-RobinHood „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 50
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - This repository contains a modified pre-trained language model designed to role-play as a Japanese person and improve multi-turn conversation skills while reducing literal translations.
  - Downloads: 38
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - A language model generated from fine-tuning Qwen2.5-7B-Instruct on datasets including user-created questions and thoughts produced by allura-org/Qwen2.5-32b-RP-Ink and AXCXEPT/EZO-Qwen2.5-32B-Instruct, requiring Chain-of-Thought responses in specified tags.
  - Downloads: 22
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - A fine-tuned LUKEmodel for Question-Answering based on luke-japanese-large-lite and DDQA dataset, achieving an accuracy of 0.863 in strict matching.
  - Downloads: 21
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - A fine-tuned DeBERTa-V2-Japanese model for QA tasks, trained on the DDQA dataset and usable for SQuAD tasks.
  - Downloads: 21
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - A Japanese-capable model, Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, merges Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1 differences with Swallow-MX-8x7b-NVE-v0.1, extending context length to 32K while enhancing instruct function.
  - Downloads: 20
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - The repository contains Lightblue's QLoRA fine-tuned OpenOrca model for Japanese closed QA, trained on SNOW TyDiQA (Ja), XLSUM (Ja) datasets totaling 13,167 samples.
  - Downloads: 18
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - A QA model for answering English questions about learning Japanese, built on the japanese-stablelm-instruct-gamma-7b base model and requiring Transformers 4.34.0 or newer.
  - Downloads: 18
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This GitHub repository contains a fine-tuned DeBERTa-v2-tiny-japanese model for QA tasks using the DDQA dataset, suitable for SQuAD tasks, and includes installation instructions for transformers and pytorch.
  - Downloads: 16
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
  - Downloads: 16
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This model, fine-tuned on Yuyuyui scenario corpus based on rinna/japanese-gpt2-medium, generates responses to input sequences of utterances withÁâπÂÆöÂ≠óÁ¨¶ÂºÄÂ§¥ÂíåEOSÁªìÂ∞æÔºåÂπ∂‰ª•<Êüê>Áî®‰∫éÁî®Êà∑ËæìÂÖ•„ÄÇ
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - A fine-tuned model based on luke-japanese-base-lite for Question-Answering tasks using JSQuAD dataset, with accuracy metrics provided.
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 „Åì„ÅÆ„É¢„Éá„É´„ÅØtokyotech-llm/Swallow-MS-7b-instruct-v0.1„ÅÆtokenizer.chat_template„Çí‰ª•‰∏ã„Å´Â§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - A model trained on instruction datasets, specifically using tiny_mixtral_ja.
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - The repository includes a named entity recognition model for Japanese medical documents, along with a prediction script that outputs XML-tagged text and provides entity normalization.
  - Downloads: 131,506
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - A named entity recognition model for Japanese, fine-tuned from cl-tohoku/bert-base-japanese-v3 using the llm-book/ner-wikipedia-dataset, as introduced in Chapter 6 of "Large Language Models Primer."
  - Downloads: 80,084
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - A Python model for named entity recognition in Japanese medical documents, including files for tags, attributes, prediction script, and input text.
  - Downloads: 5,731
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - A BERT-based model for extracting named entities in Japanese text, supporting 8 entity types.
  - Downloads: 1,904
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - A binary classification model trained with AutoNLP, achieving high accuracy and F1 score, accessible via API.
  - Downloads: 575
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - Accessing the repository requires agreeing to certain conditions.
  - Downloads: 570
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository offers fastText classifiers for assessing the educational value of Japanese web pages, including a Wiki-based and an LLM-based model, both licensed under CC BY-SA 4.0.
  - Downloads: 541
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - A LLaMA 3 Youko qlora fine-tune using a new version of the VNTL dataset for improving Japanese visual novel translations to English, without chat mode.
  - Downloads: 374
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - A LLaMA 3 Youko qlora fine-tuned with a new version of the VNTL dataset for improving Japanese visual novel translations to English, omitting chat mode and showing enhanced accuracy and stability.
  - Downloads: 330
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - The repository contains weighted/imatrix quantized GGUF files for Japanese-Starling-ChatV-7B, including i1-IQ1_S (1.7GB), with notes on usage and concatenation.
  - Downloads: 267
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository contains a fine-tuned LuKE Japanese base model for Named-Entity-Recognition using a Wikipedia dataset, with reported entity recognition accuracy metrics.
  - Downloads: 219
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - A binary classification model trained using AutoNLP with high accuracy and AUC, accessible via cURL API.
  - Downloads: 144
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - A fine-tuned Luke Japanese large model for Named-Entity-Recognition (NER) using a Wikipedia dataset, achieving an F1-score of 0.845.
  - Downloads: 98
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - A fine-tuned BERT model for named entity recognition on Japanese Wikipedia data, combining cl-tohoku/bert-base-japanese-v3 with CRF layer as described in "Â§ßË¶èÊ®°Ë®ÄË™ûÊ®°ÂûãÂÖ•ÈñÄ" Chapter 6.
  - Downloads: 89
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - A fine-tuned Reward model for evaluating the quality of Japanese novels using the TinySwallow-1.5B base model, intended for applications like generative models' reinforcement learning, with noted biases.
  - Downloads: 58
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - A Python model for named entity recognition in Japanese medical documents, including files for tagging and prediction.
  - Downloads: 53
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository contains a fine-tuned DeBERTa-v2-large-japanese model for Named Entity Recognition using a Japanese Wikipedia dataset.
  - Downloads: 36
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - A text classifier fine-tuned on ~5000 labeled sentences to assign JLPT levels, achieving good performance with varying precision, recall, and F1 scores across levels N5 to N1.
  - Downloads: 34
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - The ja_core_news_lg pipeline for spaCy version 3.7.0 includes tok2vec, morphologizer, parser, senter, attribute_ruler, and ner components, optimized for CPU with 480,443 unique vectors in a 300-dimensional space based on UD Japanese GSD v2.8.
  - Downloads: 27
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This GitHub repository provides a fine-tuned DeBERTa-v2-base-japanese model for Named Entity Recognition (NER) using a Japanese Wikipedia dataset.
  - Downloads: 25
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - A Fully Convolutional Neural Network model for classifying Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - The repository provides a dataset file named wrime-ver1.tsv.
  - Downloads: 21
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - A model for generating titles from article text.
  - Downloads: 20
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - The GitHub repository ja_core_news_trf version 3.7.2 includes a Japanese transformer pipeline with transformer, morphologizer, parser, attribute ruler, and NER components for spaCy ‚â•3.7.0 <3.8.0.
  - Downloads: 19
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - A CPU-optimized spaCy pipeline for Japanese text processing, including tokenization, parsing, named entity recognition, and 480,000+ unique vectors based on UD Japanese GSD v2.8.
  - Downloads: 17
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - A fine-tuned BERT model for Named-Entity-Recognition based on cl-tohoku/bert-large-japanese-v2 using a Japanese Wikipedia dataset, achieving an accuracy of 0.862.
  - Downloads: 16
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - A model trained on Japanese parliament proceedings from 2022 using GPU resources, demonstrated at the #ABCILLM hackathon.
  - Downloads: 16
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - A fine-tuned model scoring whether short text is sexual on a 0-1 scale with variations outside this range, using studio-ousia/luke-japanese-large-lite.
  - Downloads: 16
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - The baseline model is trained on an awesome-japanese-nlp-classification-dataset, achieving high precision and accuracy with evaluation metrics provided.
  - Downloads: 15
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This repository includes a So-vits-svc 4.0 model for generating natural, youthful female voices from one's own voice data, along with notebooks and pre-trained G_0.pth and D_0.pth files.
  - Downloads: 13
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - The repository houses a fine-tuned Llama-2-Chat 70B model for Japanese, using the CC-BY-SA 4.0 licensed izumi-lab/llm-japanese-dataset.
  - Downloads: 12
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - A QLoRA-fine-tuned LLaMA2-7B model trained on 49,000 chat and 280,000 non-chat samples from the Guanaco dataset, with improved Chinese and Japanese performance, tested using test.py.
  - Downloads: 11
### Responsible NLP
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - A personalized model for generating low-breasted female characters, requiring age editing and possibly failing with prompts for other outputs; VAE and sampling recommendations provided.
  - Downloads: 14,521
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - The repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime epoch2, details the merging ratios in ckpt filenames, and includes instructions for replacing models while avoiding coloration issues with specific SD versions.
  - Downloads: 3,093
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a specialized Japanese voice recognition model fine-tuned on 5,300 hours of anime dialogue data, excelling particularly in anime acting lines while showing unique performance even for other voices.
  - Downloads: 2,083
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - A GitHub repository for the sarashina2.2-3b-instruct-v0.1-GGUF model using the sbintuitions/sarashina2.2-3b-instruct-v0.1 base model and imatrix dataset.
  - Downloads: 581
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
  - Downloads: 178
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - matsuolab-weblab-10b-instruction-sft-gguf matsuo-lab„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãweblab-10b-instruction-sft„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 59
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - A GitHub repository containing a model mix of MoeDiffusion, HassanBlend, and VMix03 for generating black-haired ponytail faces, with adjustments to improve control and reduce NSFW outputs.
  - Downloads: 21
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 integrates WaifuDiffusion and StableDiffusion VAEs to improve coloration, and merges with DreamShaper 3.3 for broader expression, capable of generating high Beauty Score realistic portraits andidealized characters.
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - A Mixture of Experts (MoE) language model fine-tuned on SQL datasets, combining Llama-3-Umievo-itr014-Shizuko-8b with rdefog/llama-3-sqlcoder-8b using MergeKit.
  - Downloads: 12
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 11
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - Ê¶ÇË¶Å „ÄåLOCAL AI HACKATHON„Äç„Å´„Åä„Åë„Çã„ÄÅ„ÉÅ„Éº„É†DataPilot,4„Å§„ÇÅ„ÅÆÊàêÊûúÂìÅ„Åß„Åô„ÄÇ
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - By agreeing, users accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 390
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - AJapanese ELECTRA Small model fine-tuned for cyberbullying detection, pretrained on a 5.6 billion-word corpus and fine-tuned onbalanced datasets from BBS and Twitter comments.
  - Downloads: 222
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - By agreeing, users consent to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 131
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - A model fine-tuned on manually labeled social media comments using the Twitter/twhin-bert-large, with 27 epochs, batch size 16, and max token length 256, achieving macro F1 scores of 64.8%.
  - Downloads: 109
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - The model, fine-tuned for 27 epochs with a batch size of 16 and maximum token length of 256 on manually annotated SNS comments, achieves macro F1 scores of 64.0% for offensive content detection.
  - Downloads: 106
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - A fine-tuned model based on Twitter/twhin-bert-base using a manually annotated dataset for social media comments, achieving macro F1 scores of 64.7% and other metrics with 27 epochs, batch size 16, and maximum token length 256.
  - Downloads: 106
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - By agreeing, users accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 104
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository is a private demo-only project.
  - Downloads: 86
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - An ELECTRA Base model for Japanese language fine-tuning focused on cyberbullying detection, derived from Megagon Labs' base and trained on a balanced dataset combining two major Japanese datasets. Licenses: CC BY-SA 4.0.
  - Downloads: 44
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - By agreeing, users accept the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 32
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged model with strong background and detail expression, inspired by HimawariMix but tuned according to Riga's ideas, prohibited for commercial use or altering model permissions.
  - Downloads: 22
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 20
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - A CreativeML OpenRAIL-M licensed model for non-harmful use with redistribution of weights allowed.
  - Downloads: 19
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - The repository includes the instructions for the llm-jp language model.
  - Downloads: 14
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - An ELECTRA Small model for Japanese language fine-tuned on balanced datasets for automatic cyberbullying detection, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Sentiment Analysis
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This fine-tuned model based on Luke-japanese-large-lite analyzes eight emotions in text using the wrime dataset.
  - Downloads: 37,013
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - A model trained from scratch on the chABSA dataset for Japanese sentiment analysis, achieving perfect accuracy and F1 score.
  - Downloads: 17,350
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - The GitHub repository houses a BERT Base model for Japanese sentiment analysis and irony detection, fine-tuned on ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 822
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - A sentiment analysis model trained to classify Japanese stock-related comments as either bullish or bearish.
  - Downloads: 102
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 37
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 36
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - A BERT model fine-tuned from scratch on the Japanese Sentiment Polarity Dictionary dataset for sentiment analysis.
  - Downloads: 23
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - A specialized BERT model adapted for Japanese Twitter, trained on a Twitter corpus, suitable for tasks like sentiment analysis and defamation detection, with applications in fined-tuning for specific NLP tasks.
  - Downloads: 22
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - A BERT-based Japanese finance sentiment analysis model trained on translated Financial PhraseBank data that classifies news into positive, negative, and neutral sentiments.
  - Downloads: 21
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - A Japanese ELECTRA Base model finetuned for irony detection,licensed under CC BY-SA 4.0, derived from transformers-ud-japanese-electra-ginza and trained on ironic tweets.
  - Downloads: 20
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - A fine-tuned ELECTRA small Japanese model for irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA small Japanese is a finetuned model for automatic irony detection in tweets, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - A BERT Base model for Japanese language fine-tuned on a balanced dataset for automatic cyberbullying detection,licensed under CC BY-SA 4.0.
  - Downloads: 13
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - A model fined-tuned on the „Å§„Åè„Çà„Åø„Å°„ÇÉ„Çì dataset using calm-2-7b-chat, available for free use within specified licenses.
  - Downloads: 11
### Reasoning
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - A Japanese fine-tuned version of DeepSeek-R1 for consistent bilingual English and Japanese reasoning.
  - Downloads: 2,222
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - A multilingual model combining Chinese and Japanese capabilities using a Mixture of Experts approach, optimized for GSM8K evaluation with 20GB VRAM, available in code example.
  - Downloads: 1,622
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuiseki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãYuisekinAIEvol-Mistral-7B-ja-math-v0.1.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 811
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is a risk-optimized model derived from Stable Diffusion and Wifu Diffusion, featuring anime-style illustrations with transparent LoRA weights and datasets.
  - Downloads: 396
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - A Japanese natural language inference model trained on JGLUE-JNLI and JSICK datasets using SentenceTransformers Cross-Encoder, outputting scores for contradiction, entailment, and neutral labels.
  - Downloads: 140
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This fine-tuned model for the JCommonsenseQA task based on luke-japanese-large achieves an accuracy of 83.82%.
  - Downloads: 99
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - A modified Japanese LLaMA 8B model enhanced with reasoning capabilities from a distilled DeepSeek model.
  - Downloads: 63
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository contains a fine-tuned DeBERTa-v2-tiny-japanese model for CommonsenseQA tasks using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 27
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This GitHub repository contains a fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA tasks using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 26
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a fine-tuned Japanese version of the COMET model using causal language modeling on TimeATOMIC data.
  - Downloads: 18
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - A fine-tuned LUKEmodel for JNLI task with accuracy of 0.898, useful for natural language inference on Japanese text.
  - Downloads: 14
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository contains a fine-tuned DeBERTa-V2-Base-Japanese model for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman morphological analysis tool installation.
  - Downloads: 12
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - A fine-tuned Luke-Japanese-base model for commonsense QA tasks, achieving high accuracy of 80.07, based on the JGLUE JCommonsenseQA dataset.
  - Downloads: 11
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSe v2 is a general Japanese text embedding model optimized for retrieval tasks, running on CPU, and excelling in measuring semantic similarity and passage retrieval.
  - Downloads: 258,150
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - Hotaru Jujo's LoRA collection, dual-licensed under MIT or CreativeML Open RAIL-M, is available for download with optional social sharing.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT is an initial release of a Japanese-only document retrieval model that outperforms previous models and approaches the performance of multilingual ones.
  - Downloads: 523
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - A fine-tuned passage encoder for the BPR document retrieval model using bert-base-japanese-v3, based on Chapter 9 of "Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ," with related notebooks and datasets available.
  - Downloads: 43
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - The repository contains a 7B-parameter Japanese instruction-following model fine-tuned on custom datasets, evaluated with a score of 6.65 on the JA MT-Bench benchmark.
  - Downloads: 17
## Datasets üß†

This list is sorted by downloads as of March 25, 2025.
477 datasets are listed.

### Information Extraction & Text Mining
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - A high-quality educational Japanese dataset consisting of approximately 120 million texts totaling around 89.3 billion tokens, filtered from FineWeb2.
  - Downloads: 4,667
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a dataset of sentences and translations that can be loaded by language pair code.
  - Downloads: 2,217
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - The JSICK dataset is a Japanese translation of the SICK dataset, used for NLI and STS tasks, with an accompanying stress test set to evaluate multilingual compositional inference capabilities.
  - Downloads: 921
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - A cleaned raw dataset of fineweb-2-edu-japanese's small_tokens text column after Unicode normalization (NFKC) and noise inference using fineweb-2-japanese-text-cleaner, with noise spans indicated by start_pos and end_pos.
  - Downloads: 897
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - A dataset of 5,000 annotated Japanese tweets for detecting defamatory content, labeled with targets and types of abuse.
  - Downloads: 613
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - A filtered Japanese subset of XL-Sum with PaLM 2 filters, containing 4,215 training examples, 758 validation examples, and 766 test examples.
  - Downloads: 538
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository contains shardable Japanese parquet files from the cc100 dataset.
  - Downloads: 510
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - A dataset of approximately 5% of ja subset documents, sampled using DSIR and closest to specific Japanese texts, for training language models.
  - Downloads: 507
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - The repository contains cleaned Japanese news articles from Common Crawl's news subset for July to October 2024, totaling 612M tokens using llm-jp tokenizer.
  - Downloads: 474
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - This repository contains a Japanese Wikipedia-derived dataset of sentences, including article and section titles, processed from dumps using a specific script.
  - Downloads: 457
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset is a cleaned version of the livedoor News Corpus, used in the "Large Language Models Introduction" book, licensed under CC BY-ND 2.1 JP.
  - Downloads: 392
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a machine-learning-friendly dataset from Aozora Bunko, including code to extract and process public-domain Japanese literary works.
  - Downloads: 391
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - The WRIME dataset includes subjective and objective emotional intensity annotations for social network posts by 50 participants.
  - Downloads: 332
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - A multi-label annotated dataset for NLP research field classification from GitHub repository descriptions and related content.
  - Downloads: 270
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - Atsushi Nakajima's Daikinrin website offers indexed summaries of thousands of mycological papers and manually extracted comparisons of diagnostic characters for fungal species.
  - Downloads: 258
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository contains version 2.0 of a Japanese named entity recognition dataset created by Stockmark for use in the book "Introduction to Large Language Models," citing a 2021 ANLP conference paper.
  - Downloads: 233
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository contains three parquet files with Japanese data extracted from the wiki40b dataset, generated using specific Python code.
  - Downloads: 209
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - A dataset card for "japanese_alpaca_data," derived from masa3141's Japanese-Alpaca-LoRA work, with relevant repository references.
  - Downloads: 208
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - A dataset containing 53,640 Japanese tweets annotated for COVID-19 relevance, covering January to June 2020.
  - Downloads: 196
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - A partial dataset of Nene'sÂπ≤Â£∞ (talking voice) recordings from the game PJSK, with a todo list to complete and standardize the dataset.
  - Downloads: 188
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - This GitHub repository mirrors data from https://registry.opendata.aws/abeja-cc-ja/, as described in the tech blog entry at https://tech-blog.abeja.asia/entry/abeja-cc-ja-202409.
  - Downloads: 184
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - The repository contains the AnswerCarefully Dataset, licensed for use in improving LLM safety including commercial purposes, but restricting its re-distribution and requiring attribution for derived data.
  - Downloads: 176
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - A JSON-based anime dataset with metadata and cross-references to MAL, ANIDB, AniList, and other popular sites.
  - Downloads: 169
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - A dataset containing only September and October news from kajuma/CC-news-2024-July-October-cleaned, with output token limit of 1024 and adjusted to about 1000 tokens for efficient learning using llm-jp/llm-jp-3-13b tokenizer.
  - Downloads: 149
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - A dataset for Japanese-English aligned sentences from 2003, provided with scripts for downloading, parsing, and preprocessing, but not for redistribution.
  - Downloads: 144
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - The repository contains a Japanese image classification dataset comprising four tasks specifically for Japanese cultural content, including the recognition of 101 types of Japanese dishes and ingredients.
  - Downloads: 144
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - A stopwords list for Japanese text analysis using the nagisa library, derived from top frequently used words in CC-100 dataset and Wikipedia.
  - Downloads: 140
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - A dataset consisting of a parquet file with Japanese Wikipedia data extracted and processed using Python code from the Hugging Face datasets library.
  - Downloads: 139
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - A Japanese named entity extraction dataset created using Wikipedia, licensed under CC-BY-SA 3.0 by Stockmark Inc.
  - Downloads: 136
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - The repository contains a dataset for training conditional language models for kana-to-Chinese character conversion, including over 190 million context-input-output triples, and features three model sizes (medium, small, xsmall) along with an evaluation benchmark.
  - Downloads: 135
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - The Japanese-Heron-Bench dataset includes 21 images categorized into Conversation, Detail, and Complex with 102 questions across seven subcategories to evaluate Japanese VLMs.
  - Downloads: 123
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - A corpus ofÁ∫¶350‰∏áÊó•Êú¨ËΩªÂ∞èËØ¥ËôöÊûÑËßíËâ≤ÂêçÁß∞ÔºåÊó®Âú®ÊîØÊåÅÊñáÂåñÊïèÊÑüÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°„ÄÇ
  - Downloads: 118
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This dataset consists of excerpts from 2014-2022 annual SEC filings in Japan, including fields like document ID, EDINET code, company name, and reporting period.
  - Downloads: 116
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - The dataset categorizes GitHub repository descriptions for relevance to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing.
  - Downloads: 115
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - The GitHub repository contains a diverse collection of anime song lyrics in Parquet format, serving as a rich resource for enthusiasts and researchers.
  - Downloads: 110
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - A manually translated English-Japanese dataset for passage-level translations of English Wikipedia article excerpts, created to provide a high-quality corpus with a permissive license due to restrictions on using existing translation tools for machine learning purposes.
  - Downloads: 101
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides validated Japanese linguistic minimal pairs in JSONL format for benchmarking.
  - Downloads: 100
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - A public RLHF dataset in Japanese, where reward model construction was reformatted into a classification task, with labels 1 for chosen and 0 for rejected sentences.
  - Downloads: 96
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - The repository has been expanded to include 100k Japanese data points from cosmopedia, with additional translations for text generation prompts.
  - Downloads: 96
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - A curated collection of inspiring anime quotes from various series, formatted as a list of dictionaries for easy access and analysis.
  - Downloads: 94
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This repository includes 8,750 law records from the official Japanese government website, each containing details like number, title, ID, effective date, and text, unique to August 1, 2023.
  - Downloads: 90
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - A cleaned dataset containing only September and October 2024 news, with dates added, for continued pre-training with outputs up to 1024 tokens, adjusted to about 1000 tokens using llm-jp/llm-jp-3-13b tokenizer.
  - Downloads: 89
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This repository contains a dataset withNamed Entity Recognition labels applied to articles from Wikinews, including 8 entity types, licensed under CC BY 2.5, for use in the book "Introduction to Large Language Models."
  - Downloads: 83
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - A dataset containingÁà¨Ëô´Êï∞ÊçÆÊù•Ëá™marusenryu.comÁΩëÁ´ôÔºåËØ•ÁΩëÁ´ôÊòØÊúÄÂ§ßÁöÑ‰ø≥Âè•ÊäïÁ®øÂπ≥Âè∞‰πã‰∏ÄÔºåÂåÖÂê´376‰∏™‰∏ªÈ¢òÂíå5346Êù°ÂõûÂ∫îÔºåÁî®‰∫éÊñáÊú¨Âà∞ÊñáÊú¨‰ªªÂä°„ÄÇ
  - Downloads: 83
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - A curated clustering dataset for training and evaluating embedded models, derived from web pages of the Pharmaceuticals and Medical Devices Agency, containing cleaned and split 'train' and 'test' subsets with preserved label proportions.
  - Downloads: 78
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - A dataset of furigana characters created from National Diet Library bibliographic data.
  - Downloads: 77
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - A dataset and open-source code for the SLG framework, supporting multi-task learning in Japanese sentence classification and named entity recognition.
  - Downloads: 74
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a benchmark for Japanese LLM performance on long-context tasks, including extractive QA and abstractive summarization from various sources and synthetic data.
  - Downloads: 71
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is an unsupervised dataset primarily in English from Touhou wiki, structured in JSONL format for text generation and classification tasks.
  - Downloads: 68
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-response dataset with 6,259 annotated pairs for AI training and usage examples in Python.
  - Downloads: 65
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - Automated Q&A generation from Wikipedia Japanese edition using Mixtral 8x22b GGUF model, with filters recommended due to potential hallucinations.
  - Downloads: 64
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - A clustering dataset for training and evaluating embedded models, derived from customs pre-instruction responses (ÂïÜÂìÅÂàÜÁ±ª) with "general item names" and "cargo overview" combined, split into train and test sets while preserving label proportions.
  - Downloads: 59
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - A cleaned version of the JParaCrawl dataset, the largest publicly available English-Japanese parallel corpus, for easy loading with Hugging Face Datasets.
  - Downloads: 58
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - A multilingual dataset for classifying similar and dissimilar paragraph pairs, including English and Japanese text from PubChem and Wikipedia.
  - Downloads: 52
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - The GitHub repository contains Japanese summaries and indexed information from thousands of mycology taxonomy papers in the form of "Three-line Paper Summaries," last updated on September 28, 2024.
  - Downloads: 50
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This GitHub repository contains a question-answer formatted synthetic dataset derived from the sakura_japanese_dataset, created using Nurture-intelligence/Gemma-2-108B-DPO-v0.1, licensed under the original dataset's license and Gemma Terms of Use.
  - Downloads: 50
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository contains a format-converted Japanese subset of the original NTX v1 dataset into the Aya instructions format, licensed under CC-BY-SA 4.0.
  - Downloads: 49
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - The repository contains the GIELLM dataset for Japanese information extraction, built on the livedoor news corpus.
  - Downloads: 49
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - A dataset comprising excerpts from specific chapters of SEC filings submitted to EDINET in 2024, including metadata like document ID, company name, and filing dates.
  - Downloads: 47
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - The repository contains parsed, cleaned, and UTF-8 encoded Parquet files from the OpenSubtitles database for Japanese subtitles, including text, metadata, and source titles.
  - Downloads: 44
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - A labeled dataset for Japanese racing horses with 9 categories of named entities, derived from Wikipedia articles, including new additions specific to horse names.
  - Downloads: 39
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - A dataset containing question texts for culinary searches and their search keywords, categorized into four types (AREA, TYPE, SZN, INGR), along with notebooks for creating the dataset, fine-tuning language models, and using the models in an application.
  - Downloads: 39
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - A converted version of the Japanese Wikipedia Typo Dataset for use with HuggingFace, sourced from Kyoto University Language Media Research Laboratory, licensed under CC-BY-SA 3.0.
  - Downloads: 38
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - A JSONL version of the dolly-15k-jp dataset for use with SFTTrainer's dataset_text_field property, licensed under CC BY SA 3.0.
  - Downloads: 36
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - The GitHub repository hosts collaborative efforts to create a high-quality French-Japanese dictionary and aligned bilingual corpus, using initial data from various sources including Japanese-to-French dictionaries and JMdict.
  - Downloads: 36
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - The AutoTrain dataset for project tam_jp contains Japanese language data structured as JSON objects with context fields.
  - Downloads: 33
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - The repository contains the AnswerCarefully Dataset, intended for enhancing LLM safety with permissive licensing for derivative works, but strictly prohibiting misuse and non-commercial redistribution.
  - Downloads: 32
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - A dataset of 221 haikus, including comments from authors and judges for approximately 200 of them, from the Oi-Ocha New Haiku Award.
  - Downloads: 19
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - This repository contains a dataset for named entity recognition (J-NER) in large language models, including 157 types of entities from Wikipedia pages, each with 5 positive and 5 negative examples.
  - Downloads: 14
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíUVR„Çí‰ΩøÁî®„Åó„Å¶BGM„ÇÑ„Éé„Ç§„Ç∫Èô§Âéª„Åó„Åü„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a largescale crowdsourced anime illustration dataset with over 5 million tagged images and average 30 tags per image for training various visual tasks.
  - Downloads: 6,495
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - VOICEVOX-based artificial voice dataset including 445,793 .wav files totaling 577 hours and 51 minutes.
  - Downloads: 4,911
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a large-scale, crowdsourced anime illustration dataset containing over 1.2 million high-quality images and diverse tags.
  - Downloads: 3,862
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - japanese-anime-speech is an audio-text dataset for training ASR models, comprising thousands of clips from visual novels to improve transcription accuracy of anime dialogue.
  - Downloads: 1,996
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese-Anime-Speech-V2 is a dataset with 292,637 audio clips and transcriptions from visual novels for training ASR models.
  - Downloads: 1,684
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This non-official project aims to dataset-ize Sakura Miko's voice from Hololive Partnership for use in speech recognition, adhering to their guidelines and without any license rights. Contributions are welcome following the specified procedures.
  - Downloads: 1,389
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a multimodal benchmark for evaluating LMM performance in Japanese across various disciplines, incorporating expert input from native speakers.
  - Downloads: 1,134
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - A diverse collection of high-quality images depicting various aspects of Japan captured between 2022 and 2024, for AI training purposes.
  - Downloads: 772
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - The ReazonSpeech dataset includes over 35,000 hours of diverse Japanese terrestrial television speech in FLAC format sampled at 16kHz, suitable only for Japanese copyright-compliant ASR research.
  - Downloads: 703
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This repository provides a refined dataset for evaluating vision-language models in Japanese, derived from the original Japanese-Heron-Bench benchmark.
  - Downloads: 496
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - Classify images from the KMNIST dataset into 10 classes of Japanese characters.
  - Downloads: 451
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This repository contains crawled data from HomeMate Research's photo haiku contest, including images and structured responses, with 435 themes and 1767 submissions for use in the YANS hackathon.
  - Downloads: 411
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - The Lux Japanese Speech Corpus includes raw and cleaned 96kHz/16bit WAV audio files of character Lux reading texts, along with transcription metadata.
  - Downloads: 265
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - The J-ResearchCorpus is a high-quality text dataset containing 1,343 papers from the NLP2024 conference and 360 papers from the Natural Language Processing journal, licensed under CC-BY-4.0, totaling approximately 39 million words for pre-training language models or other applications.
  - Downloads: 242
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - MashiroSA/sovits-emu-dataset contains 2735 unmodified WAV voice recordings from the Project Sekai character Emu Otori, for research use only under CC-BY-NC 4.0 license.
  - Downloads: 235
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - A copyright-free dataset of places in Japan for training text-to-image models.
  - Downloads: 234
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - A dataset of anime-style illustrations with accompanying CC-0 licensed Japanese captions created using AI.
  - Downloads: 234
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This repository contains crawled data from senryu poetry submission websites, including original HTML files and processed content, for training image-to-text and text-to-text tasks in senryu poetry creation.
  - Downloads: 226
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - The JA-VG-VQA-500 is a 500-sample subset of the Japanese Visual Genome VQA dataset used for evaluating EvoVLM-JP-v1-7B, available under CC BY 4.0 License.
  - Downloads: 222
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - The repository contains processed MS MARCO Japanese translation data with hard negative mining and compares a trained SPLADE model against mMARCO.
  - Downloads: 215
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - A public-domain dataset of Japanese scenery images suitable for training text-to-image models.
  - Downloads: 211
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - A transcription dataset using Whisper for Japanese ASR, excluding audio files.
  - Downloads: 173
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - A JSONL dataset using Music2Emotion for analyzing emotions in Japanese music, including predicted moods and valence/arousal scores.
  - Downloads: 170
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - A dataset from FineWeb2 Japanese web scraping data, containing 300K+ training and 30K test samples with noise spans identified by LLM, specifically using cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese.
  - Downloads: 164
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - The CABank Japanese Sakura Corpus is a 31-participant audio study from Japan, available as acopy from the original at https://ca.talkbank.org/access/Sakura.html.
  - Downloads: 150
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - Avoice dataset in WAV format from Project Sekai character Emu Otori, size 2735 files, intended for so-vits-svc 4.0 research use only under CC-BY-NC 4.0 license.
  - Downloads: 123
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - The repository contains transcriptions and duration data for voice recordings of characters from Umamusume, including ‰∏úÂïÜÂèòÈù© („Çπ„Ç§„Éº„Éó„Éà„Ç¶„Ç∑„Éß„Ç¶) with a total time of 799.44 seconds.
  - Downloads: 116
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - The repository preprocesses the Tanaka Corpus for HF Datasets, extracting Japanese and English text into a structured format.
  - Downloads: 104
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - A dataset pairing images of PDF pages (resized to 896px, 700px, or 588px) with questions generated from the OCR text using Qwen/Qwen2.5-14B-Instruct.
  - Downloads: 99
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This repository contains crawled data from "ÂÜôÁúüÂ∑ùÊü≥" and "Â∑ùÊü≥ÊäïÁ®ø„Åæ„Çã„Åõ„Çì" websites, including 70 image-to-text and 30 text-to-text tasks forÂ∑ùÊü≥ poetry creation, with submissions for a leaderboard evaluation.
  - Downloads: 92
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - A dataset split from the Japanese-anime-speech-v2 project.
  - Downloads: 87
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - A synthetic dataset generated using Qwen/Qwen2-VL-7B-Instruct and Qwen/Qwen2.5-32B-Instruct-AWQ models based on photos from ThePioneer/japanese-photos.
  - Downloads: 77
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - The CABank Japanese CallHome Corpus includes 120 audio recordings from US-based speakers and is used for telephone speech research.
  - Downloads: 72
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - A dataset of 1024x1024 PNG images of Kanji characters, each with a descriptive text meaning.
  - Downloads: 70
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - A clustered dataset for training and evaluating embedded models, derived from extracted "DescriptionOfBusinessTextBlock" data with sector codes from EDINET, split into train and test sets while maintaining label proportions.
  - Downloads: 68
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - The analysis using speechMOS on the Common Voice Corpus 17.0 resulted in MOS values saved as audio_analysis_results_speechMOS.json, including histogram data and counts of files above specific SNR values.
  - Downloads: 67
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - A corpus of Japanese text generated using Phi-3 from randomly extracted sources, with some processing done on TokyoTech's TSUBAME4.0 supercomputer.
  - Downloads: 67
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - The repository contains audio analysis results using speechMOS to evaluate voice quality, saved in `audio_analysis_results_speechMOS.json`, and includes histograms of the data.
  - Downloads: 66
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - A synthetic Japanese roleplay instruction dataset of about 1000 entries created by applying Magpie's method to nvidia/Nemotron-4-340B-Instruct, using DeepInfra with potential low-quality records.
  - Downloads: 64
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - A clustered dataset of 6,127 XML files from e-Gov, containing concatenated text and labels for laws and regulations, split into train and test sets.
  - Downloads: 64
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 curates over 240,000 animation clips from enthusiasts, aimed at addressing the scarcity of animation-related video datasets for AI and generative models.
  - Downloads: 63
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - A dataset containing only CC-BY-SA-4.0 licensed quiz data from the official AI King Quartz Knowledge Quiz Dataset (JAQKET).
  - Downloads: 59
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - The repository contains Pok√©mon BLIP captions in English and Japanese for training a text-to-image model, derived from Few Shot Pok√©mon images.
  - Downloads: 59
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a large-scale, crowdsourced anime illustration dataset with over 5 million images and detailed tags for training various vision tasks.
  - Downloads: 59
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This repository contains a dataset for testing hilarious text and image generation tasks, including text-to-text, image-to-text, and text-image-to-text sub-tasks.
  - Downloads: 58
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - A dataset of 3.3 million Japanese text entries with hiragana annotations derived from ÈùíÁ©∫ÊñáÂ∫´ÂíåËê®ÁöÆÂüÉÁöÑÈü≥Â£∞DAISYÊï∞ÊçÆ„ÄÇÂéªÈáçÂπ∂Ê∏ÖÁêÜ‰∫ÜÊñáÊú¨Êñá‰ª∂‰∏≠ÁöÑÈáçÂ§çÈ°πÂèä‰∏çÂê´Ê±âÂ≠óÁöÑÊù°ÁõÆ„ÄÇ
  - Downloads: 54
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - A Japanese-translated trial dataset for NVIDIA's SteerLM, useful for alignment testing and training method references.
  - Downloads: 54
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - A curated dataset combining high-quality subsets from oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja for JGLUE evaluation, licensed under Apache 2.0, CC-BY-SA-3.0, or MIT as original.
  - Downloads: 53
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - The repository contains datasets for three tasks from crawled data of theÂ§ßÂñúÂà© website Bokete, including text_to_text, image_to_text, and text_image_to_text, with specified numbers of items.
  - Downloads: 47
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - A dataset converting Japanese Visual Genome VQA and docci_ja training data into LLaVA-Instruct format, licensed under Apache License 2.0.
  - Downloads: 46
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - The repository contains "Rakuten-Alpaca-Data-32K," a dataset of Japanese instructional data auto-generated using Rakuten/RakutenAI-7B-chat, with seed tasks from seed_tasks_japanese.jsonl, licensed under Apache 2.0.
  - Downloads: 45
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - A dataset of 30,800 Japanese voice-text recordings from 66.4 hours of FGO playable characters' voices, each with one voice actor, for ASR/ASV model training and evaluation.
  - Downloads: 44
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - A cleaned-up version of Calvin-Xu/Furigana-Aozora-Speech with 2,536,041 entries out of 3,361,443, including sanity-checking for common kanji readings.
  - Downloads: 41
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - A dataset of 101,702 Japanese words with their pronunciations in hiragana, curated by Japanese linguists, for use in Japanese ASR technology research.
  - Downloads: 38
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository contains embedded voice embeddings of Japanese parliament members' speeches for tasks such as speaker separation and analysis of parliamentary broadcasts.
  - Downloads: 35
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - The jaCappella corpus includes musical scores and audio recordings of Japanese a cappella children's songs with six voice parts.
  - Downloads: 25
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - A Japanese translation of the LLaVA Visual Instruct 150K dataset, aiming to support research and applications in theJapanese language domain, licensed under CC BY-NC-4.0.
  - Downloads: 19
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - A Japanese veterinary medicine audio dataset categorized into drugs, diseases, and symptoms, suitable for training purposes.
  - Downloads: 12
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - The VNTL leaderboard ranks LLMs based on their Japanese Visual Novel to English translations, with comparisons to traditional translation tools.
  - Downloads: 3,306
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - A manually corrected Japanese translation of the HumanEval dataset for evaluating large language models' code generation abilities, preserving some errors from the English version.
  - Downloads: 1,305
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - The defunct multilingual Amazon reviews dataset "amazon_reviews_multi" included product reviews in multiple languages from 2015 to 2019 for text classification.
  - Downloads: 1,198
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - The repository contains the Japanese translation of the English subset of ReLAION-5B, translated using open-weight LLMs and vLLM.
  - Downloads: 598
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - The repository contains Japanese queries from the MMarco dataset with up to 25 hard negative samples retrieved by mE5 and 10 by BM25.
  - Downloads: 490
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - A translated dataset from the Qwen/Qwen2.5-32B-Instruct model of Japanese text in the kaken subset of llm-jp-corpus-v3 to English, intended as an open Japanese-English parallel corpus.
  - Downloads: 483
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - The GitHub repository contains Japanese translations of the ms_marco dataset using google/madlad400-3b-mt, processed with a HuggingFace structure but lower quality compared to mMARCO.
  - Downloads: 476
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - A Japanese translation of openai/gsm8k with answers extracted, using nejumi/phi-4-GPTQ-Int4-calib-ja-1k, containing some invalid data.
  - Downloads: 418
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This repository contains Japanese web novel chapters aligned with their English fan translations, structured for document translation tasks and including metadata like series titles, genres, and alignment scores.
  - Downloads: 205
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - The repository contains the Japanese translation of Meta's LIMA dataset, named LIMA-JA, along with minor adjustments, and provides a dataset card for easy loading using Hugging Face's datasets library.
  - Downloads: 198
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - A portion of the Guanaco dataset in Japanese, comparable to datasets like Alpaca-Guanaco-Japanese-GPT-1b.
  - Downloads: 185
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - A subset of the bluemoon-fandom-1-1-rp-cleaned RP translated to Japanese using command-r-08-2024, leveraging OpenRouter's API for efficient and high-quality uncensored nsfw translation.
  - Downloads: 170
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - A curated subset of 1M high-quality rows from ntt's JParaCrawl v3 English-Japanese corpus, filtered by an LLM to improve alignment and reduceË¥®ÈáèÈóÆÈ¢ò„ÄÇ
  - Downloads: 144
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository offers a Japanese translation of an English dataset from oasst2, created by LLM-jp for instruction tuning.
  - Downloads: 139
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - A dataset of 50,000 English sentences extracted from Synthetic-JP-EN-Coding-Dataset-801k. Refer to the original dataset's overview for details andÊ≥®ÊÑè‰∫ãÈ°π„ÄÇ
  - Downloads: 129
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a high-quality, cleanly/manual-curated dataset of 100 sets for Chain-of-Thought reasoning in Japanese, with both merged and separated CoT and output parts.
  - Downloads: 120
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - A modified version of kunishou/hh-rlhf-49k-ja excluding ng_translation == 1 examples.
  - Downloads: 108
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - A dataset for instruction tuning developed by the collaborative LLM-jp project, comprising a subset of Aratako/Synthetic-JP-EN-Coding-Dataset-801k.
  - Downloads: 108
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - A converted CC-BY-SA 4.0 licensed Japanese instruction dataset (~2.46M rows) for LLMs, originally v1.0.0, credits to Jian Wu for conversion.
  - Downloads: 108
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - The Nexdata/English-Japanese_Parallel_Corpus_Data repository contains 850,000 desensitized and quality-checked bilingual texts (23 words on average) covering various fields, suitable for machine translation and text data analysis.
  - Downloads: 108
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - A corrected dataset for Japanese translations in MBZUAI/multilingual-llava-bench-in-the-wild, based on liuhaotian/llava-bench-in-the-wild.
  - Downloads: 105
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - A randomized English-Japanese translation dataset derived fromhttps://tatoeba.org/en/downloads with duplicates removed.
  - Downloads: 101
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - The repository includes Apache 2.0 licensed Japanese-English parallel texts for translation tasks, sourced from various documents with variable licensing conditions.
  - Downloads: 99
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - A GPT-3.5-Turbo-translated Japanese version of MMLU dataset for MultilingualSIFT research.
  - Downloads: 99
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - A dataset containing only the Japanese-English translations from the ALT Parallel Corpus, utilizing data from the HuggingFace repository.
  - Downloads: 97
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - A JSON conversion of NilanE/ParallelFiction-Ja_En-100k for text-generation-webui, containing Japanese web novel chapters and their English translations.
  - Downloads: 95
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - A dataset for long-text instruction data, primarily using Aozorabunko cleaning data, without filtered QA pairs due to performance limitations, suitable for fine-tuning with model-specific effects. Licensed under CC BY 4.0.
  - Downloads: 81
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository contains a Japanese translation of the mbpp dataset created by LLM-jp, using DeepL for English to Japanese translation.
  - Downloads: 79
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - A corrected translation of MT-Bench-ja using AI inflection, including some questions from Stability AI's Japanese MT-Bench.
  - Downloads: 77
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - The Tanaka-corpus dataset contains Japanese-English pairs compiled by Professor Yasuhito Tanaka and his students, originally released at Pacling2001.
  - Downloads: 76
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - A 20,000-item synthetic Japanese-English translation dataset created using Magpie's approach applied to nvidia/Nemotron-4-340B-Instruct, featuring code and instructions for data generation with potential low-quality records.
  - Downloads: 76
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - The repository includes filtered training and validation sets from JSNLI v1.1 for the book "Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®" and is licensed under CC BY-SA 4.0.
  - Downloads: 76
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - A processed English-Japanese parallel corpus fromWikidata, ready for translation tasks using Hugging Face transformers, with filtering applied due to non-direct translations.
  - Downloads: 76
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - A Japanese-translated FED dataset using Google Cloud Translate API v2, with potential inconsistencies between dimensions and annotations that should be noted.
  - Downloads: 74
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - A Japanese translation of the LLaVA Pretrain dataset using DeepL API, licensed under CC-3M and BLIP, for use in Japanese-language applications.
  - Downloads: 71
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - A Chinese-Japanese parallel corpus dataset with 9.83 million sentence pairs, covering multiplefields and desensitized for text analysis and machine translation.
  - Downloads: 70
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - The OPUS-MIT-5M dataset contains 5 million randomly sampled sentence pairs from the OPUS corpus, ensuring balanced representation across 20 language pairs for multilingual image translation.
  - Downloads: 68
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - This repository offers a 12,000-entry Japanese translation dataset derived from hh-rlhf using DeepL for human preference learning in LLMs, comprising 3,000 samples from four training groups.
  - Downloads: 68
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository contains 1000 generated Japanese responses using the deepseek-ai/DeepSeek-R1-Distill-Llama-8B model, with some formatting issues and reduced accuracy due to 8-bit generation.
  - Downloads: 67
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - A summary dataset from a long text corpus using the Aozora Bunko dataset, licensed under CC BY 4.0.
  - Downloads: 66
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - A question-answer dataset generated from the wiki40b-ja.
  - Downloads: 66
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - A MIT-licensed dataset for fine-tuning Plan RLHF, handcrafted by ebisuke.
  - Downloads: 64
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - The Ja-miracl dataset converts the Japanese section of the miracl dataset to BeIR format for use with mteb.
  - Downloads: 62
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - Translated sentence pairs from Japanese to Vietnamese.
  - Downloads: 61
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - A Japanese translation dataset for PiQA using Facebook's MBART-large-50 model, licensed under the same terms as the original.
  - Downloads: 58
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - The repository contains the dataset used for enhancing cross-lingual transfer in LLMs through translation-assisted chain-of-thought processes, as detailed in the TaCo paper. Please cite when using the dataset.
  - Downloads: 58
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - A dataset created by machine-translating "ViQuAE" answers into Japanese, including translated `original_answer_ja` but not translating `answer`.
  - Downloads: 56
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - The repository collects translations of English text into Korean, Chinese, and Japanese from OpenOrca datasets, using various APIs and similarity measures for matching.
  - Downloads: 53
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - A cleaned TALPCo dataset with Japanese-English translation pairs in HuggingFace format, without whitespace characters from tokenization, licensed under CC-BY 4.0.
  - Downloads: 52
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual form understanding benchmark dataset with key-value pairs labeled in 7 languages for form parsing and extraction.
  - Downloads: 50
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - A chunked 4096-token version of NilanE/ParallelFiction-Ja_En-100k in Alpaca format for use with augmxnt/shisa-base-7b-v1 model, with the original dataset containing aligned Japanese web novel chapters and English translations.
  - Downloads: 49
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository includes a dataset formatted according to TaCo guidelines for use in cross-lingual instruction-following tasks.
  - Downloads: 42
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - A Japanese translation dataset for the "sciq" dataset using Facebook's mBART, licensed under CC BY-NC 3.0.
  - Downloads: 41
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - A dataset of Japanese Wikipedia paragraphs vectorized with the multilingual-e5-base model and indexed using Faiss for efficient similarity search.
  - Downloads: 38
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - A 380,000-group Japanese-English parallel corpus, excluding sensitive vocabulary, suitable for text analysis and machine translation.
  - Downloads: 35
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - The repository contains a function for evaluating the accuracy and quality of Japanese to English translations, rejecting those with errors or low quality.
  - Downloads: 20
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - A Japanese to English translation project licensed under Creative Commons Attribution 4.0.
  - Downloads: 11
### Semantic Text Processing
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a benchmark for evaluating Japanese text embedding models consisting of 6 tasks and 16 datasets.
  - Downloads: 4,997
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - The repository contains scripts to convert Japanese Wikipedia text into various Japanese embeddings and a faiss index, along with Hugging Face Space demos for RAG and vector search tasks, and evaluation data for multiple Japanese embeddings in Q&A and vector search tasks.
  - Downloads: 3,271
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a benchmark dataset and evaluation framework for assessing Japanese biomedical LLMs.
  - Downloads: 2,424
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - The GitHub repository contains a dataset loading script for JGLUE, a Japanese general language understanding evaluation corpus developed by Yahoo Japan Corporation and Kawahara Lab at Waseda.
  - Downloads: 1,815
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - This repository includes synthetic Japanese and English conversation datasets derived from LMSYS-Chat-1M, used for post-training large language models.
  - Downloads: 992
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - A dataset generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b, with random sampling that may result in missing examples between collections, distributed under CC-BY-SA 4.0.
  - Downloads: 894
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - The repository contains a Japanese instruction dataset for tuning LLMs, particularly for chat tasks using LoRA, with updates addressing licensing changes and data issues.
  - Downloads: 619
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - A Japanese BERT base model passage embedding dataset for the "AI King" competition, used in the book "Introduction to Large Language Models," with binary vector embeddings added. Licenses include CC BY-SA 3.0 and GFDL for Wikipedia content.
  - Downloads: 601
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - A Japanese translation dataset of databricks-dolly-15k developed by LLM-jp for instruction tuning, with model card authors listed.
  - Downloads: 346
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - A simple dataset for zunda mon characters, curated from online research and provided for testing character LLMs, with usage licenses to be reviewed.
  - Downloads: 285
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - A refined dataset from izumi-lab/llm-japanese-dataset, excluding translations, for tuning LLM models on Japanese chat tasks using techniques like LoRA.
  - Downloads: 272
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - A dataset for Japanese roleplay, created using gpt-4o-mini and expanded toÁ∫¶39,600 pieces with added system messages, licensed under CC-BY-NC-SA 4.0.
  - Downloads: 230
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - The repository merges 256-character lines from three cleaned Japanese datasets: C4, CC100, and Oscar.
  - Downloads: 225
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - A dataset scoring fineWeb-2 Japanese texts from 0 to 4 for educational content using the Deepseek API, with a training set of about 280,000 and a test set of about 30,000 samples.
  - Downloads: 214
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - A dataset of 190,854 Japanese synthetic preference examples created using five models for instruction and judgment, including Qwen variants and Tanuki models.
  - Downloads: 194
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - A dataset of Wikipedia sentences for the book "Large Language Models Introduction," using data from the singletongue/wikipedia-utils repository, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 166
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - A dataset of conversation text generated using GPT-3.5-Turbo based on the Wikipedia Japanese edition (izumi-lab/wikipedia-ja-20230720), with commercial use prohibited due to GPT-3.5-Turbo restrictions.
  - Downloads: 158
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - The repository contains executable files and models for running Japanese GPT-2 on Windows, but warns that one file has an issue.
  - Downloads: 154
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - CC100-JA documents are combined line-by-line segments of the cc100-ja dataset, licensed under terms similar to the original cc100.
  - Downloads: 152
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - This GitHub repository contains a collection of short stories generated by gpt-4o-mini and other models, including annotations and multiple language versions.
  - Downloads: 151
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - A dataset converted from Open-Platypus-Japanese-masked to OpenAI Messages format.
  - Downloads: 143
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - A new Japanese dataset generated using GPT-4 with Self-Instruct, for fine-tuning non-English open-source models, updated continuously.
  - Downloads: 143
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - The repository contains processed versions of the RyokoAI/ShareGPT52K dataset in Markdown, with whitespace corrections and language labeling, using tools for HTML to Markdown conversion, whitespace handling, and language detection.
  - Downloads: 125
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - A dataset of simple Japanese sentences using calm3-22b, including various patterns like affirmative and negative statements, politeness forms, and more.
  - Downloads: 121
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - A 5 million clean Japanese sentence dataset for unsupervised semantic similarity learning.
  - Downloads: 111
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - A dataset of simple Japanese sentences using calm3-22b, including various grammatical patterns like affirmative and negative statements, politeness forms, wishes, progressive tense, requests, and more.
  - Downloads: 99
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - Data set for the 5th session of the 2024 LLM course, containing excerpts from outputs of two models in response to manually created inputs. Uses models from https://huggingface.co/watashiha/watashiha-gpt-6b and https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft, for educational and research purposes only.
  - Downloads: 96
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - A filtered and modified Japanese/Chinese language pair dataset from WikiMatrix v1, including regex-based filtering, semantic similarity filtering with a threshold of 0.6, and conversion of Traditional Chinese to Simplified Chinese.
  - Downloads: 96
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - The repository uses Magpie method to extract prompts on rinna/llama-3-youko-8b, employing translated system prompts from a paper, though the instruct model adaptation may be inappropriate.
  - Downloads: 89
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - NVIDIA's Japanese translation of the HelpSteer2 trial dataset for use with SteerLM, also compatible with Nemotron-4-430B-Reward.
  - Downloads: 81
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - The repository databricks-dolly-15k-ja-scored includes BERTScore translation quality scores for the Japanese translation of databricks-dolly-15k, with notes on its data quality issues.
  - Downloads: 80
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - A manually created instruction dataset for LLMs, available for training and testing splits.
  - Downloads: 78
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - A chat-formatted dataset derived from oasst2-135k-ja, suitable for multiturn dialogue fine-tuning, with large token lengths requiring significant computational resources; formatted in ShareGPT.
  - Downloads: 70
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - The dataset includes a DSIR-sampled subset of MADLAD-400 JA/EN tokens in a 90%/10% split for training shisa-base-7b-v1.
  - Downloads: 65
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - Á∫¶26,500Êù°ÁªèËøáÂ§ÑÁêÜÁöÑÊó•Êú¨ËØ≠Êåá‰ª§Êï∞ÊçÆÈõÜÔºåÈÄöËøáÂ∫îÁî®MagpieÊñπÊ≥ïÂπ∂‰ΩøÁî®ËÅöÁ±ªÂíåÊ®°ÂûãÁîüÊàêÁ≤æÈÄâËÄåÊù•„ÄÇ
  - Downloads: 65
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - A model fine-tuned on additional personality tweets after being pretrained on GPT-3.5, scored from 10 to 8 points for tweet generation.
  - Downloads: 64
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository contains a dataset of approximately 60,000 Japanese instructions created using the Self-Instruct method with Qwen2.5-72B, categorizing and generating new instructions based on existing ones.
  - Downloads: 64
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - A trial dataset for evaluation, including 50 queries generated by ChatGPT-4o from five perspectives, with manually created answers for 10 excluded questions involving patent introductions.
  - Downloads: 64
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - A dataset of Wikipedia paragraphs for the book "Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂÖ•Èó®," derived from the singletongue/wikipedia-utils repository, licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 62
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - A dataset of approximately 69,000 Japanese-English coding dialogues created using Magpie with various large language models, including NVIDIA Nemotron-4-340B-Instruct, Microsoft Phi-3-medium-4k-instruct, Mistral-8x22B-Instruct-v0.1, and Calm3-22b-chat.
  - Downloads: 59
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One aimed at increasing painting realism and complexity, with instructions for installation.
  - Downloads: 57
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - Created to address model weaknesses identified by @pokutuna, this GitHub repository contains manually crafted variations of IME and kakko tasks for transforming suggestions and matching parentheses.
  - Downloads: 57
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese dataset for evaluating language models' temporal inference abilities, consisting of templates and categorized test/training data.
  - Downloads: 57
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - A dataset recording Pok√©mon (VGC) Regulation F selection data collected from public broadcasted battles, presented at the 2024 Remote Pok√©mon Society community.
  - Downloads: 55
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This GitHub repository contains the Japanese translation of the Dolly project developed by Databricks, licensed under CC BY-SA 3.0.
  - Downloads: 54
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - Human-rated responses for ELYZA-tasks-100 used to evaluate Japanese LLMs, with automatic grading by GPT-4o and Claude 3.5 Sonnet achieving scores of 3.69 and 4.42, respectively.
  - Downloads: 52
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - A dataset of approximately 10,000 Japanese coding dialogues created by applying Magpie's method to Nvidia/Nemotron-4-340B-Instruct, with code for dataset creation and some post-hoc changes.
  - Downloads: 51
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - A Japanese translation dataset for roleplay learning using GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awq, translated via DeepInfra with 3-shot prompting and truncated at 8000 tokens.
  - Downloads: 47
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset documents VTubers' characteristics, activities, collaborations, and more, using prompts for detailed natural language descriptions from GPT-4o Search Preview.
  - Downloads: 40
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - A dataset combining human-created text (OSCAR) and LLM-generated text (GPT-3.5 Turbo) for evaluating LLM Japanese text detection performance.
  - Downloads: 37
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - A fine-tuned XLSR-53 model for Japanese speech recognition using Common Voice 6.1, CSS10, and JSUT datasets, sampled at 16kHz.
  - Downloads: 36
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - A dataset of Japanese fake news converted for HuggingFace datasets, including labels for truthfulness and text lengths.
  - Downloads: 36
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - A mirror of the LLM-jp Corpus v3 Japanese portion excluding Wikipedia, which is licensed under CC-BY-SA.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Natural Language Interfaces
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a dataset containing about 14,000 Japanese conversations that include speakers' personas and personality traits, with usage restrictions to protect individual privacy.
  - Downloads: 3,861
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese Q&A dataset for evaluating retrieval augmentation in large language model question answering.
  - Downloads: 893
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - This repository contains 40 Japanese questions across history, society, government, and geography to evaluate AI assistants' capabilities in Japanese.
  - Downloads: 523
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - The GitHub repository contains a dataset and loading script for JAQKET, a Japanese open-domain QA dataset derived from Wikipedia article titles, supporting multiple-choice task definitions.
  - Downloads: 503
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696-question-answer pair dataset for Japanese machine reading comprehension, derived from Wikipedia articles and achieving F1 scores of 78.92% when fine-tuning BERT-Japanese.
  - Downloads: 431
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - This repository contains evaluation datasets for benchmarking LLM Japanese role-play capabilities, including details such as genre, age range, world setting, and dialogue tone.
  - Downloads: 428
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository contains text datasets (totaling 1.56B tokens) from multiple sources for pre-training the AKU-d_ms-0.5B-chat-v0.1 model, including license information and processing scripts.
  - Downloads: 405
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - A dataset for evaluating Japanese large language models using LLM-jp's "llm-jp-eval," derived from multiple datasets for automatic assessment of LLMs as described in the book "Introduction to Large Language Models II."licensed under Apache 2.0.
  - Downloads: 334
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a Japanese pharmacology QA dataset including 13 years of past exam questions, answers, and commentaries, released under CC BY 4.0.
  - Downloads: 292
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The BSD dataset is a Japanese-English parallel corpus of written conversations across various business scenarios, created by first writing monolingual scenarios in each language and then translating them.
  - Downloads: 265
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - The GitHub repository contains evaluation scores for ensuring replicability and the source code for SB Intuitions'‰øÆÊ≠£Áâà in JEMHopQA, a Japanese multi-hop QA dataset that evaluates internal reasoning by generating answers and derivations with derivation steps.
  - Downloads: 226
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - The JIC-VQA dataset includes Japanese images of 101 food types with multiple-choice questions designed to evaluate VLMs.
  - Downloads: 224
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - A dataset of responses generated by Qwen/Qwen2.5-72B-Instruct from inputs extracted due to excellent input quality from Magpie-Tanuki-Qwen2.5-72B-Answered, licensed under Apache 2.0 with Qwen License considerations.
  - Downloads: 171
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval dataset consisting of 5000 questions and about 500,000 web page titles or snippets sourced from various Web genres filtered from Hagetani Bookmarks.
  - Downloads: 156
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - A chatbot response generator using aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, prompted by Chatbot Arena Conversations JA (calm2) instructions translated from lmsys/chatbot_arena_conversations.
  - Downloads: 151
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - Automatically generated multi-turn dataset using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF from open data sources, with some computation using Tokyo Tech's TSUBAME4.0 supercomputer.
  - Downloads: 145
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - The repository includes augmented answers for the validation set of AIÁéã V2.0, a Japanese quiz dataset, with fields for question ID, competition name, timestamp, split, and original question.
  - Downloads: 133
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - A multithreaded dataset generated using Calm3-22b to auto-generate Q&A from open-source data sources, with some computational assistance from TSUBAME 4.0 supercomputer.
  - Downloads: 123
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - A Japanese translation of the OpenOrca dataset, with about 20% translated and ongoing work, available for commercial use.
  - Downloads: 118
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - Conversations from public-domain Japanese books are extracted using a heuristic approach and grouped into utterance sequences.
  - Downloads: 118
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - A corpus of Japanese role-play dialogues from forums, with records filtered to include only those with multiple unique posters and longer post lengths.
  - Downloads: 117
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - A manually created dataset for developing a Japanese chatbot with expanding conversation content.
  - Downloads: 113
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - A multi-turn conversation dataset created from Japanese Wikipedia using Orion14B-Chat, available for commercial use under a complex license.
  - Downloads: 105
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - A synthetic Japanese roleplay dataset using gpt-4o-mini, containing 39,600 dialogues with up to 10 turns each, including various settings and tones.
  - Downloads: 104
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - A processed QA dataset from Japanese Stack Exchange data dumps, with formatted questions and answers, suitable for machine learning tasks related to Japanese language.
  - Downloads: 94
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository contains a Japanese dialog summarization dataset translated from dialogsum and CSDS.
  - Downloads: 93
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - A processed Japanese Stack Overflow dataset with questions and answers paired, containing markdownified text, base64-encoded image placeholders, and two sub-sets for default and simplified access.
  - Downloads: 86
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - A large-scale Japanese instruction-following dataset combining 16 instruction datasets, formatted as JSON with structures including instructions, optional inputs, and outputs, derived from various tasks and dialog formats.
  - Downloads: 84
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository contains the Japanese Vicuna QA Benchmark dataset for evaluating Japanese LLMs across 10 categories, licensed under Apache 2.0.
  - Downloads: 80
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - A forked dataset filtering rows with Êñ∞Â≠óÊñ∞‰ªÆÂêç in the "ÊñáÂ≠óÈÅ£„ÅÑÁ®ÆÂà•" metadata column.
  - Downloads: 78
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - A cleaned Japanese translation dataset of Lurunchik/WikiHowNFQA.
  - Downloads: 74
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - A dataset of 3,000 usable multi-turn conversations based on the Japanese Wikipedia, generated using llama2Pro8B and published for commercial use.
  - Downloads: 72
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - Japanese multi-turn conversation data generated using Qarasu14B from Wikipedia, available for non-commercial use and suitable for training with Axolotl.
  - Downloads: 69
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - The repository contains over 80,000 multi-turn conversations derived from Wikipedia, generated using llama2Pro8B and automatically screened for commercial use.
  - Downloads: 68
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - A multi-turn conversation dataset derived from Japanese Wikipedia using Orion14B-Chat, subject to a complex license that requires careful reading before commercial use.
  - Downloads: 65
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - A dataset ofÁ∫¶10000 Japanese instruction tuning samples created by applying Magpie's method to Nvidia/Nemotron-4-340B-Instruct, with code and creation details included.
  - Downloads: 64
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - A dataset of 3,000 usable multi-turn conversations generated from Japanese Wikipedia using llama2Pro8B, licensed for commercial use.
  - Downloads: 62
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - A Japanese-translated "databricks-dolly-15k" dataset with endings changed to "„Å´„ÇÉ„ÇìÔºÅ‚Äù using ArrowPro-7B-KUJIRA, licensed under the original dataset's licensing.
  - Downloads: 58
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a large Japanese multi-domain task-oriented dialogue dataset with 4,246 dialogues across 6 domains, annotated for dialogue state tracking.
  - Downloads: 56
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - A dataset created using AI Gemini 2.0 Flash Experimental from subtitles, suitable for chatbot training with potential errors in Turkish and Japanese.
  - Downloads: 56
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - The repository contains 60,000 multi-turn conversations generated from the Japanese Wikipedia using LLaMA-Pro-8B, available for commercial use but potentially containing unÁ≠õÊü•ÁöÑÂØπËØù„ÄÇ---‰øÆÊ≠£ÂêéÁöÑÁÆÄÊ¥ÅÂè•Â≠êÂ¶Ç‰∏ãÔºöËØ•‰ªìÂ∫ìÂåÖÂê´60,000‰∏™Â§öËΩÆÂØπËØùÔºåÁî±Êó•ËØ≠Áª¥Âü∫Êï∞ÊçÆÁîüÊàêÔºå‰ΩøÁî®LLaMA-Pro-8BÊ®°ÂûãÔºåÂπ∂ÂèØÁî®‰∫éÂïÜÁî®‰ΩÜÂèØËÉΩÂåÖÂê´Êú™Á≠õÈÄâÁöÑÂØπËØù„ÄÇ
  - Downloads: 55
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - The repository contains about 1000 hours of natural conversational speech from face-to-face interactions on various topics, manually transcribed with high accuracy.
  - Downloads: 55
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - The repository includes 11,808 multi-turn conversational instructions about Japanese images generated using the GPT-4o model from an original dataset sourced from Hugging Face.
  - Downloads: 55
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - It includes the Multi-Genre Four CrossRef (MC4) dataset for natural language processing.
  - Downloads: 52
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - A dataset generated by Swallow-MX from instructions manually checked and corrected, containing possibly erroneous outputs, created during the "LOCAL AI HACKATHON #000."
  - Downloads: 48
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - A dataset of approximately 10,000 Japanese roleplay dialogues with each consisting of about 10 turns, generated using the Nemotron-4 model and Magpie technique.
  - Downloads: 45
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - A dataset with data containing prompts matched from chatbot-arena-ja-calm2-7b-chat removed.
  - Downloads: 39
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository contains machine-translated Japanese versions of the everyday-conversations-llama3.1-2k dataset using DeepL, topic-wise dialog pairs, licensed under Apache 2.0.
  - Downloads: 39
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - A Japanese translation dataset of Lurunchik/WikiHowNFQA.
  - Downloads: 37
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - A dataset containing formatted human-assistant dialogues in Japanese from the OASST1 conversations, presented as paired human and assistant messages without full conversation context.
  - Downloads: 37
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - A dataset of simple Japanese sentences created using the Elementray_m calm3-22b model, including various grammatical patterns such as affirmative and negative statements, polite forms, wishes, presence, requests, and more.
  - Downloads: 35
### Text Generation
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese language understanding benchmark consisting of translated MMLU questions and culturally relevant Japanese questions to evaluate large language models.
  - Downloads: 124,526
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This GitHub repository contains Bokete crawl data for three tasks: text_to_text, image_to_text, and text_image_to_text, with current counts of 100, 500 images and 2355 responses respectively.
  - Downloads: 1,138
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - A dataset generated by using LLMs to automatically create questions from Japanese Wikipedia and answering them with cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, distributed under CC-BY-SA 4.0.
  - Downloads: 725
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard ÊòØ‰∏Ä‰∏™Ê∂µÁõñ‰∫î‰∏™Ë°å‰∏öÈ¢ÜÂüüÔºàÈáëËûç„ÄÅ‰ø°ÊÅØÈÄö‰ø°„ÄÅÂà∂ÈÄ†„ÄÅÂÖ¨ÂÖ±„ÄÅÊµÅÈÄöÈõ∂ÂîÆÔºâ„ÄÅÂü∫‰∫éÊó•ËØ≠ÁöÑÁªºÂêàRAGÊÄßËÉΩËØÑ‰º∞Âü∫ÂáÜÔºåÂåÖÂê´ÂøÖË¶ÅÊï∞ÊçÆÈõÜÂíåÈ™åËØÅÁéØÂ¢É„ÄÇ
  - Downloads: 599
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - A synthetic Japanese-English coding dataset with 801,262 entries, expanded using the Evol-Instruct method from an original dataset of 69,000 by Magpie.
  - Downloads: 260
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - A cleaned web corpus of Japanese corpora such as mc4-ja, clustered into about 10,000 text instances using unsupervised learning models, with some files parquetized and a file list available in the out folder.
  - Downloads: 241
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - A Japanese preference dataset created from modified magpie-sft-v1.0, using Aratako/Llama-Gemma-2-27b-SFT-trial1 to regenerate responses and judge between original and regenerated answers by quality.
  - Downloads: 224
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual NL-to-Code benchmark with 945 samples in four languages, accessible via Python dataset loading.
  - Downloads: 224
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This repository contains Bokete cringe joke dataset for three tasks: text-to-text, image-to-text, and text-image-to-text, derived from CLoT-Oogiri-Go data.
  - Downloads: 213
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - The GitHub repository contains an updated code-instruction dataset with 5,200 records, including 180 newly added records of Java code data from licensed educational content, along with details on the distribution and licensing of various code-related tasks.
  - Downloads: 170
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends CommonCatalog CC-BY with additional information such as English and Japanese captions, and includes sample code for loading the dataset.
  - Downloads: 158
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - A dataset converted from llm-jp-corpus-v3's kaken subset to HF format, including article titles fetched from original URLs, licensed under CC-BY 4.0.
  - Downloads: 144
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - Extracted items from neody/oscar-ja-cleaned with 256 characters or less.
  - Downloads: 114
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - A dataset containing all the topics and corresponding text-to-text responses from the mobile puns show "Keitai Oogeri" aired on NHK, including episode IDs and response details.
  - Downloads: 90
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - A synthetic instruction dataset using Q4_K_M for CALM3-22B, generated by providing Wikipedia text and instructions, containing some hallucinations that require filtering.
  - Downloads: 88
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15 provides a dataset of 50,000 manually simplified Japanese sentences with translations, suitable for text simplification and translation tasks.
  - Downloads: 79
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository contains a Japanese Preference Dataset created by generating responses using Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter1 and scoring them with Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8.
  - Downloads: 79
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - A formatted dataset of three-line summaries for the LiveRailNews corpus, enhanced with prompts for Llama v2, and suggests using [R_START] [R_END] as special tokens.
  - Downloads: 79
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - A collection of automatically generated Q&A data using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, based on Common Crawl with cleaned random text snippets to reduce dependency on original articles.
  - Downloads: 77
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - Automated Q&A generated using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF from Common Crawl data, with recommendations for cleaning due to low similarity and potential unnatural text.
  - Downloads: 76
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This repository contains a dataset for testing joke generation tasks, including text-to-text and image-to-text sub-tasks.
  - Downloads: 75
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - A Japanese dataset from ELYZA-japanese-Llama-2-13b-instruct for evaluating AI-generated text detection, derived from GPT-4-Self-Instruct-Japanese instructions.
  - Downloads: 72
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - The repository contains 1000 Japanese responses generated using a distilled Qwen-32B model, showing decreased accuracy and frequent missing <think> tokens.
  - Downloads: 71
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This repository contains a Japanese translation of the Open_o1_sft_Pro dataset using Qwen2.5-14B-Instruct, including template conversations and retaining the original dataset's license and acknowledgments.
  - Downloads: 66
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - A dataset resulting from machine-translating the "nlvr" dataset into Japanese.
  - Downloads: 60
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This repository contains a dataset of cleaned questions used to train the Qwen model, licensed under Apache 2.0 for the questions only, sourced and processed on ollama.
  - Downloads: 59
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository contains an iterative Japanese preference dataset created from Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k, using specific models andËØÑÂàÜÊñπÊ≥ïÔºåÊéíÈô§‰ΩéÂàÜÂõûÁ≠î„ÄÇËÆ∏ÂèØËØÅÊñπÈù¢ÂèóÂà∞Áõ∏ÂÖ≥Ê®°ÂûãËÆ∏ÂèØÂΩ±ÂìçÔºåÈááÁî®META LLAMA 3.1 COMMUNITY LICENSEÂíåGemma Terms of Use„ÄÇ
  - Downloads: 57
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset aimed at advancing multimodal ad text generation models.
  - Downloads: 57
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - A Japanese dataset for evaluating AI-generated text detection methods, created with the Qwen/Qwen1.5-14B model and adapted from GPT-4-Self-Instruct-Japanese.
  - Downloads: 56
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - A dataset of approximately 3000 synthetic Japanese children's stories using simple words, generated by GPT-4o-mini and described in arXiv:2305.07759.
  - Downloads: 56
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - A Japanese translation using KUJIRA of an instruction dataset primarily about investment, Berkshire Hathaway, and Warren Buffett, originally in English by glaive-ai.
  - Downloads: 55
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset is synthetic, created using null-instruct-ja and DeepSeek-v2.5 q4, with ollama and A5000 GPUs, licensed under DeepSeek's license.
  - Downloads: 55
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - A dataset of level2-filtered data from llm-jp-corpus-v3 converted to HF format, including article titles extracted from URLs, licensed under CC-BY 4.0.
  - Downloads: 54
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - A Japanese translation dataset of English quotes generated with the llm-jp-3-3.7b-instruct model, licensed under CC BY 4.0.
  - Downloads: 50
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - A templated version of about 40 Japanese open-source datasets for instruction tuning LLMs, containing up to 20,000 samples each with half used for 0-shot and half for few-shot examples.
  - Downloads: 47
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - The analysis of audio quality using WADA SNR for reazonspeech-v2[all] data is stored in reazonspeech-all-wada-snr.json, containing file names, SNR values, and transcriptions, with 1,208,360 entries having a WAND SNR value of 100 or higher.
  - Downloads: 45
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - A public RLHF dataset in Japanese with a reward model reformatted as a classification task, consisting of labeled chosen (label 1) and rejected (label 0) sentences from synthetic text and machine translation.
  - Downloads: 43
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - A Japanese-English glossary for AI terminology, intended to aid in smooth translations with GPT-4.
  - Downloads: 38
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
### Syntactic Text Processing
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - The repository integrates data from various sources for 2056 anime, including details of specific shows like "Yami Shibai," "Kimi to Idol Precure," and "One Piece: Gyojin Tou-hen."
  - Downloads: 946
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - A transformed Japanese dataset for easy learning with SentenceTransformers, structured in (anchor, positive), (anchor, positive, negative) formats, filtered for positive and negative samples based on reranking scores from specified repositories.
  - Downloads: 892
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - A multilingual dataset licensed under MIT.
  - Downloads: 550
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - The repository contains automatically generated Q&A using MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF, derived from CC-BY or Apache-2.0 licensed data sources, with cleaned random text excerpts to reduce similarity to original content.
  - Downloads: 390
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - A text dataset extracted from a January 1, 2024, Wikipedia HTML dump, maintaining document structure and excluding markup, suitable for various NLP tasks.
  - Downloads: 383
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - A fun sticker collection by„Çã„Çä.
  - Downloads: 178
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - This repository contains cleaned synthetic Japanese data generated using the Alpaca method, curated by HachiML and licensed under Apache 2.0.
  - Downloads: 160
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - A modified parsing and chunking method for Wikipedia data crawled from December 5 to 8, 2023, based on oshizo/wikipedia-utils.
  - Downloads: 139
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - A dataset derived from Magpie-Tanuki-8B-dpo-v1.0, annotated with difficulty, quality, and category using cyberagent/calm3-22b-chat for instructions.
  - Downloads: 122
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - The repository relates to "hachiwari" or "chiikawa," likely referring to a project with origins in Japanese language or cultural elements.
  - Downloads: 100
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - A Japanese dataset of approximately 1,300 Q&A pairs about Databricks, sourced from the Databricks Japan blog and FAQ posts.
  - Downloads: 95
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - The repository includes a large dataset (about 2800 images) of high-beauty-score female images, primarily for training realistic models, with versions 2.1 and 2.6 focusing on different aspects of beauty.
  - Downloads: 94
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - A Magpie-generated dataset of 97,269 Japanese dialogues applied to weblab-GENIAC/Tanuki-8B-dpo-v1.0 without post-filtering.
  - Downloads: 77
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - A 3-turn multi-turn instruction dataset generated by Qwen, containing mixed English and Chinese records that may require filtering. Based on Aratako/Magpie-Tanuki-8B-annotated-96k instructions.
  - Downloads: 76
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - GitHub repo for data extracted from the CommonCrawl PDF collection specifically focused on the Japanese domain.
  - Downloads: 75
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - LLMChatÁ≥ªÁªüÊî∂ÈõÜ‰∫Ü2139Êù°Áî±13ÁßçÊ®°ÂûãÁîüÊàêÁöÑÂõûÁ≠îÂèä‰∫∫Â∑•ËØÑ‰ª∑Êï∞ÊçÆÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãË°®Áé∞„ÄÇ
  - Downloads: 74
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - A 525k Japanese translation of ApolloCorpus medical instruction tuning dataset from the English version, with notes on potential translation errors.
  - Downloads: 69
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - A dataset of approximately 150,000 paired Danbooru and Japanese tags created on 2024/10/15, with improved filtering to reduce non-Japanese tag inclusion.
  - Downloads: 68
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - A validated dataset of on-yomi readings from Chinese characters in the National Diet Library bibliography, with 5,064 instances corrected.
  - Downloads: 64
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - A Japanese-localized version of part of the original LLaVA v1.5 Visual Instruct 655K dataset, translated using DeepL API, for use in Japanese-language applications.
  - Downloads: 62
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - Atsushi NakajimaÁª¥Êä§ÁöÑ‚ÄúÂ§ßËèåËΩÆ‚ÄùÁΩëÁ´ôÊèê‰æõ‰∫Ü‰∏ÄÂ•ó‰ª•ÂçäËá™Âä®ÊñπÂºè‰ªéÁúüËèåËÆ∞Ëø∞‰∏≠ÊèêÂèñÂπ∂Áî®Ê†áÂáÜÂåñËØçÊ±áÊï¥ÁêÜÁöÑÂΩ¢Ë¥®Êï∞ÊçÆÈõÜÔºå‰æõÈùûÂ≠¶ÊúØÁî®ÈÄî‰∏¥Êó∂‰ΩøÁî®„ÄÇ
  - Downloads: 61
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthesis dataset created using the Evol-Instruction method on mistralai/Mixtral-8x22B-Instruct-v0.1, based on Stanford Alpaca seed tasks, with code available on GitHub.
  - Downloads: 60
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - The repository hosts the website for JSEC.
  - Downloads: 58
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - Errors in a dataset of kana markings derived from Braille texts of "Aozora"Bunko and Sapio, including mismatches in sentences from works by authors like Tanizaki Junichir≈ç and Haibara.
  - Downloads: 57
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - A collection of 20,000 auto-generated Japanese instructions and their responses in JSON format, created using the Qwen2.5-32B-Instruct model for task learning and evaluation.
  - Downloads: 55
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - A processed embedding dataset from 50,000 posts by t_w to de.light, unusable for redistribution.
  - Downloads: 52
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - Metadata scraping from dengekibunko.jp/novecomi/novel/.
  - Downloads: 48
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - A dataset containing quiz questions with free reuse permissions, suitable for use in search expansion generation and document search systems.
  - Downloads: 46
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - A dataset containing quizzes from Quiz Works, free for reuse as stated on the Quiz Works website, suitable for RAG and document search system development.
  - Downloads: 43
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data.
  - Downloads: 40
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - A fine-tuning dataset for small-scale Japanese LLM chatbots, consisting of 26,728 annotated instances from Aratako/Magpie-Tanuki-8B, focusing on specific information-seeking, reasoning, and planning queries.
  - Downloads: 39
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - A dataset of adult-content Japanese manga in CBZ format, suitable for image analysis and text recognition research.
  - Downloads: 39
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - Seven Moon Cats voice data for the game Hopeless Lost School Day.
  - Downloads: 38
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - A dataset of 280 illustrations of females generated with nijijourney v5 for LoRA training, including some copyrighted characters, with accompanying text files; use for model transparency but not for illegal or harmful activities.
  - Downloads: 36
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - A processed embedding learning dataset for 50,000 posts from t_w's Delight submissions, fixing missing text issues and altering data structure, with restricted reuse under Japanese law.
  - Downloads: 35
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - The repository contains a cleaned version of the first 950 items from meta-math/MetaMathQA, translated using RekaAI/reka-flash-3, with improperly formatted items removed.
  - Downloads: 30
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository contains a dataset of Japanese web pages extracted from the CommonCrawler using cc-downloader-rs, provided by the IPA's ICSCoE for research purposes only.
  - Downloads: 20
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
### Responsible NLP
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository contains a Japanese-language dataset extracted from CommonCrawler using cc-downloader-rs, provided with usageÈôêÂà∂Ê®°ÂºèÂ∑≤ÊøÄÊ¥ªÔºåË∂ÖÂá∫ÊòæÁ§∫ËÉΩÂäõ„ÄÇp≈Çyw<|im_start|>"user"Summarize the GitHub repository description in a single concise sentence focusing on the key points. Ensure that the summary is clear and includes important information such as the source of the data and the purpose of use.Repository Description:CC-MAIN-2019-39„Å∏„Çà„ÅÜ„Åì„Åù Êú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØCommonCrawler„Å®Âëº„Å∞„Çå„Çã„ÇÇ„ÅÆ„Åã„ÇâÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ Âà©Áî®„Åó„Åü„ÇÇ„ÅÆ„ÅØcc-downloader-rs„Åß„Åô„ÄÇ „Å™„ÅäIPA„ÅÆICSCoE„Å®Âëº„Å∞„Çå„Çã„Å®„Åì„Çç„Åã„ÇâË≥áÊ∫ê„ÇíÂÄü„Çä„Å¶„ÇÑ„Çä„Åæ„Åó„Åü„ÇÜ„Åà„Å´„ÄÅ„Åø„Å™„Åï„ÇìIPA„Å´ÊÑüË¨ù„Åó„Åæ„Åó„Çá„ÅÜ„ÄÇ ‚Äª IPA„ÅØÁã¨Á´ãË°åÊîøÊ≥ï‰∫∫ ÊÉÖÂ†±Âá¶ÁêÜÊé®ÈÄ≤Ê©üÊßã„ÅÆ„Åì„Å®„Åß„Åô„ÄÇ„ÉÜ„Çπ„Éà„Å´Âá∫„Åæ„Åô„ÅÆ„ÅßË¶ö„Åà„Åæ„Åó„Çá„ÅÜ„ÄÇ Âà©Áî®„Å´„Å§„ÅÑ„Å¶ Êú¨Âà©Áî®„ÅØÁ†îÁ©∂ÁõÆÁöÑ„ÅÆ„Åø„Å®„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åô„ÄÇ „Åù„Çå‰ª•Â§ñ„ÅÆÂà©Áî®„Å´„Å§„Åç„Åæ„Åó„Å¶„ÅØÈÄîÊñπ„ÇÇ„Åè„Çå„Å™„ÅÑÊï∞„ÅÆËëó‰ΩúÊ®©ËÄÖ„Å´Ë®±ÂèØ„ÇíÊ±Ç„ÇÅ„Å¶„Åç„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 5,391
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This GitHub repository contains Japanese web corpora from 2009, converted and marked with sentence boundaries through morphological analysis for research purposes.
  - Downloads: 2,028
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository contains a Japanese dataset extracted from CommonCrawler using cc-downloader-rs, courtesy of IPA's ICSCoE, for research purposes only.
  - Downloads: 851
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - The repository contains randomly extracted text from various sources, re-generated with phi3, and includes large parquet files; download requires git lfs, with some datasets unavailable through standard libraries due to size limitations; partial supercomputing resources from Tokyo Tech's TSUBAME4.0 were used for calculations.
  - Downloads: 661
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - A cleaned and de-duplicated mqa dataset with preprocessed noisy text and indexed positive and negative query-passage pairs.
  - Downloads: 306
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset comprises hand-extracted FAQ questions and answers from Japanese government agency websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models.
  - Downloads: 246
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - A dataset generated by replacing text in Japanese Wikipedia and providing queries and responses to LLMs, licensed under CC-BY-SA 4.0.
  - Downloads: 230
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - A curated dataset of Japanese Wikipedia input errors, specifically focusing on Kanji conversion mistakes, designed for use with HuggingFace.
  - Downloads: 217
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - A dataset of 20,000 Japanese instructional prompts and their responses generated by the LLM-JP 3.13B model, formatted in JSON, for instruction-following task learning and evaluation.
  - Downloads: 153
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - A corpus of Japanese text randomly extracted from sources like Wikibooks and Wikipedia, re-generated with Phi-3, automatically translated to English, and stored in large parquet files requiring git lfs for download.
  - Downloads: 153
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository contains a Japanese dataset extracted from CommonCrawler using cc-downloader-rs, provided by the IPA's ICSCoE for research purposes only.
  - Downloads: 142
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - Japanese translation of GuanacoDataset using langdetect.
  - Downloads: 127
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - A low-quality dataset forËµã‰∫àËÅäÂ§©ËØ≠Ë®ÄÊ®°ÂûãPythonÂáΩÊï∞Ë∞ÉÁî®ËÉΩÂäõÔºåÂåÖÂê´1002‰∏™Ê†∑Êú¨ÔºåÁî±Qwen2.5-32b-instructÁîüÊàêÊåá‰ª§ÔºåPhi-4ÁîüÊàêÂõûÁ≠îÔºåÂ≠òÂú®‰∏Ä‰∫õÂ∑•ÂÖ∑‰ΩøÁî®Áõ∏ÂÖ≥ÈóÆÈ¢ò„ÄÇ
  - Downloads: 121
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - A converted chat-format dataset from oasst1-89k-ja for multi-turn conversation fine-tuning, in ShareGPT format, requiring significant computational resources.
  - Downloads: 93
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - Users must download publicly available models, datasets, and other content while agreeing not to use it for commercial purposes and taking responsibility for legal compliance.
  - Downloads: 88
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - The repository contains automatically generated Japanese Q&A in RAG format using texts randomly extracted from sources like Wikibooks and legal cases, intended for instruction dataset pre-training, with some computation done on TSUBAME4.0 supercomputer.
  - Downloads: 76
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - A dataset translated from English Wikipedia to Japanese using cyberagent's DeepSeek-R1-Distill-Qwen-32B model, including processed Japanese translations and original English texts, with few-shot examples and CC-BY-SA 4.0 licensing.
  - Downloads: 73
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - A dataset generated using Phi-3 to create sentences in Japanese based on ConceptNet 5.7 triples with a specific prompt.
  - Downloads: 66
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset includes 1243 tweets selected from the author's tweets between May 16, 2022, and May 24, 2024, focusing on those that convey difficult concepts or have unique worlds, to enhance a base model's expression.
  - Downloads: 64
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - A Japanese translation of the "OpenAssistant/oasst2" dataset, converted into chat format using DeepL, along with conversion code for fine-tuning models.
  - Downloads: 58
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - The repository includes a subset of Japanese conversations from the Open Assistant dataset, available at https://huggingface.co/datasets/timdettmers/openassistant-guanaco.
  - Downloads: 48
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - Automatically generated conversation data created from excerpts of "I Will Not Say I Am a Dog" randomly selected from the Qingkong Literature Archive using Calm3-22B-chat, with a light cleaning.
  - Downloads: 48
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - A processed dataset with unique queries, preprocessed for encoding issues and NFKC normalization, where query IDs correspond to collection subset indices.
  - Downloads: 47
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - Based on Japan Post's international mail content items English-Chinese translation and HS codes (as of May 9, 2024).
  - Downloads: 45
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset includes questions and answers about characters from the Touhou Project, structured in CSV files, suitable for training chatbots or machine learning models.
  - Downloads: 41
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - A collection of approximately 16,000 automatically generated Japanese instructions and their corresponding inferences, initial responses, and refined answers, designed for large language model training and evaluation.
  - Downloads: 40
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - The dataset includes slightly cleaned dialogue from the anime "My Lady is a Despicable Person," containing lines mostly from Lay, including some responses from Claire. Usage of the dataset is at the user's own risk as the animator's rights are not owned.
  - Downloads: 38
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - Download and use the published models and datasets, acknowledging no guarantees are made regarding compliance with laws or quality, and agree not to exploit copyrighted works for personal enjoyment.
  - Downloads: 31
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - Download the publicly available models and datasets while agreeing not to use them for profiting from the expressed ideas or emotions, understanding that no legal compliance or quality guarantees are provided.
  - Downloads: 24
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - Users agree not to use the publicly downloadable models and datasets for commercial or unauthorized personal gain,È°ªÈÅµÂÆàÊ≥ïÂæãÂπ∂Âú®Êä´Èú≤ÂÜÖÂÆπÊó∂ÂêåÊ†∑Ë¶ÅÊ±ÇÁ¨¨‰∏âÊñπÈÅµÂÆà„ÄÇ
  - Downloads: 15
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - Download the publicly shared models and datasets, agree not to use them for profit, and understand you bear responsibility for legal compliance.
  - Downloads: 14
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - Download this repository's publicly shared models, datasets, and other content while agreeing not to use them for exploiting the expressed ideas or emotions they contain, understanding that no warranties are provided and you alone are responsible for legal compliance.
  - Downloads: 13
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - Users must download publicly shared models, datasets, and other content while agreeing not to use the copyrighted materials for profit or to exploit the expressed ideas or emotions they contain, and to comply with laws when utilizing and sharing the content.
  - Downloads: 12
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - Users must agree not to use the downloaded public models and datasets for commercial purposes and understand that the repository provider does not guarantee legal compliance or quality, disclaiming liability for their use.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository contains a dataset of Japanese content extracted from the CommonCrawler using cc-downloader-rs, provided by IPA's ICSCoE for research purposes only.
  - Downloads: 11
### Reasoning
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a handcrafted Japanese dataset for logical reasoning tasks, suitable for both pre-training and post-training.
  - Downloads: 329
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - The JSNLI dataset is a Japanese translation of the SNLI benchmark for natural language inference, structured in TSV format and preprocessed with JUMAN++.
  - Downloads: 318
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a high-quality synthetic dataset for Japanese math problems with assured correctness and chain-of-thought reasoning, derived from English seeds translated using Qwen2-7B-Instruct.
  - Downloads: 275
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - A commercially usable, Japanese-translated dataset of 180K mathematical instruction tuning examples, derived from synthetic solutions to GSM8K and MATH benchmark questions, licensed by NVIDIA for commercial use.
  - Downloads: 227
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - This repository contains a clone of the JSQuAD dataset for Japanese reading comprehension, including SB Intuitions corrections and evaluation score improvements.
  - Downloads: 202
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - The abc-multiple-choice dataset contains multiple-choice questions from the "abc" quiz competition, with evaluation scripts available for research purposes only.
  - Downloads: 151
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - The repository contains code and datasets for ensuring evaluation score reproducibility and includes a Japanese version of CommonsenseQA (JCommonsenseQA) licensed under CC BY-SA 4.0.
  - Downloads: 134
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - The LogicJa dataset includes 105 multi-turn tasks covering various domains like math, writing, and coding to evaluate Japanese language models' reasoning capabilities.
  - Downloads: 130
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - The GitHub repository contains a high-quality, small-scale Japanese dataset for commercial use, including commonsense_qa, Calc-ape210k, and japanese-commonsense-openqa, licensed under the Database Contents License v1.0.
  - Downloads: 117
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - The repository offers an enhanced Japanese math dataset with 50k samples for reinforcing multi-step reasoning, complementing the original NuminaMath CoT dataset.
  - Downloads: 88
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE is a Japanese NLI dataset consisting of premise and hypothesis sentences labeled as entailment, contradiction, or neutral.
  - Downloads: 86
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - A Japanese-translated subset of 100k samples from the original NuminaMath CoT dataset, including math problems and their Chain of Thought solutions.
  - Downloads: 84
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench evaluates advanced Japanese reasoning capabilities using mathematics entrance exam questions from Kyoto University for testing Large Language Models.
  - Downloads: 82
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - A dataset of synthesized math instructions and corresponding answers created using Magpie, with two generations per instruction and only consistent pairs retained.
  - Downloads: 82
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - Filtered dataset from magpie-reasoning-llama-nemotron-70b-100k with "ÊîπËâØ" excluded, in OpenAI messages format.
  - Downloads: 81
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - Highly informative multi-turn conversation data in Japanese, created from synthesized datasets based on cosmopedia, focusing on dialogues about mathematics education.
  - Downloads: 77
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - A curated Japanese dataset of 1800 high-quality instruction, reasoning, and answer triples generated using the Qwen2.5-32B-Instruct model.
  - Downloads: 73
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - A Japanese-translated version of the OpenO1-SFT dataset containing 77,312 Chain of Thought reasoning examples for fine-tuning language models.
  - Downloads: 70
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - A collection of 125,000 Japanese instruction-response pairs auto-generated using the Qwen2.5-32B-instruct model for task learning and evaluation, formatted in JSONL.
  - Downloads: 59
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - A dataset of 200 simplified tasks based on Kendamarron/jimba-instruction-1k-beta, created for reproducing the in-depth evolving of Wizard LM, with plans to increase record numbers and detailed information available elsewhere.
  - Downloads: 44
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suite that extends the FraCaS suite to evaluate recognizing textual entailment in natural language processing.
  - Downloads: 42
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - A dataset of 17k math instruction examples generated using Magpie with specific system prompts, including logical and mathematical reasoning outputs and their corresponding Python code executions.
  - Downloads: 40
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - The JCQ dataset is a Japanese creative thinking assessment consisting of seven tasks with 100 questions each, inspired by TTCT and Zhao's research, evaluating creativity through tasks like unusual uses, consequences, and assumptions.
  - Downloads: 32
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Responsible & Trustworthy NLP
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - The JaNLI dataset provides Japanese examples to test models' understanding of linguistic nuances and identify vulnerabilities.
  - Downloads: 218
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - The repository contains 42K Japanese-English pairs for instruction tuning, part of Swallow-Magpie-Ultra v0.1, used to train Llama models.
  - Downloads: 214
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - The Gendec framework uses machine learning to detect gender from Japanese names, as detailed in a paper accepted at ISDA'23.
  - Downloads: 176
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - A dataset of Japanese Medical Licensing Exam questions from exams 110 to 117, intended for model evaluation and tasks like evolutionary model merging, with some unstructured questions excluded, licensed under CC-BY-NC-ND 4.0.
  - Downloads: 172
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - The AttaQ-JA dataset comprises 1402 adversarial questions in Japanese to evaluate LLMs for harmful responses, containing offensive content.
  - Downloads: 126
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - A Japanese unsupervised speech dataset covering 28 domains, compliant with data protection regulations, and tested by AI companies for use in real-world tasks.
  - Downloads: 118
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - The LLM-jp Toxicity Dataset is a Japanese-language dataset for identifying harmful content.
  - Downloads: 104
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - Data from pairwise evaluations of responses from two LLM models using various models, created for verifying consistency with manual and open-LLM automatic evaluations.
  - Downloads: 58
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - The dataset is not publicly shared to prevent leakage into LLM training data.
  - Downloads: 49
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - The repository contains a dataset with keys for identifying game states, units, and positions, including team-specific unit classes, unit states, locations, and more.
  - Downloads: 37
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - You agree to the terms of the LICENSE when using this dataset.
  - Downloads: 32
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - The Cauldron-JA is a dataset translated from 'The Cauldron' using DeepL API, excluding certain datasets, specifically for fine-tuning a vision-language model.
  - Downloads: 10,190
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - A Japanese translation of the "databricks-dolly-15k" dataset, licensed under CC-BY-SA-3.0, last updated on 2023-05-11.
  - Downloads: 871
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - A dataset for training translation models, containing paired Japanese-Korean text from the Helsinki-NLP/Tatoeba-Challenge repository, limited to non-commercial use.
  - Downloads: 251
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - A dataset with Japanese translations of "OpenAssistant/oasst1," including failed translations marked by "ng_translation" and manually corrected code-related data.
  - Downloads: 225
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a machine-translated subset of ultrachat_200k, containing 6,537 training and 995 test samples, with some IDs missing.
  - Downloads: 182
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - The GitHub repository contains a deduplicated Japanese-translated version of radiology reports from the CT-RATE dataset, facilitating Japanese medical AI model development.
  - Downloads: 113
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - A Japanese translation dataset of 69K examples from "databricks-dolly-15k," licensed under CC BY SA 3.0, last updated on 2023-04-18.
  - Downloads: 101
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - The Kaidan Nihonbunka dataset collects Japanese ghost stories, or kaidan, related to the Hyakumonogatari tradition, preserving traditional Japanese folklore.
  - Downloads: 73
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository collects ~40 high-quality Japanese open-source datasets for downstream tasks,Êú™ÁøªËØëÂÆåÊàêÔºå‰ΩÜÊÄªÁªì‰ø°ÊÅØÂ¶Ç‰∏ãÔºöËøô‰ªìÂ∫ìÊ±áÈõÜ‰∫ÜÁ∫¶40‰∏™È´òË¥®ÈáèÁöÑÊó•Êú¨ÂºÄÊ∫ê‰∏ãÊ∏∏‰ªªÂä°Êï∞ÊçÆÈõÜÔºå‰∏çÂåÖÂê´Êó•ËØ≠Êú∫Âô®ÁøªËØëÔºåÂπ∂Êåâ‰ªªÂä°ÂàÜÁ±ª„ÄÇ
  - Downloads: 69
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - A Japanese-English parallel corpus containingÁ∫¶26‰∏áÂè•Ê≥ïÂæã sentences, extracted from the Japanese-English Legal Parallel Corpus and available for use via Hugging Face datasets.
  - Downloads: 65
### Information Retrieval
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - A large-scale Japanese QA dataset generated from Wikipedia text using Swallow-MX, suitable for QA model training and RAG system development.
  - Downloads: 706
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - A dataset of 1 million retrieval-based multi-turn chat entries synthesized using large language models, released a year ago for continued pre-training and use in studies on data and internet culture.
  - Downloads: 343
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - A JSONL dataset of metadata for YouTube channels, including VTuber and non-VTuber channels, for text classification tasks, with a biased class distribution.
  - Downloads: 275
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - The repository contains a Japanese QA dataset where human workers retrieve information from Wikipedia in response to questions.
  - Downloads: 215
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This dataset, used in the book "Introduction to Large Language Models," is based on passages from the AI King competition, licensed under CC BY-SA 3.0 and GFDL, derived from datasets in the cl-tohoku/quiz-datasets repository.
  - Downloads: 130
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - A QA dataset for training a document retrieval model using passages from the "Â§ßË¶èÊ®°Ë®ÄË™ûÊ®°ÂûãÂÖ•ÈñÄ" book, based on datasets from cl-tohoku/quiz-datasets.
  - Downloads: 111
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset contains automatically translated text from cosmopedia-100k index 20k to 100k into Japanese, excluding records with translation errors due to text length.
  - Downloads: 93
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - The repository contains passages of up to 400 characters from Japanese Wikipedia, used as a dataset for AI question-answering competitions.
  - Downloads: 36
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - A dataset containing archived chat logs from the entire history of NicoNico Live's streaming platform, collected and curated by community effort to preserve over 11 years of past comments before the service's transition and eventual API discontinuation.
  - Downloads: 2,826,723
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - A dataset of 100 complex Japanese instructions for evaluating instruction-tuned language models, annotated for reliable assessment across various tasks.
  - Downloads: 2,562
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - A binary sentiment analysis dataset for Japanese text, derived from the WRIME dataset, labeled as positive or negative based on Avg. Readers_Sentiment values.
  - Downloads: 547
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - The repository contains a corpus of recordings from an 81-year-old woman, with both noisy and cleaned wav files and phoneme labels, available for download from Google Drive or Hugging Face hub.
  - Downloads: 149
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - A manually checked and corrected Japanese Instruction dataset created from the output of cyberagent/calm2-7b-chat.
  - Downloads: 95
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - A modified Japanese dataset based on Dolly-15K, altered to emulate Yuki Nagato's speaking style, with "„Åß„Åô„ÄÅ„Åæ„Åô" and "„Å†„ÄÅ„Åß„ÅÇ„Çã" replaced, for personal use but freely accessible.
  - Downloads: 76
### Linguistics & Cognitive NLP
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - A curated translation of helpful-base chosen English texts from https://github.com/anthropics/hh-rlhf, excluding and correcting poorly translated parts with fuguMT.
  - Downloads: 60
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - NewsQ, a Japanese QA benchmark on news information, is freely available on Hugging Face with registration required upon agreeing to terms and conditions.
  - Downloads: 21
