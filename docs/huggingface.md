# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP.

At present, 1337 models and 509 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [Êó•Êú¨Ë™û (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [ÁπÅÈ´î‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [ÁÆÄ‰Ωì‰∏≠Êñá (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)

## Contents üìñ

The following categories were constructed with reference to the research from [Exploring-NLP-Research](https://github.com/sebischair/Exploring-NLP-Research).

Thank you for your efforts in categorizing the field of natural language processing.

 * [Models](#models)
	 * [Semantic Text Processing](#Semantic-Text-Processing)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing)
	 * [Multilinguality](#Multilinguality)
	 * [Text Generation](#Text-Generation)
	 * [Multimodality](#Multimodality)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining)
	 * [Responsible NLP](#Responsible-NLP)
	 * [Reasoning](#Reasoning)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP)
	 * [Sentiment Analysis](#Sentiment-Analysis)
	 * [Information Retrieval](#Information-Retrieval)
	 * [Information Retrieval and Information Extracrtion & Text Mining](#Information-Retrieval-and-Information-Extracrtion-&-Text-Mining)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation)

 * [Datasets](#datasets)
	 * [Information Extraction & Text Mining](#Information-Extraction-&-Text-Mining-1)
	 * [Multimodality](#Multimodality-1)
	 * [Multilinguality](#Multilinguality-1)
	 * [Semantic Text Processing](#Semantic-Text-Processing-1)
	 * [Natural Language Interfaces](#Natural-Language-Interfaces-1)
	 * [Text Generation](#Text-Generation-1)
	 * [Syntactic Text Processing](#Syntactic-Text-Processing-1)
	 * [Responsible NLP](#Responsible-NLP-1)
	 * [Reasoning](#Reasoning-1)
	 * [Responsible & Trustworthy NLP](#Responsible-&-Trustworthy-NLP-1)
	 * [Multilinguality and Text Generation](#Multilinguality-and-Text-Generation-1)
	 * [Information Retrieval](#Information-Retrieval-1)
	 * [Sentiment Analysis](#Sentiment-Analysis-1)
	 * [Linguistics & Cognitive NLP](#Linguistics-&-Cognitive-NLP)


## The latest additions üéâ

**Models**
6 models have been added.

- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)


**Datasets**
3 datasets have been added.

- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)


## Models üß†

This list is sorted by downloads as of May 06, 2025.
1337 models are listed.

### Semantic Text Processing
- [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets with a 16kHz sampling rate.
  - Downloads: 1,396,376
- [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from xlm-roberta-base using a dataset from Japanese Wikipedia, classifying tokens into PER and ORG entities.
  - Downloads: 591,852
- [cl-nagoya/ruri-base](https://huggingface.co/cl-nagoya/ruri-base)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring installation of `sentence-transformers`, `fugashi`, `sentencepiece`, and `unidic-lite`, and specifying "„ÇØ„Ç®„É™: " or "ÊñáÁ´†: " prefixes for optimal query/passage performance.
  - Downloads: 531,071
- [globis-university/deberta-v3-japanese-large](https://huggingface.co/globis-university/deberta-v3-japanese-large)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 446,296
- [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)
  - This repository provides a Japanese BERT model pretrained using Unidic-lite word-level tokenization and whole word masking on CC-100 and JawiWiki datasets.
  - Downloads: 368,380
- [retrieva-jp/amber-large](https://huggingface.co/retrieva-jp/amber-large)
  - AMBER is a 315M parameter text embedding model by Retrieva, built on modernbert-ja-310m, and optimized for both Japanese and English text understanding.
  - Downloads: 295,622
- [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16)
  - Rinna‚Äôs japanese-cloob-vit-b-16 is a Japanese contrastive language-image pre-training (CLIP) model, installable via pip and usable for image-text matching tasks.
  - Downloads: 244,828
- [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese)
  - This repository provides a Japanese BERT base model pretrained on Japanese text using word and WordPiece tokenization based on the IPA dictionary, mirroring the original BERT base architecture.
  - Downloads: 150,838
- [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2)
  - This repository provides a Japanese BERT model pre-trained on jawiki-20200831, utilizing character and word-level (Unidic 2.1.2) tokenization with whole word masking for masked language modeling.
  - Downloads: 117,934
- [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking)
  - This repository provides a Japanese BERT model pre-trained with IPA dictionary-based word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 112,735
- [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char)
  - This repository provides a Japanese BERT base model pretrained using both word and character tokenization, mirroring the original BERT base architecture with 12 layers and 768 dimensions.
  - Downloads: 107,118
- [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3)
  - This repository provides a Japanese BERT model pre-trained with character and word-level tokenization (using Unidic) and whole word masking on CC-100 and JA Wiki datasets.
  - Downloads: 103,588
- [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
  - This repository provides a refined Japanese sentence-BERT model (v2), trained with MultipleNegativesRankingLoss for improved accuracy over v1, and requiring Fugashi and IPAdic for inference.
  - Downloads: 91,589
- [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese)
  - This repository provides a tiny Japanese DeBERTa V2 model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 80,838
- [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja)
  - GLuCoSE is a Japanese sentence embedding model, built on LUKE and trained on diverse data, for tasks like semantic similarity and search.
  - Downloads: 55,632
- [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)
  - This repository provides a Japanese Sentence-BERT model (version 1) for generating sentence embeddings, with a more accurate version 2 also available, leveraging the `transformers` library and PyTorch.
  - Downloads: 48,176
- [cl-nagoya/ruri-small-v2](https://huggingface.co/cl-nagoya/ruri-small-v2)
  - Ruri provides small, pre-trained Japanese text embeddings using Sentence Transformers, requiring `fugashi`, `sentencepiece`, and `unidic-lite` with specific prefixes ("„ÇØ„Ç®„É™:" or "ÊñáÁ´†:") for optimal query/passage processing.
  - Downloads: 46,496
- [pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)
  - PLaMo-Embedding-1B is a top-performing Japanese text embedding model by Preferred Networks, excelling in tasks like information retrieval and achieving state-of-the-art results on the JMTEB benchmark.
  - Downloads: 40,242
- [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 38,693
- [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 36,345
- [elyza/Llama-3-ELYZA-JP-8B](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)
  - Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter large language model built on Meta Llama 3 and optimized for Japanese language tasks through pre-training and instruction tuning.
  - Downloads: 27,345
- [sbintuitions/sarashina-embedding-v1-1b](https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b)
  - Sarashina-Embedding-v1-1B is a state-of-the-art Japanese text embedding model, built on Sarashina2.1-1B, that generates 1792-dimensional vectors for semantic similarity and search tasks, achieving top performance on JMTEB.
  - Downloads: 23,648
- [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium)
  - This repository hosts a medium-sized Japanese GPT-2 language model trained by rinna Co., Ltd., and readily usable with the `transformers` library.
  - Downloads: 21,263
- [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment)
  - This repository provides a BERT-based model fine-tuned for Japanese sentiment analysis of Amazon product reviews, classifying sentence polarity as positive or negative.
  - Downloads: 19,234
- [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE)
  - LaBSE is a multilingual BERT-based model providing sentence embeddings for 109 languages, trained with masked and translation language modeling for tasks like bi-text retrieval, and migrated to PyTorch from TensorFlow.
  - Downloads: 18,222
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3)
  - Llama 3.1 Swallow are 8B and 70B large language models extending Meta‚Äôs Llama 3.1 with enhanced Japanese language capabilities via pre-training on a 200 billion token corpus of Japanese and English data.
  - Downloads: 18,145
- [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm)
  - This repository provides a Japanese DeBERTa V2 large language model pre-trained on diverse Japanese datasets using character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 17,248
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 16,575
- [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, trained on 312.5B tokens from Japanese datasets and achieving a perplexity of 8.68.
  - Downloads: 15,492
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 14,967
- [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall)
  - This repository hosts rinna‚Äôs extra-small Japanese GPT-2 language model, offering code for use with the `transformers` library.
  - Downloads: 14,481
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 14,385
- [sbintuitions/sarashina2.2-3b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1)
  - SB Intuitions‚Äô repository hosts sarashina2.2-3b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English benchmarks alongside other models like Qwen and RakutenAI.
  - Downloads: 13,820
- [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2)
  - This repository provides a Japanese BERT model pretrained on Jawiki-20200831 using Unidic-lite word-level tokenization and whole word masking for improved language understanding.
  - Downloads: 13,176
- [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1)
  - This repository provides a series of Japanese cross-encoder rerankers (xsmall to large) with varying layer and hidden size configurations, offering models for improved Japanese information retrieval.
  - Downloads: 12,640
- [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese)
  - LINE DistilBERT Japanese is a pre-trained DistilBERT model for Japanese natural language processing, built by LINE Corporation on 131GB of web text and based on their in-house BERT-base model.
  - Downloads: 12,446
- [kotoba-tech/kotoba-whisper-v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)
  - Kotoba-Whisper-v2.2 is a Japanese ASR model built on Whisper v2.0, enhanced with speaker diarization, punctuation, and integrated into Hugging Face Transformers (v4.39+).
  - Downloads: 11,783
- [cl-nagoya/ruri-small](https://huggingface.co/cl-nagoya/ruri-small)
  - Ruri provides pre-trained Japanese sentence embeddings using Sentence Transformers, requiring `fugashi`, `sentencepiece`, and `unidic-lite` and specifying prefixes ("„ÇØ„Ç®„É™:" or "ÊñáÁ´†:") for input text.
  - Downloads: 11,754
- [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model finetuned for instruction-following conversations using translated Anthropic HH, FLAN, and Stanford datasets.
  - Downloads: 11,381
- [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite)
  - This repository provides a Japanese sentence-transformers model, `sbert-jsnli-luke-japanese-base-lite`, for embedding sentences and paragraphs into 768-dimensional vectors suitable for semantic search and clustering.
  - Downloads: 9,195
- [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2)
  - This repository provides a Japanese BERT large model, pretrained using Unidic-lite word segmentation and whole word masking on CC-100 and JAWIKI datasets for masked language modeling.
  - Downloads: 8,541
- [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings from Wikipedia to generate contextualized representations of words and entities.
  - Downloads: 8,369
- [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small)
  - This repository hosts a small Japanese GPT-2 model trained by rinna, easily usable with the `transformers` library for causal language modeling.
  - Downloads: 7,680
- [tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1)
  - Llama3 Swallow is a family of language models continually pre-trained on Japanese data, built from Meta‚Äôs Llama 3, and including 8B and 70B parameter Instruct and Chat versions released July 1, 2024.
  - Downloads: 6,850
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and distributed as a Python package for advanced Japanese natural language processing with GiNZA v5.
  - Downloads: 6,842
- [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm)
  - This repository provides a tiny Japanese DeBERTa V2 model, pre-trained on Japanese text data with character-level tokenization, for masked language modeling tasks.
  - Downloads: 6,817
- [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base)
  - This repository offers a base-sized Japanese RoBERTa model, trained via rinna's code, and loadable through the `transformers` library for masked language modeling tasks.
  - Downloads: 6,724
- [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b)
  - Rinna's japanese-gpt-1b is a 1.3 billion parameter Japanese GPT model for causal language modeling, easily implemented with Hugging Face Transformers.
  - Downloads: 6,650
- [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and improved performance through dropout management and layer separation.
  - Downloads: 6,501
- [llm-jp/llm-jp-3-7.2b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct3)
  - LLM-jp-3-7.2b-instruct3 is a 7.2 billion parameter, instruction-tuned Japanese large language model developed by NII, available in Hugging Face Transformers format with specific PyTorch and Transformers version requirements.
  - Downloads: 6,316
- [kotoba-tech/kotoba-whisper-v2.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0)
  - Kotoba-Whisper provides faster, distilled Japanese Automatic Speech Recognition (ASR) models based on OpenAI's Whisper, utilizing large-v3 and stable-ts/punctuation for improved performance.
  - Downloads: 6,234
- [cl-nagoya/ruri-large](https://huggingface.co/cl-nagoya/ruri-large)
  - Ruri provides pre-trained Japanese text embeddings (v3 models ranging from 30M to 315M parameters) using Sentence Transformers, offering high JMTEB scores for tasks like semantic search and text similarity.
  - Downloads: 5,976
- [mmnga/Vecteus-v1-gguf](https://huggingface.co/mmnga/Vecteus-v1-gguf)
  - Vecteus-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãVecteus-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 5,901
- [sbintuitions/modernbert-ja-310m](https://huggingface.co/sbintuitions/modernbert-ja-310m)
  - ModernBERT-Ja-310M is a computationally efficient Japanese BERT model trained on 4.09T tokens, utilizing local and global attention with RoPE for improved long sequence handling and a 102,400 vocabulary.
  - Downloads: 5,679
- [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 5,355
- [tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1)
  - Llama 3.1 Swallow is a series of 8B and 70B large language models continually pre-trained on a massive Japanese web corpus and multilingual data to enhance Japanese language capabilities while preserving English proficiency.
  - Downloads: 5,080
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 5,052
- [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web)
  - This repository provides a 32K vocabulary T5 (Text-to-Text Transfer Transformer) model pre-trained on a large corpus of Japanese web text, including mC4 and Wiki40b, with training code available.
  - Downloads: 5,015
- [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base)
  - Rinna‚Äôs Japanese wav2vec2.0 Base model is a 12-layer transformer trained on nearly 19,000 hours of Japanese speech data, replicating the original wav2vec 2.0 Base architecture.
  - Downloads: 4,937
- [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including models varying in layer count and hidden size, designed to improve search relevance and detailed in accompanying technical reports.
  - Downloads: 4,856
- [sbintuitions/modernbert-ja-130m](https://huggingface.co/sbintuitions/modernbert-ja-130m)
  - ModernBERT-Ja-130M is a computationally efficient Japanese BERT model leveraging local & global attention, RoPE, and trained on 4.39T tokens with a 102.4K vocabulary for long sequence handling.
  - Downloads: 4,639
- [cl-nagoya/ruri-v3-310m](https://huggingface.co/cl-nagoya/ruri-v3-310m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, offering improved performance and efficiency through extended sequence/vocabulary support and FlashAttention.
  - Downloads: 4,537
- [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin)
  - This repository provides a Japanese BERT small model pretrained on Wikipedia and financial texts, utilizing a 12-layer, 256-dimensional architecture.
  - Downloads: 4,428
- [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large)
  - Rinna‚Äôs Japanese HuBERT Large model is a 24-layer transformer trained on 19,000 hours of Japanese speech data from the Rea corpus, mirroring the original HuBERT Large architecture.
  - Downloads: 4,388
- [mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-deepseek-R1K-RL-EZO-gguf)
  - This repository provides a GGUF format conversion of the AXCXEPT-phi-4-deepseek-R1K-RL-EZO model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 4,108
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-14B and 32B Japanese language models, trained with the TFMC/imatrix dataset and usable with llama.cpp.
  - Downloads: 4,103
- [pfnet/plamo-2-1b](https://huggingface.co/pfnet/plamo-2-1b)
  - PLaMo 2 1B is a 1 billion parameter English/Japanese language model utilizing a hybrid Mamba/sliding window attention architecture‚Äîlike Samba, but with added normalization for stable training‚Äîdeveloped by Preferred Elements, Inc.
  - Downloads: 4,007
- [cyberagent/llava-calm2-siglip](https://huggingface.co/cyberagent/llava-calm2-siglip)
  - llava-calm2-siglip is an experimental vision-language model enabling image question answering in Japanese, built on the Llava architecture.
  - Downloads: 3,879
- [llm-jp/llm-jp-3-1.8b](https://huggingface.co/llm-jp/llm-jp-3-1.8b)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct versions) developed by NII, utilizing Hugging Face Transformers and requiring specific library versions like PyTorch, Transformers, and Accelerate.
  - Downloads: 3,818
- [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base)
  - Rinna‚Äôs Japanese HuBERT Base model is a 12-layer transformer trained on 19,000 hours of Reazon Japanese speech data, mirroring the original HuBERT Base architecture.
  - Downloads: 3,798
- [mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf](https://huggingface.co/mmnga/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the deepseek-r1-distill-qwen2.5-bakeneko-32b language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 3,795
- [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jsts) fine-tuned on the JSTS dataset for semantic similarity calculations, as detailed in chapter 5 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ.‚Äù
  - Downloads: 3,788
- [cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)
  - Cyberagent‚Äôs DeepSeek-R1-Distill-Qwen-14B-Japanese is a Japanese-language finetuned model built on DeepSeek-R1-Distill-Qwen-14B, designed for use with the Transformers library.
  - Downloads: 3,731
- [Rakuten/RakutenAI-7B-chat](https://huggingface.co/Rakuten/RakutenAI-7B-chat)
  - RakutenAI-7B is a high-performing 7B Japanese LLM built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 3,632
- [mmnga/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/mmnga/Fugaku-LLM-13B-instruct-gguf)
  - This repository provides a GGUF-formatted version of the Fugaku-LLM-13B-instruct language model, trained with the imatrix dataset, and requires acceptance of the associated terms of use for usage with llama.cpp.
  - Downloads: 3,494
- [cl-nagoya/ruri-large-v2](https://huggingface.co/cl-nagoya/ruri-large-v2)
  - Ruri provides large Japanese text embeddings using Sentence Transformers, requiring `fugashi`, `sentencepiece`, and `unidic-lite` and utilizing prefixes like "„ÇØ„Ç®„É™:" or "ÊñáÁ´†:" for query/passage text.
  - Downloads: 3,293
- [tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Meta's Llama 3.3, significantly enhancing Japanese language capabilities while preserving English proficiency using a 315 billion token corpus.
  - Downloads: 3,186
- [Rakuten/RakutenAI-2.0-8x7B-instruct](https://huggingface.co/Rakuten/RakutenAI-2.0-8x7B-instruct)
  - RakutenAI-2.0-8x7B-instruct is a 13B-parameter Japanese large language model fine-tuned for enhanced instruction-following, fluency, and contextual understanding.
  - Downloads: 3,131
- [rinna/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides GGUF quantized versions of the 32B rinna/qwen2.5-bakeneko-32b-instruct model, compatible with llama.cpp applications, alongside other quantization formats like AWQ and GPTQ.
  - Downloads: 3,003
- [elyza/Llama-3-ELYZA-JP-8B-GGUF](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF)
  - ELYZA‚Äôs Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B-parameter large language model based on Meta Llama 3, provided in a quantized GGUF (Q4_K_M) format for use with llama.cpp.
  - Downloads: 2,842
- [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 2,715
- [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)
  - This repository provides a series of Japanese cross-encoder rerankers‚Äîincluding xsmall, small, base, and large models with varying layer and hidden sizes‚Äîfor improved text retrieval performance.
  - Downloads: 2,580
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)
  - This repository provides GGUF format conversions of the ELYZA-japanese-Llama-2-7b and CodeLlama models, including a fast-instruct version optimized for speed and reduced token cost.
  - Downloads: 2,559
- [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 7B model, supported by a16z and utilizing hardware from Massed Compute.
  - Downloads: 2,544
- [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite)
  - This repository provides a Japanese Sentence-LUKE model, trained similarly to Japanese Sentence-BERT, demonstrating improved qualitative and comparable quantitative accuracy, and requiring SentencePiece for inference.
  - Downloads: 2,475
- [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 2,296
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-gguf-japanese-imatrix)
  - This repository provides a Japanese-finetuned version of the DeepSeek-R1-Distill-Qwen-14B model, enabling it to generate thought processes and creative text‚Äîlike novel introductions‚Äîdirectly in Japanese.
  - Downloads: 2,277
- [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b)
  - Japanese-StableLM-Instruct-Beta-70B is a 70B-parameter language model, fine-tuned for instruction following using datasets like Dolly-15k, with smaller 7B and fast-tokenizer versions also available.
  - Downloads: 2,235
- [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation's Japanese large language model, `japanese-large-lm-1.7b-instruction-sft`, for use with llama.cpp.
  - Downloads: 2,235
- [cl-nagoya/ruri-reranker-large](https://huggingface.co/cl-nagoya/ruri-reranker-large)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to improve relevance by scoring sentence pairs based on a query.
  - Downloads: 2,230
- [mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository provides GGUF-formatted versions of the DeepSeek-R1-Distill-Qwen-32B Japanese language model, trained with the TFMC/imatrix dataset, and intended for use with llama.cpp.
  - Downloads: 2,212
- [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf)
  - This repository provides GGUF conversions of the Line Corporation‚Äôs 1.7B Japanese language model, alongside instructions and a script for use with llama.cpp.
  - Downloads: 2,162
- [mmnga/japanese-stablelm-2-instruct-1_6b-gguf](https://huggingface.co/mmnga/japanese-stablelm-2-instruct-1_6b-gguf)
  - This repository provides a GGUF-formatted version of StabilityAI's japanese-stablelm-2-instruct-1_6b language model, trained with the imatrix dataset, and requires agreement to the terms of use, including membership for commercial applications.
  - Downloads: 2,099
- [pkshatech/RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
  - RoSEtta is a Japanese sentence embedding model based on RoFormer, optimized for long-sequence (1024 tokens) retrieval and semantic similarity tasks, and runnable on CPUs.
  - Downloads: 2,085
- [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base)
  - StableLM-3B-4E1T is a 3B-parameter Japanese language model pretrained from its English counterpart to excel in Japanese language modeling and downstream tasks.
  - Downloads: 2,056
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
  - PLaMo-13B is a 13B LLaMA-based language model pre-trained on English and Japanese datasets by Preferred Networks, released under Apache 2.0 and usable via the `transformers` pipeline.
  - Downloads: 2,012
- [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo)
  - This repository hosts a 3.6 billion parameter Japanese GPT-NeoX language model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 1,999
- [tokyotech-llm/Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1)
  - Llama3 Swallow is a continually pre-trained language model based on Meta‚Äôs Llama 3, enhanced with Japanese language data and available in 8B and 70B parameter Instruct and Chat versions released July 1, 2024.
  - Downloads: 1,990
- [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 1,928
- [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf)
  - This repository provides a GGUF conversion of the AIBunCho japanese-novel-gpt-j-6b model, intended for use with llama.cpp, but potentially incompatible with future updates to llama.cpp's core implementations.
  - Downloads: 1,855
- [hotchpotch/japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2)
  - This repository provides a high-performance Japanese SPLADE v2 model for converting text to sparse vectors, with a WebUI demo and utilizing YAST/YASEM for training and easy inference/token inspection.
  - Downloads: 1,844
- [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-marc_ja) fine-tuned on the JGLUE MARC-ja dataset for sentiment analysis, as detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 1,730
- [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama 7b models, including a fast version with reduced token cost and increased speed.
  - Downloads: 1,712
- [llm-jp/llm-jp-3-1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct3)
  - This repository hosts the llm-jp-3-1.8b-instruct3, a 1.8 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring PyTorch 2.3.0+ and Transformers 4.40+.
  - Downloads: 1,626
- [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 1,585
- [cl-nagoya/ruri-reranker-base](https://huggingface.co/cl-nagoya/ruri-reranker-base)
  - Ruri-Reranker is a Japanese sentence reranking model, built on Sentence Transformers, designed to improve search relevance by scoring sentence pairs based on their semantic similarity.
  - Downloads: 1,554
- [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm)
  - This repository provides a Japanese character-level DeBERTa V2 base model pre-trained on large-scale Japanese text corpora for masked language modeling tasks.
  - Downloads: 1,545
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Gamma 7B model, created with support from a16z and hardware from Massed Compute.
  - Downloads: 1,524
- [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking)
  - This repository provides a Japanese BERT model pretrained with character and word-level tokenization and whole word masking for improved masked language modeling performance.
  - Downloads: 1,489
- [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube)
  - This repository provides HuBERT-base model weights trained on JTubeSpeech for speech recognition tasks, specifically functioning as an encoder to embed audio into latent variables, unlike generative models like ChatGPT.
  - Downloads: 1,487
- [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos)
  - This repository provides a RoBERTa-small Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 1,458
- [cl-nagoya/ruri-v3-30m](https://huggingface.co/cl-nagoya/ruri-v3-30m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, featuring extended sequence length support (8192 tokens), a larger 100K vocabulary, and integrated FlashAttention for improved performance and efficiency.
  - Downloads: 1,455
- [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 7B models, including instruct and fast-tuned variants, as well as CodeLlama versions, for optimized performance.
  - Downloads: 1,442
- [Rakuten/RakutenAI-7B](https://huggingface.co/Rakuten/RakutenAI-7B)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 1,432
- [koshin2001/Japanese-to-emotions](https://huggingface.co/koshin2001/Japanese-to-emotions)
  - Japanese to emotions I fine-tuned LINE DistillBERT as the base model using WRIME Ver2 as the teacher data.
  - Downloads: 1,430
- [llm-jp/llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct-tuned variants) developed by NII's R&D Center for LLMs, with support from GENIAC, in Hugging Face Transformers format.
  - Downloads: 1,418
- [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese)
  - This repository provides a Japanese BERT large model, pretrained on Japanese text with Unidic-lite word-level tokenization and whole word masking for improved masked language modeling.
  - Downloads: 1,409
- [llm-jp/llm-jp-modernbert-base](https://huggingface.co/llm-jp/llm-jp-modernbert-base)
  - llm-jp-modernbert-base is a Japanese language model based on modernBERT, trained on a 3.4TB corpus with support for 8192 sequence length using the llm-jp-tokenizer.
  - Downloads: 1,403
- [mmnga/llm-jp-3-8x1.8b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x1.8b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x1.8b-instruct3 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 1,364
- [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including `hotchpotch/japanese-bge-reranker-v2-m3-v1`, varying in size (layers, hidden_size) for improved sentence ranking performance.
  - Downloads: 1,326
- [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)
  - This repository hosts ABEJA's 2.7B-parameter Japanese GPT-NeoX model, compatible with transformers v4.23+, for text generation tasks.
  - Downloads: 1,310
- [KoichiYasuoka/modernbert-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-wikipedia)
  - modernbert-base-japanese-wikipedia Model Description
  - Downloads: 1,303
- [mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf](https://huggingface.co/mmnga/lightblue-DeepSeek-R1-Distill-Qwen-7B-Japanese-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B Japanese language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 1,302
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf)
  - This repository provides GGUF and GPTQ formatted versions of ELYZA's Japanese-language CodeLlama-7b-instruct model, along with related Llama 2 models optimized for speed and token cost.
  - Downloads: 1,288
- [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, enhanced with additional pre-training for improved Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 1,250
- [sbintuitions/modernbert-ja-30m](https://huggingface.co/sbintuitions/modernbert-ja-30m)
  - ModernBERT-Ja-30M is a computationally efficient Japanese BERT model trained on 4.39T tokens, utilizing local & global attention and RoPE for long sequence handling with a 102,400 vocabulary.
  - Downloads: 1,207
- [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) fine-tuned on JA Wiki data using unsupervised SimCSE for sentence embeddings and semantic similarity tasks.
  - Downloads: 1,175
- [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI‚Äôs Japanese StableLM Instruct Beta 70B model, supported by a16z and hardware from Massed Compute.
  - Downloads: 1,167
- [cl-nagoya/ruri-base-v2](https://huggingface.co/cl-nagoya/ruri-base-v2)
  - Ruri provides Japanese general text embeddings with v3 models (30m-315m parameters, max length 8192) offering up to 77.24 JMTEB scores, utilizing Sentence Transformers, Fugashi, SentencePiece, and Unidic-Lite.
  - Downloads: 1,156
- [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite)
  - Luke-Japanese-Large-Lite is a lightweight, pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 1,136
- [cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - CyberAgent‚Äôs DeepSeek-R1-Distill-Qwen-32B-Japanese is a finetuned 32B parameter language model for Japanese text generation, utilizing the DeepSeek-R1-Distill-Qwen architecture.
  - Downloads: 1,120
- [cl-nagoya/ruri-v3-70m](https://huggingface.co/cl-nagoya/ruri-v3-70m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model built on ModernBERT-Ja, offering extended sequence length (8192 tokens) and a larger 100K vocabulary for improved performance and efficiency.
  - Downloads: 1,115
- [cyberagent/Mistral-Nemo-Japanese-Instruct-2408](https://huggingface.co/cyberagent/Mistral-Nemo-Japanese-Instruct-2408)
  - CyberAgent‚Äôs Mistral-Nemo-Japanese-Instruct-2408 is a continually pre-trained Japanese instruction-following model built upon Mistral-Nemo-Instruct-2407, utilizing the Hugging Face Transformers library.
  - Downloads: 1,106
- [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b)
  - This repository hosts LINE Corporation's 3.6B parameter Japanese language model for text generation, with accompanying usage examples and a technical blog.
  - Downloads: 1,098
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF formatted versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, optimized for speed and reduced token cost, alongside related models like 7B versions and CodeLlama adaptations.
  - Downloads: 1,097
- [tokyotech-llm/Llama-3.1-Swallow-8B-v0.2](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2)
  - Llama 3.1 Swallow are 8B and 70B large language models continually pre-trained on Llama 3.1 to significantly improve Japanese language capabilities while maintaining English proficiency, utilizing a 200 billion token corpus of Japanese web data, Wikipedia, and code.
  - Downloads: 1,083
- [mmnga/llm-jp-3-8x13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-8x13b-instruct3-gguf)
  - This repository provides a GGUF-formatted version of the llm-jp-3-8x13b-instruct3 Japanese language model, utilizing data from TFMC/imatrix-dataset-for-japanese-llm and intended for use with llama.cpp.
  - Downloads: 1,078
- [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b)
  - Llama 3 Youko 8B is a continually pre-trained 8B parameter Llama 3 model‚Äîenhanced with 22B Japanese/English tokens‚Äîimproving performance on Japanese language tasks, available in both base and instruction-tuned versions with HF and GPTQ formats.
  - Downloads: 1,072
- [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)
  - Japanese-StableLM-Base-Alpha-7B is a 7B-parameter language model pre-trained on Japanese and English data to maximize performance in Japanese language modeling and downstream tasks, with an instruction-following version also available.
  - Downloads: 1,066
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,040
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.2-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-8B-Instruct-v0.2„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,020
- [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)
  - ABEJA‚Äôs repository hosts a large Japanese GPT-2 model for text generation, requiring sentencepiece installation and utilizing the ‚Äúabeja/gpt2-large-japanese‚Äù model name with the transformers pipeline.
  - Downloads: 1,018
- [llm-jp/llm-jp-3-13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct3)
  - This repository hosts llm-jp-3-13b-instruct3, a 13 billion parameter Japanese large language model developed by NII‚Äôs R&D Center, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 999
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.3-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 982
- [mmnga/cyberagent-open-calm-7b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-7b-gguf)
  - This repository provides GGUF format conversions of the CyberAgent Open Calm 7B language model, compatible with llama.cpp, but potentially subject to future incompatibility with updates to llama.cpp's GPT-Neox implementation.
  - Downloads: 978
- [Aratako/calm3-22b-RP-v2-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-v2-GGUF)
  - This repository provides a GGUF quantized version of the Aratako/calm3-22b-RP-v2 model, distributed under a CC-BY-NC-SA 4.0 license due to training data including outputs from OpenAI's GPT-4o-mini and Anthropic‚Äôs Claude 3.5 Sonnet.
  - Downloads: 947
- [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF)
  - This repository provides GGUF quantized versions of Stability AI's Japanese StableLM Base Beta 70B language model, created with support from a16z and Massed Compute.
  - Downloads: 940
- [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf)
  - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8b-Cosmopedia-japanese„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 919
- [owner203/japanese-llama-3-8b](https://huggingface.co/owner203/japanese-llama-3-8b)
  - Japanese-LLaMA-3-8B Japanese-LLaMA-3-8B„ÅØÂü∫Áõ§„É¢„Éá„É´„ÄÅ„Éï„É´„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 919
- [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, enhanced with additional pre-training for improved performance as an assistant.
  - Downloads: 918
- [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf)
  - This repository provides a GGUF-formatted conversion of the stockmark-gpt-neox-japanese-1.4b language model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation within llama.cpp.
  - Downloads: 886
- [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)
  - LINE Corporation's repository hosts a 3.6 billion parameter Japanese language model, fine-tuned for instruction-following and conversational ability.
  - Downloads: 877
- [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf)
  - haqishen-Llama-3-8B-Japanese-Instruct-gguf haqishen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Japanese-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 861
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
  - PLaMo-13B-Instruct is a 13B parameter, Apache 2.0 licensed, Japanese instruct-tuned language model built on PLaMo-13B with an 8192 context length.
  - Downloads: 853
- [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese)
  - This repository provides a Japanese DeBERTa V3 base model pre-trained on the LLM-jp corpus v1.0, usable for masked language modeling tasks via Hugging Face Transformers.
  - Downloads: 834
- [mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf](https://huggingface.co/mmnga/alfredplpl-Llama-3-8B-Instruct-Ja-gguf)
  - alfredplpl-Llama-3-8B-Instruct-Ja-gguf alfredplpl„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-8B-Instruct-Ja„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 823
- [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b)
  - OpenCALM-1B is a 1 billion parameter decoder-only language model pre-trained on Japanese data by CyberAgent, enabling text generation via Hugging Face Transformers.
  - Downloads: 820
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model, including a 2.0GB i1-IQ1_S quantized file, with usage guidance referencing TheBloke's documentation.
  - Downloads: 816
- [abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-7b-Japanese-v0.1)
  - ABEJA-Qwen2.5-7b-Japanese-v0.1 is a Japanese language model based on Qwen2.5-7B-Instruct, enhanced via distillation from a larger ABEJA model and instruction-following via ChatVector differences, without post-training.
  - Downloads: 786
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-8B-Instruct-v0.3-gguf)
  - This repository provides a GGUF-formatted conversion of the tokyotech-llm Llama-3.1-Swallow-8B-Instruct-v0.3 model, trained with the TFMC/imatrix-dataset-for-japanese-llm, for use with llama.cpp.
  - Downloads: 773
- [mmnga/Moonlight-16B-A3B-Instruct-gguf](https://huggingface.co/mmnga/Moonlight-16B-A3B-Instruct-gguf)
  - This repository provides a GGUF-formatted version of the Moonshot AI Moonlight-16B-A3B-Instruct model, trained with Japanese LLM data, and intended for testing with llama.cpp.
  - Downloads: 758
- [rinna/youri-7b](https://huggingface.co/rinna/youri-7b)
  - Youri-7b is a continually pre-trained 7-billion parameter Llama 2 model‚Äîusing a 32-layer transformer‚Äîon 40B Japanese & English tokens to enhance Japanese language performance.
  - Downloads: 758
- [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b)
  - Nekomata-7B is a continually pre-trained version of Qwen-7B on 30B Japanese & English tokens, enhancing performance on Japanese tasks with an expanded vocabulary for efficient text processing.
  - Downloads: 758
- [mmnga/r1-1776-distill-llama-70b-gguf](https://huggingface.co/mmnga/r1-1776-distill-llama-70b-gguf)
  - This repository provides a GGUF-formatted conversion of the r1-1776-distill-llama-70b language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and intended for use with llama.cpp.
  - Downloads: 754
- [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2)
  - Japanese-StableLM-Instruct-Alpha-7B-v2 is a 7B parameter language model fine-tuned for instruction following, built upon Japanese-StableLM-Base-Alpha-7B and designed for Japanese text generation.
  - Downloads: 748
- [tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3)
  - Llama 3.1 Swallow are large language models (8B, 70B) continually pre-trained on Llama 3.1 to significantly enhance Japanese language capabilities while maintaining English proficiency, utilizing a 200 billion token corpus.
  - Downloads: 741
- [weblab-GENIAC/Tanuki-8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0)
  - Tanuki-8B-dpo-v1.0 is an 8B parameter language model, pre-trained on 1.3T tokens and fine-tuned for dialogue using SFT and DPO, with available quantized versions (AWQ, GPTQ, GGUF).
  - Downloads: 737
- [abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1](https://huggingface.co/abeja/ABEJA-Qwen2.5-32b-Japanese-v0.1)
  - ABEJA-Qwen2.5-32b-Japanese-v0.1 is a Japanese-focused language model built upon Qwen2.5-32B-Instruct, enhanced with ChatVector for improved instruction-following, and detailed in the linked ABEJA tech blog.
  - Downloads: 732
- [Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)
  - RakutenAI-7B is a high-performing 7B Japanese language model built on the Mistral architecture, achieving state-of-the-art results on Japanese benchmarks while remaining competitive in English.
  - Downloads: 731
- [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b)
  - Rinna‚Äôs nekomata-14b is a continually pre-trained version of Qwen-14b‚Äîusing 66B Japanese/English tokens‚Äîthat significantly enhances Japanese language performance with an expanded vocabulary and long sequence support.
  - Downloads: 731
- [Aratako/calm3-22b-RP-GGUF](https://huggingface.co/Aratako/calm3-22b-RP-GGUF)
  - calm3-22b-RP-GGUF Ê¶ÇË¶Å Aratako/calm3-22b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 731
- [cl-nagoya/ruri-v3-130m](https://huggingface.co/cl-nagoya/ruri-v3-130m)
  - Ruri v3 is a state-of-the-art Japanese text embedding model based on ModernBERT-Ja, offering improved performance and efficiency with support for longer sequences (8192 tokens) and an expanded 100K token vocabulary.
  - Downloads: 730
- [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp)
  - This repository provides a Japanese SimCSE model (pkshatech/simcse-ja-bert-base-clcmlp) for generating sentence embeddings from Japanese text, built on cl-tohoku/bert-base-japanese-v2 and trained with the JSNLI dataset, requiring fugashi and unidic-lite for tokenization.
  - Downloads: 720
- [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese Llama 2 and CodeLlama 7b models, including standard, fast, and instruct-tuned variants.
  - Downloads: 712
- [bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf)
  - This repository provides GGUF quantized versions of the DeepSeek-R1-Distilled Qwen-14B language model, specifically for Japanese language processing, with a MIT license.
  - Downloads: 711
- [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b)
  - LINE Corporation‚Äôs japanese-large-lm-1.7b is a 1.7 billion parameter Japanese language model for text generation, readily usable with Hugging Face Transformers and detailed in a tech blog.
  - Downloads: 700
- [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)
  - This repository hosts rinna's 3.6B parameter Japanese GPT-NeoX model, finetuned with a new data split for improved instruction-following conversational abilities.
  - Downloads: 697
- [QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/QuantFactory/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, a Japanese language model based on Mistralai's Mistral-Nemo-Instruct, optimized for use with llama.cpp.
  - Downloads: 695
- [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf)
  - This repository provides GGUF and GPTQ quantized versions of ELYZA‚Äôs Japanese-language CodeLlama-7b and Llama-2-7b models, including both base and instruct versions, optimized for performance and reduced token cost.
  - Downloads: 694
- [umiyuki/Umievo-itr012-Gleipnir-7B](https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B)
  - Gleipnir-7B is a merged 7B Japanese language model evolved from Japanese-Starling-ChatV-7B, Ninja-v1-RP-expressive-v2, Vecteus-v1, and Japanese-Chat-Umievo-itr004.
  - Downloads: 681
- [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b)
  - Japanese Stable LM Base Gamma 7B is a 7-billion parameter decoder-only language model pretrained on Japanese data based on Mistral-7B-v0.1, optimized for Japanese language tasks.
  - Downloads: 666
- [sbintuitions/sarashina1-7b](https://huggingface.co/sbintuitions/sarashina1-7b)
  - Sarashina1-7B This repository provides Japanese language models trained by SB Intuitions.
  - Downloads: 661
- [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct)
  - This repository provides a 3B-parameter Japanese language model, fine-tuned for instruction following, based on the StableLM-3B-4E1T base and offering a smaller alternative to the 7B Gamma version.
  - Downloads: 652
- [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, accessible via Hugging Face Transformers.
  - Downloads: 650
- [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b)
  - Japanese-StableLM-Base-Beta-70B is a 70B-parameter language model fine-tuned on diverse Japanese data, built upon Llama-2-70B, for enhanced performance in Japanese language tasks.
  - Downloads: 637
- [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained on Japanese & multilingual data and fine-tuned with continual learning via SteerLM to create a chat-optimized version.
  - Downloads: 637
- [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft)
  - LINE Corporation‚Äôs repository hosts a 1.7B parameter Japanese language model fine-tuned for instruction following and improved dialogue performance.
  - Downloads: 636
- [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli)
  - This repository provides a Japanese BERT model (llm-book/bert-base-japanese-v3-jnli) fine-tuned on the JGLUE MARC-ja dataset for Natural Language Inference (NLI) tasks, as detailed in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 631
- [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b)
  - CyberAgentLM2-7B is a 7B parameter decoder-only language model pre-trained on 1.3T Japanese and English tokens, requiring transformers >= 4.34.1 for use.
  - Downloads: 626
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-based question-answering model pretrained on the Japanese Aozora corpus for dependency parsing and head-detection, utilizing masked tokens for ambiguous word clarification.
  - Downloads: 617
- [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b)
  - Stability AI's Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter decoder-only language model fine-tuned for instruction following, based on the Japanese Stable LM Base Gamma 7B.
  - Downloads: 616
- [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b)
  - Japanese-StableLM-Instruct-Beta-7B is a 7 billion parameter, decoder-only language model fine-tuned for instruction following, built on japanese-stablelm-base-beta-7b and utilizing datasets like Databricks Dolly-15k, with larger 70B and faster versions also available.
  - Downloads: 613
- [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1)
  - Karakuri LM is a Japanese-enhanced Llama 2 language model, pretrained with expanded vocabulary and further trained on mixed corpora, with a chat-optimized, continually-learned fine-tuned version (Karakuri LM Chat) utilizing SteerLM.
  - Downloads: 595
- [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct)
  - Stockmark-13b-instruct is a 13 billion parameter instruction-tuned Japanese LLM developed by Stockmark Inc., trained using data from the Project of Development of Japanese Instruction data for LLM.
  - Downloads: 591
- [elyza/Llama-3-ELYZA-JP-8B-AWQ](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-AWQ)
  - ELYZA's Llama-3-ELYZA-JP-8B is a Japanese-enhanced, 8B parameter large language model based on Meta Llama 3, provided in a quantized AutoAWQ format.
  - Downloads: 590
- [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b)
  - Japanese-StableLM-Instruct-7B is a 7B-parameter Japanese language model fine-tuned for instruction following, utilizing an expanded Japanese vocabulary for improved performance.
  - Downloads: 584
- [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b)
  - Japanese-StableLM-Base-7B is a 7 billion parameter language model fine-tuned on diverse Japanese data, utilizing an expanded Japanese vocabulary for improved performance on Japanese language tasks.
  - Downloads: 584
- [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b)
  - Stockmark-13b is a 13 billion parameter large language model pretrained on a 220B Japanese token corpus by Stockmark Inc., with an instruction-tuned version (stockmark-13b-instruct) and AWS support.
  - Downloads: 581
- [llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1](https://huggingface.co/llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1)
  - LLM-jp provides collaboratively developed, Japanese and English instruction-tuned large language models‚Äîincluding 13B variants like llm-jp-13b-instruct‚Äîbased on Dolly and OASST datasets, with versions 1.0 and 1.1 available.
  - Downloads: 577
- [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1)
  - This repository provides a series of Japanese cross-encoder rerankers, including xsmall, small, base, and large models with varying layer and hidden size configurations, alongside technical reports and usage instructions.
  - Downloads: 574
- [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)
  - ELYZA-japanese-Llama-2-7b is a Japanese language model built upon Llama 2, further pre-trained to enhance its Japanese capabilities and designed for instruction-following tasks.
  - Downloads: 571
- [llm-jp/llm-jp-3-980m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct3)
  - This repository hosts llm-jp-3-980m-instruct3, a Japanese large language model developed by NII's R&D Center for LLMs, utilizing the Hugging Face Transformers format and requiring PyTorch &gt;=2.3.0 and Transformers &gt;=4.40.
  - Downloads: 569
- [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf)
  - This repository provides GGUF-formatted conversions of the ELYZA-japanese-Llama-2-13b-fast model‚Äîa Japanese language model optimized for speed and reduced token cost‚Äîalong with related models and CodeLlama versions.
  - Downloads: 568
- [tokyotech-llm/Llama-3.1-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1)
  - Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 563
- [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2 with additional pre-training to enhance its Japanese capabilities, available via Hugging Face Transformers.
  - Downloads: 560
- [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo)
  - This repository hosts a 3.8 billion parameter English-Japanese bilingual GPT-NeoX language model, aligned via Reinforcement Learning from Human Feedback (RLHF) for instruction-following conversational AI.
  - Downloads: 554
- [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf)
  - karakuri-lm-70b-chat-v0.1-gguf karakuri-ai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãkarakuri-lm-70b-chat-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 545
- [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small)
  - This repository hosts a small Japanese GPT-NeoX language model, trained with EleutherAI‚Äôs codebase and compatible with Hugging Face‚Äôs official GPT-NeoX implementation.
  - Downloads: 542
- [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct)
  - ELYZA-japanese-Llama-2-13b is a Japanese language model built upon Llama 2, further pre-trained for enhanced performance and designed for fast instruction-following.
  - Downloads: 541
- [mmnga/qwq-bakeneko-32b-gguf](https://huggingface.co/mmnga/qwq-bakeneko-32b-gguf)
  - This repository provides a GGUF-formatted version of the qwq-bakeneko-32b language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and is compatible with llama.cpp for inference.
  - Downloads: 538
- [llm-jp/llm-jp-3-440m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-440m-instruct3)
  - This repository hosts llm-jp-3-440m-instruct3, a 440M parameter Japanese language model from NII's LLM-jp-3 series, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 524
- [Aratako/calm3-22b-RP-v2](https://huggingface.co/Aratako/calm3-22b-RP-v2)
  - This repository provides a QLoRA-finetuned version of cyberagent/calm3-22b-chat, optimized for roleplaying using the ChatML format and offering a demo and GGUF version.
  - Downloads: 518
- [mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-T2-2B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-T2-2B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 516
- [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b)
  - Japanese-StableLM-Base-Beta-7B is a 7B-parameter language model fine-tuned on Japanese data, built upon Llama-2-7b, and designed for strong performance in Japanese language tasks, with a corresponding instruction-following model available.
  - Downloads: 511
- [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b)
  - Stockmark‚Äôs GPT-NeoX-Japanese-1.4B is a 1.4 billion parameter language model pre-trained on a 20 billion token Japanese corpus, designed for use with PyTorch and Transformers.
  - Downloads: 507
- [mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf](https://huggingface.co/mmnga/pfnet-Llama3-Preferred-MedSwallow-70B-gguf)
  - pfnet-Llama3-Preferred-MedSwallow-70B-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-Preferred-MedSwallow-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 503
- [llm-jp/llm-jp-3-150m-instruct3](https://huggingface.co/llm-jp/llm-jp-3-150m-instruct3)
  - This repository hosts the llm-jp-3-150m-instruct3, a 150 million parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 499
- [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc)
  - PLaMo-13B-Instruct-NC is a non-commercial, 13B parameter language model fine-tuned on Japanese datasets for instruction following, based on the 8192-context PLaMo-13B and released under CC-BY-NC-4.0.
  - Downloads: 496
- [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese)
  - This repository provides a large Japanese DeBERTa V2 model pre-trained on extensive Japanese text corpora for masked language modeling tasks.
  - Downloads: 492
- [llm-jp/llm-jp-3-980m](https://huggingface.co/llm-jp/llm-jp-3-980m)
  - This repository hosts llm-jp-3-980m, a 980 million parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 485
- [mmnga/llm-jp-3-13b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-13b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-13b-instruct3 Japanese language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp (excluding custom chat templates/`-cvn`).
  - Downloads: 473
- [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head)
  - This repository provides a Japanese BERT model pretrained on Wikipedia data for dependency parsing and question answering, built upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 467
- [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1)
  - This repository provides a pre-trained Japanese ALBERT model (‚Äúken11/albert-base-japanese-v1‚Äù) intended for fine-tuning on various tasks, with specific instructions for handling the [MASK] token when using Sentencepiece tokenizer.
  - Downloads: 462
- [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja)
  - This repository provides a Japanese Sentence BERT base model, pretrained on colorfulscoop/bert-base-ja v1.0 and fine-tuned using the Japanese SNLI dataset for sentence similarity and understanding tasks.
  - Downloads: 459
- [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite)
  - Luke-Japanese is a lightweight, pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities, excluding Wikipedia entity embeddings.
  - Downloads: 445
- [mmnga/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the RakutenAI-2.0-mini-instruct language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and compatible with llama.cpp.
  - Downloads: 445
- [mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf)
  - tokyotech-llm-Llama-3.1-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 431
- [sbintuitions/modernbert-ja-70m](https://huggingface.co/sbintuitions/modernbert-ja-70m)
  - ModernBERT-Ja-70M is a computationally efficient Japanese BERT model, trained on 4.39T tokens with RoPE and a 102,400 vocabulary, designed for handling long sequences.
  - Downloads: 426
- [hotchpotch/japanese-splade-base-v1](https://huggingface.co/hotchpotch/japanese-splade-base-v1)
  - È´òÊÄßËÉΩ„Å™Êó•Êú¨Ë™û SPLADE (Sparse Lexical and Expansion Model) „É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 426
- [QuantFactory/shisa-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-7b-v1-GGUF)
  - QuantFactory/shisa-7b-v1-GGUF This is quantized version of augmxnt/shisa-base-7b-v1 created using llama.cpp Model Description shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
  - Downloads: 417
- [llm-jp/llm-jp-3-150m](https://huggingface.co/llm-jp/llm-jp-3-150m)
  - This repository hosts llm-jp-3-150m, a 150 million parameter Japanese large language model developed by NII, utilizing Hugging Face Transformers and requiring specific torch, transformers, and tokenizers versions.
  - Downloads: 408
- [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese)
  - This repository provides a Japanese language model based on XLNet, requiring Mecab and SentencePiece, and utilizing NFKD normalization which removes Japanese accent marks.
  - Downloads: 404
- [rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides GGUF quantized versions of the rinna/qwen2.5-bakeneko-32b-instruct-v2 Japanese language model, compatible with llama.cpp applications, including merged reasoning models like DeepSeek R1 Distill.
  - Downloads: 401
- [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base)
  - LUKE-Japanese is a pre-trained Japanese language model leveraging knowledge-based embeddings to generate contextualized representations of words and entities from text, utilizing Wikipedia entity embeddings.
  - Downloads: 392
- [alfredplpl/gemma-2-2b-jpn-it-gguf](https://huggingface.co/alfredplpl/gemma-2-2b-jpn-it-gguf)
  - This repository provides GGUF quantized versions of Google's Gemma 2B Japanese-Italian model, enabling use with tools like llama.cpp and LM Studio, built using a process based on npaka's LLM-jp-3.
  - Downloads: 390
- [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus)
  - This repository provides a T5-base model fine-tuned on the livedoor-news corpus for Japanese text summarization, as featured in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 371
- [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese)
  - This repository provides a pretrained DeBERTa V2 base model for Japanese masked language modeling, utilizing code from retarfi/language-pretraining and a specific sentencepiece tokenizer.
  - Downloads: 360
- [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base)
  - This repository details a Japanese typo detection model‚Äîbuilt on RoBERTa‚Äîthat identifies and scores the probability of various character-level errors like deletions, insertions, substitutions, and incorrect kanji conversions within text.
  - Downloads: 359
- [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-Japanese speech recognition model, trained on public datasets like Common Voice, requiring 16kHz sampled input, with a more accurate, larger version available upon request.
  - Downloads: 357
- [mmnga/karakuri-lm-32b-thinking-2501-exp-gguf](https://huggingface.co/mmnga/karakuri-lm-32b-thinking-2501-exp-gguf)
  - This repository provides a GGUF-formatted conversion of the karakuri-lm-32b-thinking-2501-exp large language model, trained with imatrix data and designed for use with llama.cpp.
  - Downloads: 353
- [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese)
  - This repository provides a fine-tuned Japanese version of OpenAI‚Äôs Whisper-small speech recognition model, trained on Japanese datasets and requiring 16kHz audio input.
  - Downloads: 351
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 350
- [llm-jp/llm-jp-3-3.7b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct3)
  - This repository hosts the llm-jp-3-3.7b-instruct3, a 3.7 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.
  - Downloads: 349
- [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model featuring GEGLU activation and offering improved performance through pre-training optimizations like disabled dropout.
  - Downloads: 348
- [QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF
  - Downloads: 348
- [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese)
  - This repository provides a pretrained Japanese RoBERTa base model for masked language modeling, trained on Japanese Wikipedia and CC-100 data.
  - Downloads: 346
- [sbintuitions/sarashina2.2-1b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-1b-instruct-v0.1)
  - SB Intuitions' repository hosts the sarashina2.2-1b-instruct-v0.1, a Japanese autoregressive language model evaluated on Japanese and English tasks, alongside performance comparisons to other models.
  - Downloads: 343
- [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli)
  - This repository provides a Japanese Natural Language Inference (NLI) model‚ÄîakiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick‚Äîtrained with SentenceTransformers on the JSNLI dataset, outputting entailment scores for sentence pairs.
  - Downloads: 339
- [alter-wang/bert-base-japanese-emotion-lily](https://huggingface.co/alter-wang/bert-base-japanese-emotion-lily)
  - This is a BERT Base model for emotion analysis in Japanese additionally fine-tuned for emotion detection and classification.
  - Downloads: 338
- [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides GGUF conversions of Japanese-Starling-ChatV-7B, a 7B Japanese chat model built upon chatntq-ja-7b-v1.0 and enhanced with a chat vector derived from Starling-LM-7B-beta.
  - Downloads: 337
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf)
  - This repository provides GGUF versions of Stability AI's Japanese-StableLM-3B-4E1T-Base language model, noting Llama.cpp GPU offloading limitations to 34 layers.
  - Downloads: 334
- [Fugaku-LLM/Fugaku-LLM-13B-instruct](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed through a collaborative project utilizing the Fugaku supercomputer, permitting both commercial and non-commercial use, modification, and redistribution.
  - Downloads: 329
- [llm-jp/llm-jp-3-3.7b](https://huggingface.co/llm-jp/llm-jp-3-3.7b)
  - This repository hosts large Japanese language models (llm-jp-3, ranging from 1.8b to 172b parameters, including instruct versions) developed by NII, formatted for Hugging Face Transformers and requiring specific library versions.
  - Downloads: 321
- [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune)
  - This repository provides a Japanese BERT base model fine-tuned on the WRIME dataset for predicting emotion intensities (eight emotions) in Japanese tweets, as detailed in a related research paper.
  - Downloads: 320
- [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf)
  - This repository provides GGUF versions of rinna's japanese-gpt-neox-3.6b model, intended for use with llama.cpp, but may become incompatible upon official GPT-Neox implementation within llama.cpp.
  - Downloads: 308
- [rinna/qwq-bakeneko-32b](https://huggingface.co/rinna/qwq-bakeneko-32b)
  - QwQ Bakeneko 32B is an instruction-tuned, Japanese language model built upon rinna/qwen2.5-bakeneko-32b, optimized for reasoning and available in various quantized formats (AWQ, GGUF, GPTQ).
  - Downloads: 299
- [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese)
  - This repository provides a BERT small model pre-trained on Japanese Wikipedia data for financial text applications, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 297
- [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering)
  - This repository provides a JaQuAD-fine-tuned RoBERTa base model for Japanese question answering, with example code using the `transformers` library.
  - Downloads: 292
- [2121-8/canary-tts-0.5b](https://huggingface.co/2121-8/canary-tts-0.5b)
  - Canary-TTS-0.5B is a text-to-speech model built on sarashina2.2‚Äë0.5b‚Äëinstruct‚Äëv0.1 and XCodec2, offering fine-grained voice control via prompt-based adjustments to pitch, gender, and noise.
  - Downloads: 276
- [mmnga/cyberagent-open-calm-3b-gguf](https://huggingface.co/mmnga/cyberagent-open-calm-3b-gguf)
  - This repository provides a GGUF format conversion of the cyberagent-open-calm-3b language model, intended for use with llama.cpp and potentially subject to compatibility changes with future updates.
  - Downloads: 269
- [mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-7B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 259
- [cl-nagoya/ruri-v3-pt-30m](https://huggingface.co/cl-nagoya/ruri-v3-pt-30m)
  - Ruri provides pretrained and fine-tuned Japanese text embedding models‚Äîbased on ModernBERT-Ja and available in various sizes‚Äîfor general-purpose natural language processing, evaluated using JMTEB scores.
  - Downloads: 250
- [nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf](https://huggingface.co/nappa0326/llama-3-elyza-jp-8b-ft-functioncalling-gguf)
  - This repository provides a Japanese Llama-3-ELYZA-JP-8B model finetuned on a specific dataset and accelerated with Unsloth & TRL, licensed under Apache 2.0.
  - Downloads: 248
- [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional)
  - This repository provides a Japanese BERT base model further pretrained on financial text, utilizing the cl-tohoku/bert-base-japanese model and a 12-layer, 768-dimension architecture.
  - Downloads: 243
- [llm-jp/llm-jp-3-8x13b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x13b-instruct3)
  - This repository hosts the llm-jp-3-8x13b-instruct3, a large language model developed by NII's R&D Center for LLMs, utilizing Hugging Face Transformers and requiring torch >= 2.3.0.
  - Downloads: 232
- [mmnga/qwen2.5-bakeneko-32b-instruct-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-gguf)
  - This repository provides a GGUF-formatted version of the Qwen2.5-bakeneko-32b-instruct language model, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 226
- [kajuma/DiffLlama-1B](https://huggingface.co/kajuma/DiffLlama-1B)
  - DiffLlama-1B is a 1 billion parameter language model pre-trained on 100B tokens, utilizing a Differential Transformer architecture with Differential Attention to improve focus and reduce noise, and employing techniques like patch-level training and the Muon optimizer for efficient learning.
  - Downloads: 226
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the Llama tokenizer.
  - Downloads: 226
- [llm-jp/llm-jp-3-8x1.8b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-8x1.8b-instruct3)
  - This repository hosts the llm-jp-3-8x1.8b-instruct3, an 8x1.8B parameter Japanese large language model developed by NII's R&D center, utilizing the Hugging Face Transformers format and requiring torch>=2.3.0.
  - Downloads: 217
- [mmnga/RakutenAI-2.0-8x7B-instruct-gguf](https://huggingface.co/mmnga/RakutenAI-2.0-8x7B-instruct-gguf)
  - This repository provides GGUF-formatted conversions of the RakutenAI-2.0-8x7B-instruct large language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and demonstrates its usage with llama.cpp.
  - Downloads: 216
- [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1)
  - VecTeus is a Mistral-7B-based LLM fine-tuned for high-quality Japanese & English generation with a 128k context window and improved long-context memory, including NSFW variants.
  - Downloads: 213
- [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF)
  - This repository provides GGUF versions of the ELYZA-japanese-Llama-2-13b-fast-instruct model, optimized for LlamaEdge (v0.2.8+) with a 5120 context size and specific llama-2-chat prompt formatting.
  - Downloads: 210
- [LoneWolfgang/bert-for-japanese-twitter-sentiment](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 210
- [retrieva-jp/amber-base](https://huggingface.co/retrieva-jp/amber-base)
  - AMBER is a 132M parameter Japanese & English text embedding model built on the modernbert-ja-130m architecture, developed by Retrieva for improved text representation.
  - Downloads: 205
- [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese)
  - This repository provides a pre-trained Japanese BART base model, `ku-nlp/bart-base-japanese`, for conditional generation tasks, requiring word segmentation with Juman++ for input.
  - Downloads: 205
- [dahara1/gemma-2-2b-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-it-gguf-japanese-imatrix)
  - This repository provides a Japanese-optimized, quantized GGUF version of Gemma-2-2B-it, leveraging iMatrix and compatible with llama.cpp's speculative decoding for faster performance.
  - Downloads: 204
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 202
- [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training for enhanced Japanese capabilities, designed for assisting with code and general tasks.
  - Downloads: 201
- [cyberagent/Llama-3.1-70B-Japanese-Instruct-2407](https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407)
  - This repository provides a Japanese instruction-tuned large language model, based on Meta-Llama-3.1-70B-Instruct, for use with the Transformers library.
  - Downloads: 197
- [mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-14B-gguf)
  - This repository provides a GGUF-formatted conversion of the DeepSeek-R1-Distill-Qwen-14B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 197
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model, offering various size/quality options for use with tools like those detailed in TheBloke‚Äôs READMEs.
  - Downloads: 187
- [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta)
  - This repository provides a fine-tuned RoBERTa model for Japanese medical named entity recognition, extracting entities like diseases, body parts, medications, and tests using IOB2 tagging, trained with specific hyperparameters on the MedTxt-CR dataset.
  - Downloads: 182
- [tokyotech-llm/Llama-3.3-Swallow-70B-v0.4](https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4)
  - Llama 3.3 Swallow is a 70B large language model continually pre-trained on Meta‚Äôs Llama 3.3, significantly enhancing Japanese language capabilities alongside retained English proficiency using a 315 billion token corpus.
  - Downloads: 180
- [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
  - GPTSAN is a Japanese language model based on a Switch Transformer architecture, utilizing a Prefix-LM structure and a unique "Spout" vector for controlling generation tendency via pre-training or fine-tuning.
  - Downloads: 180
- [reazon-research/japanese-wav2vec2-large-rs35kh](https://huggingface.co/reazon-research/japanese-wav2vec2-large-rs35kh)
  - This repository provides a fine-tuned wav2vec 2.0 Large model for Japanese Automatic Speech Recognition (ASR) using the ReazonSpeech v2.0 corpus, compatible with the `transformers` library.
  - Downloads: 179
- [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese)
  - This repository provides a large Japanese BART model, pre-trained on Japanese Wikipedia, for conditional generation tasks using the `transformers` library.
  - Downloads: 176
- [dahara1/weblab-10b-instruction-sft-GPTQ](https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ)
  - This repository provides a 6.3GB GPTQ-quantized version of the 10 billion parameter, Japanese-centric multilingual Weblab-10b-instruction-sft model, offering faster speed with a slight performance trade-off.
  - Downloads: 175
- [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf)
  - This repository provides GGUF versions of StabilityAI‚Äôs Japanese-StableLM-3B-4E1T-Instruct language model, noting current Llama.cpp limitations with GPU offloading for these models.
  - Downloads: 172
- [llm-jp/llm-jp-3-172b-instruct3](https://huggingface.co/llm-jp/llm-jp-3-172b-instruct3)
  - This repository hosts Japanese large language models (llm-jp-3, ranging from 1.8b to 172b parameters) developed by NII‚Äôs R&D Center, including both base models and instruction-tuned variants.
  - Downloads: 172
- [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-large model, pretrained on Japanese Wikipedia & CC-100, supporting sequences up to 512 tokens, and usable for masked language modeling via Hugging Face Transformers.
  - Downloads: 168
- [Rakuten/RakutenAI-2.0-mini](https://huggingface.co/Rakuten/RakutenAI-2.0-mini)
  - RakutenAI-2.0-mini is a lightweight, transformer-based Japanese language foundation model optimized for efficient use in resource-limited settings, and serves as a base for instruct models like RakutenAI-2.0-mini-instruct.
  - Downloads: 167
- [alfredplpl/llm-jp-3-3.7b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-3.7b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-3.7b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for creating them from the original LLM-jp-3 model.
  - Downloads: 160
- [mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-phi-4-v2_900-gguf)
  - This repository provides a GGUF-formatted conversion of the EZO-phi-4-v2_900 language model, trained with imatrix Japanese LLM data, and intended for use with llama.cpp.
  - Downloads: 158
- [nlp-waseda/roberta_jtruthfulqa](https://huggingface.co/nlp-waseda/roberta_jtruthfulqa)
  - This repository provides a Waseda RoBERTa model finetuned for evaluating truthfulness in generated answers on the JTruthfulQA dataset.
  - Downloads: 155
- [nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1](https://huggingface.co/nitky/Llama-3.3-SuperSwallow-70B-Instruct-v0.1)
  - This repository hosts Llama-3.3-SuperSwallow-70B-Instruct-v0.1, a merged language model demonstrating Japanese-RP conversational ability with an example dialogue showcasing interaction with Gemini-2.0-flash-exp.
  - Downloads: 153
- [mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-32B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Qwen-32B language model, utilizing the TFMC/imatrix dataset for Japanese LLM training and runnable with llama.cpp.
  - Downloads: 153
- [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese RoBERTa-base model pretrained on Japanese Wikipedia and CC-100, designed for masked language modeling tasks.
  - Downloads: 151
- [Kendamarron/Tokara-0.5B-Chat-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„Å´chat vector„ÅßÂØæË©±ËÉΩÂäõ„ÇíÂä†„Åà„Åü„É¢„Éá„É´„Å´„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 150
- [mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Qwen-1.5B-gguf)
  - This repository provides a GGUF-formatted conversion of the DeepSeek-R1-Distill-Qwen-1.5B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm, and usable with llama.cpp.
  - Downloads: 147
- [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf)
  - This repository provides GGUF-converted versions of Line Corporation‚Äôs Japanese large language model (3.6B parameters) for use with llama.cpp, acknowledging potential future incompatibility with upstream updates.
  - Downloads: 147
- [alfredplpl/gemma-2-baku-2b-it-gguf](https://huggingface.co/alfredplpl/gemma-2-baku-2b-it-gguf)
  - This repository provides GGUF quantized versions of the Japanese-Italian Gemma 2B model, built using npaka's LLM-jp-3 and compatible with llama.cpp, LM Studio, and LLMFarm.
  - Downloads: 147
- [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese)
  - This repository provides a Japanese extractive question answering model fine-tuned from rinna/japanese-roberta-base on the JaQuAD dataset, enabling question answering from Japanese Wikipedia articles.
  - Downloads: 143
- [HPLT/hplt_bert_base_ja](https://huggingface.co/HPLT/hplt_bert_base_ja)
  - HPLT Bert for Japanese This is one of the encoder-only monolingual language models trained as a first release by the HPLT project.
  - Downloads: 142
- [DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1](https://huggingface.co/DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1)
  - DataPilot/ArrowMint-Gemma3-4B-YUKI-v0.1 is a Japanese language model fine-tuned from Google‚Äôs Gemma-3-4B-it, specializing in multi-turn, prompt-following conversational ability for AI VTubers with a focus on lightweight performance.
  - Downloads: 141
- [eliashasnat/phi-3](https://huggingface.co/eliashasnat/phi-3)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 141
- [bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf](https://huggingface.co/bluepen5805/DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf)
  - This repository hosts GGUF versions of the DeepSeek-R1-Distilled Qwen 32B language model, specifically fine-tuned for Japanese language processing, with a permissive MIT license.
  - Downloads: 140
- [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 140
- [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient)
  - This repository likely contains research and experimental data related to the Tsukuba Science City area, potentially involving student projects and analysis from a national university campus in Ibaraki Prefecture, Kanto region.
  - Downloads: 139
- [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja)
  - This repository provides a Japanese pre-trained MobileBERT model, offering a faster inference speed alternative to standard BERT, and is compatible with the `transformers` library using the Tohoku University tokenizer.
  - Downloads: 138
- [mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf](https://huggingface.co/mmnga/AXCXEPT-phi-4-open-R1-Distill-EZOv1-gguf)
  - This repository provides a GGUF-formatted conversion of AXCXEPT's phi-4-open-R1-Distill-EZOv1 model, utilizing the TFMC/imatrix dataset for Japanese language LLM training and compatible with llama.cpp.
  - Downloads: 135
- [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf)
  - This repository provides GGUF conversions of Line Corporation‚Äôs Japanese-large-lm-3.6b-instruction-sft model, intended for use with llama.cpp, but potentially incompatible with future updates to llama.cpp‚Äôs core implementations.
  - Downloads: 132
- [nitky/AtheneX-V2-72B-instruct](https://huggingface.co/nitky/AtheneX-V2-72B-instruct)
  - AtheneX-V2-72B-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 130
- [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, for masked language modeling tasks.
  - Downloads: 129
- [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking)
  - This repository provides japanese-gpt-1b-PII-masking, a 1B Japanese GPT model fine-tuned to mask Personally Identifiable Information (PII) like names, birthdays, phone numbers, addresses, and IDs from Japanese text.
  - Downloads: 129
- [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese)
  - This repository provides a fine-tuned Japanese Whisper model (Ivydata/whisper-base-japanese) built on OpenAI‚Äôs whisper-base, optimized for speech recognition with 16kHz audio using datasets like Common Voice and JVS.
  - Downloads: 127
- [mmnga/matsuolab-weblab-10b-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-gguf)
  - This repository provides a GGUF-formatted conversion of the matsuo-lab weblab-10b model, compatible with llama.cpp examples, utilizing a development branch for faster updates.
  - Downloads: 127
- [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L)
  - This repository provides a base model card template and details for a small, pretrained Japanese/English T5 text-to-text transfer transformer model, requiring further information on its development, type, language specifics, and license.
  - Downloads: 126
- [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 125
- [ssanjay22/japanese_partner](https://huggingface.co/ssanjay22/japanese_partner)
  - Developed by: SANJAY S License: MIT Finetuned from model: Meta's LLAMA 3.2 (3B)
  - Downloads: 124
- [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick)
  - Cross-Encoder for Natural Language Inference(NLI) for Japanese This model was trained using SentenceTransformers Cross-Encoder class.
  - Downloads: 123
- [llm-jp/llm-jp-3-7.2b-instruct](https://huggingface.co/llm-jp/llm-jp-3-7.2b-instruct)
  - This repository hosts LLM-jp-3-7.2b-instruct, a large language model developed by NII, with Hugging Face Transformers compatibility and specific library version requirements.
  - Downloads: 121
- [cinmodel/electra-small-japanese-discriminator](https://huggingface.co/cinmodel/electra-small-japanese-discriminator)
  - This repository provides a pre-trained Japanese ELECTRA-small model, built on Japanese Wikipedia subword units with MeCab tokenization, for use with the `transformers` library.
  - Downloads: 119
- [MIL-UT/Asagi-14B](https://huggingface.co/MIL-UT/Asagi-14B)
  - Asagi-14B is a 14-billion parameter Japanese Vision & Language Model trained on a large, diverse dataset‚Äîincluding synthetically generated data from CALM3 and Phi3.5-vision‚Äîwith open-source output usage rights.
  - Downloads: 118
- [MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-T2-2B-gemma-2-it-GGUF)
  - HODACHIÊßò„ÅÆ EZO-Common-T2-2B-gemma-2-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 117
- [rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8](https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-gptq-int8)
  - This repository provides a 32B Qwen2.5 Bakeneko instruct model quantized to 8-bit using AutoGPTQ for reduced memory usage and faster inference.
  - Downloads: 113
- [DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE](https://huggingface.co/DataPilot/ArrowNeo-AME-4x3B-v0.1-MoE)
  - ArrowNeo-AME-4x3B-v0.1 is a 4x3B MoE model built upon sarashina-2.2-instruct-v0.1, combining a base model with three specialized expert models‚Äîcoding, instruction-following, and multi-turn‚Äîoptimized for AItuber applications with a focus on lightweight performance.
  - Downloads: 111
- [second-state/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/second-state/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with LlamaEdge LlamaEdge version: v0.10.1 and above Prompt template Prompt type: llama-3-chat Prompt string &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; {{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; {{ user_message_1 }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; {{ model_answer_1 }}&lt;|eot_id|&gt;&lt;|start_header
  - Downloads: 108
- [weblab-GENIAC/Tanuki-8x8B-dpo-v1.0](https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0)
  - Tanuki-8x8B-dpo-v1.0 is a 47B parameter language model, fine-tuned for dialogue using SFT and DPO, and available in quantized formats (AWQ, GPTQ, GGUF) requiring flash attention for inference.
  - Downloads: 107
- [llm-jp/llm-jp-3-vila-14b](https://huggingface.co/llm-jp/llm-jp-3-vila-14b)
  - LLM-jp-3 VILA 14B is a 14B parameter vision language model developed by NII Japan, requiring Python 3.10.12 and specific library installations including flash-attention.
  - Downloads: 107
- [mmnga/DeepSeek-R1-Distill-Llama-8B-gguf](https://huggingface.co/mmnga/DeepSeek-R1-Distill-Llama-8B-gguf)
  - This repository provides a GGUF-formatted version of the DeepSeek-R1-Distill-Llama-8B language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 104
- [alfredplpl/llm-jp-3-1.8b-instruct-gguf](https://huggingface.co/alfredplpl/llm-jp-3-1.8b-instruct-gguf)
  - This repository provides GGUF quantized versions of the LLM-jp-3-1.8b-instruct language model, compatible with llama.cpp, LM Studio, and LLMFarm, along with instructions for creating the quantized models.
  - Downloads: 100
- [mmnga/WabiSabi-V1-gguf](https://huggingface.co/mmnga/WabiSabi-V1-gguf)
  - This repository provides a GGUF-formatted version of the WabiSabi-V1 Japanese language model, built using the TFMC/imatrix-dataset, and designed for use with llama.cpp.
  - Downloads: 99
- [nitky/EZO-QwQ-32B-Preview](https://huggingface.co/nitky/EZO-QwQ-32B-Preview)
  - EZO-QwQ-32B-Preview This is a merge of pre-trained language models created using mergekit.
  - Downloads: 98
- [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia and CC-100, for masked language modeling tasks.
  - Downloads: 95
- [cl-nagoya/ruri-reranker-small](https://huggingface.co/cl-nagoya/ruri-reranker-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 94
- [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli)
  - This repository provides a fine-tuned Japanese BERT model (‚Äúbert-base-japanese-jsnli‚Äù) achieving 92.88% accuracy on the JSNLI dataset for zero-shot text classification.
  - Downloads: 91
- [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall)
  - This repository provides a Japanese-specific DeBERTa V3 model that avoids morphological analysis and better respects word boundaries during inference.
  - Downloads: 89
- [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1)
  - Fio-base-japanese-v0.1 is a proof-of-concept Japanese embedding model, based on bert-base-japanese-v3 and trained on limited data for tasks like similarity, entailment, and multilingual retrieval.
  - Downloads: 87
- [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium)
  - This repository details a Japanese-pretrained T5 v1.1 Transformer model with GEGLU activation and optimized dropout settings for improved performance.
  - Downloads: 87
- [MIL-UT/Asagi-8B](https://huggingface.co/MIL-UT/Asagi-8B)
  - Asagi-8B is a large-scale Japanese Vision & Language Model trained on a diverse dataset, largely synthesized using CALM3-22B-Chat and Phi3.5-vision-instruct, with outputs free from restrictive usage licenses.
  - Downloads: 87
- [llm-jp/llm-jp-3-13b-instruct2](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct2)
  - This repository hosts the llm-jp-3-13b-instruct2, a 13 billion parameter Japanese large language model developed by NII, compatible with Hugging Face Transformers and requiring torch>=2.3.0 and transformers>=4.40.1.
  - Downloads: 87
- [tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/tensorblock/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository provides GGUF quantized versions of the cyberagent/Mistral-Nemo-Japanese-Instruct-2408 model, optimized for llama.cpp and runnable via the TensorBlock client.
  - Downloads: 87
- [kcoopermiller/llm-jp-1.3b-v1.0-aya](https://huggingface.co/kcoopermiller/llm-jp-1.3b-v1.0-aya)
  - This repository hosts llm-jp-1.3b-v1.0-aya, a 1.3 billion parameter Japanese language model fine-tuned on Cohere's aya dataset, achieving a kcoopermiller evaluation score of 0.0698.
  - Downloads: 86
- [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat)
  - SambaLingo-Japanese-Chat is a Japanese/English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 85
- [Miwa-Keita/zenz-v1-checkpoints](https://huggingface.co/Miwa-Keita/zenz-v1-checkpoints)
  - zenz-v1 Checkpoints zenz-v1 is a language model specialized for kana-kanji conversion tasks based on the GPT-2 architecture.
  - Downloads: 85
- [DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit](https://huggingface.co/DeL-TaiseiOzaki/llm-jp-3-172b-instruct3-4bit)
  - This repository provides a 4-bit quantized derivative of the llm-jp-3-172b-instruct3 large language model, reducing GPU/memory usage while maintaining performance through BitsAndBytes (fp4) quantization with float16/float32 computation.
  - Downloads: 83
- [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece)
  - JMedRoBERTa-base-sentencepiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 82
- [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard)
  - Deepreneur-blue-lizard is a high-performing, 7 billion parameter Japanese language model‚Äîfine-tuned from Meta‚Äôs Llama-2-7b‚Äîthat outperforms ChatGPT-3.5 on the JGLUE benchmark without using JGLUE data in its training.
  - Downloads: 82
- [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos)
  - This repository provides a Japanese BERT model pre-trained for Universal Part-Of-Speech tagging and dependency parsing on Japanese Wikipedia text.
  - Downloads: 80
- [TheBloke/japanese-stablelm-instruct-beta-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI's Japanese StableLM Instruct Beta 7B language model, supported by a16z and utilizing hardware from Massed Compute.
  - Downloads: 80
- [mm/japanese-e5-mistral-7b_slerp_gguf](https://huggingface.co/mm/japanese-e5-mistral-7b_slerp_gguf)
  - Japanese E5 Mixtral 7B Slerp GGUF GGUF conversion of oshizo/japanese-e5-mistral-7b_slerp Avaiable formats: Q2_K.gguf Q3_K.gguf Q4_K.gguf Q5_K.gguf
  - Downloads: 80
- [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b)
  - This repository provides a 6B-parameter Japanese GPT-2 language model, trained on a 477B token corpus and fine-tuned with 6.9M comedic responses, optimized for AWS Trn1 instances using the Megatron-LM framework and licensed under Apache 2.0.
  - Downloads: 76
- [Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed through a collaborative project utilizing the Fugaku supercomputer, allowing both commercial and non-commercial use including modification, redistribution, and service implementation.
  - Downloads: 76
- [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0)
  - Heron GIT Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing the `transformers` library for implementation.
  - Downloads: 75
- [AIJapanese/Moriyasu_Qwen2_JP_7B](https://huggingface.co/AIJapanese/Moriyasu_Qwen2_JP_7B)
  - Moriyasu_Qwen2_JP_7B Model Description Moriyasu_Qwen2_JP_7B is a large language model trained by Moriyasu.
  - Downloads: 75
- [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp)
  - This repository merges intfloat‚Äôs E5-Mistral-7B-Instruct and StabilityAI‚Äôs Japanese-StableLM-Base-Gamma-7B models, detailing the process to resolve class compatibility issues during merging.
  - Downloads: 73
- [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ)
  - This repository details a 4.11GB quantized version of the Japanese-enhanced Meta Llama 2 7B model, optimized for speed but with some performance trade-offs due to quantization.
  - Downloads: 73
- [Aratako/sarashina2.2-3b-RP-v0.1](https://huggingface.co/Aratako/sarashina2.2-3b-RP-v0.1)
  - Sarashina2.2-3b-RP-v0.1 is a 3B parameter model fine-tuned for role-playing, utilizing a system prompt to define character and scenario for interactive conversations, with a GGUF version available for use with tools like Ollama.
  - Downloads: 70
- [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000)
  - JMedRoBERTa-base is a Japanese RoBERTa model pre-trained on a large corpus of medical academic articles from JST, licensed under CC BY-NC-SA 4.0.
  - Downloads: 69
- [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT model pre-trained on Wikipedia for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 69
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository hosts GGUF quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese language model, offering options optimized for different VRAM capacities (8GB, 12GB, 16GB).
  - Downloads: 68
- [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b)
  - This repository provides a Japanese novel-focused language model, GPT-J-6B, pre-trained on Japanese data and fine-tuned on novel data, runnable on Google Colab.
  - Downloads: 67
- [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa)
  - This repository provides a Japanese BERT model (bert-base-japanese-v3) fine-tuned on the JCommonsenseQA dataset for multiple-choice question answering, as detailed in the book ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 66
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted files for the japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 model, enabling easier use with tools supporting the GGUF format.
  - Downloads: 66
- [AXCXEPT/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/AXCXEPT/Llama-3.1-70B-EZO-1.1-it)
  - This repository provides Llama 3.1-70B-EZO-1.1-it, a Japanese language model fine-tuned from Meta‚Äôs Llama 3.1 achieving top open-source performance on ElyzaTasks-100, and is subject to the Llama 3.1 Community License.
  - Downloads: 65
- [stockmark/Stockmark-2-100B-Instruct-beta](https://huggingface.co/stockmark/Stockmark-2-100B-Instruct-beta)
  - Stockmark-2-100B-Instruct-beta is a 100B Japanese-focused large language model pre-trained on 1.5T tokens and instruction-tuned with synthetic data generated by Qwen2.5.
  - Downloads: 64
- [hotchpotch/luke-japanese-base-lite-xlm-roberta](https://huggingface.co/hotchpotch/luke-japanese-base-lite-xlm-roberta)
  - luke-japanese-base-lite-xlm-roberta studio-ousia/luke-japanese-base-lite„ÅÆÈáç„Åø„ÅÆÂêçÂâç„ÇíXLMRobertaÂΩ¢Âºè„Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅXLMRoberta„É¢„Éá„É´„Å®„Åó„Å¶Êâ±„Åà„Çã„Çà„ÅÜ„Å´„Åó„ÅüÁâ©„Åß„Åô„ÄÇ
  - Downloads: 62
- [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2)
  - This repository provides a Japanese BERT large model pretrained on CC-100 and JAWIKI data using character-level tokenization with whole word masking and the Unidic 2.1.2 dictionary.
  - Downloads: 61
- [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja)
  - This repository provides a GPT-2 small model trained on a 540M token Japanese Wikipedia dataset for Japanese language generation.
  - Downloads: 61
- [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix)
  - REV-Mix is an anime and realistic style diffusion model for Stable Diffusion, inspired by DateMix and RDtMix, optimized for samplers like DDIM/DPM++ Karras with suggested settings and benefiting from custom embeddings.
  - Downloads: 59
- [QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF](https://huggingface.co/QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF)
  - QuantFactory/Llama-3.1-Swallow-8B-v0.1-GGUF This is quantized version of tokyotech-llm/Llama-3.1-Swallow-8B-v0.1 created using llama.cpp Original Model Card Llama 3.1 Swallow - Built with Llama Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on the Meta Llama 3.1 models.
  - Downloads: 59
- [mmnga/Stockmark-2-100B-Instruct-beta-gguf](https://huggingface.co/mmnga/Stockmark-2-100B-Instruct-beta-gguf)
  - This repository provides a GGUF-formatted version of the Stockmark-2-100B-Instruct-beta language model, trained on the imatrix dataset and optimized for use with llama.cpp.
  - Downloads: 58
- [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja)
  - This repository provides a fine-tuned luke-japanese-base model for binary sentiment classification (positive/negative) using the JGLUE MARC-ja dataset, achieving 0.9 accuracy.
  - Downloads: 58
- [cl-nagoya/ruri-pt-base](https://huggingface.co/cl-nagoya/ruri-pt-base)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 58
- [QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF)
  - QuantFactory/Llama3-ArrowSE-8B-v0.3-GGUF This is quantized version of DataPilot/Llama3-ArrowSE-8B-v0.3 created using llama.cpp Original Model Card Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 58
- [rinna/qwen2.5-bakeneko-32b](https://huggingface.co/rinna/qwen2.5-bakeneko-32b)
  - Qwen2.5 Bakeneko 32B is a continually pre-trained 32B parameter language model based on Qwen2.5, enhanced for improved Japanese language performance using an 18B token dataset.
  - Downloads: 57
- [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf)
  - This repository provides GGUF-formatted weights for ELYZA-japanese-Llama-2-13b-fast-instruct, a Japanese language model based on Llama 2, optimized for use with llama.cpp.
  - Downloads: 57
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-4bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 57
- [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf)
  - Rinna/nekomata-14b-instruction-gguf provides a GGUF quantized version of the 14B instruction-tuned Nekomata model, optimized for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 56
- [MIL-UT/Asagi-4B](https://huggingface.co/MIL-UT/Asagi-4B)
  - Asagi-4B is a large-scale Japanese Vision & Language Model trained on a diverse, synthetically-generated Japanese dataset, leveraging models like CALM3 and Phi3.5-vision without usage restrictions on outputs.
  - Downloads: 56
- [retrieva-jp/bert-1.3b](https://huggingface.co/retrieva-jp/bert-1.3b)
  - RetrievaBERT is a pre-trained, Japanese-language Transformer Encoder built with Megatron-LM, featuring PreNorm and addressing a recent bias initialization bug.
  - Downloads: 54
- [yamatazen/HMS-Slerp-12B](https://huggingface.co/yamatazen/HMS-Slerp-12B)
  - This repository provides a ChatML model created by SLERP merging yamatazen/Himeyuri-Magnum-12B with shisa-ai/shisa-v2, representing the HMS model family.
  - Downloads: 53
- [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base)
  - SambaLingo-Japanese-Base is a pretrained, bilingual Japanese-English language model adapting Llama-2-7b with 42 billion tokens from Cultura-X, achieving state-of-the-art results in perplexity and translation.
  - Downloads: 53
- [llm-book/bert-base-japanese-v3-bpr-question-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-question-aio)
  - This repository provides a question encoder for a BERT-based Bi-directional Preference Ranking (BPR) document search model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of ‚ÄúIntroduction to Large Language Models.‚Äù
  - Downloads: 50
- [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF)
  - Japanese-WizardLM2-ChatV-7B is a 7B GGUF language model blending WizardLM-2's performance with ChatNTQ's Japanese language capabilities via ChatVector subtraction and addition.
  - Downloads: 49
- [takuyadayo/ozisan](https://huggingface.co/takuyadayo/ozisan)
  - This project provides code and a LoRA-tuned, Unsloth-based Japanese language model‚Äîoptimized with GRPO‚Äîto transform standard text into the characteristic "ojisan" (middle-aged man) speaking style found in online communication.
  - Downloads: 48
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF)
  - This repository provides GGUF formatted files for the Japanese-StableLM-Instruct-gamma-7B model, based on Mistral-7B-Instruct-v0.1, enabling easier use with tools supporting the GGUF format.
  - Downloads: 48
- [cl-nagoya/ruri-reranker-stage1-large](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-large)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 48
- [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 47
- [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja)
  - This repository provides a Japanese-adapted version of the Llama 3 8B model, commercially usable under the Llama 3 license, with instructions for quick deployment via demo, Colab, or local installation using transformers and accelerate.
  - Downloads: 46
- [cl-nagoya/ruri-pt-large](https://huggingface.co/cl-nagoya/ruri-pt-large)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 46
- [yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated](https://huggingface.co/yamatazen/Shisa-v2-Mistral-Nemo-12B-Abliterated)
  - This repository provides a merged language model created with mergekit, utilizing the TIES method and combining shisa-ai/shisa-v2-mistral-nemo-12b with natong19/Mistral-Nemo-Instruct-2407-abliterated.
  - Downloads: 44
- [toshi456/chat-vector-llava-v1.5-7b-ja](https://huggingface.co/toshi456/chat-vector-llava-v1.5-7b-ja)
  - Chat-Vector-LLaVA-v1.5-7b-JA Model Card Model detail Model type: Chat-Vector-LLaVA-v1.5-7b-JA is a vision-language model that can converse about input images in Japanese.
  - Downloads: 44
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit)
  - Tanuki-8x8B-dpo-v1.0-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 44
- [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer)
  - This repository provides a Japanese pre-trained ALBERT model (`ken11/albert-base-japanese-v1-with-japanese-tokenizer`) designed for fine-tuning on various downstream Japanese NLP tasks, utilizing an improved BertJapaneseTokenizer.
  - Downloads: 43
- [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese)
  - This repository provides a Japanese language model, fine-tuned from studio-ousia/luke-japanese-large, for automatic defamation detection, classifying comments into categories like threats, insults, or reputational harm.
  - Downloads: 43
- [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece)
  - JMedRoBERTa-base-manbyo-wordpiece is a Japanese RoBERTa model pre-trained on medical academic articles from JST, released under CC BY-NC-SA 4.0.
  - Downloads: 43
- [gaianet/Llama-3-8B-Japanese-Instruct-GGUF](https://huggingface.co/gaianet/Llama-3-8B-Japanese-Instruct-GGUF)
  - Llama-3-8B-Japanese-Instruct-GGUF Original Model haqishen/Llama-3-8B-Japanese-Instruct Run with Gaianet Prompt template: prompt template: llama-3-chat Context size: chat_ctx_size: 4096 Run with GaiaNet:
  - Downloads: 43
- [DataPilot/Llama3.1-ArrowSE-v0.4](https://huggingface.co/DataPilot/Llama3.1-ArrowSE-v0.4)
  - This repository provides a Japanese-enhanced version of the Llama3.1-8B-instruct model, fine-tuned with Mergekit, designed for improved performance as a helpful Japanese assistant.
  - Downloads: 42
- [DataPilot/Llama3-ArrowSE-8B-v0.3](https://huggingface.co/DataPilot/Llama3-ArrowSE-8B-v0.3)
  - Ê¶ÇË¶Å elyza/Llama-3-ELYZA-JP-8B„ÇíÂÖÉ„Å´chat vector„ÇíÁî®„ÅÑ„Å¶ÊîπËâØ„ÅóAItuber„Å´ÁâπÂåñ„Åï„Åõ„Åæ„Åó„Åü„ÄÇ
  - Downloads: 42
- [aerner/lm-v2](https://huggingface.co/aerner/lm-v2)
  - Aerner LM-v2 is a Japanese-language LLaMA-based language model, pre-trained on a largely Wikipedia dataset with 76,000 training steps, designed to run on 24GB VRAM and utilizing Flash Attention.
  - Downloads: 41
- [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf)
  - Rinna‚Äôs Nekomata-7B-instruction model is provided in GGUF format for lightweight inference using llama.cpp, with a recommendation for GGUF q4_K_M quantization due to potential stability issues with other methods.
  - Downloads: 41
- [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm)
  - ku-nlp/roberta-large-japanese-char-wwm is a Japanese RoBERTa-large model pre-trained on Japanese Wikipedia & CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 40
- [KoichiYasuoka/roberta-base-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-char)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 39
- [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium)
  - This repository details a pre-trained Japanese T5 v1.1 Transformer model‚Äîan encoder-decoder architecture improved with GEGLU activation and optimized for fine-tuning with dropout.
  - Downloads: 38
- [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char)
  - This repository provides a Japanese character-level GPT-2 Medium (310M) language model pre-trained on diverse Japanese text data for text generation tasks.
  - Downloads: 38
- [mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-i1-GGUF)
  - This repository provides weighted and static GGUF quantized versions of the japanese-llama-3-8b-instruct-v2 model, offering various quantizations (including IQ1_S) for optimized size and usage.
  - Downloads: 38
- [KoichiYasuoka/ltgbert-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/ltgbert-base-japanese-ud-goeswith)
  - ltgbert-base-japanese-ud-goeswith Model Description
  - Downloads: 38
- [KoichiYasuoka/deberta-base-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia)
  - DeBERTa-v2 pre-trained on Japanese Wikipedia and Aozora Bunko texts provides a foundation for fine-tuning various Japanese NLP tasks like POS-tagging and dependency parsing.
  - Downloads: 37
- [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char)
  - This repository details a 90M parameter Japanese character-level GPT-2 Small language model pre-trained on diverse Japanese text corpora and readily usable for text generation via the `transformers` pipeline.
  - Downloads: 37
- [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja)
  - This repository provides a Japanese-pretrained version of the 275.86M parameter Mixtral model, demonstrated with code for text generation using a specified tokenizer and parameters.
  - Downloads: 37
- [aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF](https://huggingface.co/aplulu/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF)
  - This repository offers a MIT-licensed, GGUF-quantized version of the DeepSeek-R1-Distill-Qwen-32B-Japanese large language model.
  - Downloads: 36
- [Miwa-Keita/zenz-v1](https://huggingface.co/Miwa-Keita/zenz-v1)
  - Zenz-v1 is a 90M parameter Japanese GPT-2 language model finetuned for kana-to-kanji conversion, intended for use with the Zenzai neural conversion system and licensed under CC-BY-SA 4.0.
  - Downloads: 36
- [aken12/splade-japanese](https://huggingface.co/aken12/splade-japanese)
  - This repository provides a Japanese SPLADE model‚Äîfine-tuned from bert-base-japanese-v2 on the mMARCO dataset‚Äîfor sparse retrieval, including code for encoding queries.
  - Downloads: 35
- [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese)
  - This repository provides a Japanese GPT-2 model pre-trained on Japanese Wikipedia and CC-100, designed for text generation and fine-tuning with text pre-processed via Juman++.
  - Downloads: 34
- [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct)
  - This repository provides Japanese-finetuned versions of the Meta-Llama-3-8B-Instruct model, trained on a 49k conversation dataset using LLaMA-Factory, and compatible with both transformers and the original llama3 codebase with an 8192 context length.
  - Downloads: 34
- [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 34
- [QuantFactory/Oumuamua-7b-instruct-GGUF](https://huggingface.co/QuantFactory/Oumuamua-7b-instruct-GGUF)
  - Oumuamua-7b-instruct-GGUF This is quantized version of nitky/Oumuamua-7b-instruct created using llama.cpp Model Description This is a merge of pre-trained language models created using mergekit.
  - Downloads: 33
- [AXCXEPT/phi-4-open-R1-Distill-EZOv1](https://huggingface.co/AXCXEPT/phi-4-open-R1-Distill-EZOv1)
  - This repository provides a Japanese-specialized, reasoner version of the phi-4 model, built using the open-r1 methodology inspired by Deepseek-R1's distillation techniques, and capable of flexible English integration.
  - Downloads: 32
- [Formzu/roberta-base-japanese-jsnli](https://huggingface.co/Formzu/roberta-base-japanese-jsnli)
  - This repository provides a fine-tuned RoBERTa-base Japanese model for zero-shot text classification on the JSNLI dataset, achieving 93.28% accuracy and requiring Juman++ word segmentation for input.
  - Downloads: 31
- [loiccabannes/MambaSan-370m](https://huggingface.co/loiccabannes/MambaSan-370m)
  - MambaSan-370m is a 370 million parameter Japanese chat language model built on the Mamba state-space architecture for efficient, linear-time sequence modeling.
  - Downloads: 31
- [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b)
  - This repository details a 1.3B parameter Japanese GPT-2 model (‚Äúdolly-japanese-gpt-1b‚Äù) fine-tuned with RLHF on datasets like ‚Äúdatabricks-dolly-15k-ja‚Äù and ‚Äúoasst1-89k-ja‚Äù for interactive AI applications, requiring 7GB VRAM/RAM, though recent updates show decreased accuracy on factual questions.
  - Downloads: 30
- [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese)
  - This repository provides a Japanese Sentence-LUKE model, trained with the same data and settings as Japanese Sentence-BERT, demonstrating improved qualitative and comparable quantitative accuracy, built upon studio-ousia/luke-japanese-base-lite and requiring SentencePiece for inference.
  - Downloads: 30
- [megagonlabs/electra-base-japanese-discriminator](https://huggingface.co/megagonlabs/electra-base-japanese-discriminator)
  - This repository provides a pretrained Japanese ELECTRA model (specifically, the discriminator) tokenized with SudachiTra WordPiece, trained on 200M Japanese sentences and usable with the Hugging Face Transformers library.
  - Downloads: 30
- [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512)
  - This repository provides a large Japanese RoBERTa model, pretrained on Japanese Wikipedia & CC-100, supporting sequences up to 512 tokens for masked language modeling tasks.
  - Downloads: 28
- [sakuraumi/Sakura-13B-Galgame](https://huggingface.co/sakuraumi/Sakura-13B-Galgame)
  - SakuraLLM provides open-source, locally deployable Japanese-to-Chinese translation models‚Äîfine-tuned on general and ACGN-style data‚Äîfor light novels and galgames, released under a non-commercial license.
  - Downloads: 28
- [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese)
  - This repository provides a Japanese BigBird base model pre-trained on large-scale Japanese text datasets‚ÄîJapanese Wikipedia, CC-100, and OSCAR‚Äîfor masked language modeling tasks.
  - Downloads: 28
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-4bit)
  - This repository provides a 4-bit quantized MLX version of the DeepSeek-R1-Distill-Qwen-32B-Japanese language model, enabling efficient inference with the `mlx-lm` library.
  - Downloads: 27
- [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt)
  - This repository provides a Japanese T5 language model fine-tuned on a 100GB corpus of Japanese text‚Äîincluding Wikipedia, OSCAR, and CC-100‚Äîfor adapted language modeling (next token prediction).
  - Downloads: 27
- [llm-jp/llm-jp-3-980m-instruct2](https://huggingface.co/llm-jp/llm-jp-3-980m-instruct2)
  - This repository hosts llm-jp-3-980m-instruct2, a Japanese large language model from the National Institute of Informatics, compatible with Hugging Face Transformers and requiring torch >=2.3.0 and transformers >=4.40.
  - Downloads: 27
- [masato12/bert-base-japanese-v3-jsts-with-tokenizer](https://huggingface.co/masato12/bert-base-japanese-v3-jsts-with-tokenizer)
  - bert-base-japanese-v3-jsts „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑèÂë≥È°û‰ººÂ∫¶Ë®àÁÆó)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad)
  - This repository provides a Japanese question answering model fine-tuned on the JaQuAD dataset, achieving F1 scores of 77.35/78.92 and exact match scores of 61.01/63.38 on development/test sets, based on the BERT base Japanese model.
  - Downloads: 26
- [tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq8192-alpha)
  - TohokuNLP‚Äôs BERT-alpha 500M is a Japanese BERT model based on the Llama architecture, enabling long sequence input (up to 8,192 tokens) as an encoder-type language model.
  - Downloads: 26
- [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b)
  - This repository provides an 8-layer, 800,000-sentence trained Japanese language model based on intfloat/e5-mistral-7b-instruct, detailed in a linked Japanese article.
  - Downloads: 26
- [nptdat/bert-japanese-12M](https://huggingface.co/nptdat/bert-japanese-12M)
  - Overview of bert-japanese-12M The bert-japanese-12M model is a transformer-based model with BERT architecture, which is designed to be used on Japanese text.
  - Downloads: 26
- [owner203/japanese-llama-3-8b-instruct-v2-gguf](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2-gguf)
  - Japanese-LLaMA-3-8B-Instruct-v2-GGUF Japanese-LLaMA-3-8B-Instruct-v2-GGUF„ÅØJapanese-LLaMA-3-8B-Instruct-v2„ÅÆGGUFÂΩ¢Âºè„Åß„Åô„ÄÇ
  - Downloads: 26
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese-8bit)
  - This repository provides the 32B Japanese language model DeepSeek-R1-Distill-Qwen converted to the MLX format for efficient inference using the `mlx-lm` library.
  - Downloads: 25
- [MIL-UT/Asagi-2B](https://huggingface.co/MIL-UT/Asagi-2B)
  - Asagi-2B is a large-scale Japanese Vision & Language Model trained on a diverse, primarily synthesized dataset‚Äîleveraging CALM3 and Phi3.5‚Äîwhile avoiding LLMs with restrictive output usage policies.
  - Downloads: 25
- [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1)
  - This repository provides fine-tuned Mistral-7B-based large language models (Ninja-v1, etc.) optimized for high-quality Japanese and English generation with enhanced long-context memory capabilities.
  - Downloads: 25
- [Fugaku-LLM/Fugaku-LLM-13B](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B)
  - This repository outlines the usage terms for Fugaku-LLM, a large language model developed collaboratively using the Fujaku supercomputer, permitting both commercial and non-commercial use, modification, and redistribution.
  - Downloads: 25
- [TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-base-lite-offensiveness-estimation)
  - This repository hosts a fine-tuned Japanese language model (based on sonoisa/sentence-luke-japanese-base-lite) for social media comment offensiveness detection, achieving a 71.3% F1-score on a manually-labeled dataset, and presented at NLP2024.
  - Downloads: 25
- [TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ)
  - This repository provides GPTQ quantized versions of Stability AI's Japanese StableLM Instruct Gamma 7B language model, offering various parameter permutations for optimized performance.
  - Downloads: 25
- [Local-Novel-LLM-project/Vecteus-V2-7B](https://huggingface.co/Local-Novel-LLM-project/Vecteus-V2-7B)
  - Vecteus-V2-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ„Éô„ÇØ„Éà„É´„Éû„Éº„Ç∏„Å™„Å©„ÇíÁî®„ÅÑ‰ΩúÊàê„Åï„Çå„ÅüÈ´òÊÄßËÉΩ„Éô„Éº„Çπ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [kishizaki-sci/phi-4-AWQ-4bit-EN-JP](https://huggingface.co/kishizaki-sci/phi-4-AWQ-4bit-EN-JP)
  - This repository provides a 4-bit AutoAWQ quantized version of the phi-4 language model, calibrated with both Japanese and English data for improved performance.
  - Downloads: 24
- [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm)
  - ku-nlp/roberta-base-japanese-char-wwm is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and CC-100, utilizing character-level tokenization and whole word masking for masked language modeling tasks.
  - Downloads: 24
- [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B)
  - GPT-Neo 1.3B is a pre-trained Japanese language model‚Äîsimilar to GPT2/GPT3‚Äîtrained on a large Japanese corpus and usable via the `transformers` pipeline for text generation.
  - Downloads: 24
- [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char)
  - This repository provides a Japanese BERT large model pre-trained on jawiki-20200831 using character and word-level tokenization with whole word masking for improved masked language modeling.
  - Downloads: 24
- [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1)
  - This repository details swallow-hermes-st-v1, a Japanese language model created by merging English language model vectors (chat and narrative) using an evolutionary strategy to produce a storytelling LLM intended as a relaxing bedtime read, addressing the perceived stiffness and overly positive tone of models like GPT-4.
  - Downloads: 24
- [espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest](https://huggingface.co/espnet/kan-bayashi_tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest)
  - ESPnet2 TTS pretrained model kan-bayashi/tsukuyomi_tts_finetune_full_band_jsut_vits_raw_phn_jaconv_pyopenjtalk_prosody_latest ‚ôª
  - Downloads: 24
- [fukugawa/transformer-lm-japanese-1.0b](https://huggingface.co/fukugawa/transformer-lm-japanese-1.0b)
  - transformer-lm-japanese-1.0b This is a JAX/Flax-based transformer language model trained on a Japanese dataset.
  - Downloads: 24
- [Helsinki-NLP/opus-mt-ja-he](https://huggingface.co/Helsinki-NLP/opus-mt-ja-he)
  - This repository provides a Japanese-to-Hebrew transformer model (jpn-heb) trained on OPUS data, utilizing SentencePiece preprocessing and including evaluation metrics for BLEU and chr-F on the Tatoeba test set.
  - Downloads: 23
- [skytnt/gpt2-japanese-lyric-medium](https://huggingface.co/skytnt/gpt2-japanese-lyric-medium)
  - This repository provides a pre-trained Japanese GPT2 model (‚Äúskytnt/gpt2-japanese-lyric-medium‚Äù) for generating Japanese song lyrics from a title and/or prompt text.
  - Downloads: 23
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 1.7B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following.
  - Downloads: 23
- [Miwa-Keita/zenz-v2.5-medium](https://huggingface.co/Miwa-Keita/zenz-v2.5-medium)
  - Zenz v2.5 is a finetuned, Japanese GPT-2 language model specializing in Kana-to-Kanji conversion, available in small (91M), medium (310M), and xsmall (26M) sizes under a CC-BY-SA 4.0 license.
  - Downloads: 23
- [if001/llama2_ja_small_instruct](https://huggingface.co/if001/llama2_ja_small_instruct)
  - This repository provides a Japanese instruction-tuned Llama 2 model (if001/llama2_ja_small) fine-tuned on an instruction dataset, utilizing the lit-gpt training script and supporting sentencepiece Japanese tokenization.
  - Downloads: 23
- [webbigdata/C3TR-Adapter_gptq](https://huggingface.co/webbigdata/C3TR-Adapter_gptq)
  - Model card Ëã±Êó•„ÄÅÊó•Ëã±ÁøªË®≥Áî®„É¢„Éá„É´C3TR-Adapter„ÅÆGPTQ4bitÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 23
- [cl-nagoya/ruri-reranker-stage1-base](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-base)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 23
- [jri-advtechlab/layoutlm-wikipedia-ja](https://huggingface.co/jri-advtechlab/layoutlm-wikipedia-ja)
  - LayoutLM-wikipedia-ja is a Japanese language LayoutLM model pretrained for token classification, finetuned from cl-tohoku/bert-base-japanese-v2 and licensed under CC BY-SA 3.0.
  - Downloads: 22
- [makiart/jp-ModernBert-base-preview](https://huggingface.co/makiart/jp-ModernBert-base-preview)
  - jp-ModernBert-base-preview is a 150M parameter Japanese language model, trained on 300B tokens of Japanese data (fineweb2), featuring an 8192 context length and utilizing BertJapaneseTokenizer for efficient inference, potentially with FlashAttention.
  - Downloads: 22
- [KoichiYasuoka/roberta-small-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora-char)
  - This repository provides a small RoBERTa model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora)
  - DeBERTa-large-japanese-aozora is a pre-trained Japanese language model, based on DeBERTa V2 and trained on the Aozora Bunko corpus, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [KoichiYasuoka/deberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora)
  - DeBERTa-base-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 22
- [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese)
  - COMET-GPT2 is a Japanese language model finetuned from GPT-2 using the ATOMIC dataset and causal language modeling, enabling text generation with reproducible results via the `transformers` pipeline.
  - Downloads: 22
- [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator)
  - This repository provides a Japanese ELECTRA model, pretrained on 200M sentences using SudachiTra WordPiece tokenization, for use with the `transformers` library.
  - Downloads: 22
- [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner)
  - This repository provides a Japanese named entity recognition (NER) model fine-tuned from Kyoto University‚Äôs BERT Japanese Pretrained model using the ner-wikipedia-dataset, requiring separate downloads of the tokenizer and installation of Juman++ & pyknp.
  - Downloads: 21
- [p1atdev/kakuyomu-genre-bert](https://huggingface.co/p1atdev/kakuyomu-genre-bert)
  - This repository provides a BERT model, fine-tuned on Japanese novels, for classifying text (titles & summaries) into specific genres, built upon the cl-tohoku/bert-base-japanese-char-v3 base model.
  - Downloads: 21
- [cl-nagoya/ruri-pt-small](https://huggingface.co/cl-nagoya/ruri-pt-small)
  - Ruri: Japanese General Text Embeddings Usage First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference.
  - Downloads: 21
- [nitky/Oumuamua-7b-instruct](https://huggingface.co/nitky/Oumuamua-7b-instruct)
  - Oumuamua-7b-instruct This is a merge of pre-trained language models created using mergekit.
  - Downloads: 21
- [makiart/jp-ModernBERT-large-preview](https://huggingface.co/makiart/jp-ModernBERT-large-preview)
  - jp-modernbert-large-preview is a 396M parameter Japanese language model built by Algomatic, featuring 8192 context length, 50K vocabulary, and trained on ~100B tokens including fineweb2 data, utilizing BertJapaneseTokenizer and optimized for FlashAttention.
  - Downloads: 20
- [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf)
  - Nekomata-7b-gguf provides a GGUF quantized version of the rinna/nekomata-7b model, optimized for lightweight inference using llama.cpp, with a recommendation for q4_K_M 4-bit quantization due to potential stability issues in other formats.
  - Downloads: 20
- [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char)
  - This repository provides a 717M parameter Japanese character-level GPT-2 language model pre-trained on diverse Japanese text datasets for text generation tasks.
  - Downloads: 20
- [Aratako/Qwen3-8B-RP-v0.1](https://huggingface.co/Aratako/Qwen3-8B-RP-v0.1)
  - This repository provides a fine-tuned 8B parameter Qwen3 model (Qwen3-8B-RP-v0.1) optimized for role-playing, configurable via system prompts to define character settings and scenarios, and runnable with tools like Ollama.
  - Downloads: 20
- [Aratako/Ninja-v1-RP](https://huggingface.co/Aratako/Ninja-v1-RP)
  - This repository provides a merged, role-playing language model (Ninja-v1-RP) built with mergekit, enhancing instruction following and expressiveness via Task Vector addition and Model Stock merging, and utilizing the Vicuna chat template with specific multi-turn conversation formatting requirements.
  - Downloads: 20
- [yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF](https://huggingface.co/yasu-oh/Llama-3.3-Swallow-70B-Instruct-v0.4-GGUF)
  - This repository provides GGUF versions of the Llama-3.3-Swallow-70B-Instruct-v0.4 language model, fine-tuned with the imatrix Japanese language dataset.
  - Downloads: 20
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-commonsenseQA)
  - This repository provides a Japanese BERT-large model fine-tuned on the JGLUE/JCommonsenseQA dataset for performing commonsense question answering tasks.
  - Downloads: 20
- [sehiro/AI-buncho-novel-ct2](https://huggingface.co/sehiro/AI-buncho-novel-ct2)
  - This repository provides a ctranslate2-compatible dataset converted from the AIBunCho Japanese novel GPT-J-6B model, featuring 8-bit quantization which may impact precision‚Äîcurrently lacking quantitative data.
  - Downloads: 20
- [akineAItech/Jeneri-SAMA-6B](https://huggingface.co/akineAItech/Jeneri-SAMA-6B)
  - This repository provides a Japanese novel generation model‚Äîfine-tuned on 216 highly-rated web novels, Aozora Bunko texts, and Wikipedia‚Äîusing QLoRA, guided by genre, keywords, and prompts, though it remains an unverified prototype.
  - Downloads: 20
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha-merged)
  - This repository provides a Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, created via difference merging of an intermediate training model and evaluated on the ABEJA tech blog.
  - Downloads: 20
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 20
- [knok/japanese-distilgpt2](https://huggingface.co/knok/japanese-distilgpt2)
  - This repository provides a Japanese GPT-2 distillation model trained using rinna/japanese-gpt2-medium as the teacher, achieving a perplexity of around 40 on Wikipedia data, and utilizing Hugging Face Transformers with modified training code.
  - Downloads: 19
- [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf)
  - Rinna/nekomata-14b-gguf provides a GGUF quantized version of the rinna/nekomata-14b model, optimized for lightweight inference using llama.cpp, with recommendations for GGUF q4_K_M quantization to avoid stability issues.
  - Downloads: 19
- [KoichiYasuoka/bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)
  - KoichiYasuoka/bert-large-japanese-char-extended is a pre-trained Japanese BERT model‚Äîderived from bert-large-japanese-char‚Äîwith extended character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 19
- [Local-Novel-LLM-project/Vecteus-v1-abliterated](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1-abliterated)
  - This repository provides an uncensored (abliterated) version of vecteus v1, a high-performance Japanese large language model specializing in novel text generation and natural language understanding, offering greater freedom in text output while disclaiming responsibility for generated content.
  - Downloads: 19
- [if001/llama2_ja_small](https://huggingface.co/if001/llama2_ja_small)
  - This repository provides a Japanese-trained Llama 2 model (417.12M parameters) built using Lit-GPT, with example code demonstrating its use via the Transformers library for text generation.
  - Downloads: 19
- [KoichiYasuoka/deberta-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-based Japanese model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 19
- [DataPilot/sarashina2.2-3Bx8-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx8-moe)
  - Sarashina2.2-3Bx8-moe is a Mixture of Experts (MoE) language model built upon sbintuitions/sarashina2.2-3b-instruct-v0.1, integrating eight specialized models via mergekit-moe for diverse and high-quality Japanese text generation.
  - Downloads: 19
- [TheBloke/japanese-stablelm-base-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GPTQ)
  - This repository offers quantized (GPTQ) versions of Stability AI's Japanese StableLM Base Beta 70B language model, with various parameter options for optimized performance.
  - Downloads: 19
- [doshisha-mil/llama-2-70b-chat-4bit-japanese-v1](https://huggingface.co/doshisha-mil/llama-2-70b-chat-4bit-japanese-v1)
  - This repository provides a 4-bit quantized, Japanese-instruction-tuned version of Llama-2-Chat 70B, fine-tuned on the alpaca_ja dataset, requiring adherence to the Meta LLaMA license.
  - Downloads: 19
- [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base)
  - SpiralAI Spiral-RetNet-3b-base We have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621)
  - Downloads: 19
- [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF)
  - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF
  - Downloads: 19
- [Local-Novel-LLM-project/Assistance-GGUF](https://huggingface.co/Local-Novel-LLM-project/Assistance-GGUF)
  - Assistance „ÅÆGGUFÁâà Our Models for GGUF Vecteus-GGUF Ninja-v1-GGUF Ninja-v1-NSFW-GGUF Ninja-v1-128k-GGUF Ninja-v1-NSFW-128k-GGUF
  - Downloads: 19
- [KoichiYasuoka/gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-ud-causal)
  - gpt2-small-japanese-ud-causal Model Description
  - Downloads: 19
- [espnet/kan-bayashi_jsut_vits_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_vits_accent_with_pause)
  - This repository hosts a pretrained ESPnet2 Text-to-Speech (TTS) model, jsut_vits_accent_with_pause, trained on the JSUT dataset and imported from Zenodo.
  - Downloads: 18
- [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for live Japanese speech-to-text translation, trained on a combination of Japanese datasets including Common Voice, JSUT, and others.
  - Downloads: 18
- [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small)
  - This repository provides a pre-trained Japanese GPT2 model (‚Äúskytnt/gpt2-japanese-lyric-small‚Äù) for generating Japanese lyrics, with example code and a demo website available at lyric.fab.moe.
  - Downloads: 18
- [okazaki-lab/japanese-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-gpt2-medium-unidic)
  - This repository provides a medium-sized Japanese GPT-2 model with a BERT-like tokenizer (Unidic), built on PyTorch and Hugging Face Transformers for text generation.
  - Downloads: 18
- [fukugawa/transformer-lm-japanese-0.1b](https://huggingface.co/fukugawa/transformer-lm-japanese-0.1b)
  - This repository provides a JAX/Flax-based Japanese transformer language model (0.1B parameters) trained on a Japanese dataset, with support for FlaxAutoModelForCausalLM and benchmark scores.
  - Downloads: 18
- [megagonlabs/transformers-ud-japanese-electra-base-ginza-520](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-520)
  - This repository distributes a spaCy v3-finetuned ELECTRA model (ja_ginza_electra) pretrained on Japanese mC4 data and built upon megagonlabs/transformers-ud-japanese-electra-base, offering custom pipeline components via GiNZA v5.
  - Downloads: 18
- [nakamura196/roberta-small-hi-char](https://huggingface.co/nakamura196/roberta-small-hi-char)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 18
- [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja)
  - This repository provides a Japanese BERT base model and tokenizer trained on the June 2021 Japanese Wikipedia dataset for natural language processing tasks.
  - Downloads: 18
- [Miwa-Keita/zenz-v2.5-small](https://huggingface.co/Miwa-Keita/zenz-v2.5-small)
  - Zenz-v2.5 is a finetuned GPT-2-based conditional language model for Japanese kana-to-kanji conversion, offered in small, medium, and large sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 18
- [Local-Novel-LLM-project/WabiSabi-V1](https://huggingface.co/Local-Novel-LLM-project/WabiSabi-V1)
  - Wabisabi-v1.0 is a fine-tuned Mistral-7B-v0.1 LLM boasting a 128k context window, high-quality Japanese & English generation, and robust long-context memory, even for NSFW content.
  - Downloads: 18
- [Miwa-Keita/zenz-v2-gguf](https://huggingface.co/Miwa-Keita/zenz-v2-gguf)
  - Zenz-v2 is a 90M parameter Japanese GPT-2 language model finetuned for kanji-to-kana conversion, building upon ku-nlp/gpt2-small-japanese-char and designed for use in the Zenzai system, licensed under CC-BY-SA 4.0.
  - Downloads: 18
- [KoichiYasuoka/deberta-large-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-large Japanese question-answering model, pretrained on Aozora Bunko for dependency parsing and optimized for handling ambiguous words using [MASK] tokens.
  - Downloads: 18
- [ketman/whisper_for_dominion](https://huggingface.co/ketman/whisper_for_dominion)
  - This repository provides a fine-tuned Japanese LLM for Whisper, specifically trained on Dominion card terminology to improve speech-to-text accuracy, having learned all cards as of December 19, 2023.
  - Downloads: 18
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2)
  - This model merges differences between Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x7B-v0.1 based on Swallow-MX-8x7b-NVE-v0.1, improving Japanese language output and offering top performance for 32k context local LLMs as of March 2024.
  - Downloads: 18
- [tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer](https://huggingface.co/tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer)
  - Made using Gpt-Small from scratch for learning purpose.
  - Downloads: 18
- [cl-nagoya/ruri-reranker-stage1-small](https://huggingface.co/cl-nagoya/ruri-reranker-stage1-small)
  - Ruri-Reranker: Japanese General Reranker Usage Direct Usage (Sentence Transformers)
  - Downloads: 18
- [KoichiYasuoka/modernbert-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/modernbert-large-japanese-aozora)
  - ModernBERT-large-japanese-aozora is a pre-trained Japanese language model based on ÈùíÁ©∫ÊñáÂ∫´ texts, designed for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 17
- [TylorShine/distilhubert-ft-japanese-50k](https://huggingface.co/TylorShine/distilhubert-ft-japanese-50k)
  - This repository provides a fine-tuned DistilHuBERT speech model‚Äîtrained for 50k steps on a combined Japanese corpus including JVS, Tsukuyomi-Chan, and custom ITA data‚Äîsubject to the JVS corpus's terms of use.
  - Downloads: 17
- [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0)
  - Heron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model, built with the Heron library, enabling conversational interaction about images using a 7B parameter GitGPTNeoX model.
  - Downloads: 17
- [TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI's Japanese StableLM Instruct Gamma 7B language model, optimized for efficiency using hardware from Massed Compute and supported by a16z grant.
  - Downloads: 17
- [izumi-lab/electra-small-japanese-generator](https://huggingface.co/izumi-lab/electra-small-japanese-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 17
- [AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-1b-japanese-hiragana-katakana)
  - This repository provides a wav2vec2-xls-r-1b model fine-tuned on the Japanese Common Voice 8.0 dataset, achieving a Word Error Rate (WER) of 1.0132 with specific training hyperparameters (learning rate 7.5e-05, batch size 32).
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-char-luw-upos)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of character-level long-unit-words.
  - Downloads: 17
- [izumi-lab/electra-small-paper-japanese-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data, utilizing a 12-layer, 64-dimensional architecture with 1 attention head.
  - Downloads: 17
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-large model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing models and utilizing the goeswith subword tokenization.
  - Downloads: 17
- [llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models‚Äîincluding LoRA and full instruction-tuned variants‚Äîwith Hugging Face checkpoint format.
  - Downloads: 17
- [Elizezen/SlaughterHouse-exp-7B](https://huggingface.co/Elizezen/SlaughterHouse-exp-7B)
  - SlaughterHouse Exp 7B is a 7-billion parameter Japanese language model, fine-tuned from Japanese Stable LM, with some ACG (anime, manga, VN) knowledge for experimental fanfiction generation.
  - Downloads: 17
- [ebisuke/liz-nojaloli-ja](https://huggingface.co/ebisuke/liz-nojaloli-ja)
  - This repository provides a Japanese chat model, ‚Äúliz-nojaloli-ja,‚Äù fine-tuned from rinna/japanese-gpt-neox-3.6b, designed for personal use and iteratively updated with a specific dataset (ebisuke/liz-nojaloli-ja-ds) and a defined input/output format.
  - Downloads: 17
- [KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-juman-ud-goeswith)
  - This repository provides a Japanese RoBERTa-base model, pretrained for POS-tagging and dependency parsing with fugashi, built upon roberta-base-japanese and utilizing the goeswith subword approach.
  - Downloads: 17
- [KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt-neox-small-japanese-ud-causal)
  - rinna-gpt-neox-small-japanese-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-5mb-ud-causal)
  - goldfish-gpt2-japanese-5mb-ud-causal Model Description
  - Downloads: 17
- [KoichiYasuoka/gpt2-small-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-upos)
  - gpt2-small-japanese-upos Model Description
  - Downloads: 17
- [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2)
  - This repository provides a Japanese BERT base model pretrained on jawiki-20200831 using character-level tokenization and whole word masking, offering a Fugashi/Unidic-free alternative to cl-tohoku/bert-base-japanese-char-v2 with a basic tokenizer.
  - Downloads: 16
- [nlp-waseda/gpt2-small-japanese-wikipedia](https://huggingface.co/nlp-waseda/gpt2-small-japanese-wikipedia)
  - This repository provides a Japanese GPT-2 model pretrained on Japanese Wikipedia, requiring Juman++ word segmentation for text generation or fine-tuning tasks.
  - Downloads: 16
- [KoichiYasuoka/deberta-large-japanese-wikipedia](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia)
  - DeBERTa-large-japanese-wikipedia is a pre-trained Japanese language model based on DeBERTaV2, trained on Japanese Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 16
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-8bit-1g-actorder_True)
  - This repository hosts a 1.7B parameter, 8-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following and causal language modeling.
  - Downloads: 16
- [p1atdev/t5-base-xlsum-ja](https://huggingface.co/p1atdev/t5-base-xlsum-ja)
  - This repository provides a T5-base model fine-tuned on the XLSum dataset for Japanese summarization, achieving specified Rouge scores with a learning rate of 0.0001, though further details on data, limitations, and procedure are needed.
  - Downloads: 16
- [Yokohide031/rust_cl-tohoku_bert-large-japanese](https://huggingface.co/Yokohide031/rust_cl-tohoku_bert-large-japanese)
  - This repository provides a Rust implementation of the Tohoku University BERT large Japanese model, enabling its use in Rust projects with instructions for cloning, project setup, and basic usage via the `rust-bert` crate.
  - Downloads: 16
- [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens)
  - This repository provides a Japanese sentence-T5 model, pre-trained using sonoisa/t5-base-japanese, requiring sentencepiece for inference and achieving comparable accuracy to sonoisa/sentence-bert-base-ja-mean-tokens.
  - Downloads: 16
- [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator)
  - This repository provides a pre-trained Japanese ELECTRA-small model, utilizing Byte-Pair Encoding on Japanese Wikipedia data and requiring proper MeCab dictionary configuration for optimal results.
  - Downloads: 16
- [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying)
  - This repository provides a Japanese BERT base model finetuned for cyberbullying detection using a unified dataset of BBS and Twitter comments, licensed under CC BY-SA 4.0.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa-v2-based Japanese NLP model for part-of-speech tagging and dependency parsing, pre-trained on large Japanese corpora and utilizing the `goeswith` subword strategy, requiring `fugashi` for use.
  - Downloads: 16
- [KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-based Japanese model pretrained on the Aozora corpus for universal dependency parsing and part-of-speech tagging, utilizing the goeswith subword tokenizer.
  - Downloads: 16
- [microsoft/unihanlm-base](https://huggingface.co/microsoft/unihanlm-base)
  - UnihanLM is a self-supervised Chinese-Japanese masked language model pretraining framework leveraging the Unihan database and a coarse-to-fine approach to capitalize on shared character morphology between the two languages.
  - Downloads: 16
- [huranokuma/es](https://huggingface.co/huranokuma/es)
  - This repository details a fine-tuned Japanese GPT-2 model for generating *ES* (entry sheets/applications) using over 20,000 samples from successful candidates, with a demo web app available and built upon rinnakk/japanese-pretrained-models.
  - Downloads: 16
- [inu-ai/alpaca-guanaco-japanese-gpt-1b](https://huggingface.co/inu-ai/alpaca-guanaco-japanese-gpt-1b)
  - This repository provides a 1.3B parameter Japanese GPT model (‚Äúalpaca-guanaco-japanese-gpt-1b‚Äù) trained on Japanese datasets‚Äîalpaca_ja and GuanacoDataset‚Äîrequiring 7GB VRAM/RAM for conversational AI.
  - Downloads: 16
- [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k)
  - This repository provides a Japanese T5 text-to-text transformer model, pre-trained on 8K vocabulary from large-scale web and Wikipedia corpora, with training code available and a 32K version also offered.
  - Downloads: 16
- [ce-lery/dolly-japanese-gpt-1b-clone](https://huggingface.co/ce-lery/dolly-japanese-gpt-1b-clone)
  - This repository provides an inference model cloned from rinna's "japanese-gpt-1b" and fine-tuned using the ‚Äúdatabricks-dolly-15k-ja‚Äù dataset, with detailed environment setup for local fine-tuning and inference.
  - Downloads: 16
- [A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw](https://huggingface.co/A-Funakoshi/bert-finetuned-multilingual-sentiments-adamw)
  - This repository details hyperparameter optimization, using Optuna with a bert-base Japanese model and multilingual sentiment dataset, achieving best results with a cosine learning rate schedule (2.82e-05), gradient accumulation of 1, and weight decay of 0.00017.
  - Downloads: 16
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-32g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized, instruction-tuned Japanese language model fine-tuned by LINE Corporation for efficient use.
  - Downloads: 16
- [A-Funakoshi/bert-base-japanese-v3-wrime-v1](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v1)
  - This repository details Japanese sentiment analysis experiments using bert-base, the WRIME dataset, and Adafactor optimization, exploring hyperparameters like learning rate, batch size, and weight decay with Optuna.
  - Downloads: 16
- [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo)
  - Tanuki-Zero is a 13B Japanese language model built on llm-jp/llm-jp-13b-v1.0 and fine-tuned with a 15k sample Jaster instruction dataset.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese)
  - This repository provides a Japanese language model, Mixtral-8x7B-Instruct-v0.1-japanese, built upon Mixtral-8x7B-Instruct-v0.1 with vocabulary expansion and continued pre-training.
  - Downloads: 16
- [abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese-alpha)
  - This repository provides an early-access, Japanese-enhanced version of Mixtral-8x7B-Instruct-v0.1, pre-trained with expanded vocabulary and evaluated on ABEJA‚Äôs tech blog.
  - Downloads: 16
- [Aratako/Oumuamua-7b-RP](https://huggingface.co/Aratako/Oumuamua-7b-RP)
  - Oumuamua-7b-RP GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-v3-base-japanese-ud-goeswith)
  - deberta-v3-base-japanese-ud-goeswith Model Description
  - Downloads: 16
- [yohida/yoshida_gpt](https://huggingface.co/yohida/yoshida_gpt)
  - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
  - Downloads: 16
- [KoichiYasuoka/gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-ud-causal)
  - gpt2-large-japanese-ud-causal Model Description
  - Downloads: 16
- [KoichiYasuoka/roberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-aozora)
  - KoichiYasuoka's roberta-small-japanese-aozora is a pre-trained RoBERTa model for Japanese natural language processing, utilizing the LUW tokenizer and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [vumichien/wav2vec2-large-pitch-recognition](https://huggingface.co/vumichien/wav2vec2-large-pitch-recognition)
  - This repository provides a fine-tuned Wav2Vec2 model (facebook/wav2vec2-large-xlsr-53) for Japanese accent recognition, achieving a 15.82% Word Error Rate on 16kHz audio input.
  - Downloads: 15
- [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese)
  - This repository provides a 1.5B parameter Japanese GPT2 model pretrained on Japanese Wikipedia and CC-100, requiring Juman++ word segmentation for text generation or fine-tuning.
  - Downloads: 15
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A and Retrieval-Augmented Generation (RAG) with Japanese context, and quantized using both GPTQ and AutoAWQ for improved performance approaching GPT-3.5.
  - Downloads: 15
- [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese BERT base model, optimized for super short unit words (SSUW) and requiring full-width character input and prior segmentation (like with KyTea) for masked language modeling tasks.
  - Downloads: 15
- [paulhindemith/fasttext-jp-embedding](https://huggingface.co/paulhindemith/fasttext-jp-embedding)
  - This repository provides a pretrained FastText word embedding model for Japanese, along with a Google Colaboratory example demonstrating its usage with transformers, PyTorch, and MeCab.
  - Downloads: 15
- [KoichiYasuoka/deberta-small-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-aozora)
  - DeBERTa-small-japanese-aozora is a pre-trained Japanese language model based on DeBERTa V2, trained on the Aozora Bunko corpus and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 15
- [Miwa-Keita/zenz-v2.5-xsmall](https://huggingface.co/Miwa-Keita/zenz-v2.5-xsmall)
  - Zenz-v2.5 is a Japanese kana-to-kanji conversion model based on the GPT-2 architecture, offered in small (91M), medium (310M), and xsmall (26M) sizes, and licensed under CC-BY-SA 4.0.
  - Downloads: 15
- [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese)
  - This repository provides a fine-tuned Wav2Vec2-large-xlsr-53 model for Japanese speech recognition, trained on Common Voice, JVS, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 15
- [izumi-lab/electra-small-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-generator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text generation, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 15
- [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese)
  - RoBERTa-long-japanese is a Japanese language model pretrained on 200M sentences, extending RoBERTa-base with a 1282 max position embedding for longer inputs and requiring Juman++ pre-tokenization followed by SentencePiece tokenization.
  - Downloads: 15
- [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp)
  - This repository provides a Japanese DeBERTa V2 base model pre-trained on extensive Japanese text data for masked language modeling tasks.
  - Downloads: 15
- [sonoisa/t5-base-japanese-mC4-Wikipedia](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on an 890GB corpus of Japanese text, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 15
- [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-13B language model, available on Hugging Face.
  - Downloads: 15
- [KoichiYasuoka/bert-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-unidic-luw-upos)
  - This repository provides a Japanese BERT-large model pre-trained on Wikipedia for Universal Part-Of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 15
- [Kendamarron/LongWriter-llm-jp-3-3.7b-instruct](https://huggingface.co/Kendamarron/LongWriter-llm-jp-3-3.7b-instruct)
  - This repository hosts llm-jp-3.7b-instruct, a Japanese language model fine-tuned with the Japanese-LongWriter-3k dataset to enable long-form text generation, trained with a learning rate of 1e-05 and specific batch sizes.
  - Downloads: 15
- [k-ush/xlm-roberta-base-ance-en-jp-warmup](https://huggingface.co/k-ush/xlm-roberta-base-ance-en-jp-warmup)
  - This repository provides a XLM-RoBERTa-base model fine-tuned on Japanese mMARCO data using an ANCE warmup strategy, achieving a peak MRR@100 of 0.242 at 50k steps, with dataset preparation scripts linked.
  - Downloads: 15
- [hiroshi-matsuda-rit/bert-base-sudachitra-v11](https://huggingface.co/hiroshi-matsuda-rit/bert-base-sudachitra-v11)
  - Bert-base-sudachitra-v11 is a Japanese language model based on SudachiTra, differing from the original chiTra v1.1 through surface form normalization and vocabulary adjustments for improved tokenization.
  - Downloads: 15
- [owner203/japanese-llama-2-7b-gguf](https://huggingface.co/owner203/japanese-llama-2-7b-gguf)
  - This repository provides the GGUF quantized version of the Japanese-LLaMA-2-7B language model, available on Hugging Face.
  - Downloads: 15
- [rinna/llama-3-youko-70b](https://huggingface.co/rinna/llama-3-youko-70b)
  - Rinna‚Äôs Llama 3 Youko 70B is a continually pre-trained version of Meta Llama 3 70B, enhanced with 5B Japanese/English tokens to improve performance on Japanese language tasks, and available in 8B and instruct-tuned quantized versions.
  - Downloads: 15
- [LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-8.0bpw-h8-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, offering human-aligned conversational abilities.
  - Downloads: 15
- [MuneK/bert-large-japanese-v2-finetuned-jed](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-jed)
  - bert-large-japanese-v2-finetuned-wrime
  - Downloads: 15
- [KoichiYasuoka/gpt2-medium-japanese-unidic-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-upos)
  - gpt2-medium-japanese-unidic-upos Model Description
  - Downloads: 15
- [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base)
  - This repository provides a Japanese language model, ‚Äúluke-japanese-wordpiece-base‚Äù, built upon studio-ousia/luke-japanese-base, pre-trained on Japanese Wikipedia data, and utilizing WordPiece tokenization to handle unknown entities.
  - Downloads: 14
- [Aratako/Japanese-Novel-Reward-modernbert-ja-310m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-310m)
  - This repository provides a fine-tuned modernbert-ja-310m reward model for evaluating the quality of Japanese novels, intended for use with reinforcement learning in novel generation, by predicting user ratings as a regression task‚Äîthough results may be affected by biases beyond text quality.
  - Downloads: 14
- [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite)
  - This repository provides a Japanese natural language processing pipeline featuring a bert-base transformer, parser, and named entity recognition (NER) components, built with spaCy and utilizing the UD_Japanese-GSD dataset under a CC BY-SA 4.0 license.
  - Downloads: 14
- [G-Root/deberta-v2-base-japanese](https://huggingface.co/G-Root/deberta-v2-base-japanese)
  - This repository provides a pre-trained Japanese DeBERTa V2 base model, fine-tuned on Japanese Wikipedia, CC-100, and OSCAR, for masked language modeling tasks.
  - Downloads: 14
- [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss)
  - This repository provides a 130.78M parameter Llama2 model trained on Japanese text, utilizing the `transformers` library and a provided training script for causal language modeling.
  - Downloads: 14
- [oshizo/qa-refine-japanese-gpt-1b](https://huggingface.co/oshizo/qa-refine-japanese-gpt-1b)
  - This repository details a Japanese GPT-1B-based model fine-tuned for extractive question answering and answer refinement within the gpt-index (v0.2.5) framework, utilizing specific prompt templates for context-based responses.
  - Downloads: 14
- [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens)
  - This repository provides a Japanese+English Sentence-BERT model (‚Äúsonoisa/sentence-bert-base-ja-en-mean-tokens‚Äù) pretrained on cl-tohoku/bert-base-japanese-whole-word-masking, demonstrating improved English STS benchmark accuracy with fugashi and ipadic dependencies.
  - Downloads: 14
- [ascktgcc/Mistral-nemo-ja-rp-v0.2](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2)
  - This repository provides a Japanese-enhanced, fine-tuned version of Mistral-Nemo (v0.2) for roleplaying, utilizing an expanded dataset and optimized for Japanese output with a recommended temperature of 0.3.
  - Downloads: 14
- [ClassCat/gpt2-base-japanese-v2](https://huggingface.co/ClassCat/gpt2-base-japanese-v2)
  - ClassCat/gpt2-base-japanese-v2 is a Japanese GPT-2 base model trained on Japanese Wikipedia and web crawl data, utilizing a 60,000-token BPE tokenizer for text generation with transformers v4.19.2.
  - Downloads: 14
- [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa)
  - This repository showcases a merged Stable Diffusion model (‚ÄúKokuwa‚Äù) built upon KiwiMix and other models, specializing in uniquely stylized, slightly quirky character generation with potential for future refinement and stability improvements.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_sudachi-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and WordPiece for efficient Japanese text processing.
  - Downloads: 14
- [KoichiYasuoka/deberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-luw-upos)
  - This repository provides a DeBERTa-large Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [KoichiYasuoka/deberta-small-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-upos)
  - This repository provides a DeBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-luw-upos)
  - KoichiYasuoka's repository provides a RoBERTa-large Japanese language model, pre-trained on Aozora texts, for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 14
- [kz/mt5base-finetuned-ECC-japanese-small](https://huggingface.co/kz/mt5base-finetuned-ECC-japanese-small)
  - This repository provides a Japanese fine-tuned mt5-base model for single-task error detection and correction, trained on a 20,000 text pair dataset with a "correction: " prefix.
  - Downloads: 14
- [KoichiYasuoka/roberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-large Japanese model, pretrained on the Aozora corpus for dependency parsing and question answering, building upon existing character-level and UD Japanese resources.
  - Downloads: 14
- [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos)
  - This repository provides a BERT-large Japanese model pre-trained for universal part-of-speech (UPOS) tagging and dependency parsing on Japanese Wikipedia, extending bert-large-japanese-char-extended with long-unit-word tagging.
  - Downloads: 14
- [dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn](https://huggingface.co/dahara1/DeepSeek-R1-Distill-Qwen-14B-unsloth-jpn)
  - This repository provides a Japanese-finetuned version of the DeepSeek-R1-Distill-Qwen-14B model, optimized for Japanese language output and featuring a system prompt defining it as a helpful Japanese assistant.
  - Downloads: 14
- [jweb/japanese-soseki-gpt2-1b](https://huggingface.co/jweb/japanese-soseki-gpt2-1b)
  - This repository offers a 1.3B parameter Japanese GPT2 model finetuned by jweb (based on rinna's work) in both PyTorch and Rust formats, requiring T5Tokenizer for use.
  - Downloads: 14
- [Formzu/bart-large-japanese](https://huggingface.co/Formzu/bart-large-japanese)
  - This repository provides a BART large language model for Japanese, converted from Kyoto University‚Äôs original release and compatible with the `transformers` library via `BartJapaneseTokenizer`.
  - Downloads: 14
- [Mizuiro-sakura/t5-CAMERA-title-generation](https://huggingface.co/Mizuiro-sakura/t5-CAMERA-title-generation)
  - This repository provides a fine-tuned T5-base-Japanese model for Japanese title generation from input text, pretrained on a 100GB Japanese corpus including Wikipedia and OSCAR data.
  - Downloads: 14
- [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ)
  - This repository hosts GPTQ-quantized models of Stability AI's Japanese StableLM Instruct Beta 70B, offering various parameter options for efficient large language model use.
  - Downloads: 14
- [yukismd/JapaneseQuizChatbot_v1](https://huggingface.co/yukismd/JapaneseQuizChatbot_v1)
  - This repository provides a fine-tuned open-calm-7b language model, trained with H2O LLM Studio on a Japanese quiz dataset, and ready for text generation using the transformers library with specified dependencies.
  - Downloads: 14
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-4bit-128g-actorder_False)
  - This repository hosts a 3.6B parameter, 4-bit quantized Japanese language model fine-tuned by LINE Corporation for instruction following.
  - Downloads: 14
- [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram)
  - This repository provides a Japanese BERT-base tokenizer utilizing the Nothing + Unigram algorithm, requiring a downloaded dictionary file for loading and use with Transformers.
  - Downloads: 14
- [kaiinui/kotoba-whisper-v2.0-mlx](https://huggingface.co/kaiinui/kotoba-whisper-v2.0-mlx)
  - kotoba-whisper-v2.0-mlx This repository contains a converted mlx-whisper model of kotoba-whisper-v2.0 which is suitable for running with Apple Silicon.
  - Downloads: 14
- [litagin/vits-japros-pretrained](https://huggingface.co/litagin/vits-japros-pretrained)
  - A pretrained Japanese TTS model intended for use in VITS-JaPros-WebUI.
  - Downloads: 14
- [nitky/Oumuamua-7b-base](https://huggingface.co/nitky/Oumuamua-7b-base)
  - Oumuamua-7b-base This is a merge of pre-trained language models created using mergekit.
  - Downloads: 14
- [KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-medium-japanese-ud-causal)
  - rinna-gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 14
- [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-v2 model for Japanese, trained on the Aozora Bunko corpus with BertJapaneseTokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short)
  - This repository details a T5 v1.1 Transformer model pre-trained on Japanese text, featuring GEGLU activation and optimized for fine-tuning with dropout re-enabled.
  - Downloads: 13
- [TheBloke/japanese-stablelm-base-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI‚Äôs Japanese StableLM Base Beta 70B language model, optimized for efficiency and accuracy with support from a16z and Massed Compute.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-unidic](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic)
  - This repository provides a pre-trained DeBERTa-large model for Japanese language tasks, utilizing the BertJapaneseTokenizer and trained on the Aozora Bunko corpus, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [izumi-lab/electra-small-paper-japanese-fin-generator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-generator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text generation, based on the original ELECTRA architecture.
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-char-luw-upos)
  - This repository provides a RoBERTa-large Japanese character-level model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing.
  - Downloads: 13
- [KoichiYasuoka/roberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora)
  - KoichiYasuoka/roberta-large-japanese-aozora is a pre-trained RoBERTa-large model for Japanese, trained on the Aozora Bunko corpus with Japanese-LUW tokenization, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 13
- [vlzcrz/vlzcrz-whisper-small-japanese-2](https://huggingface.co/vlzcrz/vlzcrz-whisper-small-japanese-2)
  - Whisper Small Ja is a fine-tuned version of OpenAI‚Äôs Whisper-small model, trained on the Common Voice 17.0 dataset with specific hyperparameters for improved Japanese speech recognition.
  - Downloads: 13
- [KoichiYasuoka/roberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-luw-upos)
  - This repository provides a Japanese RoBERTa model pre-trained on Aozora corpus texts for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [okazaki-lab/japanese-reversed-gpt2-medium-unidic](https://huggingface.co/okazaki-lab/japanese-reversed-gpt2-medium-unidic)
  - This repository provides a medium-sized, right-to-left (reversed) Japanese GPT-2 model utilizing a BERT-like tokenizer and requiring PyTorch, Fugashi with Unidic-lite, and Hugging Face Transformers.
  - Downloads: 13
- [llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0](https://huggingface.co/llm-jp/llm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0)
  - LLM-jp provides 13B and 1.3B Japanese language models, including instruction-tuned and LoRA variants built from pre-trained checkpoints, as part of a collaborative Japanese project.
  - Downloads: 13
- [kit-nlp/bert-base-japanese-basic-char-v2-irony](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-irony)
  - This repository provides a Japanese BERT-base model finetuned for irony detection using ironic and sarcastic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa(V2) language model pretrained on Japanese Wikipedia and Aozora Bunko for universal dependencies tasks like POS-tagging and dependency parsing, utilizing the goeswith subword tokenizer.
  - Downloads: 13
- [KoichiYasuoka/roberta-small-japanese-char-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-char-luw-upos)
  - This repository provides a RoBERTa-small Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of character-level long-unit words.
  - Downloads: 13
- [KoichiYasuoka/deberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-small-japanese-luw-upos)
  - This repository provides a DeBERTa(V2) model pre-trained on Japanese ÈùíÁ©∫ÊñáÂ∫´ text for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Wikipedia and Aozora Bunko texts for Universal Part-of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [KoichiYasuoka/bert-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-large-japanese-wikipedia-ud-head)
  - This repository provides a BERT-large model pretrained on Japanese Wikipedia for dependency parsing and question answering, building upon existing Japanese BERT models and the UD_Japanese-GSDLUW dataset.
  - Downloads: 13
- [isek-ai/isekai-bert-v1](https://huggingface.co/isek-ai/isekai-bert-v1)
  - Isekai-BERT-v1 is a fine-tuned Japanese BERT model (cl-tohoku/bert-base-japanese-v3) with reported loss of 1.9164, but lacking detailed information on data, training, and intended use.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-goeswith)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for universal dependencies tasks like POS-tagging and dependency parsing, utilizing the goeswith subword approach.
  - Downloads: 13
- [KoichiYasuoka/deberta-base-japanese-unidic-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-unidic-ud-head)
  - This repository provides a DeBERTa-based Japanese question-answering model, pretrained on Aozora Bunko for dependency parsing and optimized for handling ambiguous words using the [MASK] token.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 13
- [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator)
  - This repository provides a Japanese ELECTRA base model, pretrained on Japanese Wikipedia data, featuring 12 layers, 768 hidden dimensions, and 12 attention heads.
  - Downloads: 13
- [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b)
  - ELYZA-japanese-CodeLlama-7b is a Japanese language model built upon Code Llama with additional pre-training to enhance its capabilities, particularly for code generation and instruction following.
  - Downloads: 13
- [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-instruct-gamma-7b using Slerp merging across the first 32 layers, based on Mistral-7B as the base model.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-juman-ud-goeswith)
  - This repository provides a DeBERTa-v2-large Japanese model, pretrained for universal dependency parsing and POS-tagging with the `goeswith` subword tokenizer, requiring the `fugashi` library for use.
  - Downloads: 13
- [KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-luw-upos)
  - This repository provides a DeBERTa-large Japanese language model pre-trained on Wikipedia and Aozora Bunko texts for Universal Part-of-Speech tagging and dependency parsing with long-unit-word support.
  - Downloads: 13
- [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-32g-actorder_False)
  - LINE Corporation‚Äôs repository hosts a 1.7B parameter, 4-bit quantized Japanese language model fine-tuned for instruction following.
  - Downloads: 13
- [ohwi/japanese-stablelm-instruct-gamma-7b-repro](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-repro)
  - This repository reproduces a 7B-parameter Japanese language model, fine-tuned for instruction-following using translated Dolly-15k and other datasets, built upon Japanese Stable LM Base Gamma 7B with the Notus codebase.
  - Downloads: 13
- [TheBloke/japanese-stablelm-instruct-beta-70B-AWQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-AWQ)
  - This repository provides AWQ-quantized files for Stability AI's Japanese StableLM Instruct Beta 70B, optimized using hardware from Massed Compute and supported by a16z and Patreon contributions.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-5.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat is a Japanese and English language model finetuned from Llama-2-7b using direct preference optimization on a large Japanese dataset, built by SambaNova Systems.
  - Downloads: 13
- [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo)
  - „É¢„Éá„É´ „Éô„Éº„Çπ„É¢„Éá„É´Ôºömicrosoft/Phi-3-mini-4k-instruct Â≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºöllm-jp/hh-rlhf-12k-ja Â≠¶ÁøíÊñπÂºèÔºö„Éï„É´„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ „Çµ„É≥„Éó„É´ import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", trust_remote_code=True, ) model = AutoModelForCausalLM.from_pretrained( "ryota39/Phi-3-mini-4k-instruct-dpo", device_map="auto", torch_dtype='auto', trust_remote_code=True, ) text = "&lt;|user|&gt;\n‰∏é„Åà„Çâ„Çå„ÅüË≥™Âïè„Å´ÂØæ„Åó„Å¶Ëã±Ë™û„ÅßÊÄùËÄÉ„Åó„ÄÅÊó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
  - Downloads: 13
- [Elizezen/Omnia-2x7B](https://huggingface.co/Elizezen/Omnia-2x7B)
  - Omnia 2x7B Description This repository hosts Omnia-2x7B, an advanced Japanese language model specifically trained for generating novels.
  - Downloads: 13
- [fznx92/openai-whisper-large-v2-ja-transcribe-colab](https://huggingface.co/fznx92/openai-whisper-large-v2-ja-transcribe-colab)
  - Model Card for Model ID Japanese transcription, testing in progress to see results, main personal use cases are japanese comedy usage 9GB vram with this Lora Model Details Model Description openai-whisper-large-v2-LORA-ja Developed by: FZNX Model type: PEFT LORA Language(s) (NLP):
  - Downloads: 13
- [hotchpotch/bert-base-japanese-v3-retromae](https://huggingface.co/hotchpotch/bert-base-japanese-v3-retromae)
  - tohoku-nlp/bert-base-japanese-v3 „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/japanese-splade-base-v1-dummy-fast-tokenizer-for-tei)
  - japanese-splade-base-v1„Çí huggingface/text-embeddings-inference„ÅßÂãï„Åã„Åô„Åü„ÇÅ„ÅÆ fork „Åß„Åô„ÄÇ
  - Downloads: 13
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF)
  - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF Ê¶ÇË¶Å Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 13
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit The Model mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-8bit was converted to MLX format from cyberagent/Llama-3.1-70B-Japanese-Instruct-2407 using mlx-lm version 0.16.1.
  - Downloads: 13
- [KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-xsmall-japanese-ud-causal)
  - rinna-gpt2-xsmall-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/abeja-gpt2-large-japanese-ud-causal)
  - abeja-gpt2-large-japanese-ud-causal Model Description
  - Downloads: 13
- [espnet/kan-bayashi_jsut_transformer_accent_with_pause](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent_with_pause)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent_with_pause ‚ôª
  - Downloads: 13
- [KoichiYasuoka/gpt2-medium-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-ud-causal)
  - gpt2-medium-japanese-ud-causal Model Description
  - Downloads: 13
- [KoichiYasuoka/gpt2-large-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-large-japanese-upos)
  - gpt2-large-japanese-upos Model Description
  - Downloads: 13
- [LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter-sentiment-mixed-label)
  - BERT for Sentiment Analysis of Japanese Twitter
  - Downloads: 13
- [youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF](https://huggingface.co/youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF)
  - youhansun/Llama-3-70B-japanese-suzume-vector-v0.1-Q2_K-GGUF
  - Downloads: 13
- [y-oikawa/Information-triage-for-disaster-tweets](https://huggingface.co/y-oikawa/Information-triage-for-disaster-tweets)
  - This repository provides a Japanese ELECTRA model, pretrained and finetuned on disaster tweets, for information triage tasks, licensed under CC BY-SA 4.0.
  - Downloads: 12
- [line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft-8bit-1g-actorder_True)
  - LINE Corporation‚Äôs repository hosts a quantized 3.6B parameter Japanese language model, fine-tuned for instruction following and optimized for use with 1GB of memory.
  - Downloads: 12
- [TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GPTQ)
  - This repository hosts quantized (GPTQ) versions of Stability AI's Japanese StableLM Instruct Beta 7B language model, offering various parameter options.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_jumanpp-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and WordPiece for natural language processing.
  - Downloads: 12
- [ku-accms/roberta-base-japanese-ssuw](https://huggingface.co/ku-accms/roberta-base-japanese-ssuw)
  - This repository provides a pre-trained Japanese RoBERTa base model optimized for super short unit words (SSUW) requiring full-width character conversion and SSUW segmentation as pre-processing steps.
  - Downloads: 12
- [nakamura196/roberta-small-hi-char-mlm](https://huggingface.co/nakamura196/roberta-small-hi-char-mlm)
  - This repository provides a small RoBERTa model pre-trained on Hindi text using a character-level tokenizer for masked language modeling tasks.
  - Downloads: 12
- [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pre-trained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer, 256-dimensional architecture with 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/roberta-large-japanese-aozora-char](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-aozora-char)
  - This repository provides a RoBERTa-large model pre-trained on Japanese Aozora Bunko texts using a character tokenizer, suitable for fine-tuning on tasks like POS tagging and dependency parsing.
  - Downloads: 12
- [KoichiYasuoka/roberta-base-japanese-aozora](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora)
  - This repository provides a RoBERTa-base model pre-trained on Japanese Aozora texts using the Japanese-LUW-Tokenizer, suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [KoichiYasuoka/bert-base-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-base-japanese-char-extended)
  - KoichiYasuoka/bert-base-japanese-char-extended is a Japanese BERT model, pre-trained on Wikipedia and enhanced with extended character embeddings for improved performance on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [izumi-lab/electra-base-japanese-generator](https://huggingface.co/izumi-lab/electra-base-japanese-generator)
  - This repository provides a Japanese language ELECTRA base model pretrained on Japanese Wikipedia data, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [huranokuma/es_IT](https://huggingface.co/huranokuma/es_IT)
  - This repository provides a fine-tuned Japanese GPT-2 model specifically for generating *ES* (entry sheets/cover letters) for IT job applications, built upon rinna's pretrained models.
  - Downloads: 12
- [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese)
  - This repository provides a pretrained DeBERTa V2 small Japanese language model for masked language modeling tasks, with associated pretraining code available elsewhere.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_vaporetto-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer (Vaporetto + Unigram) requiring a downloaded dictionary file for use with Transformers.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_vaporetto-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-bpe)
  - This repository provides a Japanese BERT-base model (‚ÄúVaporetto + BPE‚Äù) and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_sudachi-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-bpe)
  - This repository provides a Japanese BERT-base model utilizing Sudachi and Byte Pair Encoding (BPE) tokenization, requiring a downloaded Sudachi dictionary for tokenizer loading.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_mecab-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and WordPiece for natural language processing tasks.
  - Downloads: 12
- [thefrigidliquidation/nllb-200-distilled-1.3B-bookworm](https://huggingface.co/thefrigidliquidation/nllb-200-distilled-1.3B-bookworm)
  - This repository provides a 1.3B parameter NLLB-200 model fine-tuned for Japanese-to-English translation specifically tailored to the ‚ÄúAscendance of a Bookworm‚Äù web novel.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing of long-unit words.
  - Downloads: 12
- [owner203/japanese-llama-3-8b-instruct-v2](https://huggingface.co/owner203/japanese-llama-3-8b-instruct-v2)
  - Japanese-LLaMA-3-8B-Instruct-v2 is a full, instruction-following language model based on Meta-Llama-3-8B-Instruct, developed and tested on ConoHa VPS with NVIDIA H100 GPUs.
  - Downloads: 12
- [izumi-lab/electra-small-paper-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-fin-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data for financial text discrimination, utilizing a 12-layer architecture with 256 hidden dimensions and 4 attention heads.
  - Downloads: 12
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-head)
  - This repository provides a DeBERTa-based question-answering model pre-trained on Japanese Wikipedia and Aozora texts specifically for dependency parsing and handling ambiguous words using the [MASK] token.
  - Downloads: 12
- [naclbit/gpt-j-japanese-6.8b](https://huggingface.co/naclbit/gpt-j-japanese-6.8b)
  - This repository provides a 6.8 billion parameter, Japanese language pre-trained model based on EleutherAI‚Äôs Mesh Transformer JAX, utilizing T5Tokenizer and SentencePiece for Japanese tokenization.
  - Downloads: 12
- [KoichiYasuoka/bert-large-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-upos)
  - This repository provides a BERT-large Japanese model pre-trained for Universal Part-of-Speech (UPOS) tagging and dependency parsing on Japanese Wikipedia data.
  - Downloads: 12
- [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1)
  - This repository merges Mistral-7B-Instruct-v0.1 and stabilityai/japanese-stablelm-base-gamma-7b using Slerp merging, focusing on self-attention layers, to create a Japanese language model.
  - Downloads: 12
- [KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-large-japanese-juman-ud-goeswith)
  - This repository provides a RoBERTa-large Japanese model, pretrained for universal dependency parsing and POS-tagging using Japanese Wikipedia and CC-100, and requiring the `fugashi` library.
  - Downloads: 12
- [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork)
  - This repository provides a fork of LINE‚Äôs DistilBERT model, pre-trained on a large Japanese web corpus, with an updated tokenizer compatible with transformers ‚â•4.34.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Youki-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Youki-v1)
  - This repository provides a Japanese language model exhibiting inconsistent persona (shifting gender/personality) but consistently cheerful behavior, intended for experimentation rather than merging, with specific settings for text generation (max_length 150, top_p 0.8, temperature 0.7).
  - Downloads: 12
- [ebisuke/liz-nojaloli-nxja-ja](https://huggingface.co/ebisuke/liz-nojaloli-nxja-ja)
  - This repository provides a Japanese chat model‚Äîa variant of ebisuke/liz-nojaloli-ja‚Äîfine-tuned from abeja/gpt-neox-japanese-2.7b for personal study, utilizing a specific input format for conversational turns.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_sudachi-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_sudachi-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Sudachi and Unigram, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 12
- [ganchengguang/Yoko_13B_Japanese_QLoRA](https://huggingface.co/ganchengguang/Yoko_13B_Japanese_QLoRA)
  - This repository provides a QLoRA-fine-tuned Llama-2-13b-chat-hf model, trained on a large Japanese/Chinese dataset of chat and non-chat samples, with recommended generation parameters and testing script provided.
  - Downloads: 12
- [tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa](https://huggingface.co/tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa)
  - This repository provides a Japanese-StableLM-Base-Alpha-7B model fine-tuned to emulate the speech patterns of Reimu Hakurei from *Touhou Project*, enabling conversational interactions using a specific prompting format, and released under the Apache 2.0 license with 4-bit quantization.
  - Downloads: 12
- [tsukemono/japanese-novel-gpt-j-6b-f16-marisa](https://huggingface.co/tsukemono/japanese-novel-gpt-j-6b-f16-marisa)
  - This repository provides a fine-tuned Japanese language model based on japanese-novel-gpt-j-6b, enabling conversations with the Touhou Project character Marisa Kirisame, trained with 4-bit quantization.
  - Downloads: 12
- [Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly](https://huggingface.co/Mizuiro-sakura/open-calm-large-finetuned-databricks-dolly)
  - OpenCALM-LARGE is a suite of Japanese decoder-only language models by CyberAgent, Inc., fine-tuned using LoRA and the PEFT library with PyTorch and Transformers.
  - Downloads: 12
- [hitachi-nlp/bert-base-japanese_nothing-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-bpe)
  - This repository provides a Japanese BERT-base model and instructions for loading its tokenizer using a downloaded dictionary file for the Nothing + BPE method.
  - Downloads: 12
- [loiccabannes/MambaSan-130m-instruct](https://huggingface.co/loiccabannes/MambaSan-130m-instruct)
  - MambaSan-130m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on JaQuAD data, offering a non-transformer alternative for sequence modeling.
  - Downloads: 12
- [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ)
  - This repository provides a 3.89GB AWQ-quantized version of theELYZA/ELYZA-japanese-Llama-2-7b-instruct model, a Japanese instruction-tuned Llama 2 variant optimized for Colab A100 and RTX 3000 series GPUs.
  - Downloads: 12
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1 model, currently offering only the Q4_K_M quantization.
  - Downloads: 12
- [KoichiYasuoka/modernbert-base-japanese-char](https://huggingface.co/KoichiYasuoka/modernbert-base-japanese-char)
  - ModernBERT-base-japanese-char is a pre-trained Japanese language model based on BERT, trained on Wikipedia and Aozora Bunko texts, and suitable for fine-tuning on tasks like POS-tagging and dependency parsing.
  - Downloads: 12
- [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k)
  - Our Models Vecteus Ninja-v1 Ninja-v1-NSFW Ninja-v1-128k Ninja-v1-NSFW-128k Model Card for Ninja-v1-128k The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1 Ninja-128k has the following changes compared to Mistral-7B-v0.1.
  - Downloads: 12
- [Lycoris53/style-bert-vits2-sakura-miko](https://huggingface.co/Lycoris53/style-bert-vits2-sakura-miko)
  - Style-Bert-VITS2 Japanese Only Sakura Miko „Åì„Å°„Çâ„ÅØ„Äå„Åï„Åè„Çâ„Åø„Åì„Äç„ÅÆÈü≥Â£∞„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Â≠¶Áøí„Åï„Çå„ÅüVITS-TTS„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [masato12/bert-base-japanese-v3-marc_ja](https://huggingface.co/masato12/bert-base-japanese-v3-marc_ja)
  - https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja with ONNX weights to be compatible with Transformers PHP bert-base-japanese-v3-marc_ja „ÄåÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ„Äç„ÅÆÁ¨¨5Á´†„ÅßÁ¥π‰ªã„Åó„Å¶„ÅÑ„Çã(ÊÑüÊÉÖÂàÜÊûê)„ÅÆ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei](https://huggingface.co/hotchpotch/ruri-base-dummy-fast-tokenizer-for-tei)
  - „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅtext-embeddings-inference (TEI) „Åß„ÄÅmecab / unidic „Å™„Å©„ÇíÁî®„ÅÑ„ÅüÊó•Êú¨Ë™ûTokenizer„ÅÆ„É¢„Éá„É´„Çí„ÄÅdummy „ÅÆ tokenizer.json „ÇíÁî®„ÅÑ„Å¶ÁÑ°ÁêÜ„ÇÑ„ÇäÂãï„Åã„Åô ÊñπÊ≥ï„ÅÆ„Çµ„É≥„Éó„É´„Åß„Åô„ÄÇ
  - Downloads: 12
- [mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese](https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-Japanese)
  - This repository provides the DeepSeek-R1-Distill-Qwen-32B-Japanese language model converted to MLX format for use with the `mlx-lm` library.
  - Downloads: 11
- [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news)
  - This repository hosts a base-sized Japanese BART model, pre-trained by Stockmark Inc. on news data, utilizing a transformer encoder-decoder architecture for sequence-to-sequence tasks.
  - Downloads: 11
- [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Sentence Text Similarity (JSTS) using the JGLUE dataset, achieving a Pearson correlation of 0.8971.
  - Downloads: 11
- [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k)
  - This repository provides a 7B Japanese CodeLlama instruct model quantized with GPTQ, calibrated using a 1k sample from Wikipedia and the ELYZA-tasks-100 dataset.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram)
  - This repository provides a Japanese BERT-base tokenizer utilizing Juman++ and Unigram, requiring a downloaded dictionary file for proper loading and functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-unigram)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Unigram, requiring a downloaded dictionary file for proper functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_mecab-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_mecab-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing MeCab and Byte-Pair Encoding (BPE), requiring a downloaded dictionary for proper functionality.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_jumanpp-bpe](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-bpe)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing Juman++ and Byte Pair Encoding (BPE), requiring a downloaded dictionary file for proper functionality.
  - Downloads: 11
- [Aratako/Japanese-Novel-Reward-modernbert-ja-130m](https://huggingface.co/Aratako/Japanese-Novel-Reward-modernbert-ja-130m)
  - This repository provides a fine-tuned modernbert-ja-130m Reward model for evaluating the quality of Japanese novels, primarily intended for reinforcement learning of text generation models by predicting user-based ratings.
  - Downloads: 11
- [KoichiYasuoka/bert-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-luw-upos)
  - This repository provides a Japanese BERT model‚Äîbased on bert-base-japanese-char-extended‚Äîpre-trained for Universal Part-Of-Speech (UPOS) tagging and dependency parsing of long-unit words.
  - Downloads: 11
- [izumi-lab/electra-small-paper-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-paper-japanese-discriminator)
  - This repository provides a small ELECTRA discriminator model pre-trained on Japanese Wikipedia data, mirroring the original ELECTRA small architecture with 12 layers and 256 hidden dimensions.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora-ud-head)
  - This repository provides a DeBERTa-large Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, particularly optimized for long-unit words and handling ambiguous terms with `[MASK]` tokens.
  - Downloads: 11
- [huranokuma/es2](https://huggingface.co/huranokuma/es2)
  - This repository provides a fine-tuned Japanese GPT-2 model, trained on 140,000 ES (resume/CV) examples, and accessible via the web app http://www.eswrite.com, built upon rinnakk/japanese-pretrained-models.
  - Downloads: 11
- [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese)
  - This repository provides a Japanese BART model‚Äîconverted from Kyoto University's original‚Äîfor sequence-to-sequence tasks using the BartJapaneseTokenizer and the Hugging Face Transformers library.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-goeswith)
  - This repository provides a RoBERTa-base model pretrained on Japanese Aozora texts for universal dependency parsing and part-of-speech tagging, built upon existing Japanese RoBERTa and UD_Japanese-GSDLUW resources.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-ud-goeswith)
  - This repository provides a DeBERTa-based model pretrained on Japanese text for universal dependency parsing and part-of-speech tagging, built upon existing Japanese UD datasets and utilizing the goeswith subword tokenizer.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-wikipedia-ud-head)
  - DeBERTa-large-japanese-wikipedia-ud-head is a Japanese DeBERTa V2 model pretrained on Wikipedia & Aozora Bunko texts for dependency parsing and question answering, utilizing masked tokens for disambiguation.
  - Downloads: 11
- [KoichiYasuoka/deberta-large-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-upos)
  - This repository provides a DeBERTa-large Japanese model pre-trained on Aozora texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 11
- [KoichiYasuoka/deberta-base-japanese-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-upos)
  - This repository provides a DeBERTa-based Japanese language model pre-trained on Aozora Bunko texts for Universal Part-Of-Speech tagging and dependency parsing.
  - Downloads: 11
- [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small)
  - This repository provides a Japanese language model, fine-tuned from Google‚Äôs mt5-base, specifically for summarizing patent claims within the pharmaceutical domain.
  - Downloads: 11
- [KoichiYasuoka/roberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/roberta-base-japanese-aozora-ud-head)
  - This repository provides a RoBERTa-base Japanese model pretrained on the Aozora corpus for dependency parsing and question answering, building upon existing char-level and UD-Japanese models.
  - Downloads: 11
- [megagonlabs/transformers-ud-japanese-electra-base-ginza](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza)
  - This repository provides ja_ginza_electra, a spaCy v3-finetuned ELECTRA model pretrained on Japanese mC4 data and using SudachiTra tokenization for advanced natural language processing with GiNZA v5.
  - Downloads: 11
- [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator)
  - This repository provides a small ELECTRA model pretrained on Japanese Wikipedia data, featuring 12 layers, 256 hidden dimensions, and 4 attention heads, with pretraining code available at retarfi/language-pretraining.
  - Downloads: 11
- [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert)
  - Nagisa-BERT is a Japanese BERT model and tokenizer for use with the Transformers library, requiring Python 3.7+ on Linux or macOS and installable via pip.
  - Downloads: 11
- [astremo/JAINU](https://huggingface.co/astremo/JAINU)
  - JAINU is an experimental T5-based machine translation model for translating between Japanese and the Ainu language.
  - Downloads: 11
- [Gustav114514/work](https://huggingface.co/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 wav2vec2 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets with a 16kHz sampling rate.
  - Downloads: 11
- [A-Funakoshi/bert-base-japanese-v3-wrime-v2](https://huggingface.co/A-Funakoshi/bert-base-japanese-v3-wrime-v2)
  - This repository details Japanese sentiment analysis training using cl-tohoku's BERT base model on the wrime-sentiment dataset, optimized with AdamW and tuned via Optuna to achieve best performance with a cosine learning rate schedule, learning rate of 3.9e-5, batch size of 128, weight decay of 5.2e-5, and 100 epochs with early stopping.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_vaporetto-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_vaporetto-wordpiece)
  - This repository provides a Japanese BERT-base model (‚ÄúVaporetto + WordPiece‚Äù) and instructions for loading its tokenizer using a downloadable dictionary file.
  - Downloads: 11
- [aerner/lm-v1](https://huggingface.co/aerner/lm-v1)
  - Aerner LM-v1 is a small, fast, Japanese-language LLaMA-based model pre-trained entirely on Japanese text, producing surprisingly coherent (though not always helpful) responses with a focus on generation quality via careful configuration.
  - Downloads: 11
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - This repository provides a 7B language model, `youri-7b`, fine-tuned for Q&A with context and quantized using AutoGPTQ/AutoAWQ, aiming for GPT-3.5-level performance.
  - Downloads: 11
- [hitachi-nlp/bert-base-japanese_nothing-wordpiece](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-wordpiece)
  - This repository provides a Japanese BERT-base model and tokenizer utilizing the Nothing + WordPiece algorithm, requiring a downloaded dictionary file for loading.
  - Downloads: 11
- [hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2](https://huggingface.co/hiroshi-matsuda-rit/electra-base-japanese-discriminator-v2)
  - This repository provides a Japanese ELECTRA model, pretrained on 200M sentences and tokenized with SudachiTra/WordPiece, readily usable with the Hugging Face Transformers library.
  - Downloads: 11
- [slplab/wav2vec2-xls-r-300m-japanese-hiragana](https://huggingface.co/slplab/wav2vec2-xls-r-300m-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech-to-text, operating at 16kHz and outputting continuous character sequences without word breaks.
  - Downloads: 11
- [loiccabannes/MambaSan-370m-instruct](https://huggingface.co/loiccabannes/MambaSan-370m-instruct)
  - MambaSan-370m-instruct is a Japanese chat language model built on the Mamba state-space architecture and fine-tuned on the JaQuAD dataset for conversational ability.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-3.0bpw-h6-exl2)
  - Stability AI's Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, decoder-only Japanese language model fine-tuned for instruction following, building upon the Japanese Stable LM Base Gamma 7B.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_full_band_vits_prosody](https://huggingface.co/espnet/kan-bayashi_jsut_full_band_vits_prosody)
  - This repository provides a pretrained ESPnet2 text-to-speech (TTS) model, `jsut_full_band_vits_prosody`, trained on the JSUT dataset and available via Zenodo.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-8.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following based on Japanese Stable LM Base Gamma, requiring Transformers 4.34.0+.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-6.0bpw-h6-exl2)
  - Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter Japanese language model fine-tuned for instruction following, based on the Stable LM Base Gamma 7B and requiring Transformers 4.34.0+.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-5.0bpw-h6-exl2)
  - Stability AI‚Äôs Japanese Stable LM Instruct Gamma 7B is a 7-billion parameter, instruction-following Japanese language model fine-tuned from the base Gamma 7B, requiring Transformers 4.34.0+.
  - Downloads: 11
- [LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/stabilityai_japanese-stablelm-instruct-gamma-7b-4.0bpw-h6-exl2)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned for instruction following, based on Japanese Stable LM Base Gamma and requiring Transformers 4.34.0+.
  - Downloads: 11
- [traintogpb/llama-3-mmt-xml-it-sft-adapter](https://huggingface.co/traintogpb/llama-3-mmt-xml-it-sft-adapter)
  - Pretrained LM beomi/Llama-3-Open-Ko-8B (MIT License)
  - Downloads: 11
- [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2)
  - ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b „ÅØ„ÄÅ Llama2„Çí„Éô„Éº„Çπ„Å®„Åó„Å¶Êó•Êú¨Ë™ûËÉΩÂäõ„ÇíÊã°Âºµ„Åô„Çã„Åü„ÇÅ„Å´ËøΩÂä†‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„Å£„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [espnet/kan-bayashi_jsut_transformer_accent](https://huggingface.co/espnet/kan-bayashi_jsut_transformer_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_transformer_accent ‚ôª
  - Downloads: 11
- [KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal](https://huggingface.co/KoichiYasuoka/rinna-gpt2-small-japanese-ud-causal)
  - rinna-gpt2-small-japanese-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal](https://huggingface.co/KoichiYasuoka/goldfish-gpt2-japanese-10mb-ud-causal)
  - goldfish-gpt2-japanese-10mb-ud-causal Model Description
  - Downloads: 11
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit)
  - Tanuki-8B-dpo-v1.0-4k-GPTQ-8bit Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGPTQ 8bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-unidic-ud-causal)
  - gpt2-medium-japanese-unidic-ud-causal Model Description
  - Downloads: 11
- [KoichiYasuoka/gpt2-small-japanese-juman-upos](https://huggingface.co/KoichiYasuoka/gpt2-small-japanese-juman-upos)
  - gpt2-small-japanese-juman-upos Model Description
  - Downloads: 11
- [Aratako/calm3-22b-RP-v0.1](https://huggingface.co/Aratako/calm3-22b-RP-v0.1)
  - calm3-22b-RP-v0.1 cyberagent/calm3-22b-chat„Çí„Éô„Éº„Çπ„Å´„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´QLoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [KoichiYasuoka/gpt2-medium-japanese-upos](https://huggingface.co/KoichiYasuoka/gpt2-medium-japanese-upos)
  - gpt2-medium-japanese-upos Model Description
  - Downloads: 11
- [HODACHI/glm-4-9b-chat-FT-ja-v0.3](https://huggingface.co/HODACHI/glm-4-9b-chat-FT-ja-v0.3)
  - Ê¶ÇË¶Å GLM-4-9B-Chat„Çí„ÄÅÊó•Êú¨Ë™û„ÅÆWiki„Éá„Éº„Çø„ÇíÈÅ∏ÂÆö„Åó„ÄÅËøΩÂä†Â≠¶Áøí„Åó„ÅüÊó•Êú¨Ë™û„Å´ÈùûÂ∏∏„Å´Âº∑„ÅÑ„Çπ„Ç≥„Ç¢„ÇíÂá∫„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Syntactic Text Processing
- [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1)
  - Model Card for Model ID ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô /
  - Downloads: 3,944
- [mmnga/Ninja-v1-NSFW-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-gguf)
  - This repository provides GGUF-formatted conversions of the Ninja-v1 NSFW language model, trained on the TFMC/imatrix-dataset for Japanese LLMs, and demonstrates its usage with llama.cpp.
  - Downloads: 3,637
- [mmnga/Ninja-v1-NSFW-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-NSFW-128k-gguf)
  - This repository provides a GGUF-formatted, 128k context version of the Ninja-v1-NSFW language model, trained on the imatrix dataset and compatible with llama.cpp for local novel generation.
  - Downloads: 3,085
- [mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Japanese-Instruct-2407-gguf)
  - Llama-3.1-70B-Japanese-Instruct-2407-gguf cyberagent„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-Japanese-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 3,004
- [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix)
  - This repository distributes a model based on CreativeML Open RAIL-M, adding ‚Äú‰ΩêÂüéÈÉéÁîª‚Äù (sazyou_roukaku) as an additional author, while adhering to the original license and its usage restrictions, particularly prohibiting use for illegal or specialized applications like medical imaging.
  - Downloads: 2,717
- [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 2,665
- [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small)
  - OpenCALM is a suite of decoder-only language models pre-trained by CyberAgent on Japanese datasets, readily usable via the Hugging Face Transformers library.
  - Downloads: 2,348
- [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,114
- [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 2,019
- [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 2,002
- [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old)
  - This repository stores archived, experimental, and merged Stable Diffusion 1.5 models‚Äîspecifically a ‚Äúlametta‚Äù-style material (v1745 + littleMonsters_anime) designed for strong deformation and stylistic effects when merged with other models.
  - Downloads: 1,748
- [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3)
  - This repository presents evaluation results for several Japanese information retrieval models (including variations of splade-japanese) on MIRACL and JQaRA datasets, reporting metrics like nDCG, Recall, and MRR.
  - Downloads: 1,495
- [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium)
  - OpenCALM-Medium is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., readily usable with the Hugging Face Transformers library.
  - Downloads: 1,465
- [mmnga/Mistral-7B-Instruct-v0.3-gguf](https://huggingface.co/mmnga/Mistral-7B-Instruct-v0.3-gguf)
  - Mistral-7B-Instruct-v0.3-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-7B-Instruct-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,413
- [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1)
  - Swallow-MX-8x7b-NVE-v0.1 is a Japanese/English language model continuously pre-trained from Mixtral-8x7B-Instruct-v0.1, utilizing the same tokenizer and focused on improving Japanese language performance.
  - Downloads: 1,231
- [mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,101
- [mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-13b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-13b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-13b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,057
- [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b)
  - OpenCALM-3B is a suite of 3 billion parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., and accessible via the `transformers` library.
  - Downloads: 987
- [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large)
  - OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets by CyberAgent, Inc., enabling text generation via Hugging Face Transformers.
  - Downloads: 987
- [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 954
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF)
  - About static quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF Usage If you are unsure how to use GGUF files, refer to one of TheBloke's READMEs for more details, including on how to concatenate multi-part files.
  - Downloads: 854
- [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)
  - OpenCALM-7B is a suite of 7-billion parameter decoder-only language models pre-trained on Japanese data by CyberAgent, Inc., readily usable with transformers for text generation.
  - Downloads: 837
- [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf)
  - umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf umiyuki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãJapanese-Chat-Umievo-itr001-7b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 830
- [mmnga/ArrowPro-7B-KillerWhale-gguf](https://huggingface.co/mmnga/ArrowPro-7B-KillerWhale-gguf)
  - ArrowPro-7B-KillerWhale-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KillerWhale„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 815
- [grapevine-AI/gemma-2-2b-jpn-it-gguf](https://huggingface.co/grapevine-AI/gemma-2-2b-jpn-it-gguf)
  - What is this?
  - Downloads: 793
- [mmnga/rinna-llama-3-youko-8b-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-8b-gguf)
  - rinna-llama-3-youko-8b-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-8b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-70b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-70b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-70b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 732
- [QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF](https://huggingface.co/QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF)
  - QuantFactory/TinySlime-1.1B-Chat-v1.0-GGUF
  - Downloads: 726
- [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf)
  - This repository provides GGUF format conversions of the shisa-7b-v1 language model, demonstrated with examples for Pok√©mon-related question answering and Japanese-to-English translation using llama.cpp.
  - Downloads: 713
- [QuantFactory/plamo-13b-GGUF](https://huggingface.co/QuantFactory/plamo-13b-GGUF)
  - QuantFactory/plamo-13b-GGUF
  - Downloads: 697
- [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 685
- [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf)
  - c4ai-command-r-plus-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãc4ai-command-r-plus„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 675
- [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 630
- [mmnga/Ninja-v1-gguf](https://huggingface.co/mmnga/Ninja-v1-gguf)
  - Ninja-v1-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 628
- [mmnga/aixsatoshi-Honyaku-13b-gguf](https://huggingface.co/mmnga/aixsatoshi-Honyaku-13b-gguf)
  - aixsatoshi-Honyaku-13b-gguf aixsatoshi„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHonyaku-13b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 604
- [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 590
- [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 589
- [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 584
- [tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha](https://huggingface.co/tohoku-nlp/tohokunlp-bert-500m-sq4096-alpha)
  - (English part follows Japanese one.
  - Downloads: 576
- [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 565
- [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and available in 7b, 13b, and 70b instruction-tuned versions (v0.1 released April 26, 2024).
  - Downloads: 550
- [mmnga/llm-jp-3-7.2b-instruct3-gguf](https://huggingface.co/mmnga/llm-jp-3-7.2b-instruct3-gguf)
  - This repository provides GGUF-formatted versions of the llm-jp-3-7.2b-instruct3 Japanese language model, alongside instructions for use with llama.cpp, noting incompatibility with custom chat templates (-cvn).
  - Downloads: 548
- [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as previews starting April 26, 2024.
  - Downloads: 535
- [mmnga/Qwen1.5-110B-Chat-gguf](https://huggingface.co/mmnga/Qwen1.5-110B-Chat-gguf)
  - Qwen1.5-110B-Chat-gguf Qwen„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãQwen1.5-110B-Chat„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 518
- [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and released in instruction-tuned versions (7b, 13b, 70b) as of April 26, 2024.
  - Downloads: 511
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Jp-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Jp„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 498
- [QuantFactory/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/QuantFactory/gemma-2-2b-jpn-it-GGUF)
  - QuantFactory/gemma-2-2b-jpn-it-GGUF
  - Downloads: 485
- [mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Common-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Common-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Common-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 451
- [mmnga/Ninja-v1-128k-gguf](https://huggingface.co/mmnga/Ninja-v1-128k-gguf)
  - Ninja-v1-128k-gguf Local-Novel-LLM-project„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãNinja-v1-128k„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 408
- [mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF](https://huggingface.co/mradermacher/ABEJA-Qwen2.5-7b-Japanese-v0.1-GGUF)
  - This repository provides statically quantized versions of the ABEJA-Qwen2.5-7b-Japanese-v0.1 language model in GGUF format, offering various quantization levels (like Q2_K) for different size/quality trade-offs.
  - Downloads: 384
- [QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF](https://huggingface.co/QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF)
  - QuantFactory/Llama3.1-ArrowSE-v0.4-GGUF This is quantized version of DataPilot/Llama3.1-ArrowSE-v0.4 created using llama.cpp Original Model Card Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØllama3.1-8B-instruct„Çí„ÇÇ„Å®„Å´Êó•Êú¨Ë™ûÊÄßËÉΩ„ÇíÈ´ò„ÇÅ„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´Mergekit&amp;„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - Downloads: 354
- [QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF](https://huggingface.co/QuantFactory/Umievo-itr012-Gleipnir-7B-GGUF)
  - Umievo-itr012-Gleipnir-7B-GGUF
  - Downloads: 353
- [mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf](https://huggingface.co/mmnga/HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf)
  - HODACHI-Borea-Phi-3.5-mini-Instruct-Common-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãBorea-Phi-3.5-mini-Instruct-Common„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 312
- [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-v1-7B base model, a merged language model built upon Shisa Gamma, WizardMath, and Abel, usable with llama.cpp for tasks like generating Japanese text.
  - Downloads: 282
- [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja)
  - This repository provides a Stanza NLP model for Japanese, offering tools for linguistic analysis including syntax and entity recognition, automatically prepared via Hugging Face integration.
  - Downloads: 270
- [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf)
  - This repository provides a GGUF-formatted, 7B parameter version of the Deepreneur Blue Lizard model, a Japanese assistant based on the Llama 2 license, and includes usage instructions for llama.cpp.
  - Downloads: 266
- [mmnga/Mistral-Large-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Large-Instruct-2407-gguf)
  - Mistral-Large-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Large-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 249
- [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps)
  - This repository provides a faster, CTranslate2-converted Japanese Whisper large-v2 model for efficient speech-to-text transcription, utilizing the `faster-whisper` library.
  - Downloads: 228
- [mmnga/matsuolab-weblab-10b-instruction-sft-gguf](https://huggingface.co/mmnga/matsuolab-weblab-10b-instruction-sft-gguf)
  - This repository provides a GGUF-formatted conversion of the matsuo-lab weblab-10b-instruction-sft model, compatible with llama.cpp for instruction-following and inference.
  - Downloads: 214
- [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the SakanaAI EvoLLM-JP-A-v1-7B base model, utilizing models like Shisa, Gamma, and Mistral 7B, and is designed for use with llama.cpp.
  - Downloads: 193
- [QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF](https://huggingface.co/QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF)
  - QuantFactory/ELYZA-japanese-Llama-2-7b-instruct-GGUF
  - Downloads: 184
- [kawaimasa/wanabi_24b_preview_gguf](https://huggingface.co/kawaimasa/wanabi_24b_preview_gguf)
  - Wanabi-24B is a preview of a 24B language model fine-tuned for Japanese novel writing assistance, built on Mistral-Small-24B, and excelling at idea generation, text creation, and continuation, currently available in GGUF format as a proof-of-concept with limited training.
  - Downloads: 170
- [lmg-anon/vntl-llama3-8b-202409-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-gguf)
  - This repository contains some GGUF quantizations of the merged VNTL LLaMA3 8B 202409 qlora model, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 161
- [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha)
  - Japanese InstructBLIP Alpha is a vision-language model generating Japanese descriptions from images and optional text inputs, built on Llama and utilizing transformers for instruction following.
  - Downloads: 157
- [mmnga/stockmark-100b-gguf](https://huggingface.co/mmnga/stockmark-100b-gguf)
  - stockmark-100b-gguf stockmark„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãstockmark-100b„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 137
- [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1)
  - Swallow is a continually pre-trained language model based on Llama 2, enhanced with Japanese data and offering instruction-tuned versions (7b, 13b, 70b) released as of April 26, 2024.
  - Downloads: 134
- [Ryoma0302/gpt_0.76B_global_step3000_japanese](https://huggingface.co/Ryoma0302/gpt_0.76B_global_step3000_japanese)
  - Model Card for Model ID Model Details Model Description
  - Downloads: 133
- [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps)
  - This repository provides a research-focused, 5000-step fine-tuned version of OpenAI‚Äôs whisper-large-v2 model specifically for Japanese speech recognition using the CommonVoice v11 dataset, achieving a 0.7449 WER.
  - Downloads: 131
- [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine)
  - Model Card for Model ID ÊñôÁêÜ„ÇíÊ§úÁ¥¢„Åô„Çã„Åü„ÇÅ„ÅÆË≥™ÂïèÊñá„Åã„Çâ„ÄÅÊ§úÁ¥¢Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ„Åß„ÅÇ„ÇãÂõ∫ÊúâË°®Áèæ„ÇíÊäΩÂá∫„Åó„Åæ„Åô Model Details Model Description ‰æã„Åà„Å∞„ÄÅ„ÄåÊù±‰∫¨„ÅÆËÇâÊñôÁêÜ„Åß„ÄÅÊò•„Å´È£ü„Åπ„Çâ„Çå„Çã„ÄÅÈ∂èËÇâ„Çí‰Ωø„Å£„ÅüÊñôÁêÜ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊñáÁ´†„ÇíÂÖ•Âäõ„Åô„Çã„Å®„ÄÅ „ÄåÊù±‰∫¨ ‚Üí ÈÉΩÈÅìÂ∫úÁúå/Âú∞Êñπ(AREA)„Äç „ÄåËÇâÊñôÁêÜ ‚Üí Á®ÆÈ°û(TYPE)„Äç „ÄåÊò• ‚Üí Â≠£ÁØÄ(SZN)
  - Downloads: 127
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-breadcrumbs„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 121
- [QuantFactory/shisa-gamma-7b-v1-GGUF](https://huggingface.co/QuantFactory/shisa-gamma-7b-v1-GGUF)
  - QuantFactory/shisa-gamma-7b-v1-GGUF
  - Downloads: 117
- [2121-8/japanese-parler-tts-large-bate](https://huggingface.co/2121-8/japanese-parler-tts-large-bate)
  - This repository releases a retrained, lightweight, high-quality Japanese text-to-speech model based on Parler-TTS-large-v1, featuring a custom tokenizer and currently in beta.
  - Downloads: 114
- [MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF](https://huggingface.co/MCZK/Llama-3.1-8B-EZO-1.1-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3.1-8B-EZO-1.1-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 112
- [Aratako/Oumuamua-7b-RP-GGUF](https://huggingface.co/Aratako/Oumuamua-7b-RP-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Oumuamua-7b-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 111
- [QuantFactory/llama-3-youko-8b-GGUF](https://huggingface.co/QuantFactory/llama-3-youko-8b-GGUF)
  - QuantFactory/llama-3-youko-8b-GGUF
  - Downloads: 108
- [Aratako/Ninja-v1-RP-expressive-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 99
- [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B)
  - Japanese-Starling-ChatV-7B is a 7B parameter Japanese chat model based on chatntq-ja-7b-v1.0 and Mistral-7B, with details and GGUF versions available in the repository.
  - Downloads: 94
- [AELLM/Llama-3.2-Chibi-3B](https://huggingface.co/AELLM/Llama-3.2-Chibi-3B)
  - Preface Small parameter LLMs are ideal for navigating the complexities of the Japanese language, which involves multiple character systems like kanji, hiragana, and katakana, along with subtle social cues.
  - Downloads: 94
- [Aratako/Ninja-v1-RP-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-GGUF)
  - Ninja-v1-RP-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 93
- [mradermacher/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-GGUF)
  - This repository provides statically quantized versions of the Japanese-Starling-ChatV-7B language model in GGUF format, offering various quantization levels (Q2_K, Q3_K_S) for different size/quality trade-offs.
  - Downloads: 91
- [mmnga/RakutenAI-7B-gguf](https://huggingface.co/mmnga/RakutenAI-7B-gguf)
  - RakutenAI-7B-gguf Rakuten„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãRakutenAI-7B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 85
- [Aratako/Ninja-v1-RP-expressive-v2-GGUF](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2-GGUF)
  - Ninja-v1-RP-expressive-GGUF Ê¶ÇË¶Å Aratako/Ninja-v1-RP-expressive-v2„ÅÆÈáèÂ≠êÂåñÊ∏à„ÅøGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 81
- [Respair/Japanese_Phoneme_to_Grapheme_LLM](https://huggingface.co/Respair/Japanese_Phoneme_to_Grapheme_LLM)
  - Model Card for Model ID
  - Downloads: 65
- [JhonVanced/whisper-large-v3-japanese-4k-steps-ct2](https://huggingface.co/JhonVanced/whisper-large-v3-japanese-4k-steps-ct2)
  - This repository converts the drewschaub/whisper-large-v3-japanese-4k-steps speech recognition model to the CTranslate2 format for faster, more efficient transcription using CTranslate2 or faster-whisper.
  - Downloads: 54
- [Aratako/Ninja-v1-RP-expressive](https://huggingface.co/Aratako/Ninja-v1-RP-expressive)
  - Ninja-v1-RP-expressive GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 54
- [NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF)
  - NikolayKozloff/gemma-2-2b-jpn-it-Q8_0-GGUF
  - Downloads: 46
- [MCZK/Tora-7B-v0.1-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.1-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF)
  - This repository provides a GGUF conversion of ChatNTQ-JA-7b-v1.0, a Japanese chat model fine-tuned from stabilityai/japanese-stablelm-base-gamma-7b (based on Mistral 7B).
  - Downloads: 44
- [ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF](https://huggingface.co/ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF)
  - ascktgcc/Mistral-Nemo-Japanese-Instruct-2408-Q4_K_S-GGUF
  - Downloads: 44
- [mradermacher/japanese-llama-3-8b-instruct-v2-GGUF](https://huggingface.co/mradermacher/japanese-llama-3-8b-instruct-v2-GGUF)
  - This repository provides statically quantized GGUF versions of the japanese-llama-3-8b-instruct-v2 model, offering various quantization levels (like Q2_K) for efficient use, with size details and a reference to TheBloke‚Äôs documentation for usage.
  - Downloads: 41
- [Aratako/Japanese-Novel-Reward-sarashina2.1-1b](https://huggingface.co/Aratako/Japanese-Novel-Reward-sarashina2.1-1b)
  - This repository provides a Japanese novel quality assessment Reward Model, fine-tuned from sbintuitions/sarashina2.1-1b for use in applications like reinforcement learning of text generation, predicting user evaluation scores while acknowledging potential biases beyond text quality.
  - Downloads: 40
- [MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF](https://huggingface.co/MCZK/Japanese-Chat-Umievo-itr004-7b-GGUF)
  - umiyukiÊßò„ÅÆ Japanese-Chat-Umievo-itr004-7b „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 38
- [alfredplpl/gemma-2b-it-ja-poc-2](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-2)
  - „ÅØ„Åò„ÇÅ„Å´ „Å™„Çì„ÅãÊó•Êú¨Ë™û„ÅåË©±„Åõ„ÇãÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Å™AI„Åß„Åô„ÄÇ
  - Downloads: 35
- [MCZK/Llama-3-EZO-8b-Common-it-GGUF](https://huggingface.co/MCZK/Llama-3-EZO-8b-Common-it-GGUF)
  - HODACHIÊßò„ÅÆ Llama-3-EZO-8b-Common-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 33
- [HODACHI/EZO-InternVL2-26B](https://huggingface.co/HODACHI/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 32
- [Aratako/Ninja-v1-RP-expressive-v2](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-v2)
  - Ninja-v1-RP-expressive-v2 GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 30
- [mmnga/Tanuki-ZeRo-gguf](https://huggingface.co/mmnga/Tanuki-ZeRo-gguf)
  - This repository provides a GGUF-formatted version of the Tanuki-ZeRo base language model, compatible with llama.cpp for inference and testing.
  - Downloads: 27
- [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese)
  - This repository provides Mixtral-8x7B-v0.1-japanese, a Japanese language model built upon Mixtral-8x7B-v0.1 with extended vocabulary and continued pre-training.
  - Downloads: 23
- [Akimite/Qwen2-7b-Instruct-Boku-v2](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v2)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 22
- [MCZK/Tora-7B-v0.2-GGUF](https://huggingface.co/MCZK/Tora-7B-v0.2-GGUF)
  - ryota39Êßò„ÅÆ Tora-7B-v0.2 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 21
- [grapevine-AI/sarashina2-70b-gguf](https://huggingface.co/grapevine-AI/sarashina2-70b-gguf)
  - What is this?
  - Downloads: 21
- [natsusakiyomi/AnzuMix](https://huggingface.co/natsusakiyomi/AnzuMix)
  - AnzuMixSeries VAE„ÅÆÂÜÖËáì„ÅØ„Å™„ÅÑ„ÅûÔºÅ„Å®Ë®Ä„Çè„Åõ„Å™„ÅÑ„ÅûÔºÅÔºÅÔºÅÔºÅ
  - Downloads: 21
- [AXCXEPT/EZO-InternVL2-26B](https://huggingface.co/AXCXEPT/EZO-InternVL2-26B)
  - [EZO model card]
  - Downloads: 20
- [sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-IQ4_XS-GGUF
  - Downloads: 20
- [Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0](https://huggingface.co/Ryu-m0m/16bit-japanese-finetuned-mistral-7b-v0)
  - Model Overview: Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åô„Çã„Å®„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„ÇíÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
  - Downloads: 20
- [hibikaze/tiny_mixtral_ja_with_tokenizer](https://huggingface.co/hibikaze/tiny_mixtral_ja_with_tokenizer)
  - 275.86M„ÅÆmixtral„ÇíÊó•Êú¨Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßpretraining„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô sample from transformers import AutoTokenizer, AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")
  - Downloads: 19
- [Chottokun/modernBERT_japanese_30m_ner_wikipedia](https://huggingface.co/Chottokun/modernBERT_japanese_30m_ner_wikipedia)
  - This repository demonstrates Named Entity Recognition (NER) using the modernBERT Japanese language model‚Äîspecifically ‚Äúsbintuitions/modernbert-ja-130m‚Äù‚Äîwith custom tokenization and a defined label set for person, organization, location, facility, product, and event names.
  - Downloads: 18
- [KoichiYasuoka/RakutenAI-7B-upos](https://huggingface.co/KoichiYasuoka/RakutenAI-7B-upos)
  - RakutenAI-7B-upos Model Description
  - Downloads: 18
- [Elizezen/Phos-7B](https://huggingface.co/Elizezen/Phos-7B)
  - Phos 7B „Äå„Å©„ÅÜ„Åã„ÅäÊÖàÊÇ≤„Çí „ÇÇ„ÅÜ Áñ≤„ÇåÊûú„Å¶„Åæ„Åó„Åü„Äç ÁîüÊàê‰æã [Â§™Â≠ó‰ª•Èôç„ÅåAIÁîüÊàê] „Äå„Å©„ÅÜ„Åã„Äç ‚Äù„Åù„Çå‚Äù„ÅØÊááÈ°ò„Åó„Åü„ÄÇ
  - Downloads: 17
- [Aratako/Ninja-v1-RP-expressive-breadcrumbs](https://huggingface.co/Aratako/Ninja-v1-RP-expressive-breadcrumbs)
  - Ninja-v1-RP-expressive-breadcrumbs GGUFÁâà„ÅØ„Åì„Å°„Çâ/Click here for the GGUF version Ê¶ÇË¶Å This is a merge of pre-trained language models created using mergekit.
  - Downloads: 16
- [Local-Novel-LLM-project/Ocuteus-v1](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1)
  - Vecteus„Çí„Éô„Éº„Çπ„Å´LLava„Å´ÂØæÂøú„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [lightblue/kurage-ja](https://huggingface.co/lightblue/kurage-ja)
  - Kurage Kurage is a multipurpose RAG model from Lightblue.
  - Downloads: 15
- [aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF](https://huggingface.co/aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF)
  - aashish1904/gemma-2-2b-jpn-it-Q2_K-GGUF
  - Downloads: 15
- [ybelkada/japanese-dummy-tokenizer](https://huggingface.co/ybelkada/japanese-dummy-tokenizer)
  - Japanese Dummy Tokenizer Repository containing a dummy Japanese Tokenizer trained on snow_simplified_japanese_corpus dataset.
  - Downloads: 14
- [mpasila/Llama-3-Nymeria-ELYZA-8B](https://huggingface.co/mpasila/Llama-3-Nymeria-ELYZA-8B)
  - Llama-3-Nymeria-ELYZA-8B Experimental merge between a Llama 3 model that has had continued pre-training with Japanese data and a regular RP model to see how well it keeps its Japanese capability and RP capability.
  - Downloads: 14
- [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius)
  - This repository shares experimental, lightly-tuned model merges‚Äîoriginally from the spekulatius project and similar to lametta‚Äîbuilt upon lametta_v1921 and various other models like vorpal_v1 and snowpearAnime_v10.
  - Downloads: 13
- [ganchengguang/USA-7B-instruction-incontext-learning](https://huggingface.co/ganchengguang/USA-7B-instruction-incontext-learning)
  - This repository provides a Japanese language model example utilizing AutoTokenizer, AutoModelForCausalLM, and Unifine formatting for in-context and instruction learning.
  - Downloads: 13
- [naclbit/trin_tokenizer_v3](https://huggingface.co/naclbit/trin_tokenizer_v3)
  - This repository provides a 52,000-vocabulary SentencePiece tokenizer specifically trained for Japanese creative writing with AI Novelist‚Äôs SuperTrin and Damsel 20B models.
  - Downloads: 13
- [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc)
  - Suzume-POC is a commercially usable, Japanese-adapted base model derived from Google‚Äôs Gemma-2B, optimized for smaller devices despite potential instruction tuning challenges.
  - Downloads: 13
- [Hemlok/ArcanaMix](https://huggingface.co/Hemlok/ArcanaMix)
  - ‚óÜArcanaMix ‰∫åÊ¨°ÂÖÉ„Ç§„É©„Çπ„Éà„Çí‰∏≠ÂøÉ„Å´„ÄÅ„Åã„Çè„ÅÑ„ÅÑ„Ç§„É©„Çπ„Éà„ÅåÂá∫Âäõ„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ„ÄÇ
  - Downloads: 13
- [sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF](https://huggingface.co/sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF)
  - sehiro/EvoLLM-JP-A-v1-7B-Q4_K_M-GGUF
  - Downloads: 13
- [Akimite/Gemma2-9B-it-Boku-v1](https://huggingface.co/Akimite/Gemma2-9B-it-Boku-v1)
  - ÂÆüÈ®ì„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1)
  - Shisa-base-7b-v1 is a 7B language model built upon Mistral 7B, enhanced with 8B Japanese tokens from MADLAD-400 (and some English) and a 120k token tokenizer for improved Japanese language efficiency (~2.3 characters/token).
  - Downloads: 12
- [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation)
  - This repository provides a model for generating article content from titles, as detailed in the linked Qiita article.
  - Downloads: 12
- [DataPilot/sarashina2.2-3Bx4-moe](https://huggingface.co/DataPilot/sarashina2.2-3Bx4-moe)
  - Sarashina2.2-3Bx4-moe is a ~12B parameter Mixture of Experts (MoE) model created by merging a base model with four sbintuitions/sarashina2.2-3b-instruct-v0.1 expert models to improve performance and generate high-quality responses.
  - Downloads: 12
- [Aruno/Bloom-JP-160m](https://huggingface.co/Aruno/Bloom-JP-160m)
  - This repository provides a Japanese language Bloom model with 10,000 vocabulary size, 12 layers, and 8 attention heads.
  - Downloads: 12
- [SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall](https://huggingface.co/SousiOmine/sarashina2.2-3b-instruct-v0.1-Pythonic-FunctionCall)
  - This repository details a QLoRA fine-tuned language model (sbintuitions/sarashina2.2-3b-instruct-v0.1) enhanced with the ability to call Python functions, utilizing datasets from Kendamarron/jimba-instruction-all and SousiOmine/Japanese-Pythonic-FunctionCall, and formatted for tool use via specific tags.
  - Downloads: 12
- [Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2](https://huggingface.co/Atotti/TinySwallow-GRPO-TakahashiMethod-v0.2)
  - This model fine-tuned from SakanaAI/TinySwallow-1.5B-Instruct generates numerous, concise, bullet-point slides‚Äîeach with a maximum of 15 characters‚Äîin the style of the Takahashi Method for impactful presentations.
  - Downloads: 12
- [DataPilot/Arrival-32B-Instruct-v0.4](https://huggingface.co/DataPilot/Arrival-32B-Instruct-v0.4)
  - DataPilot's Arrival-32B-Instruct-v0.4 is a Japanese-enhanced, instruct-tuned language model fine-tuned from Abeja's Qwen2.5-32B base (using negative chat vectors due to limited base model access) and incorporating DeepSeek-R1-Distill-Qwen-32B-Japanese.
  - Downloads: 12
- [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese)
  - This repository implements a quote-based reasoning model for inference and potentially natural language understanding.
  - Downloads: 12
- [zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2](https://huggingface.co/zaq-hack/Orion-14B-LongChat-bpw600-h6-exl2)
  - Orion-14B üåêEnglish | üá®
  - Downloads: 12
- [dummy-foo/ChatGLM3-Japanese](https://huggingface.co/dummy-foo/ChatGLM3-Japanese)
  - ChatGLM3-6BÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠Â§ßÊ®°ÂûãÔºåÊú¨È°πÁõÆ‰∏∫ChatGLM3-6BÂä†ÂÖ•Êó•ÊñáËÉΩÂäõ„ÄÇ
  - Downloads: 12
- [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese)
  - YACIS-ELECTRA-Small is a Japanese language model pretrained on a 5.6 billion-word blog corpus using the ELECTRA architecture with 12 layers, 128 hidden dimensions, and WordPiece tokenization.
  - Downloads: 11
- [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL)
  - This repository provides AfterRealXL_beta2, a realistic image generation model licensed under CreativeML Open RAIL++-M, with usage governed by that license‚Äîprohibiting illegal or specialized uses like medical imaging‚Äîand includes merged checkpoints from nightvisionXLPhotorealisticPortrait and cbluePencilXL.
  - Downloads: 11
- [tealgreen0503/japanese-gpt2-medium-ppo-araisan](https://huggingface.co/tealgreen0503/japanese-gpt2-medium-ppo-araisan)
  - paper: Âº∑ÂåñÂ≠¶Áøí„ÇíÁî®„ÅÑ„Å¶„Ç≠„É£„É©„ÇØ„Çø„Çâ„Åó„Åï„Çí‰ªò‰∏é„Åó„ÅüÈõëË´áÂøúÁ≠î„ÅÆÁîüÊàê
  - Downloads: 11
- [KoichiYasuoka/karasu-1.1B-upos](https://huggingface.co/KoichiYasuoka/karasu-1.1B-upos)
  - KoichiYasuoka/karasu-1.1B-upos Model Description
  - Downloads: 11
- [NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF](https://huggingface.co/NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF)
  - NikolayKozloff/h2o-Llama-3-8B-Japanese-Instruct-Q8_0-GGUF
  - Downloads: 11
- [atsuki-yamaguchi/tigerbot-7b-base-random-ja](https://huggingface.co/atsuki-yamaguchi/tigerbot-7b-base-random-ja)
  - TigerBot-7B Japanese
  - Downloads: 11
- [shinyice/chatvector-llava-v1.5-plus-houou-v3-7b](https://huggingface.co/shinyice/chatvector-llava-v1.5-plus-houou-v3-7b)
  - Chatvector-llava-v1.5-plus-Houou-v3-7b Model Card Model Details ‚ÄªÂ•ΩÂ•áÂøÉ„Åã„ÇâÁîü„Åæ„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
- [Akimite/Qwen2-7b-Instruct-Boku-v3](https://huggingface.co/Akimite/Qwen2-7b-Instruct-Boku-v3)
  - Akimite/Qwen2-7b-Instruct-Boku-v2„ÅÆ„Éû„Ç§„Éä„Éº„ÉÅ„Çß„É≥„Ç∏Áâà„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality
- [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja)
  - FuguMT is an English-to-Japanese translation model built with Marian-NMT, transformers, and sentencepiece, readily usable via Hugging Face pipelines.
  - Downloads: 54,053
- [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en)
  - FuguMT is a Japanese-to-English translation model built with Marian-NMT, transformers, and sentencepiece, offering direct use via a Hugging Face pipeline.
  - Downloads: 51,453
- [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1)
  - Shisa-gamma-7b-v1 is a Japanese language model fine-tuned from Stable LM Base Gamma 7B, sharing strong performance‚Äîparticularly on JA MT-Bench‚Äîbased on a specific fine-tuning dataset.
  - Downloads: 50,125
- [HODACHI/Llama-3.1-8B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-8B-EZO-1.1-it)
  - [Llama-3.1-8B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 10,197
- [rinna/gemma-2-baku-2b](https://huggingface.co/rinna/gemma-2-baku-2b)
  - Gemma 2 Baku 2B (rinna/gemma-2-baku-2b)
  - Downloads: 4,384
- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  - Orion-14B is an open-source multilingual large language model series developed by OrionStarAI, trained on a 2.5T multilingual corpus with support for Chinese, English, Japanese, and Korean, and offering demos, benchmarks, and model downloads.
  - Downloads: 4,221
- [llm-jp/llm-jp-3-172b-beta2-instruct2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2-instruct2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 4,219
- [mmnga/aya-23-8B-gguf](https://huggingface.co/mmnga/aya-23-8B-gguf)
  - This repository provides a GGUF-formatted version of the Aya-23-8B language model, trained with the imatrix dataset and usable with llama.cpp.
  - Downloads: 3,695
- [Ultralytics/YOLO11](https://huggingface.co/Ultralytics/YOLO11)
  - Ultralytics YOLOv11 is a state-of-the-art, fast and flexible model for object detection, tracking, segmentation, classification, and pose estimation, building on previous YOLO versions.
  - Downloads: 2,961
- [mmnga/Llama-3.1-8B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-EZO-1.1-it-gguf)
  - Llama-3.1-8B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-8B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,805
- [AXCXEPT/Llama-3-EZO-8b-Common-it](https://huggingface.co/AXCXEPT/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 2,602
- [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf)
  - This repository provides GGUF-formatted conversions of the lightblue-suzume-llama-3-8B Japanese language model, built using the TFMC/imatrix-dataset-for-japanese-llm dataset, alongside other related lightblue/mmnga models.
  - Downloads: 2,387
- [webbigdata/gemma-2-2b-jpn-it-translate-gguf](https://huggingface.co/webbigdata/gemma-2-2b-jpn-it-translate-gguf)
  - gemma-2-2b-jpn-it-translate-gguf is a 2B parameter small language model for fast and high-quality Japanese-English/English-Japanese translation, rivaling larger 7B models in certain areas.
  - Downloads: 2,375
- [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1)
  - FinguAI-Chat-v1 is a multilingual AI model designed to improve English, Korean, and Japanese language skills through a specialized finance, investment, and legal curriculum with a global perspective.
  - Downloads: 2,037
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 1,653
- [mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf)
  - lightblue-suzume-llama-3-8B-multilingual-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãsuzume-llama-3-8B-multilingual„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,495
- [mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf)
  - pfnet-nekomata-14b-pfn-qfin-inst-merge-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin-inst-merge„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,273
- [dahara1/gemma-3-12b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-12b-it-qat-japanese-imatrix)
  - This repository provides a 4-bit quantized version of the Gemma 3B model, specifically optimized for Japanese language processing using an imatrix dataset and compatible with the latest llama.cpp for image and text input.
  - Downloads: 1,193
- [dartags/DanbotNL-2408-260M](https://huggingface.co/dartags/DanbotNL-2408-260M)
  - DanbotNL is a 260M parameter translator model, finetuned for converting Japanese and English natural language into Danbooru tags, licensed under Apache-2.0 and usable with ComfyUI.
  - Downloads: 1,040
- [mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf](https://huggingface.co/mmnga/tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf)
  - tokyotech-llm-Swallow-MS-7b-instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãSwallow-MS-7b-instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,038
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-AWQ)
  - Tanuki-8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 813
- [cardiffnlp/tweet-topic-large-multilingual](https://huggingface.co/cardiffnlp/tweet-topic-large-multilingual)
  - tweet-topic-large-multilingual This model is based on cardiffnlp/twitter-xlm-roberta-large-2022 language model and isfinetuned for multi-label topic classification in English, Spanish, Japanese, and Greek.
  - Downloads: 745
- [sbintuitions/sarashina1-13b](https://huggingface.co/sbintuitions/sarashina1-13b)
  - Sarashina1-13B
  - Downloads: 691
- [sbintuitions/sarashina1-65b](https://huggingface.co/sbintuitions/sarashina1-65b)
  - Sarashina1-65B
  - Downloads: 685
- [mmnga/Llama-3.1-70B-EZO-1.1-it-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-EZO-1.1-it-gguf)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.1-70B-EZO-1.1-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 667
- [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX transformer model, trained on a large corpus including Japanese CC-100, C4, and The Pile, and linked to the MiniGPT4 series on Hugging Face.
  - Downloads: 656
- [mmnga/pfnet-nekomata-14b-pfn-qfin-gguf](https://huggingface.co/mmnga/pfnet-nekomata-14b-pfn-qfin-gguf)
  - pfnet-nekomata-14b-pfn-qfin-gguf pfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãnekomata-14b-pfn-qfin„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 646
- [dahara1/gemma-3-27b-it-qat-japanese-imatrix](https://huggingface.co/dahara1/gemma-3-27b-it-qat-japanese-imatrix)
  - This repository provides a Japanese-inclusive, 4-bit quantized version of the Gemma 3B-27B-IT model, optimized for use with the latest llama.cpp and supporting image input via `llama-mtmd-cli` and `mmproj.gguf`.
  - Downloads: 596
- [Mitsua/elan-mt-bt-en-ja](https://huggingface.co/Mitsua/elan-mt-bt-en-ja)
  - ElanMT ElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.
  - Downloads: 529
- [Mitsua/elan-mt-bt-ja-en](https://huggingface.co/Mitsua/elan-mt-bt-ja-en)
  - ElanMT-BT-ja-en is an open-source Japanese-to-English translation model built and fine-tuned from scratch using only openly licensed and Wikipedia back-translated data, avoiding web-crawled or machine-translated sources.
  - Downloads: 505
- [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels)
  - This repository provides a 1.3B parameter NLLB model fine-tuned for Japanese to English translation of light novels, capable of processing up to 512 tokens using the `transformers` library.
  - Downloads: 494
- [HODACHI/Llama-3.1-70B-EZO-1.1-it](https://huggingface.co/HODACHI/Llama-3.1-70B-EZO-1.1-it)
  - [Llama-3.1-70B-EZO-1.1-it] Model Card „É¢„Éá„É´ÊÉÖÂ†± / Model Information „Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMeta AI „ÅÆ Llama 3.1 „Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíË°å„Å£„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 479
- [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)
  - Orion-14B is an open-source multilingual large language model series by OrionStarAI, trained on 2.5T tokens of diverse languages including Chinese, English, and Japanese, with accompanying demos, benchmarks, and documentation.
  - Downloads: 421
- [ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.2-GGUF)
  - This repository provides the GGUF version of ascktgcc/Mistral-nemo-ja-rp-v0.2, a Japanese language model‚Äîrefer to the original model for details.
  - Downloads: 399
- [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, focused on improved Japanese language performance with strong English capabilities through increased Japanese pre-training and tokenizer efficiency.
  - Downloads: 347
- [MCZK/gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/gemma-2-2b-jpn-it-GGUF)
  - googleÊßò„ÅÆ google/gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 333
- [HODACHI/Llama-3-EZO-VLM-1](https://huggingface.co/HODACHI/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 289
- [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf)
  - This repository provides GGUF-converted versions of rinna's japanese-gpt-neox-3.6b-instruction-ppo model, compatible with llama.cpp, but potentially subject to future incompatibility with updates to the main llama.cpp repository.
  - Downloads: 277
- [llm-jp/llm-jp-clip-vit-large-patch14](https://huggingface.co/llm-jp/llm-jp-clip-vit-large-patch14)
  - This repository details llm-jp-clip-vit-large-patch14, a 467M parameter Japanese CLIP model trained on a translated ReLAION dataset, enabling zero-shot image classification via `open_clip_torch`.
  - Downloads: 277
- [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator)
  - This repository provides a Japanese-to-Korean translation model leveraging BERT for Japanese encoding and KOGPT2 for Korean decoding, with a Hugging Face Spaces demo available for testing.
  - Downloads: 277
- [HODACHI/Llama-3-EZO-8b-Common-it](https://huggingface.co/HODACHI/Llama-3-EZO-8b-Common-it)
  - [Llama-3-EZO model card]
  - Downloads: 250
- [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model trained by OrionStarAI on a 2.5T corpus, offering demos, benchmarks, and technical documentation.
  - Downloads: 222
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-GGUF)
  - Tanuki-8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 215
- [Hemlok/QuinceMix](https://huggingface.co/Hemlok/QuinceMix)
  - QuinceMix ‚ÄúDefacta‚Äù is a merged Stable Diffusion model excelling in backgrounds and effects, optimized for use with DDIM/DPM++ SDE Karras samplers, and recommended settings like 20-30 steps, CFG 5-8, and denoising strength of 0.4-0.7 with EasyNegative.
  - Downloads: 194
- [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k)
  - This repository offers a 3.8 billion parameter English-Japanese bilingual GPT-NeoX model with an extended 8192 context length, built upon EleutherAI‚Äôs GPT-NeoX and fine-tuned with RoPE positional interpolation.
  - Downloads: 158
- [natsusakiyomi/SakuraMix](https://huggingface.co/natsusakiyomi/SakuraMix)
  - This repository provides a VAE-integrated model, SakuraMixSeries, prioritizing both background and character quality, licensed under a modified CreativeML OpenRAIL-M license permitting commercial use and modification with certain conditions.
  - Downloads: 146
- [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix)
  - This repository provides a checkpoint model built upon CreativeML Open RAIL-M, adding ‚Äúsazyou_roukaku‚Äù as an additional author while retaining the original license and its usage restrictions, particularly prohibiting criminal or specialized (e.g., medical) applications as outlined in restriction A.
  - Downloads: 137
- [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en)
  - This project provides a Japanese-to-English machine translation model fine-tuned for Weiss Schwarz (WS) trading card game text, available as a Gradio app and for local execution.
  - Downloads: 129
- [stockmark/stockmark-100b](https://huggingface.co/stockmark/stockmark-100b)
  - stockmark/stockmark-100b Stockmark-100b is a 100 billion parameter LLM pretrained from scratch based on Japanese and English corpus of about 910 billion tokens.
  - Downloads: 125
- [shisa-ai/shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 112
- [kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1](https://huggingface.co/kenyano/Llama3-ELAINE-medLLM-instruct-8B_v0.1)
  - ELAINE-medLLM is a trilingual (English, Japanese, Chinese) biomedical large language model built on Llama-3-8B, trained with continued pre-training and supervised fine-tuning for enhanced domain expertise and multilingual capabilities.
  - Downloads: 109
- [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)
  - Orion-14B is an open-source, multilingual (English, Chinese, Japanese, Korean) large language model series trained by OrionStarAI on a 2.5T corpus, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 108
- [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)
  - Orion-14B is an open-source multilingual large language model series created by OrionStarAI, trained on 2.5T tokens of diverse languages including Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 105
- [shisa-ai/shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 103
- [beomi/gemma-mling-7b](https://huggingface.co/beomi/gemma-mling-7b)
  - Gemma-Mling is a 7B multilingual language model continually pretrained on Korean, English, Chinese, Japanese, and a 500-language corpus, based on Google's Gemma-7B.
  - Downloads: 91
- [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2)
  - This repository provides a work-in-progress Japanese-to-English translation model built on tinyllama, optimized for 500-1000 token inputs and requiring deterministic inference settings (do_sample=False or temperature=0) with a specific prompt format.
  - Downloads: 90
- [mav23/Sakura-13B-Galgame-GGUF](https://huggingface.co/mav23/Sakura-13B-Galgame-GGUF)
  - SakuraLLM Sakura: SFT And RLHF models using Knowledge of Universal Character and Relationship Attributes for Japanese to Chinese Translation in Light Novel &amp; Galgame Domain.
  - Downloads: 88
- [shisa-ai/shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 83
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ)
  - Tanuki-8x8B-dpo-v1.0-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 82
- [shisa-ai/shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b)
  - Shisa V2 is a family of improved, bilingual (Japanese/English) chat models by Shisa.AI, focusing on enhanced Japanese language performance with robust English capabilities through increased Japanese pre-training and tokenizer efficiency.
  - Downloads: 77
- [Aratako/Ninja-v1-RP-WIP](https://huggingface.co/Aratako/Ninja-v1-RP-WIP)
  - Ninja-v1-RP-WIP Ê¶ÇË¶Å Local-Novel-LLM-project/Ninja-v1-NSFW„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 71
- [llm-jp/llm-jp-3-172b](https://huggingface.co/llm-jp/llm-jp-3-172b)
  - „ÄåLLM-jp-3 172B„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 66
- [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved Japanese language performance.
  - Downloads: 64
- [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0)
  - This repository provides a Japanese-adapted refiner model for Stable Diffusion XL 1.0, fine-tuning the OpenCLIP-ViT/G or CLIP-ViT/L text encoder with Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer to enable Japanese text input.
  - Downloads: 63
- [MCZK/Ninja-V2-7B-GGUF](https://huggingface.co/MCZK/Ninja-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 63
- [llm-jp/llm-jp-3-172b-beta2](https://huggingface.co/llm-jp/llm-jp-3-172b-beta2)
  - „ÄåLLM-jp-3 172B beta2„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta2„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 59
- [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)
  - Orion-14B is an open-source, multilingual large language model series developed by OrionStarAI, trained on 2.5T tokens of diverse text including Chinese, English, and Japanese, with accompanying demos, benchmarks, and technical documentation.
  - Downloads: 55
- [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
  - Model llm-jp/llm-jp-3-3.7b-instruct„ÇíCoT„Éá„Éº„Çø„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çã„Åì„Å®„Åß‰ΩúÊàê„Åó„Åüreasoning„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 54
- [shisa-ai/shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)
  - Shisa V2 is a family of bilingual (Japanese/English) chat models by Shisa.AI, improved for superior Japanese language performance with increased pre-training and tokenizer efficiency.
  - Downloads: 48
- [MCZK/Ninja-V3-GGUF](https://huggingface.co/MCZK/Ninja-V3-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Ninja-V3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 45
- [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b)
  - Leia-Swallow-7B is a Japanese-English bilingual LLM based on LLaMA 2, trained with the LEIA technique to improve cross-lingual transfer and performance on Japanese question-answering tasks.
  - Downloads: 43
- [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b)
  - Leia-Swallow-13B is a Japanese-English bilingual LLM built on LLaMA 2, enhanced with the LEIA technique to improve cross-lingual transfer and achieve strong performance on Japanese question answering benchmarks.
  - Downloads: 39
- [lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA](https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA)
  - Overview This model is based on rinna's [rinna/llama-3-youko-8b], fine-tuned using LoRA on a small number of parallel sentences from English to Japanese.
  - Downloads: 39
- [llm-jp/llm-jp-clip-vit-base-patch16](https://huggingface.co/llm-jp/llm-jp-clip-vit-base-patch16)
  - This repository details llm-jp-clip-vit-base-patch16, a 248M parameter Japanese CLIP model trained on a translated subset of the ReLAION-5B dataset, and provides instructions for its use with `open_clip_torch` for zero-shot image classification.
  - Downloads: 37
- [llm-jp/llm-jp-3-172b-beta1-instruct](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1-instruct)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 36
- [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF)
  - Orion-14B is an open-source multilingual large language model series, trained by OrionStarAI on a 2.5T multilingual corpus‚Äîincluding Chinese, English, and Japanese‚Äîwith accompanying demos, benchmarks, and technical documentation.
  - Downloads: 34
- [KoichiYasuoka/llm-jp-1.3b-upos](https://huggingface.co/KoichiYasuoka/llm-jp-1.3b-upos)
  - llm-jp-1.3b-upos Model Description
  - Downloads: 34
- [lightblue/Karasu-DPO-7B](https://huggingface.co/lightblue/Karasu-DPO-7B)
  - Karasu-DPO-7B is a Japanese DPO-trained version of Qwen2.5-7B-Instruct, excelling in multilingual chat and recommended for general conversational AI applications.
  - Downloads: 32
- [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix)
  - Yaki-Dofu-Mix Ê¶ÇË¶Å / Overview Yaki-Dofu-Mix„ÅØ„ÄÅ„Ç¢„Éã„É°È¢®„ÅÆÁîªÈ¢®„Å´ÁâπÂåñ„Åó„Åü„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 32
- [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja)
  - This repository provides a fine-tuned Japanese-to-English translation model based on Helsinki-NLP/opus-mt-ja-en, trained on the bsd_ja_en dataset.
  - Downloads: 31
- [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin)
  - Orion-14B is an open-source multilingual large language model series developed by OrionStarAI, trained on a 2.5T multilingual corpus and offering demos, benchmarks, and downloads for Chinese, English, Japanese, and Korean.
  - Downloads: 28
- [upskyy/gte-base-korean](https://huggingface.co/upskyy/gte-base-korean)
  - upskyy/gte-korean-base This model is korsts and kornli finetuning model from Alibaba-NLP/gte-multilingual-base.
  - Downloads: 27
- [tohoku-nlp/stable-diffusion-xl-jp-base-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-base-1.0)
  - This repository releases a Japanese-input-compatible version of the SDXL 1.0 base model, achieved by fine-tuning the OpenCLIP/CLIP-ViT text encoders with Japanese-English parallel data and the line-corporation/japanese-large-lm-3.6b tokenizer.
  - Downloads: 26
- [llm-jp/llm-jp-3-172b-beta1](https://huggingface.co/llm-jp/llm-jp-3-172b-beta1)
  - „ÄåLLM-jp-3 172B beta1„ÄçÂà©Áî®Ë¶èÁ¥Ñ „Åì„ÅÆÂà©Áî®Ë¶èÁ¥ÑÔºà‰ª•‰∏ã„ÄåÊú¨Ë¶èÁ¥Ñ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅØ„ÄÅÂ§ßÂ≠¶ÂÖ±ÂêåÂà©Áî®Ê©üÈñ¢Ê≥ï‰∫∫ ÊÉÖÂ†±„Éª„Ç∑„Çπ„ÉÜ„É†Á†îÁ©∂Ê©üÊßã ÂõΩÁ´ãÊÉÖÂ†±Â≠¶Á†îÁ©∂ÊâÄÔºà‰ª•‰∏ã„ÄåÊèê‰æõËÄÖ„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„Å´„Çà„ÇãÈñãÁô∫„ÅÆÊàêÊûúÁâ©„Å®„Åó„Å¶ÂÖ¨Èñã„Åô„ÇãÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÄåLLM-jp-3 172B beta1„ÄçÔºà‰ª•‰∏ã„ÄåÊú¨„Éó„É≠„Ç∞„É©„É†„Äç„Å®„ÅÑ„ÅÑ„Åæ„ÅôÔºâ„ÅÆÂà©Áî®„Å´Èñ¢„Åô„ÇãÊù°‰ª∂„ÇíÂÆö„ÇÅ„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 24
- [team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8B-dpo-v1.0-4k-AWQ)
  - Tanuki-8B-dpo-v1.0-4k-AWQ Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8B-dpo-v1.0-4k„ÅÆAWQ 4bitÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [yasyune/bert_vits2_2.1_jvnv](https://huggingface.co/yasyune/bert_vits2_2.1_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 20
- [nitky/Superswallow-70b-v0.1](https://huggingface.co/nitky/Superswallow-70b-v0.1)
  - Superswallow-70b-v0.1 is a 70B language model currently underperforming compared to its predecessor, Swallow, potentially due to identified bugs related to repetition penalty and temperature settings‚Äìthough performance improves with parameter testing.
  - Downloads: 19
- [hyperonym/barba](https://huggingface.co/hyperonym/barba)
  - Barba is a multilingual NLI/text classification model based on XLM-RoBERTa, served via TensorFlow, and trained on GLUE, CLUE, JGLUE, KLUE, and private datasets.
  - Downloads: 19
- [AXCXEPT/Llama-3-EZO-VLM-1](https://huggingface.co/AXCXEPT/Llama-3-EZO-VLM-1)
  - Llama-3-EZO-VLM-1 Based on SakanaAI/Llama-3-EvoVLM-JP-v2, it has been enhanced for Japanese usage through additional pre-training and instruction tuning.
  - Downloads: 19
- [GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2](https://huggingface.co/GralchemOz/Qwen1.5-14B-vntl-jp2zh-4.5bpw-h6-exl2)
  - This repository provides a quantized (.exl2) merged language model (Qwen-14B-vntl & Qwen1.5-14B-Chat) specifically for translating Japanese game scripts into fluent Chinese.
  - Downloads: 18
- [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1)
  - This repository hosts a Japanese language model built by merging and applying Mixture-of-Experts (MoE) to the elyza/ELYZA-japanese-Llama-2-7b-fast and instruction-tuned versions, inheriting the Llama 2 license.
  - Downloads: 18
- [sosoai/Orion-14B-Chat-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-safetensors)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T corpus, supporting Chinese, English, Japanese, and Korean with demos, benchmarks, and technical documentation available.
  - Downloads: 17
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1)
  - This repository hosts a Japanese language model, ELYZA-japanese-Llama-2-MoE-2x13B, created by merging and performing Mixture of Experts (MoE) on the elyza/ELYZA-japanese-Llama-2-13b and elyza/ELYZA-japanese-Llama-2-13b-instruct models, licensed under Llama 2.
  - Downloads: 17
- [Aratako/Swallow-MoE-2x13B-v0.1](https://huggingface.co/Aratako/Swallow-MoE-2x13B-v0.1)
  - This repository hosts Swallow-MoE-2x13B-v0.1, a Mixture-of-Experts model created by merging and MoE-ifying the tokkyotech-llm/Swallow-13b-instruct-hf and nitky/Superswallow-13b-v0.2 Llama-2 based Japanese language models, with considerations for combined licensing (Llama2 & AI2 ImpACT).
  - Downloads: 16
- [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1)
  - This repository provides a Japanese Mixture-of-Experts (MoE) language model, ELYZA-japanese-Llama-2-MoE-2x7B, created by merging and instruction-tuning the ELYZA/ELYZA-japanese-Llama-2-7b and ELYZA/ELYZA-japanese-Llama-2-7b-instruct models under the Llama 2 Community License.
  - Downloads: 16
- [owner203/japanese-llama-2-13b](https://huggingface.co/owner203/japanese-llama-2-13b)
  - This repository provides base and instruction-tuned language models‚Äîincluding full and LoRA versions‚Äîfor Japanese, built upon LLaMA 2 and Alpaca.
  - Downloads: 16
- [sbtom/karakuri-midroze-mg](https://huggingface.co/sbtom/karakuri-midroze-mg)
  - karakuri-midrose-mg „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 16
- [Language-Media-Lab/byt5-small-ain-jpn-mt](https://huggingface.co/Language-Media-Lab/byt5-small-ain-jpn-mt)
  - ByT5-small-ain-jpn-mt is a pre-trained and fine-tuned machine translation model for Ainu-to-Japanese translation, built using Google's ByT5-small architecture and web-crawled bilingual data.
  - Downloads: 15
- [doc2query/msmarco-japanese-mt5-base-v1](https://huggingface.co/doc2query/msmarco-japanese-mt5-base-v1)
  - This repository provides a multilingual T5-based (mT5) doc2query model for enhancing document retrieval by generating synthetic queries that expand search coverage and improve relevance through synonym inclusion and term re-weighting.
  - Downloads: 15
- [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA)
  - This repository provides a T5-based machine translation model, fine-tuned on the friendly_JA corpus, which simplifies Japanese by prioritizing Latin/English-derived *katakana* over traditional Sino-Japanese vocabulary.
  - Downloads: 15
- [LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built upon Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for strong performance in both languages.
  - Downloads: 14
- [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf)
  - This repository provides the Japanese-Alpaca-2-13B language model in GGUF format, sourced from Hugging Face.
  - Downloads: 13
- [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base)
  - This repository hosts a fine-tuned version of MosaicML‚Äôs MPT-7B language model, evaluated on a 100-question dataset and requiring `trust_remote_code=True` due to its custom architecture and optimized training features like FlashAttention.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B+ Japanese tokens for strong performance in both languages.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for superior Japanese language performance.
  - Downloads: 13
- [LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2)
  - Shisa 7B is a Japanese/English bilingual chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B Japanese tokens for optimized Japanese language performance.
  - Downloads: 13
- [hotchpotch/ruri-pt-base-retromae](https://huggingface.co/hotchpotch/ruri-pt-base-retromae)
  - cl-nagoya/ruri-pt-base „Çí RetroMAE „Åß‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [werty1248/Mistral-Nemo-NT-Ko-12B-sft](https://huggingface.co/werty1248/Mistral-Nemo-NT-Ko-12B-sft)
  - Mistral-Nemo-NT-Ko-12B-sft Description Mistral-Nemo-NT-Ko-12B-sft is an instruction-tuned version of mistralai/Mistral-Nemo-Base-2407, fine-tuned across four languages: English, Korean, Chinese, and Japanese.
  - Downloads: 13
- [AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2](https://huggingface.co/AbeShinzo0708/Japanese-Starling-ChatV-7B-exl2)
  - Japanese-Starling-ChatV-7B „Åì„ÅÆ„É¢„Éá„É´„ÅØ"chatntq-ja-7b-v1.0"„Çí„Éô„Éº„Çπ„Å´„Åó„Åü7B„Éë„É©„É°„Éº„Çø„ÅÆÊó•Êú¨Ë™û„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [2121-8/TinySlime-1.1B-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-v1.0)
  - TinySlime-1.1B-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [SpassMedAI/MLMedLlama3](https://huggingface.co/SpassMedAI/MLMedLlama3)
  - Model Card for Model ID MMedBench and KoreanMedMCQA Instruction Fine-Tuned Multilingual Llama3 8B 4Bit quantized model using QLoRA.
  - Downloads: 13
- [LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-4.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 13
- [staka/takomt](https://huggingface.co/staka/takomt)
  - TakoMT is a Marian-NMT translation model leveraging transformers and sentencepiece to translate from German, English, Spanish, French, Italian, Russian, and Ukrainian to Japanese, trained with ParaCrawl data.
  - Downloads: 12
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-neuron)
  - This repository provides a compiled version of the Watashiha-Llama-2-13B-Ogiri-sft model optimized for AWS Inf2 instances using Neuron, requiring an Inf2.xlarge instance with at least 256GB storage and the Deep Learning AMI Neuron PyTorch 1.13.
  - Downloads: 12
- [owner203/japanese-alpaca-2-13b](https://huggingface.co/owner203/japanese-alpaca-2-13b)
  - This repository provides full and LoRA models for Japanese-Alpaca-2-13B, an instruction-following model built on Japanese-LLaMA-2-13B.
  - Downloads: 12
- [LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2](https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2)
  - Shisa 7B is a bilingual Japanese/English chat model built on Mistral 7B, enhanced with a custom tokenizer and 8B additional Japanese tokens for improved performance in both languages.
  - Downloads: 12
- [sbtom/karakuri-MS-01](https://huggingface.co/sbtom/karakuri-MS-01)
  - karakuri-MS-01 „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 12
- [keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF](https://huggingface.co/keitokei1994/Llama-3.1-70B-EZO-1.1-it-GGUF)
  - Llama-3.1-70B-EZO-1.1-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãHODACHI/Llama-3.1-70B-EZO-1.1-it„ÅÆggufÁâà„Åß„Åô„ÄÇ
  - Downloads: 12
- [Helsinki-NLP/opus-mt-ja-ms](https://huggingface.co/Helsinki-NLP/opus-mt-ja-ms)
  - This repository provides a Japanese-to-Malay transformer model (transformer-align) trained on OPUS data, utilizing SentencePiece preprocessing and requiring a language ID token for translation.
  - Downloads: 11
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft)
  - This repository details a Japanese large language model, built upon Llama2-13b and fine-tuned for *okashi* (comedic responses), utilizing a 45,046-token vocabulary and trained with AWS Trainium instances on a 65 billion token corpus.
  - Downloads: 11
- [owner203/japanese-llama-2-7b](https://huggingface.co/owner203/japanese-llama-2-7b)
  - Japanese-LLaMA-2-7B provides both a base foundational language model and a LoRA-adapted version for Japanese language processing.
  - Downloads: 11
- [sosoai/Orion-14B-Chat-RAG-safetensors](https://huggingface.co/sosoai/Orion-14B-Chat-RAG-safetensors)
  - Orion-14B is an open-source multilingual large language model series trained by OrionStarAI on a 2.5T multilingual corpus, offering English, Chinese, Japanese, and Korean support with demos, benchmarks, and technical documentation.
  - Downloads: 11
- [yasyune/bert_vits2_2.2_jvnv](https://huggingface.co/yasyune/bert_vits2_2.2_jvnv)
  - This repository provides a Japanese BERT-VITS2 model trained on the jvnv corpus for speech synthesis.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-6.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
- [LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-3.0bpw-h6-exl2)
  - SambaLingo-Japanese-Chat SambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
  - Downloads: 11
### Text Generation
- [MCZK/EZO-gemma-2-2b-jpn-it-GGUF](https://huggingface.co/MCZK/EZO-gemma-2-2b-jpn-it-GGUF)
  - AXCXEPTÊßò„ÅÆ AXCXEPT/EZO-gemma-2-2b-jpn-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13,791
- [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) model pretrained on a 100GB corpus of Japanese text‚Äîincluding Wikipedia, OSCAR, and CC-100‚Äîrequiring fine-tuning for specific tasks and acknowledging potential biases in its outputs.
  - Downloads: 13,460
- [mmnga/Llama-3.3-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.3-70B-Instruct-gguf)
  - Llama-3.3-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3.3-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 4,835
- [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese)
  - This repository provides a T5 model pretrained on a balanced 500GB English-Japanese corpus, requiring fine-tuning for specific tasks and acknowledging potential biases inherent in large language models.
  - Downloads: 3,166
- [mmnga/Llama-3.1-8B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-8B-Instruct-gguf)
  - Llama-3.1-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 2,483
- [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0)
  - Kotoba-Whisper Kotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,472
- [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix)
  - This repository hosts the v2 release of chilled_remix and reversemix models, licensed under CreativeML Open RAIL-M with additional copyright by sazyou_roukaku, and clarifies the creator disclaims responsibility for generated content beyond license restrictions.
  - Downloads: 2,188
- [sbintuitions/sarashina2.2-0.5b-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5b-instruct-v0.1)
  - SB Intuitions‚Äô sarashina2.2-0.5B-instruct-v0.1 is a Japanese autoregressive language model evaluated on Japanese & English benchmarks, demonstrating competitive performance against other similarly sized models.
  - Downloads: 1,992
- [Kotajiro/yayoi_mix](https://huggingface.co/Kotajiro/yayoi_mix)
  - This repository provides a Japanese-style AI model licensed under CreativeML Open RAIL-M, prohibiting the generation of violent, sexually explicit, or exploitative content, especially involving minors, and requiring consent for depictions of real people.
  - Downloads: 1,484
- [mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-7b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-7b-Japanese-v0.1 large language model, built with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 1,080
- [2121-8/japanese-parler-tts-mini](https://huggingface.co/2121-8/japanese-parler-tts-mini)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality voice generation with a custom, incompatible tokenizer.
  - Downloads: 1,050
- [mmnga/Llama-3-ELYZA-JP-8B-gguf](https://huggingface.co/mmnga/Llama-3-ELYZA-JP-8B-gguf)
  - Llama-3-ELYZA-JP-8B-gguf elyza„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-ELYZA-JP-8B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,008
- [mmnga/aya-23-35B-gguf](https://huggingface.co/mmnga/aya-23-35B-gguf)
  - aya-23-35B-gguf CohereForAI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãaya-23-35B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 993
- [mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-70B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-70B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-70B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 875
- [mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf](https://huggingface.co/mmnga/Llama-3-Swallow-8B-Instruct-v0.1-gguf)
  - Llama-3-Swallow-8B-Instruct-v0.1-gguf tokyotech-llm„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama-3-Swallow-8B-Instruct-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 848
- [mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf](https://huggingface.co/mmnga/qwen2.5-bakeneko-32b-instruct-v2-gguf)
  - This repository provides a GGUF-formatted version of the Qwen2.5-bakeneko-32b-instruct-v2 language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and optimized for use with llama.cpp.
  - Downloads: 841
- [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr)
  - This repository provides a Japanese Automatic Speech Recognition (ASR) model, fine-tuned from rinna/japanese-hubert-large on reazonspeech and common_voice_11_0 datasets, specifically for Hiragana prediction.
  - Downloads: 825
- [mmnga/gemma-2-2b-it-gguf](https://huggingface.co/mmnga/gemma-2-2b-it-gguf)
  - gemma-2-2b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãgemma-2-2b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 788
- [ascktgcc/Mistral-nemo-ja-rp-v0.1](https://huggingface.co/ascktgcc/Mistral-nemo-ja-rp-v0.1)
  - Mistral-nemo„ÇíEPRÁî®ÈÄîÂêë„Åë„Å´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô ‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂçäÂàÜ„Åª„Å©„ÅåÊó•Êú¨Ë™û„Å™„ÅÆ„Åßmagnum„ÅÆ„Çà„ÅÜ„Å™„É¢„Éá„É´„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„ÅØÂº∑„ÅÑ„ÅØ„ÅöÔºü
  - Downloads: 727
- [mmnga/Phi-3-mini-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-mini-128k-instruct-gguf)
  - Phi-3-mini-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 725
- [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1)
  - This repository provides a Japanese T5 (Text-to-Text Transfer Transformer) v1.1 model pretrained on a 100GB corpus of Japanese text, requiring fine-tuning for specific tasks and acknowledging potential biases present in the training data.
  - Downloads: 717
- [Aratako/gemma-3-4b-it-RP-v0.1-GGUF](https://huggingface.co/Aratako/gemma-3-4b-it-RP-v0.1-GGUF)
  - This repository provides GGUF quantized versions of the Aratako/gemma-3-4b-it-RP-v0.1 model, inheriting the Gemma Terms of Use and Prohibited Use Policy.
  - Downloads: 709
- [mmnga/cogito-v1-preview-qwen-32B-gguf](https://huggingface.co/mmnga/cogito-v1-preview-qwen-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the Cogito-v1-preview-Qwen-32B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is intended for use with llama.cpp.
  - Downloads: 701
- [mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf](https://huggingface.co/mmnga/ryota39-Phi-3-mini-4k-instruct-dpo-gguf)
  - ryota39-Phi-3-mini-4k-instruct-dpo-gguf ryota39„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-mini-4k-instruct-dpo„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 664
- [dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K)
  - This GGUF model quantizes Qwen/Qwen2.5-3B-Instruct with a Japanese-weighted importance matrix (iMatrix) to enable accurate summarization of extremely long texts exceeding 32K tokens.
  - Downloads: 655
- [mmnga/Meta-Llama-3-8B-Instruct-gguf](https://huggingface.co/mmnga/Meta-Llama-3-8B-Instruct-gguf)
  - Meta-Llama-3-8B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3-8B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 602
- [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese)
  - This repository provides a fine-tuned mt5-small model for Japanese summarization, specifically trained on BBC news (XL-Sum Japanese) to generate summaries from news articles using the headline as the target.
  - Downloads: 559
- [mmnga/rinna-llama-3-youko-70b-instruct-gguf](https://huggingface.co/mmnga/rinna-llama-3-youko-70b-instruct-gguf)
  - rinna-llama-3-youko-70b-instruct-gguf rinna„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãllama-3-youko-70b-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 546
- [kotoba-tech/kotoba-whisper-bilingual-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0)
  - Kotoba-Whisper-Bilingual provides faster, distilled Whisper models for Japanese and English speech recognition and translation, built upon OpenAI's Whisper large-v3.
  - Downloads: 527
- [mmnga/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/mmnga/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the sarashina2.2-3b-instruct-v0.1 Japanese language model, trained with imatrix data and compatible with llama.cpp.
  - Downloads: 465
- [team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF](https://huggingface.co/team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GGUF)
  - Tanuki-8x8B-dpo-v1.0-GGUF Ê¶ÇË¶Å GENIAC ÊùæÂ∞æÁ†î LLMÈñãÁô∫„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅßÈñãÁô∫„Åï„Çå„ÅüLLM„Åß„ÅÇ„Çãweblab-GENIAC/Tanuki-8x8B-dpo-v1.0„ÅÆGGUFÈáèÂ≠êÂåñ„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 432
- [mmnga/datagemma-rag-27b-it-gguf](https://huggingface.co/mmnga/datagemma-rag-27b-it-gguf)
  - datagemma-rag-27b-it-gguf google„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãdatagemma-rag-27b-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 424
- [neody/sarashina2.2-3b-instruct-v0.1-gguf](https://huggingface.co/neody/sarashina2.2-3b-instruct-v0.1-gguf)
  - This repository provides a Japanese/English instruction-tuning dataset (Sarashina2.2) built from fineweb-edu and fineweb-2, aiming for higher accuracy than existing imatrix datasets and specifically created with Q5_K_M quantization for use with Ollama.
  - Downloads: 404
- [mmnga/Qwen3-30B-A3B-gguf](https://huggingface.co/mmnga/Qwen3-30B-A3B-gguf)
  - This repository provides a GGUF-formatted conversion of the Qwen3-30B-A3B language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset, and is intended for use with llama.cpp.
  - Downloads: 396
- [mmnga/Mistral-Nemo-Instruct-2407-gguf](https://huggingface.co/mmnga/Mistral-Nemo-Instruct-2407-gguf)
  - Mistral-Nemo-Instruct-2407-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMistral-Nemo-Instruct-2407„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 318
- [MCZK/gemma-2-baku-2b-it-GGUF](https://huggingface.co/MCZK/gemma-2-baku-2b-it-GGUF)
  - rinnaÊßò„ÅÆ rinna/gemma-2-baku-2b-it „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 296
- [lmg-anon/vntl-llama3-8b-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-gguf)
  - This repository provides GGUF quantized versions of a VNTL LLaMA 3 8B QLoRA merge, featuring a new chat mode optimized for Japanese grammar and including translation prompt examples.
  - Downloads: 272
- [mmnga/QwQ-32B-Preview-gguf](https://huggingface.co/mmnga/QwQ-32B-Preview-gguf)
  - This repository provides GGUF-formatted versions of the QwQ-32B-Preview language model, trained with the TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 244
- [mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf](https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf)
  - lightblue-Karasu-Mixtral-8x22B-v0.1-gguf lightblue„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãKarasu-Mixtral-8x22B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 211
- [Atotti/RakutenAI-2.0-mini-instruct-gguf](https://huggingface.co/Atotti/RakutenAI-2.0-mini-instruct-gguf)
  - This repository provides GGUF quantized versions of the RakutenAI-2.0-mini-instruct model, enabling its use with tools like llama.cpp and text-generation-webui.
  - Downloads: 202
- [mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf](https://huggingface.co/mmnga/AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf)
  - AXCXEPT-EZO-Qwen2.5-72B-Instruct-gguf AXCXEPT„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Qwen2.5-72B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 188
- [2121-8/japanese-parler-tts-mini-bate](https://huggingface.co/2121-8/japanese-parler-tts-mini-bate)
  - This repository provides a retrained, lightweight Japanese text-to-speech model based on Parler-TTS Mini, offering high-quality voice synthesis with a custom, incompatible tokenizer and currently in beta.
  - Downloads: 182
- [mmnga/Llama-3.1-70B-Instruct-gguf](https://huggingface.co/mmnga/Llama-3.1-70B-Instruct-gguf)
  - Llama-3.1-70B-Instruct-gguf meta-llama„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãMeta-Llama-3.1-70B-Instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 170
- [nlp-waseda/comet-v2-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-v2-gpt2-small-japanese)
  - COMET-GPT2 v2 is a finetuned, large-version GPT-2 model based on the ATOMIC dataset, enabling causal language modeling for reproducible text generation.
  - Downloads: 141
- [Kendamarron/Tokara-0.5B-Chat-dolly-jimba](https://huggingface.co/Kendamarron/Tokara-0.5B-Chat-dolly-jimba)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„ÅüTokara-0.5B-v0.1„ÇíÊó•Êú¨Ë™ûinstruction„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 139
- [Kendamarron/Tokara-0.5B-v0.1](https://huggingface.co/Kendamarron/Tokara-0.5B-v0.1)
  - „É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ Qwen/Qwen1.5-0.5B„ÇíÊó•Ëã±„Éá„Éº„Çø5B„Éà„Éº„ÇØ„É≥„ÅßÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 136
- [mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf](https://huggingface.co/mmnga/HODACHI-EZO-Humanities-9B-gemma-2-it-gguf)
  - HODACHI-EZO-Humanities-9B-gemma-2-it-gguf HODACHI„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãEZO-Humanities-9B-gemma-2-it„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 133
- [mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf](https://huggingface.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-Qwen2.5-32b-Japanese-v0.1 large language model, utilizing data from the TFMC/imatrix-dataset-for-japanese-llm and designed for use with llama.cpp.
  - Downloads: 132
- [EQUES/MedLLama3-JP-v2](https://huggingface.co/EQUES/MedLLama3-JP-v2)
  - Llama3„Éô„Éº„Çπ„ÅÆÊó•Êú¨Ë™ûÂåªÁôÇLLM MedLlama3-JP „Åì„ÅÆ„É¢„Éá„É´„ÅØLlama3„ÅÆÁ∂ôÁ∂öÂ≠¶Áøí„Å´„Çà„Çä‰ΩúÊàê„Åï„Çå„ÅüÔºîÁ®ÆÈ°û„ÅÆLLM„Åã„ÇâÊàê„Çã„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 122
- [lmg-anon/vntl-gemma2-27b-gguf](https://huggingface.co/lmg-anon/vntl-gemma2-27b-gguf)
  - This repository provides GGUF quantized versions of the VNTL Gemma 2 27B model, featuring a new chat mode primarily for Japanese grammar and including translation prompt examples.
  - Downloads: 112
- [dahara1/translate-task-thinking-test](https://huggingface.co/dahara1/translate-task-thinking-test)
  - Translation Task Thinking Test Model Model Description
  - Downloads: 106
- [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF)
  - This repository provides a quantized GGUF version of the Aratako/c4ai-command-r-v01-japanese-instruct model for Japanese language instruction-following.
  - Downloads: 100
- [dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K](https://huggingface.co/dahara1/Qwen2.5-7B-Instruct-gguf-japanese-imatrix-128K)
  - please see dahara1/Qwen2.5-3B-Instruct-gguf-japanese-imatrix-128K
  - Downloads: 94
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2)
  - Llama-3-8B-Instruct-JP-nk2t-v0.2 Model Details: Built with Meta Llama 3
  - Downloads: 91
- [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese)
  - COMET-T5 is a Japanese language model finetuned on the ATOMIC dataset for text-to-text generation, offering a pipeline for predicting subsequent events.
  - Downloads: 86
- [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese)
  - This repository provides a Japanese ByT5 model‚Äîa tokenizer-free Text-to-Text Transfer Transformer‚Äîpretrained on a 100GB corpus including Wikipedia, OSCAR, and CC-100, requiring fine-tuning for specific tasks and acknowledging potential biases in generated text.
  - Downloads: 86
- [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr)
  - This repository provides a Japanese speech recognition model fine-tuned on the uniTKU dataset, predicting only Hiragana and achieving low Word Error Rates (WER) as detailed in the training metrics.
  - Downloads: 79
- [MCZK/Vecteus-V2-7B-GGUF](https://huggingface.co/MCZK/Vecteus-V2-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Vecteus-V2-7B „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 68
- [nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3](https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.3)
  - Llama-3-8B-Instruct-JP-nk2t-v0.3 Model Details: Built with Meta Llama 3 llama-3-8b„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂öÂ≠¶Áøí„É¢„Éá„É´„Å´ChatVector„ÇíÈÅ©Áî®„Åó„ÄÅ„Åï„Çâ„Å´QLora„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 67
- [mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF](https://huggingface.co/mradermacher/Mistral-Nemo-Japanese-Instruct-2408-GGUF)
  - This repository offers statically quantized versions of the Mistral-Nemo-Japanese-Instruct-2408 model in GGUF format, with various quantizations (including IQ) available for download and usage as detailed in TheBloke's GGUF documentation.
  - Downloads: 56
- [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf)
  - Japanese-TextGen-Kage-v0.1-2x7B Kage is "ÂΩ±" in Japanese or "Shadow" in English.
  - Downloads: 55
- [mmnga/sarashina2.1-1b-sft-gguf](https://huggingface.co/mmnga/sarashina2.1-1b-sft-gguf)
  - This repository provides a GGUF-formatted version of the Sarashina 2.1-1B-SFT Japanese language model, trained with data from TFMC/imatrix-dataset-for-japanese-llm and usable with llama.cpp.
  - Downloads: 47
- [AXCXEPT/EZO2.5-gemma-3-12b-it-Preview](https://huggingface.co/AXCXEPT/EZO2.5-gemma-3-12b-it-Preview)
  - This repository details EZO2.5-gemma-3-12b-it, a Japanese language model improving upon Gemma‚Äôs base performance on MT Bench and Elyza Tasks100 through a low-cost training method integrating GRPO/PPO concepts‚Äîcurrently research-in-progress‚Äîusing a 3,000-dataset and H200-powered training.
  - Downloads: 43
- [2121-8/TinySlime-1.1B-Chat-v1.0](https://huggingface.co/2121-8/TinySlime-1.1B-Chat-v1.0)
  - TinySlime-1.1B-Chat-v1.0 TinySlime „ÅØÊó•Êú¨Ë™û„Å´ÁâπÂåñ„Åó„ÅüÂ∞èË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 40
- [hakutaku/qwen2.5-ja-zh](https://huggingface.co/hakutaku/qwen2.5-ja-zh)
  - Qwen2.5-ja-zh
  - Downloads: 38
- [llm-jp/llm-jp-3-172b-alpha1](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha1)
  - llm-jp-3-172b-alpha1
  - Downloads: 37
- [ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft](https://huggingface.co/ToPo-ToPo/line-japanese-large-lm-1.7b-kunishou-databricks-dolly-15k-ja-full-instruction-sft)
  - This repository provides a fully instruction-tuned 1.7 billion parameter Japanese language model built upon line-corporation/japanese-large-lm-1.7b.
  - Downloads: 36
- [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor)
  - This repository provides a fine-tuned, Japanese speech-to-text (ASR) model based on distil-whisper/distil-large-v2, specifically optimized for transcribing visual novel audio.
  - Downloads: 32
- [alfredplpl/sarashina2-7b-it](https://huggingface.co/alfredplpl/sarashina2-7b-it)
  - Sarashina2-7B Instruct sarashina2-7B„Çí‰ºöË©±„Åß„Åç„Çã„Çà„ÅÜ„Å´„Éï„É´„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 30
- [natsusakiyomi/KaedeMix](https://huggingface.co/natsusakiyomi/KaedeMix)
  - This repository details a modified CreativeML OpenRAIL-M license permitting commercial use, sale, and merging of the model and its generated images, even without attribution or with modified permissions for derivative works.
  - Downloads: 27
- [kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest](https://huggingface.co/kurogane/Llama3-BioYouri-8B-instruct-chatvector-mergetest)
  - kurogane/Llama3-BioYouri-8B-mergetest „Åì„ÅÆ„É¢„Éá„É´„ÅØÁîüÁâ©Â≠¶„ÉªÂåªÂ≠¶„Å´Á≤æÈÄö„Åó„ÅüOpenBioLLM-8B„Çí„Éô„Éº„Çπ„Å´„ÄÅÊó•Êú¨Ë™ûÂØæÂøú„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´Llama-3-youko-8b-instruct-chatvector„Å®„Éû„Éº„Ç∏„Åï„Åõ„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 27
- [p1atdev/zenz-v1-onnx](https://huggingface.co/p1atdev/zenz-v1-onnx)
  - Miwa-Keita/zenz-v1-checkpoints „Çí optimum Áî®„Å´ ONNX „Å´Â§âÊèõ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 24
- [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp)
  - Genji-JP 6B is a 6 billion parameter Japanese language model finetuned from EleutherAI‚Äôs GPT-J 6B on a dataset of Japanese web novels, utilizing RoPE and a 50,400 GPT-2/3 vocabulary.
  - Downloads: 23
- [watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft-vlm)
  - Watashiha-Llama-2-13B-Ogiri is a Japanese large language model fine-tuned with LLaVA on visual genome and STAIR Captions datasets to perform image-based humorous responses, utilizing a CLIP-ViT-B-32 vision encoder under the LLAMA 2 COMMUNITY LICENSE.
  - Downloads: 21
- [p1atdev/saikyou-shield-30m](https://huggingface.co/p1atdev/saikyou-shield-30m)
  - Saikyou Shield 30M is a lightweight (30M parameter) Japanese text classification model designed to classify *all* prompts as dangerous, including jailbreaks and prompt injections, for ultimate safety‚Äîreleased as an April Fool's joke.
  - Downloads: 20
- [nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview](https://huggingface.co/nitky/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-Japanese-32B-Preview)
  - This repository hosts a 32B Japanese language model‚Äîa merged and refined version of FuseO1-DeepSeekR1-QwQ-SkyT1-Flash‚Äîoptimized for code generation with specified parameters like temperature and max tokens, demonstrated by a FizzBuzz example.
  - Downloads: 17
- [llm-jp/llm-jp-3-172b-alpha2](https://huggingface.co/llm-jp/llm-jp-3-172b-alpha2)
  - llm-jp-3-172b-alpha2
  - Downloads: 16
- [tarudesu/gendec-with-distilmbert](https://huggingface.co/tarudesu/gendec-with-distilmbert)
  - Gendec is a machine learning framework for automatically detecting gender from Japanese names (provided in romaji) as presented in the ISDA'23 paper, offering male/female classification.
  - Downloads: 15
- [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst)
  - This repository hosts a fine-tuned version of MosaicML's MPT-7B-instruct, evaluated on the Jumtra 100QA dataset with 46% accuracy, requiring `trust_remote_code=True` due to its custom MPT architecture and training optimizations like FlashAttention.
  - Downloads: 14
- [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation)
  - This repository details a Japanese question generation model finetuned on a cleaned and translated SQuAD 1.1 dataset, using a Japanese T5 model to generate questions from given answers and contexts.
  - Downloads: 14
- [nlp-waseda/comet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-xl-japanese)
  - This repository provides a finetuned GPT-2 XL model (COMET-GPT2) for text generation, trained on the ATOMIC dataset using causal language modeling, and includes usage examples with the `transformers` pipeline.
  - Downloads: 14
- [mmnga/DeepSeek-V3-slice-jp64](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64)
  - DeepSeek-V3-slice-jp64 is a Japanese language model derived from DeepSeek-V3, rebuilt with a refined Mixture of Experts (MoE) architecture using only the 64 most frequently activated experts per layer to improve stability and performance.
  - Downloads: 14
- [natsusakiyomi/AsagaoMix](https://huggingface.co/natsusakiyomi/AsagaoMix)
  - This repository details a modified OpenRAIL-M license permitting commercial use, sales, and merging of the model and its outputs, even without attribution or with altered permissions for merged versions.
  - Downloads: 14
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v1)
  - This repository provides a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on machine-translated Ultrafeedback and hh-rlhf-12k-ja datasets, based on the STF and Notus codebases.
  - Downloads: 14
- [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct)
  - Introduction Who am I: Qishen Ha
  - Downloads: 14
- [sbtom/karakuri-midrose-CV](https://huggingface.co/sbtom/karakuri-midrose-CV)
  - karakuri-midroze-CV „É¢„Éá„É´„ÅÆË©≥Á¥∞„ÅØ„ÄÅ„Åì„Å°„Çâ„Åß„Åô„ÄÇ
  - Downloads: 14
- [lmg-anon/vntl-gemma2-2b-lora](https://huggingface.co/lmg-anon/vntl-gemma2-2b-lora)
  - Summary This is an Gemma 2 Baku lora, created using the VNTL 3.1 dataset.
  - Downloads: 14
- [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation)
  - This repository provides a model for automatically generating titles from article text, as detailed in the linked Qiita article.
  - Downloads: 13
- [ganchengguang/Yoko-7B-Japanese-v0](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v0)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model, trained on the Guanaco dataset with 49,000 chat samples, demonstrating improved performance in Chinese and Japanese, and includes a testing script with recommended generation parameters.
  - Downloads: 13
- [Aratako/Oumuamua-7b-instruct-v2-RP](https://huggingface.co/Aratako/Oumuamua-7b-instruct-v2-RP)
  - Oumuamua-7b-instruct-v2-RP nitky/Oumuamua-7b-instruct-v2„Çí„É≠„Éº„É´„Éó„É¨„Ç§Áî®„Å´LoRA„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [falche/opennovel_oc2_01a_7b](https://huggingface.co/falche/opennovel_oc2_01a_7b)
  - This repository hosts an alpha version of a Japanese-language assistant AI, fine-tuned from calm2-7b-chat, designed to continue writing provided text using a 150M token novel dataset and compatible with TextGen-WebUI.
  - Downloads: 12
- [ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0](https://huggingface.co/ohwi/japanese-stablelm-instruct-gamma-7b-dpo-uf-v0)
  - This repository hosts a 7B-parameter Japanese language model, fine-tuned with Direct Preference Optimization (DPO) on the Ultrafeedback dataset (machine translated), built upon the Japanese Stable LM Instruct Gamma 7B model.
  - Downloads: 12
- [Akimite/Gemma2-9b-it-Girl-v1](https://huggingface.co/Akimite/Gemma2-9b-it-Girl-v1)
  - This repository provides a Japanese language model‚Äîfine-tuned for a female/‚Äúdaughter-like‚Äù persona‚Äîwith specific settings for text generation, though merging it with other models is discouraged.
  - Downloads: 11
- [taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF](https://huggingface.co/taoki/phi3-mini-4k-qlora-jmultiwoz-dolly-amenokaku-alpaca_jp_python-GGUF)
  - This repository contains a model trained (QLoRA-SFT)
  - Downloads: 11
### Multimodality
- [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool, built on a Vision Encoder Decoder framework, designed for high-quality OCR of manga‚Äîincluding vertical text, furigana, and diverse fonts‚Äîbut also usable for general printed Japanese text.
  - Downloads: 128,903
- [mmnga/DeepSeek-V3-slice-jp64-gguf](https://huggingface.co/mmnga/DeepSeek-V3-slice-jp64-gguf)
  - This repository provides a GGUF-formatted, Japanese-focused version of DeepSeek-V3, specifically optimized with carefully selected Mixture of Experts layers and trained on Japanese examples, excluding code generation capabilities.
  - Downloads: 62,014
- [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16)
  - Rinna‚Äôs Japanese CLIP model, `rinna/japanese-clip-vit-b-16`, enables contrastive pre-training for image-text understanding in Japanese via a PyTorch package installable with `pip`.
  - Downloads: 25,155
- [line-corporation/clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)
  - This repository provides a Japanese CLIP model, trained on a billion web-collected image-text pairs, for zero-shot image classification and multimodal retrieval tasks.
  - Downloads: 12,995
- [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2)
  - This repository provides a 619M parameter, subword-based RNN-T ASR model‚Äîbuilt with an improved Conformer architecture and Longformer attention‚Äîfor efficient, long-form Japanese audio transcription using the ReazonSpeech v2.0 corpus.
  - Downloads: 3,926
- [kotoba-tech/kotoba-whisper-v2.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.1)
  - Kotoba-Whisper-v2.1 is a Japanese ASR model building upon kotoba-tech/kotoba-whisper-v2.0, enhanced with integrated punctuation and postprocessing pipelines developed in collaboration with Asahi Ushio and Kotoba Technologies.
  - Downloads: 2,956
- [mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf](https://huggingface.co/mmnga/umiyuki-Umievo-itr012-Gleipnir-7B-gguf)
  - This repository provides a GGUF-formatted version of the Umievo-itr012-Gleipnir-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm dataset and usable with llama.cpp.
  - Downloads: 2,518
- [nvidia/parakeet-tdt_ctc-0.6b-ja](https://huggingface.co/nvidia/parakeet-tdt_ctc-0.6b-ja)
  - Parakeet-tdt_ctc-0.6b-ja is a 0.6B parameter Japanese Automatic Speech Recognition (ASR) model developed by NVIDIA NeMo, transcribing speech with punctuation using a Hybrid FastConformer TDT-CTC architecture.
  - Downloads: 2,487
- [nablasinc/NABLA-VL](https://huggingface.co/nablasinc/NABLA-VL)
  - NABLA-VL is a Japanese vision-language model by NABLAS that processes images, multiple images, and videos to understand and generate text for multimodal tasks.
  - Downloads: 1,312
- [SakanaAI/Llama-3-Karamaru-v1](https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1)
  - Karamaru is a Japanese conversational AI model built on Llama 3 and trained with a large Edo-period text dataset‚Äîincluding both human and AI-transcribed historical sources‚Äîto generate responses in that era‚Äôs style.
  - Downloads: 1,058
- [FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)
  - SenseVoice is a multilingual speech foundation model offering ASR, LID, SER, and AED capabilities, with models available on Modelscope and Hugging Face, and demos for testing.
  - Downloads: 987
- [sbintuitions/sarashina2-vision-8b](https://huggingface.co/sbintuitions/sarashina2-vision-8b)
  - Sarashina2-Vision-8B is a high-performing Japanese large vision language model, built on Sarashina2-7B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 866
- [fishaudio/fish-speech-1.2-sft](https://huggingface.co/fishaudio/fish-speech-1.2-sft)
  - Fish Speech V1.2 Fish Speech V1.2 is a leading text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio data.
  - Downloads: 690
- [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)
  - This repository provides a Japanese CLIP model for encoding text and images, enabling tasks like similarity calculation, embedding, and multimodal search‚Äîwith demonstrations and accompanying articles detailing its creation, accuracy, and usage.
  - Downloads: 679
- [mmnga/Phi-3-medium-128k-instruct-gguf](https://huggingface.co/mmnga/Phi-3-medium-128k-instruct-gguf)
  - Phi-3-medium-128k-instruct-gguf microsoft„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãPhi-3-medium-128k-instruct„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 669
- [mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf](https://huggingface.co/mmnga/ELYZA-Shortcut-1.0-Qwen-7B-gguf)
  - This repository provides a GGUF-formatted conversion of the ELYZA-Shortcut-1.0-Qwen-7B language model, trained on the TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 459
- [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base)
  - This repository provides a Japanese Contrastive Language-Image Pretrained (CLIP) model, enabling zero-shot image classification, text-image retrieval, and image feature extraction for Japanese language tasks.
  - Downloads: 432
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1)
  - Heron BLIP Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, enabling image-based conversation in Japanese.
  - Downloads: 424
- [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1)
  - Kotoba-Whisper-v1.1 is a Japanese ASR model building on kotoba-whisper-v1.0, enhanced with integrated punctuation and post-processing pipelines developed jointly by Asahi Ushio and Kotoba Technologies.
  - Downloads: 410
- [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr)
  - This repository provides a fine-tuned Japanese Hubert-base ASR model, trained on the common_voice_11_0 dataset, specifically for Hiragana transcription with reported WER metrics.
  - Downloads: 343
- [mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf](https://huggingface.co/mmnga/cyberagent-Mistral-Nemo-Japanese-Instruct-2408-gguf)
  - This repository provides a GGUF-formatted conversion of the cyberagent Mistral-Nemo-Japanese-Instruct-2408 language model, using data created with TFMC/imatrix-dataset-for-japanese-llm, and intended for use with llama.cpp.
  - Downloads: 312
- [sbintuitions/sarashina2-vision-14b](https://huggingface.co/sbintuitions/sarashina2-vision-14b)
  - Sarashina2-Vision-14B is a top-performing Japanese Vision Language Model built on Sarashina2-13B and Qwen2-VL-7B, achieving state-of-the-art results on four benchmarks as of March 7, 2025.
  - Downloads: 224
- [Local-Novel-LLM-project/Ocuteus-v1-gguf](https://huggingface.co/Local-Novel-LLM-project/Ocuteus-v1-gguf)
  - Ocuteus„ÅÆGGUFÁâà„Åß„Åô„ÄÇ
  - Downloads: 179
- [rinna/japanese-data2vec-audio-base](https://huggingface.co/rinna/japanese-data2vec-audio-base)
  - Rinna‚Äôs Japanese data2vec Audio Base model is a 12-layer transformer trained on 19,000 hours of Japanese audio, replicating the original data2vec architecture and training process.
  - Downloads: 172
- [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for Japanese, trained on Common Voice and JSUT data, requiring 16kHz sampled input speech.
  - Downloads: 171
- [fishaudio/fish-speech-1.2](https://huggingface.co/fishaudio/fish-speech-1.2)
  - Fish Speech V1.2 is a multilingual text-to-speech (TTS) model trained on 300k hours of English, Chinese, and Japanese audio, with a demo available at Fish Audio.
  - Downloads: 158
- [mmnga/QwQ-32B-gguf](https://huggingface.co/mmnga/QwQ-32B-gguf)
  - This repository provides a GGUF-formatted conversion of the QwQ-32B language model, utilizing TFMC/imatrix-dataset-for-japanese-llm data and compatible with llama.cpp for inference.
  - Downloads: 156
- [Mitsua/mitsua-japanese-clip-vit-b-16](https://huggingface.co/Mitsua/mitsua-japanese-clip-vit-b-16)
  - Mitsua Japanese CLIP ViT-B-16 ÊòéÁ§∫ÁöÑ„Å™Ë®±Ë´æ„ÇíÂæó„Åü„Ç™„Éó„Éà„Ç§„É≥„Éá„Éº„Çø„ÄÅ„Ç™„Éº„Éó„É≥„É©„Ç§„Çª„É≥„Çπ„Éá„Éº„Çø„ÄÅ„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüÊó•Êú¨Ë™û/Ëã±Ë™û„Éê„Ç§„É™„É≥„Ç¨„É´CLIP (Contrastive Language-Image Pre-training)„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 149
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base)
  - This repository provides a Japanese CLIP model (ViT-H/14 base) for multimodal tasks like zero-shot image classification, embedding Japanese text and images into a shared space, licensed under CC BY-NC-SA 4.0.
  - Downloads: 128
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-deeper)
  - This repository provides a Japanese CLIP model (ViT-H/14) for multimodal tasks like zero-shot image classification, embedding Japanese text and images into a shared space under a CC BY-NC-SA 4.0 license.
  - Downloads: 126
- [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider)
  - This repository provides a Japanese CLIP model (ViT-H/14) pretrained for multimodal tasks like zero-shot image classification, mapping Japanese text and images into a shared embedding space under a CC BY-NC-SA 4.0 license.
  - Downloads: 126
- [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF)
  - Local-Novel-LLM-projectÊßò„ÅÆ Assistance „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 122
- [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 model for Japanese speech recognition, trained on Common Voice and JSUT corpora, requiring 16kHz sampled input.
  - Downloads: 110
- [Superd4/lasttest](https://huggingface.co/Superd4/lasttest)
  - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
  - Downloads: 108
- [Ivydata/wav2vec2-large-speech-diarization-jp](https://huggingface.co/Ivydata/wav2vec2-large-speech-diarization-jp)
  - This repository provides a fine-tuned XLSR-53 model for Japanese two-speaker speech diarization using phone-call data, offering direct usage examples with PyDub and Transformers.
  - Downloads: 100
- [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base)
  - This repository provides an optical character recognition (OCR) model specifically trained for Japanese text, particularly within the context of Japanese manga.
  - Downloads: 93
- [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps)
  - This repository provides a fine-tuned OpenAI Whisper-large-v3 model‚Äîtrained for 4000 steps on the Common Voice 16.1 dataset‚Äîfor improved Japanese speech recognition, achieving a 0.4057 loss and reported WER on an evaluation set.
  - Downloads: 90
- [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts)
  - This repository provides a Japanese text-to-speech model, SpeechT5 fine-tuned on the JVS dataset with 100 speakers, utilizing 16-dimensional speaker embeddings for voice independence.
  - Downloads: 63
- [mathewthe2/manga-ocr-base](https://huggingface.co/mathewthe2/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool, built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga‚Äîhandling diverse layouts, fonts, and image qualities.
  - Downloads: 49
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k)
  - Heron BLIP Japanese StableLM Base 7B is a 7B parameter vision-language model, trained with the Heron library, capable of image-based conversation and utilizing LlamaTokenizer.
  - Downloads: 48
- [Hibernates/Hibernates-JP-1.3b-Max](https://huggingface.co/Hibernates/Hibernates-JP-1.3b-Max)
  - Hibernates-JP-1.3b-Max is a 1.3B parameter Japanese vision-language model built on LLaVA with a ConvNeXt Large vision encoder, trained on a custom dataset, and licensed under Apache 2.0.
  - Downloads: 40
- [vumichien/wav2vec2-xls-r-1b-japanese](https://huggingface.co/vumichien/wav2vec2-xls-r-1b-japanese)
  - This repository provides a fine-tuned wav2vec2-xls-r-1b speech recognition model‚Äîtrained on ~60 hours of combined Japanese datasets (Common Voice, JUST, JSSS, CSS10)‚Äîachieving benchmark WER results on Common Voice.
  - Downloads: 40
- [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr)
  - This repository provides a fine-tuned wav2vec2-base model for Japanese Automatic Speech Recognition (ASR) that predicts Hiragana, achieving a Word Error Rate (WER) of 1.0 after 1000 training steps on the common_voice_11_0 dataset.
  - Downloads: 39
- [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next)
  - ReazonSpeech-ESPnet-Next provides cutting-edge Japanese Automatic Speech Recognition (ASR) models and datasets, actively incorporating community feedback for rapid research development and accessibility.
  - Downloads: 38
- [2121-8/canary-tts-150m](https://huggingface.co/2121-8/canary-tts-150m)
  - This repository details Canary-TTS-150M, a text-to-speech model based on llm-jp/llm-jp-3-150m-instruct3 and utilizing XCodec2, offering fine-grained voice control via prompt engineering‚Äîan experimental model preceding Canary-TTS 0.5B.
  - Downloads: 36
- [svjack/Stable-Diffusion-Pokemon-ja](https://huggingface.co/svjack/Stable-Diffusion-Pokemon-ja)
  - This repository provides a Japanese language Stable Diffusion model for generating Pokemon images from text prompts, trained with diffusers and licensed under CreativeML OpenRAIL-M.
  - Downloads: 35
- [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0)
  - LLaVA-JP is a Japanese vision-language model, fine-tuned from llm-jp/llm-jp-1.3b-v1.0 using the LLaVA method and trained on datasets like LLaVA-CC3M-Pretrain-595K-JA for image conversation.
  - Downloads: 31
- [RikkaBotan/style_bert_vits2_jp_extra_asmr_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_asmr_original)
  - This repository provides a free, commercially-usable voice model‚ÄîRikkaBotan ASMR‚Äîspecializing in gentle, childlike voices, with variations for emotional, English, Chinese, and logical speaking styles.
  - Downloads: 31
- [Respair/Hibiki_ASR_Phonemizer_v0.2](https://huggingface.co/Respair/Hibiki_ASR_Phonemizer_v0.2)
  - Hibiki ASR is a fine-tuned Whisper-large-v3 model for Japanese speech recognition that transcribes audio, detects non-speech sounds, and adds punctuation, requiring specific post-processing for optimal performance.
  - Downloads: 28
- [AkitoP/whisper-large-v3-japense-phone_accent](https://huggingface.co/AkitoP/whisper-large-v3-japense-phone_accent)
  - This repository provides a fine-tuned Whisper Large V3 model for Japanese speech transcription into Katakana with pitch accent, trained on Galgame-Speech and JSUT-5000 datasets.
  - Downloads: 25
- [umiyuki/Llama-3-Umievo-itr014-Shizuko-8b](https://huggingface.co/umiyuki/Llama-3-Umievo-itr014-Shizuko-8b)
  - Llama-3-Umievo-itr014-Shizuko-8b „Åì„ÅÆ„É¢„Éá„É´„ÅØÊó•Êú¨Ë™û„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„ÇãLlama-3„Éô„Éº„Çπ„ÅÆÔºî„Å§„ÅÆ„É¢„Éá„É´„ÇíÈÄ≤ÂåñÁöÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅßÈÄ≤ÂåñÁöÑ„Éû„Éº„Ç∏„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 25
- [arc-r/faster-whisper-large-v2-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-jp)
  - This repository provides a CTranslate2-compatible version of the whisper-large-v2-jp speech recognition model, enabling faster transcription with CTranslate2 and projects like faster-whisper.
  - Downloads: 24
- [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1)
  - Kotoba-Speech v0.1 is a 1.2B parameter, end-to-end Transformer model for fluent Japanese text-to-speech generation and one-shot voice cloning, building upon the metavoice framework.
  - Downloads: 24
- [vitouphy/wav2vec2-xls-r-300m-japanese](https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned Wav2Vec2 model for Japanese audio transcription directly into Hiragana, achieving a 0.2227 Character Error Rate on the Common Voice dataset after converting all text to Hiragana with pykakasi and tokenizing with fugashi.
  - Downloads: 21
- [Lycoris53/Vits-TTS-Japanese-Only-Amitaro](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Amitaro)
  - This repository provides a Japanese-only VITS TTS model, finetuned with free voice data from Amitaro, based on Plachtaa‚Äôs fast finetuning code and trained on 76 wav files for 600 epochs by Lycoris52.
  - Downloads: 20
- [yashvoladoddi37/kanji-diffusion-v1-4](https://huggingface.co/yashvoladoddi37/kanji-diffusion-v1-4)
  - Kanji Diffusion v1-4 Model Card Kanji Diffusion is a latent text-to-image diffusion model capable of hallucinating Kanji characters given any English prompt.
  - Downloads: 20
- [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0)
  - Heron BLIP Japanese StableLM Base 7B is a vision-language model, trained with the Heron library, enabling conversational interaction about images with a demo available in this repository.
  - Downloads: 19
- [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion)
  - SFCOCO is a Japanese-specific Stable Diffusion model fine-tuned for generating photo-realistic images from text prompts, built using ü§ó Diffusers.
  - Downloads: 19
- [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr)
  - Manga OCR is a Japanese text recognition tool‚Äîbuilt with a Vision Encoder Decoder‚Äîspecifically designed for high-quality OCR of manga, handling vertical/horizontal text, furigana, diverse fonts, and low-resolution images.
  - Downloads: 17
- [espnet/kan-bayashi_jsut_conformer_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_conformer_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_conformer_fastspeech2 ‚ôª
  - Downloads: 17
- [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp)
  - This repository provides a CTranslate2-compatible version of the whisper-large-v2-mix-jp speech recognition model, enabling faster transcription via CTranslate2 and projects like faster-whisper.
  - Downloads: 16
- [reazon-research/reazonspeech-espnet-v1](https://huggingface.co/reazon-research/reazonspeech-espnet-v1)
  - This repository provides a 15,000-hour ReazonSpeech-trained ESPnet model for Japanese automatic speech recognition requiring 16kHz audio input.
  - Downloads: 16
- [espnet/kan-bayashi_jsut_fastspeech2](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech2 ‚ôª
  - Downloads: 15
- [espnet/kan-bayashi_jsut_tacotron2](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2 ‚ôª
  - Downloads: 15
- [Aratako/Swallow-MoE-4x7B-lisa](https://huggingface.co/Aratako/Swallow-MoE-4x7B-lisa)
  - Swallow-MoE-4x7B-lisa Ê¶ÇË¶Å tokyotech-llm/Swallow-7b-hf„Çí„Éô„Éº„Çπ„Å´„ÄÅ‰ª•‰∏ã„ÅÆ4„É¢„Éá„É´„Çígate_mode=random„ÅßMoE„Åó„ÄÅ„Åù„ÅÆÂæåLISA„Å®„ÅÑ„ÅÜÊâãÊ≥ï„Åß„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 15
- [Dallyana/EspnetASR](https://huggingface.co/Dallyana/EspnetASR)
  - reazonspeech-espnet-v1 reazonspeech-espnet-v1 is an ESPnet model trained for Japanese automatic speech recognition (ASR).
  - Downloads: 15
- [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel)
  - Donut is a base-sized model fine-tuned on a synthetic dataset of visual novel images for document visual question answering, demonstrated with a Colab notebook and Japanese visual novel examples.
  - Downloads: 14
- [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200)
  - This repository provides a fine-tuned Wav2Vec2-Large-XLSR-53 speech recognition model for a specified language, trained on datasets like Common Voice, and requiring 16kHz audio input.
  - Downloads: 14
- [ThePioneer/MyVoiceClone-Style-Bert-VITS2](https://huggingface.co/ThePioneer/MyVoiceClone-Style-Bert-VITS2)
  - This repository provides a Style Bert VITS2 voice clone capable of text-to-speech generation in English, Japanese, and Chinese, featuring a young, neutral voice suitable for diverse applications like virtual YouTubers.
  - Downloads: 14
- [Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko](https://huggingface.co/Lycoris53/Vits-TTS-Japanese-Only-Sakura-Miko)
  - This repository provides a VITS-TTS Japanese voice model finetuned on Sakura Miko's voice data, offering hobbyist use with adherence to Cover Corporation's secondary creation guidelines.
  - Downloads: 14
- [Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/Nikolajvestergaard/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a fine-tuned Whisper tiny model for Japanese speech recognition, trained on the Common Voice dataset with a learning rate of 1e-05, achieving a WER of 225.23 on the evaluation set.
  - Downloads: 13
- [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese)
  - This repository provides a fine-tuned Wav2Vec2-XLS-R-300M model for Japanese Hiragana speech recognition, achieving 9.34% CER on Common Voice Japanese data with 16kHz audio input and continuous (word-boundary-free) sentence outputs.
  - Downloads: 13
- [agiera/manga-ocr-base](https://huggingface.co/agiera/manga-ocr-base)
  - Manga OCR is a Japanese text recognition tool built on a Vision Encoder Decoder framework, specifically designed for high-quality OCR of manga content‚Äîincluding vertical text, furigana, and varied fonts‚Äîbut also usable for general printed Japanese OCR.
  - Downloads: 13
- [espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_conformer_fastspeech2_transformer_teacher_r-truncated-f43d8f)
  - ESPnet2 TTS pretrained model kan-bayashi/jsut_tts_train_conformer_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_prosody_train.loss.ave ‚ôª
  - Downloads: 13
- [teasan/endlessMix](https://huggingface.co/teasan/endlessMix)
  - ‚ñ†endlessMix„Ç∑„É™„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶ Ê¶ÇË¶Å „Åì„ÅÆ„É¢„Éá„É´„ÅØDefacta„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÈöéÂ±§„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese)
  - This repository provides a fine-tuned wav2vec2-xls-r-300m model for Japanese speech recognition on the Common Voice 8.0 dataset, utilizing Hiragana conversion and optimized for Character Error Rate (CER) evaluation, achieving 23.64% CER on Common Voice and performance on speech-recognition-community-v2.
  - Downloads: 12
- [sonoisa/vl-t5-base-japanese](https://huggingface.co/sonoisa/vl-t5-base-japanese)
  - This repository provides a Japanese VL-T5 model, pretrained on a Japanese corpus for unifying vision-and-language tasks via text generation, based on the VL-T5 architecture.
  - Downloads: 11
- [NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model](https://huggingface.co/NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model)
  - This repository provides a Japanese-specific, fine-tuned version of OpenAI's Whisper-tiny model, trained on the Common Voice dataset, for real-time Japanese speech recognition with a WER of 301.625840.
  - Downloads: 11
- [lorenzoncina/whisper-small-ja](https://huggingface.co/lorenzoncina/whisper-small-ja)
  - Whisper Small JA is a fine-tuned speech-to-text model based on the SVJ Japanese dataset and Common Voice 11.0, achieving a 17.7261 Character Error Rate on the evaluation set.
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tacotron2_accent](https://huggingface.co/espnet/kan-bayashi_jsut_tacotron2_accent)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tacotron2_accent ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_fastspeech](https://huggingface.co/espnet/kan-bayashi_jsut_fastspeech)
  - Example ESPnet2 TTS model kan-bayashi/jsut_fastspeech ‚ôª
  - Downloads: 11
- [espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24](https://huggingface.co/espnet/kan-bayashi_jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jac-truncated-60fc24)
  - Example ESPnet2 TTS model kan-bayashi/jsut_tts_train_fastspeech2_transformer_teacher_raw_phn_jaconv_pyopenjtalk_accent_with_pause_train.loss.ave ‚ôª
  - Downloads: 11
### Natural Language Interfaces
- [tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 1,366
- [mmnga/Reflection-Llama-3.1-70B-gguf](https://huggingface.co/mmnga/Reflection-Llama-3.1-70B-gguf)
  - Reflection-Llama-3.1-70B-gguf mattshumer„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãReflection-Llama-3.1-70B„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,323
- [mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-KUJIRA-gguf)
  - DataPilot-ArrowPro-7B-KUJIRA-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-KUJIRA„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,111
- [MCZK/EZO-Common-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Common-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 1,027
- [mmnga/Llama3-ArrowSE-8B-v0.3-gguf](https://huggingface.co/mmnga/Llama3-ArrowSE-8B-v0.3-gguf)
  - Llama3-ArrowSE-8B-v0.3-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãLlama3-ArrowSE-8B-v0.3„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 700
- [mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf](https://huggingface.co/mmnga/DataPilot-ArrowPro-7B-RobinHood-gguf)
  - DataPilot-ArrowPro-7B-RobinHood-gguf DataPilot„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãArrowPro-7B-RobinHood„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 672
- [nu-dialogue/j-moshi-ext](https://huggingface.co/nu-dialogue/j-moshi-ext)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with limitations in instruction-following.
  - Downloads: 614
- [MCZK/EZO-Humanities-9B-gemma-2-it-GGUF](https://huggingface.co/MCZK/EZO-Humanities-9B-gemma-2-it-GGUF)
  - ‚Äªllama.cpp Releases b3428(7/21)
  - Downloads: 474
- [MCZK/Llama3-ArrowSE-8B-v0.3-GGUF](https://huggingface.co/MCZK/Llama3-ArrowSE-8B-v0.3-GGUF)
  - DataPilotÊßò„ÅÆ Llama3-ArrowSE-8B-v0.3 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 449
- [mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-7B-Japanese-GGUF)
  - This repository offers statically quantized weights for the DeepSeek-R1-Distill-Qwen-7B-Japanese model in GGUF format, with imatrix quants available upon request and usage guidance linked to TheBloke's resources.
  - Downloads: 375
- [tokyotech-llm/Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1)
  - Llama3 Swallow - Built with Meta Llama 3
  - Downloads: 317
- [mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF](https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Japanese-GGUF)
  - This repository provides statically quantized versions of the DeepSeek-R1-Distill-Qwen-14B-Japanese model in GGUF format, with options to request further imatrix quantizations and guidance on GGUF usage via TheBloke's documentation.
  - Downloads: 309
- [MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF](https://huggingface.co/MCZK/Llama-3-Swallow-8B-Instruct-v0.1-GGUF)
  - tokyotech-llmÊßò„ÅÆ Llama-3-Swallow-8B-Instruct-v0.1 „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 299
- [mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF](https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-i1-GGUF)
  - About weighted/imatrix quants of https://huggingface.co/nk2t/Llama-3-8B-Instruct-japanese-nk2t-v0.2 static quants are available at https://huggingface.co/mradermacher/Llama-3-8B-Instruct-japanese-nk2t-v0.2-GGUF Usage
  - Downloads: 281
- [mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF](https://huggingface.co/mradermacher/Mixtral-8x7B-v0.1-japanese-GGUF)
  - About static quants of https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese weighted/imatrix quants seem not to be available (by me) at this time.
  - Downloads: 275
- [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct)
  - This repository hosts a Japanese instruction-tuned language model (c4ai-command-r-v01) further refined with ichikara-instruction, trained on Runpod with specific LoRA parameters and evaluated on datasets like jsquad and jcommonsenseqa.
  - Downloads: 117
- [p1atdev/qwen2.5-0.5b-grpo-math-01](https://huggingface.co/p1atdev/qwen2.5-0.5b-grpo-math-01)
  - This repository demonstrates solving simple arithmetic problems using GRPO, featuring a specific prompt format with `<think>`/`<answer>` blocks for reasoning and solution output.
  - Downloads: 102
- [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA)
  - This repository provides a Japanese question-answering model fine-tuned from luke-japanese-base-lite using the DDQA dataset, achieving 84.59% exact match accuracy.
  - Downloads: 87
- [SousiOmine/minoshiro-v0.2-7B_GGUF](https://huggingface.co/SousiOmine/minoshiro-v0.2-7B_GGUF)
  - https://huggingface.co/SousiOmine/minoshiro-v0.2-7B „ÅÆGGUFÈáèÂ≠êÂåñÁâà„Åß„Åô„ÄÇ
  - Downloads: 84
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe)
  - This repository provides a Japanese DeBERTa-v2-based model fine-tuned on the DDQA dataset for Question-Answering tasks, compatible with transformers, PyTorch, and SentencePiece.
  - Downloads: 83
- [MCZK/ArrowPro-7B-KUJIRA-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-KUJIRA-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-KUJIRA „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 80
- [Mizuiro-sakura/luke-japanese-large-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-QA)
  - This repository provides a question-answering model fine-tuned from luke-japanese-large-lite using the DDQA dataset, achieving 86.3% exact match accuracy.
  - Downloads: 56
- [MCZK/ArrowPro-7B-RobinHood-GGUF](https://huggingface.co/MCZK/ArrowPro-7B-RobinHood-GGUF)
  - DataPilotÊßò„ÅÆ ArrowPro-7B-RobinHood „ÇíGGUFÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 50
- [nitky/Oumuamua-7b-instruct-v2](https://huggingface.co/nitky/Oumuamua-7b-instruct-v2)
  - Oumuamua-7b-instruct-v2 is a merged language model‚Äîoptimized for helpful, harmless responses, role-playing (especially as a Japanese person to avoid translation-like outputs), and improved multi-turn conversation‚Äîbuilt with mergekit.
  - Downloads: 31
- [nu-dialogue/j-moshi](https://huggingface.co/nu-dialogue/j-moshi)
  - J-Moshi is a Japanese full-duplex spoken dialogue system built upon the Moshi model and fine-tuned with Japanese conversational data, enabling real-time, natural turn-taking despite being a prototype with potential for unnatural responses and limited instruction-following.
  - Downloads: 28
- [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct)
  - This repository provides Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2, a Japanese language model based on Mixtral-8x7B, extending context length to 32K and significantly improving instruction-following capabilities through a vector merge.
  - Downloads: 20
- [mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit](https://huggingface.co/mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit)
  - mlx-community/Llama-3.1-70B-Japanese-Instruct-2407-4bit
  - Downloads: 20
- [aipib/karasu-lora-jp-qa-chat](https://huggingface.co/aipib/karasu-lora-jp-qa-chat)
  - karasu-lora-jp-qa-chat karasu fine tuned model by lora method with the original Q&amp;A dataset.
  - Downloads: 17
- [Noginowa/AnimaMixColorXL](https://huggingface.co/Noginowa/AnimaMixColorXL)
  - AnimagineÁ≥ª„ÅÆ„É¢„Éá„É´„Çí„Éü„ÉÉ„ÇØ„Çπ„Åó„ÅüVAEÂÜÖËîµ„Éû„Éº„Ç∏„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat](https://huggingface.co/flypg/DeepSeek-R1-Distill-Qwen-14B-Japanese-chat)
  - This repository details a permissively licensed (MIT) Japanese causal language model, finetuned from cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese for conversational use, but trained on a limited dataset.
  - Downloads: 16
- [umiyuki/Japanese-Chat-Umievo-itr004-7b](https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr004-7b)
  - japanese-chat-umievo-itr004-7b
  - Downloads: 16
- [kanxxyc/JPNsensei-V2](https://huggingface.co/kanxxyc/JPNsensei-V2)
  - JPNsensei-V2 is a question-answering model built on japanese-stablelm-instruct-gamma-7b, designed to help learners with Japanese language questions in English using a specific prompt format and requiring Transformers 4.34.0+.
  - Downloads: 15
- [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot)
  - This repository provides a Japanese chatbot model, finetuned from rinna/japanese-gpt2-medium on the Yuyuyui scenario corpus, that generates responses based on character-prefixed dialogue context.
  - Downloads: 15
- [lmg-anon/vntl-llama3-8b-202409-qlora](https://huggingface.co/lmg-anon/vntl-llama3-8b-202409-qlora)
  - Summary This is an LLaMA 3 Youko qlora, created using a custom version of the VNTL dataset combined with the VNTL-Chat dataset.
  - Downloads: 15
- [frost-beta/Llama3-33.5M-Japanese](https://huggingface.co/frost-beta/Llama3-33.5M-Japanese)
  - A very tiny 33.5M Llama3 model trained on a Macbook Pro with M3 Max for 10 hours.
  - Downloads: 15
- [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora)
  - This repository provides a small Japanese DialoGPT model trained on dialogue extracted from Aozora Bunko, a collection of public domain Japanese literature.
  - Downloads: 13
- [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)
  - Lightblue's QLoRA finetune specializes in Japanese closed-question answering, trained on a combined dataset of SNOW TyDiQA, and XLSUM for improved reference-text-based accuracy.
  - Downloads: 13
- [SousiOmine/Kuroiso-CR-7B-20250124](https://huggingface.co/SousiOmine/Kuroiso-CR-7B-20250124)
  - This repository provides a fine-tuned Qwen2.5-7B-Instruct language model capable of generating chain-of-thought reasoning from given question-answer pairs, formatted with specific XML-like tags for input and output.
  - Downloads: 13
- [Mizuiro-sakura/luke-japanese-base-lite-jsquad](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-lite-jsquad)
  - This repository provides a question-answering model fine-tuned from luke-japanese-base-lite using the JSQuAD dataset, achieving an F1 score of 0.876 and exact match of 0.758.
  - Downloads: 13
- [HachiML/Swallow-MS-7b-instruct-v0.1](https://huggingface.co/HachiML/Swallow-MS-7b-instruct-v0.1)
  - Swallow-MS-7b-v0.1 „Åì„ÅÆ„É¢„Éá„É´„ÅØtokyotech-llm/Swallow-MS-7b-instruct-v0.1„ÅÆtokenizer.chat_template„Çí‰ª•‰∏ã„Å´Â§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 13
- [Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA](https://huggingface.co/Mizuiro-sakura/deberta-v2-tiny-japanese-finetuned-QA)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-japanese model for Question-Answering, specifically trained on the DDQA dataset and usable for tasks like SQuAD.
  - Downloads: 12
- [if001/tiny_mixtral_ja_instruction](https://huggingface.co/if001/tiny_mixtral_ja_instruction)
  - This repository provides a Japanese instruction-tuned model based on TinyMixtral, trained on a dataset and available on Hugging Face.
  - Downloads: 11
- [swdq/Visual-novel-whisper](https://huggingface.co/swdq/Visual-novel-whisper)
  - https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1 ‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„ÄÅ„Ç¢„ÉÄ„É´„ÉàÁî®Ë™û„ÇíË™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
  - Downloads: 11
- [nitky/RP-7b-instruct](https://huggingface.co/nitky/RP-7b-instruct)
  - RP-7b-instruct üö® This model is tuning to RP and knowledge is likely unstable.
  - Downloads: 11
### Information Extraction & Text Mining
- [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA)
  - This repository provides a Japanese medical named entity recognition model, runnable via `predict.py`, to identify disease, medication, and key attributes within text.
  - Downloads: 117,872
- [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset)
  - This repository provides a Japanese Named Entity Recognition (NER) model, fine-tuned from `cl-tohoku/bert-base-japanese-v3` using the `llm-book/ner-wikipedia-dataset`, as presented in the book ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 78,162
- [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese)
  - This repository provides a Japanese Named Entity Recognition (NER) model built on BERT, capable of extracting eight entity types‚Äîincluding person, organization, location, and product names‚Äîfrom text.
  - Downloads: 3,255
- [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)
  - This repository provides public access to files and content upon acceptance of specified conditions.
  - Downloads: 598
- [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner)
  - This repository provides a fine-tuned luke-japanese-base model for Named Entity Recognition (NER) using a Wikipedia dataset, achieving 77% precision, recall, and F1-score for organizational names.
  - Downloads: 452
- [Kotajiro/kurumi_flux](https://huggingface.co/Kotajiro/kurumi_flux)
  - Kurumi_flux_lora_v1.0 is a non-commercial LoRA model based on flux1-dev, optimized for realistic, beautiful girl depictions, and subject to specific usage restrictions including prohibiting violent content and retraining.
  - Downloads: 218
- [mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF](https://huggingface.co/mradermacher/Japanese-Starling-ChatV-7B-i1-GGUF)
  - This repository provides weighted/imatrix quantized versions of the Japanese-Starling-ChatV-7B model in GGUF format, offering various quantization levels (including IQ1_S at 1.7GB) for use with tools like those detailed in TheBloke's READMEs.
  - Downloads: 154
- [abhishek/autonlp-japanese-sentiment-59362](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59362)
  - This repository details a binary classification model (ID 59362) trained with AutoNLP for Japanese sentiment analysis, achieving 95.27% accuracy and accessible via a Hugging Face API endpoint.
  - Downloads: 153
- [lmg-anon/vntl-llama3-8b-v2-gguf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-gguf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune‚Äîtrained on an expanded VNTL dataset‚Äîto enhance English translation of Japanese visual novels, offering improved accuracy and stability over previous versions without chat mode.
  - Downloads: 132
- [lmg-anon/vntl-llama3-8b-v2-hf](https://huggingface.co/lmg-anon/vntl-llama3-8b-v2-hf)
  - This repository provides a LLaMA 3 Youko QLoRA fine-tune, leveraging an expanded and rebuilt VNTL dataset to enhance Japanese visual novel translation to English with improved accuracy and stability.
  - Downloads: 121
- [Mizuiro-sakura/luke-japanese-large-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-finetuned-ner)
  - This repository provides a fine-tuned version of the luke-japanese-large model for Named Entity Recognition (NER) using a Wikipedia-based Japanese dataset, achieving an overall accuracy of 0.845.
  - Downloads: 98
- [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA)
  - This repository provides a pre-trained Japanese medical named entity recognition model, prediction script, and normalization method, outputting XML tags compatible with the MedTxt-CR-JA dataset.
  - Downloads: 88
- [tokyotech-llm/edu-classifier](https://huggingface.co/tokyotech-llm/edu-classifier)
  - This repository provides fastText classifiers‚Äîtrained on Wikipedia and LLM-annotated data‚Äîto assess the educational value of Japanese web pages under a CC BY-SA 4.0 license.
  - Downloads: 80
- [bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier](https://huggingface.co/bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier)
  - This repository provides a Japanese text classifier, finetuned from cl-tohoku-bert-japanese-v3, to predict JLPT levels (N1-N5) with reported precision, recall, and F1-scores up to 0.95, trained on ~5000 sentences.
  - Downloads: 42
- [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition (NER) model‚Äîbuilt by fine-tuning a BERT-base Japanese model with a CRF layer on the NER-wikipedia dataset‚Äîas presented in chapter 6 of ‚ÄúIntroduction to Large Language Models‚Äù.
  - Downloads: 36
- [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for Named Entity Recognition (NER) using a Wikipedia-based dataset.
  - Downloads: 34
- [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner)
  - This repository provides a fine-tuned version of cl-tohoku/bert-large-japanese-v2 for Japanese Named Entity Recognition (NER) using a Wikipedia dataset, achieving an overall accuracy of 0.862.
  - Downloads: 27
- [IoriU/kmnist49-classifier](https://huggingface.co/IoriU/kmnist49-classifier)
  - This repository provides a Fully Convolutional Neural Network (FCN) model trained for classifying images of 49 Japanese hiragana characters from the Kuzushiji 49 MNIST dataset.
  - Downloads: 25
- [Tomohiro/RealMedNLP_CR_JA](https://huggingface.co/Tomohiro/RealMedNLP_CR_JA)
  - This repository provides a Japanese medical named entity recognition (NER) model and script (`predict.py`) for identifying disease, treatment, and temporal expressions in medical text.
  - Downloads: 22
- [kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)
  - WRIME is a dataset of 36k weakly-supervised region identification annotations for improving visual reasoning and object detection.
  - Downloads: 21
- [spacy/ja_core_news_lg](https://huggingface.co/spacy/ja_core_news_lg)
  - `ja_core_news_lg` is a CPU-optimized spaCy 3.7+ pipeline for Japanese natural language processing, featuring tokenization, morphological analysis, parsing, sentence segmentation, named entity recognition, and 300-dimensional vectors trained on UD Japanese GSD v2.8.
  - Downloads: 20
- [spacy/ja_core_news_trf](https://huggingface.co/spacy/ja_core_news_trf)
  - `ja_core_news_trf` is a spaCy 3.7+ Japanese NLP pipeline featuring a transformer model (`cl-tohoku/bert-base-japanese-char-v2`) for tasks like morphological analysis, parsing, and named entity recognition.
  - Downloads: 20
- [ithattieu/XML-RoBERTa-NER-Japanese](https://huggingface.co/ithattieu/XML-RoBERTa-NER-Japanese)
  - XML-RoBERTa-NER-Japanese This model is a fine-tuned version of xlm-roberta-base on the Wikipedia Japanese NER dataset from Stockmark Inc.
  - Downloads: 19
- [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2)
  - This repository provides a Japanese language model, fine-tuned from `studio-ousia/luke-japanese-large-lite`, that scores short text snippets for sexual content on a 0-1 scale, aiding in content moderation.
  - Downloads: 17
- [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation)
  - This repository provides a model for automatically generating titles from article text, as detailed in the linked Qiita article.
  - Downloads: 15
- [Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-large-japanese-finetuned-ner)
  - This repository provides a Japanese Named Entity Recognition (NER) model fine-tuned from deberta-v2-large-japanese using a Wikipedia-based dataset.
  - Downloads: 15
- [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model)
  - This repository provides a baseline transformer model trained and evaluated on the awesome-japanese-nlp-classification-dataset, achieving 97% accuracy with precision, recall, and F1-scores detailed in the description.
  - Downloads: 15
- [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363)
  - This repository details a binary classification model (ID: 59363) trained with AutoNLP for Japanese sentiment analysis, achieving 97.4% recall and 97.1% AUC, and accessible via a Hugging Face API endpoint.
  - Downloads: 14
- [spacy/ja_core_news_md](https://huggingface.co/spacy/ja_core_news_md)
  - `ja_core_news_md` is a CPU-optimized spaCy 3.7+ Japanese NLP pipeline featuring tokenization, morphological analysis, parsing, sentence segmentation, and named entity recognition, trained on UD Japanese and GSD v2.8 data with 200-dimensional vectors.
  - Downloads: 13
- [ohtaman/falcon-7b-kokkai2022-lora](https://huggingface.co/ohtaman/falcon-7b-kokkai2022-lora)
  - This repository provides a language model trained on 2022 Japanese parliamentary proceedings, built as a multi-GPU/node training example for the #ABCILLM hackathon, and includes example question/answer pairs.
  - Downloads: 13
- [ThePioneer/NaturalGirlyVoice](https://huggingface.co/ThePioneer/NaturalGirlyVoice)
  - This So-vits-svc 4.0 model creates natural-sounding, friendly female vocals‚Äîsynthesized from the author's own voice and expanded with ElevenLabs‚Äîand includes essential checkpoints and a notebook for inference/training, acknowledging potential noise and occasional pronunciation errors.
  - Downloads: 13
- [Aratako/Japanese-Novel-Reward-TinySwallow-1.5B](https://huggingface.co/Aratako/Japanese-Novel-Reward-TinySwallow-1.5B)
  - Aratako/Japanese-Novel-Reward-TinySwallow-1.5B is a Japanese novel quality evaluation reward model finetuned from SakanaAI/TinySwallow-1.5B, designed for tasks like reinforcement learning of text generation and predicting user preference via regression, but may exhibit biases beyond text quality.
  - Downloads: 11
- [AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese](https://huggingface.co/AIgroup-CVM-utokyohospital/Llama-2-70b-chat-4bit-japanese)
  - This repository provides a 4-bit quantized Llama-2-70b-chat model fine-tuned on the Japanese instruction dataset ‚Äúizumi-lab/llm-japanese-dataset‚Äù for improved performance in Japanese language tasks.
  - Downloads: 11
- [ganchengguang/Yoko-7B-Japanese-v1](https://huggingface.co/ganchengguang/Yoko-7B-Japanese-v1)
  - This repository provides a QLoRA-fine-tuned LLaMA2-7B model trained on the 49000/280000 chat/non-chat Guanaco dataset, exhibiting improved Chinese and Japanese performance and testable via the provided script.
  - Downloads: 11
### Responsible NLP
- [Lasorco/lametta](https://huggingface.co/Lasorco/lametta)
  - This Stable Diffusion model merges customizations for generating low body-proportion girls with preserved eye highlights, requiring age adjustment and potentially limited compatibility with LoRA, and recommends using DPM++ 2M Karras sampler with Hires.fix settings between 0-10.
  - Downloads: 25,287
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 8,064
- [litagin/anime-whisper](https://huggingface.co/litagin/anime-whisper)
  - Anime Whisper is a Japanese speech-to-text model fine-tuned on a large anime voice acting dataset, excelling in that domain but also offering unique performance on other audio, and requiring use *without* an initial prompt to avoid hallucinations.
  - Downloads: 3,674
- [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion)
  - This repository merges CoolJapanDiffusion 2.1.1 and WaifuDiffusion 1.4 anime models (with specific ratios noted in file names) for use in Colab WebUI, addressing potential over-saturation when merging with realistic models ‚Äì best results achieved with 512-based models instead of SD 2.1 768, and includes download commands for necessary checkpoint files.
  - Downloads: 390
- [knosing/japanese_ner_model](https://huggingface.co/knosing/japanese_ner_model)
  - Model Description
  - Downloads: 178
- [Kotajiro/suzume_mix](https://huggingface.co/Kotajiro/suzume_mix)
  - Suzume_mix_v1.0 is a non-commercial, merged Stable Diffusion model based on flux1-dev, designed to soften facial features and intended for personal use with proper attribution and no retraining.
  - Downloads: 96
- [keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/Llama-3-ELYZA-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 74
- [keitokei1994/shisa-v1-qwen2-7b-GGUF](https://huggingface.co/keitokei1994/shisa-v1-qwen2-7b-GGUF)
  - shisa-v1-qwen2-7b-gguf (English explanation is below.
  - Downloads: 62
- [keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF](https://huggingface.co/keitokei1994/swallow-3-8B-sqlcoder-2x8B-GGUF)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 41
- [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus)
  - This repository details a Stable Diffusion model merge ‚Äì MoeDiffusion blended with HassanBlend and VMix03 ‚Äì optimized for generating black-haired ponytail hairstyles, with a note that it may occasionally produce NSFW content and is not recommended for those seeking to avoid NAI/Insta-style model influences.
  - Downloads: 28
- [keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B](https://huggingface.co/keitokei1994/Llama-3-Umievo-Shizuko-sqlcoder-2x8B)
  - This repository hosts Llama-3-Umievo-Shizuko-sqlcoder-2x8B, a Mixture of Experts (MoE) language model created with MergeKit, combining Japanese language skills and SQL generation through fine-tuning on SQL datasets.
  - Downloads: 24
- [yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF](https://huggingface.co/yasu-oh/sarashina2.2-3b-instruct-v0.1-GGUF)
  - Sarashina 2.2-3B-Instruct, a Japanese language model, is based on sbintuitions' work and trained with the imatrix dataset.
  - Downloads: 21
- [SoMiyagawa/AinuTrans-2.0](https://huggingface.co/SoMiyagawa/AinuTrans-2.0)
  - „Ç∑„Çµ„É†Ë™û„Å´„Çà„ÇãË™¨Êòé „Ç¢„Ç§„ÉåË™û„Å®Êó•Êú¨Ë™û„ÅÆÂèåÊñπÂêëÊ©üÊ¢∞ÁøªË®≥„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 17
- [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1)
  - MoeDiffusionPlusPlus 0.7 merges DreamShaper 3.3 with WaifuDiffusion and Stable Diffusion VAEs to improve color vibrancy and expressive range, aiming for highly realistic and beautiful images‚Äîthough it may contain NAI/Insta model influences and is runnable on Colab.
  - Downloads: 13
- [aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b](https://huggingface.co/aixsatoshi/Meta-Llama-3.1-8B-Instruct-plus-Swallow-b)
  - Swallow-8B„ÅØËøΩÂä†„ÅÆÊó•Êú¨Ë™ûÁ∂ôÁ∂ö‰∫ãÂâçÂ≠¶Áøí„Å´„Çà„ÇäÊó•Êú¨Ë™û„ÅåÂ§ßÂ§âÊµÅÊö¢„Å™Llama-3Ê¥æÁîü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 13
- [keitokei1994/Llama-3-ELYZA-hermes-2x8B](https://huggingface.co/keitokei1994/Llama-3-ELYZA-hermes-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 13
- [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion)
  - „É¢„Éá„É´Ë™¨Êòé (model explanation) YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4 „Éû„Éº„Ç∏ÂÖÉ„ÅÆ„É´„Éº„ÉÑ„Å´NAI„É™„Éº„ÇØ„ÅåÂê´„Åæ„Çå„Çã„Å®„ÅÑ„ÅÜÂôÇ„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅNAI„É™„Éº„ÇØ„Ç¢„É≥„ÉÅ„Å´„ÅØÈùûÊé®Â•® ÁêÜÊÉ≥„ÅÆÈªíÈ´™„Éù„Éã„ÉÜÈ°î„ÅåÂá∫„Åõ„ÇãYaguruMagiku„Çí„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶È°î„ÅåËøë„Åè„Å¶Âà∂Âæ°„Åó„ÇÑ„Åô„ÅÑAbyssOrangeMix2„Å®Ê∑∑„Åú„Å¶„Åø„Åü„ÄÇ
  - Downloads: 11
- [DataPilot/ArrowSmartPlus_3.6B_instruction](https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instruction)
  - Ê¶ÇË¶Å „ÄåLOCAL AI HACKATHON„Äç„Å´„Åä„Åë„Çã„ÄÅ„ÉÅ„Éº„É†DataPilot,4„Å§„ÇÅ„ÅÆÊàêÊûúÂìÅ„Åß„Åô„ÄÇ
  - Downloads: 11
- [keitokei1994/Llama-3-8B-shisa-2x8B](https://huggingface.co/keitokei1994/Llama-3-8B-shisa-2x8B)
  - „É¢„Éá„É´„ÅÆË™¨Êòé(English explanation is below.
  - Downloads: 11
### Reasoning
- [mmnga/mathstral-7B-v0.1-gguf](https://huggingface.co/mmnga/mathstral-7B-v0.1-gguf)
  - mathstral-7B-v0.1-gguf mistralai„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãmathstral-7B-v0.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 1,067
- [mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf](https://huggingface.co/mmnga/ABEJA-QwQ32b-Reasoning-Japanese-v1.0-gguf)
  - This repository provides a GGUF-formatted version of the ABEJA-QwQ32b-Reasoning-Japanese-v1.0 language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm dataset and intended for use with llama.cpp.
  - Downloads: 1,007
- [mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf](https://huggingface.co/mmnga/YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf)
  - YuisekinAIEvol-Mistral-7B-ja-math-v0.1.1-gguf yuiseki„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„ÇãYuisekinAIEvol-Mistral-7B-ja-math-v0.1.1„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ
  - Downloads: 811
- [macadeliccc/polyglot-math-4x7b](https://huggingface.co/macadeliccc/polyglot-math-4x7b)
  - This repository hosts Polyglot-math-4x7b-24b, a multilingual Mixture of Experts model merging Chinese and Japanese language capabilities, fine-tuned on GSM8K with a 20GB VRAM footprint, and provides inference code.
  - Downloads: 761
- [abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0](https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0)
  - ABEJA-QwQ32b-Reasoning-Japanese-v1.0 is a Japanese reasoning model built upon ABEJA-Qwen2.5-32b-Japanese-v0.1 by merging Qwen/QwQ-32B chat vectors and further training, outputting answers following a `<think>`-guided thought process.
  - Downloads: 632
- [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK)
  - FlexDreamHK is an anime-style diffusion model‚Äîbuilt by merging and openly documenting fine-tuned Stable Diffusion/Wifu Diffusion and distilled LoRA data‚Äîoptimized for generating solo, cute girls with thick outlines, moderate NSFW content, and responsive prompts.
  - Downloads: 468
- [lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese](https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese)
  - This repository provides a Japanese-focused fine-tune of the DeepSeek-R1-D model, addressing inconsistencies in Japanese output often found in the original bilingual (English/Chinese) DeepSeek models.
  - Downloads: 372
- [mmnga/Phi-4-reasoning-plus-gguf](https://huggingface.co/mmnga/Phi-4-reasoning-plus-gguf)
  - This repository provides a GGUF-formatted version of Microsoft's Phi-4-reasoning-plus language model, utilizing the TFMC/imatrix-dataset-for-japanese-llm for its data and intended for use with llama.cpp.
  - Downloads: 292
- [Mizuiro-sakura/luke-large-commonsenseqa-japanese](https://huggingface.co/Mizuiro-sakura/luke-large-commonsenseqa-japanese)
  - This repository provides a fine-tuned version of luke-japanese-large for the JCommonsenseQA task, achieving high accuracy (83.82%) on commonsense question answering.
  - Downloads: 99
- [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick)
  - This repository provides a Japanese Natural Language Inference model, trained on JGLUE-JNLI & JSICK datasets, that classifies sentence pair relationships as contradiction, entailment, or neutral using SentenceTransformers.
  - Downloads: 79
- [elyza/ELYZA-Thinking-1.0-Qwen-32B](https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B)
  - ELYZA-Thinking-1.0-Qwen-32B is a Japanese reasoning model built upon Qwen/Qwen2.5-32B-Instruct and improved via post-training with synthetic, long Chain of Thought data generated using Monte Carlo Tree Search.
  - Downloads: 56
- [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-tiny-japanese model for CommonsenseQA tasks, trained on the JGLUE/JCommonsenseQA dataset.
  - Downloads: 20
- [nlp-waseda/tacomet-gpt2-xl-japanese](https://huggingface.co/nlp-waseda/tacomet-gpt2-xl-japanese)
  - TaCOMET_ja is a Japanese commonsense knowledge model finetuned from COMET on the Japanese TimeATOMIC dataset using causal language modeling, detailed in a LREC-COLING2024 paper and processed with Juman++ and SentencePiece.
  - Downloads: 18
- [HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1](https://huggingface.co/HachiML/Llama-3.1-Swallow-8B-v0.2-reasoningvector-deepseek-r1)
  - This model merges reasoning abilities from DeepSeek-R1-Distill-Llama-8B into the Japanese Llama-3.1-Swallow-8B-v0.2, enhancing its reasoning capabilities.
  - Downloads: 17
- [Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-juman-finetuned-commonsenseqa)
  - This repository provides a Japanese DeBERTa-v2-base model fine-tuned for CommonsenseQA using the JGLUE/JCommonsenseQA dataset, requiring Juman for morphological analysis.
  - Downloads: 16
- [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa)
  - This repository provides a fine-tuned DeBERTa-v2-base-Japanese model for CommonsenseQA tasks, trained using the JGLUE/JCommonsenseQA dataset.
  - Downloads: 15
- [Mizuiro-sakura/luke-japanese-base-finetuned-jnli](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jnli)
  - This repository provides a fine-tuned luke-japanese-base model for Japanese Natural Language Inference (JNLI) tasks, achieving 89.77% accuracy on the JGLUE dataset.
  - Downloads: 14
- [Mizuiro-sakura/luke-japanese-base-commonsenseqa](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-commonsenseqa)
  - This repository provides a highly accurate (80.07%) Japanese language model, fine-tuned from luke-japanese-base using the JGLUE JCommonsenseQA dataset for multiple-choice commonsense question answering.
  - Downloads: 11
### Responsible & Trustworthy NLP
- [dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix](https://huggingface.co/dahara1/gemma-2-2b-jpn-it-gguf-japanese-imatrix)
  - Êú¨„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶ About this model.
  - Downloads: 816
- [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16)
  - This repository requires agreement to a License and Privacy Policy from Stability AI before access.
  - Downloads: 370
- [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition)
  - This repository provides a private demonstration project.
  - Downloads: 274
- [stabilityai/japanese-stablelm-2-instruct-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-instruct-1_6b)
  - This repository requires agreement to a License Agreement and acknowledges a Privacy Policy from Stability AI.
  - Downloads: 234
- [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before use.
  - Downloads: 116
- [TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-large-japanese-offensiveness-estimation)
  - This repository details a fine-tuned ‚Äútwhin-bert-large‚Äù model for classifying Japanese social media comments into offensive, gray-area, and non-offensive categories, achieving macro-averaged F1-score of 64.8% and accuracy of 66.1%.
  - Downloads: 109
- [TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation](https://huggingface.co/TomokiFujihara/luke-japanese-large-lite-offensiveness-estimation)
  - This repository provides a fine-tuned Japanese language model, based on studio-ousia/luke-japanese-large-lite, for classifying online comment offensiveness with 64.0% macro-averaged F-score and 65.0% accuracy.
  - Downloads: 106
- [TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation](https://huggingface.co/TomokiFujihara/twhin-bert-base-japanese-offensiveness-estimation)
  - This repository details a fine-tuned Twitter/twhin-bert-base model for classifying Japanese social media comments as non-offensive, gray-area, or offensive, achieving macro F1-score of 64.7% and accuracy of 65.6%.
  - Downloads: 106
- [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl)
  - This repository requires agreement to a License Agreement and acknowledgment of Stability AI's Privacy Policy before access.
  - Downloads: 74
- [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying)
  - This repository provides a Japanese language ELECTRA Small model finetuned for cyberbullying detection using data from online forums and Twitter, built upon a foundation pretrained with the YACIS corpus.
  - Downloads: 69
- [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion)
  - This repository provides an open-access model with a CreativeML OpenRAIL-M license, granting usage rights while prohibiting illegal/harmful outputs and assigning user accountability for generated content.
  - Downloads: 40
- [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection)
  - Riga_collection is a merged model, similar to HimawariMix, specializing in detailed backgrounds and visuals, tuned with ideas from "riga," and includes a standard VAE, but prohibits commercial use, resale, illegal outputs, and altered permissions upon sharing.
  - Downloads: 22
- [stabilityai/japanese-stablelm-2-base-1_6b](https://huggingface.co/stabilityai/japanese-stablelm-2-base-1_6b)
  - By clicking "Agree", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.
  - Downloads: 20
- [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying)
  - This repository provides a Japanese language ELECTRA model finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 15
- [DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct](https://huggingface.co/DeL-TaiseiOzaki/Tengentoppa-llm-jp-3.7B-reasoning-instruct)
  - llm-jp's instruction model facilitates instruction-following capabilities for Japanese language models.
  - Downloads: 14
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-cyberbullying)
  - This repository provides a Japanese-language ELECTRA Base model finetuned for cyberbullying detection using a combined dataset of BBS comments and Twitter data, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [hs-hf/m2v-LaBSE-distilled](https://huggingface.co/hs-hf/m2v-LaBSE-distilled)
  - m2v-LaBSE-distilled Model Card
  - Downloads: 12
### Sentiment Analysis
- [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime)
  - This repository provides a Japanese emotion analysis model, fine-tuned from Luke-japanese-large-lite on the wrime dataset, to detect eight emotions‚Äîjoy, sadness, anticipation, surprise, anger, fear, disgust, and trust‚Äîwithin text.
  - Downloads: 6,508
- [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model trained on the chABSA dataset, achieving 1.0 accuracy and F1 score, with specified training hyperparameters.
  - Downloads: 4,391
- [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony)
  - This repository provides a Japanese BERT Base model finetuned for both sentiment analysis and automatic irony detection, licensed under CC BY-SA 4.0.
  - Downloads: 664
- [c299m/japanese_stock_sentiment](https://huggingface.co/c299m/japanese_stock_sentiment)
  - This repository provides a sentiment analysis model trained on Japanese stock comments to identify bullish or bearish trends for investor and analyst support.
  - Downloads: 102
- [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying)
  - This repository provides a Japanese BERT Base model, finetuned for cyberbullying detection using a combined dataset and licensed under CC BY-SA 4.0.
  - Downloads: 75
- [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base)
  - This repository provides a Japanese BERT-based model, trained on translated Financial PhraseBank data, for analyzing sentiment in financial news as positive, negative, or neutral.
  - Downloads: 71
- [RPAmodels/PN-analysis](https://huggingface.co/RPAmodels/PN-analysis)
  - japanese-sentiment-analysis This model is the work of jarvisx17 and was trained from scratch on the chABSA dataset.
  - Downloads: 66
- [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis)
  - This repository provides a Japanese sentiment analysis model finetuned from scratch on the Japanese Sentiment Polarity Dictionary dataset, built upon the jarvisx17/japanese-sentiment-analysis pretrained model.
  - Downloads: 43
- [RikkaBotan/style_bert_vits2_jp_extra_cool_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_cool_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 37
- [RikkaBotan/style_bert_vits2_jp_extra_sweet_original](https://huggingface.co/RikkaBotan/style_bert_vits2_jp_extra_sweet_original)
  - X(Twitter) „Ç¢„Ç´„Ç¶„É≥„Éà „Åú„Å≤ÈÅä„Å≥„Å´„Åç„Å¶„Å≠„ÄÇ
  - Downloads: 36
- [LoneWolfgang/bert-for-japanese-twitter](https://huggingface.co/LoneWolfgang/bert-for-japanese-twitter)
  - This repository provides a Japanese BERT model pretrained on a Twitter corpus, optimized for Japanese social media tasks like sentiment analysis and defamation detection, and used as a base for further finetuned models.
  - Downloads: 20
- [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b)
  - This repository provides a fine-tuned calm-2-7b-chat model using the Tsukuyomi corpus for conversational AI, subject to the specified Tsukuyomi character and AI development plan licenses.
  - Downloads: 13
- [kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony](https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony)
  - This repository provides an ELECTRA Base Japanese language model finetuned for irony detection using a dataset of ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 13
- [kit-nlp/electra-small-japanese-discriminator-irony](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-irony)
  - This repository provides a Japanese ELECTRA-based model finetuned for irony detection using a dataset of ironic tweets, licensed under CC BY-SA 4.0.
  - Downloads: 11
- [kit-nlp/yacis-electra-small-japanese-irony](https://huggingface.co/kit-nlp/yacis-electra-small-japanese-irony)
  - YACIS ELECTRA Small Japanese is a finetuned language model for detecting irony and sarcasm in Japanese tweets, built upon the ELECTRA architecture and licensed under CC BY-SA 4.0.
  - Downloads: 11
### Information Retrieval
- [pkshatech/GLuCoSE-base-ja-v2](https://huggingface.co/pkshatech/GLuCoSE-base-ja-v2)
  - GLuCoSE v2 is a high-performance, CPU-runnable Japanese text embedding model optimized for semantic similarity and retrieval tasks, achieving top results on benchmarks like MIRACL.
  - Downloads: 17,048
- [JujoHotaru/lora](https://huggingface.co/JujoHotaru/lora)
  - This repository distributes LoRA models created by Hotaru Jujo, released under a dual license of MIT or CreativeML Open RAIL-M with no usage restrictions but social media credit appreciated.
  - Downloads: 7,607
- [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT)
  - JaColBERT v1 is a Japanese document retrieval model based on ColBERT, achieving near-multilingual performance despite evaluation on out-of-domain datasets and surpassing previous Japanese models.
  - Downloads: 765
- [llm-book/bert-base-japanese-v3-bpr-passage-aio](https://huggingface.co/llm-book/bert-base-japanese-v3-bpr-passage-aio)
  - This repository provides a passage encoder for a BERT-based Bi-directional Preference Ranking (BPR) document retrieval model, fine-tuned from cl-tohoku/bert-base-japanese-v3 using llm-book/aio-retriever, as detailed in chapter 9 of ‚ÄúÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ‚Äù.
  - Downloads: 31
### Information Retrieval and Information Extracrtion & Text Mining
- [classla/xlm-roberta-base-multilingual-text-genre-classifier](https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier)
  - X-GENRE classifier - multilingual text genre classifier Text classification model based on xlm-roberta-base and fine-tuned on a combination of three genre datasets: Slovene GINCO dataset (Kuzman et al.
  - Downloads: 2,946
- [Kendamarron/fineweb-edu-classifier-ja-v2](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja-v2)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßtohoku-nlp/bert-base-japanese-v3„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 25
- [Kendamarron/fineweb-edu-classifier-ja](https://huggingface.co/Kendamarron/fineweb-edu-classifier-ja)
  - HuggingFaceFW/fineweb-edu-classifier„ÇíÂÜçÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„Éá„Éº„Çø„Åßpkshatech/GLuCoSE-base-ja„ÇíÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ
  - Downloads: 11
### Multilinguality and Text Generation
- [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)
  - ChatNTQ JA 7B V1.0 is a 7-billion parameter Japanese language model, fine-tuned for instruction following on Stability AI‚Äôs Japanese Stable LM Base Gamma 7B, achieving a JA MT-Bench score of 6.65.
  - Downloads: 12
## Datasets üß†

This list is sorted by downloads as of May 06, 2025.
509 datasets are listed.

### Information Extraction & Text Mining
- [Helsinki-NLP/tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba)
  - Tatoeba is a multilingual sentence and translation dataset accessible via the `load_dataset` function, supporting customizable language pairs and versions via language codes and date specification.
  - Downloads: 2,322
- [hotchpotch/fineweb-2-edu-japanese](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese)
  - FineWeb2 Edu Japanese is a high-quality, 89.3B token dataset of 120 million filtered educational Japanese texts, including subsets for sampling and shorter text lengths.
  - Downloads: 1,435
- [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean)
  - This repository provides a machine learning-ready dataset derived from Aozora Bunko, a Japanese public-domain ebook archive, with code available for reproduction.
  - Downloads: 779
- [kajuma/ABEJA-CC-JA](https://huggingface.co/datasets/kajuma/ABEJA-CC-JA)
  - ABEJA-CC-JA is a Hugging Face mirror of the AWS Open Data Registry dataset for Japanese image captioning and visual question answering.
  - Downloads: 581
- [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja)
  - This repository provides a filtered Japanese summarization dataset (XL-Sum) refined with PaLM 2's 15-gram overlap filter, containing 4215 training, 758 validation, and 766 test examples.
  - Downloads: 555
- [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick)
  - JSICK is a Japanese natural language inference and semantic textual similarity dataset created by translating the English SICK dataset, designed for researching multilingual compositional inference and model stress-testing.
  - Downloads: 512
- [kubota/defamation-japanese-twitter](https://huggingface.co/datasets/kubota/defamation-japanese-twitter)
  - This dataset provides 5,000 annotated Japanese tweets (Feb-Jun 2022) for detecting online defamation, labeling both the target of abuse and the type of abusive content.
  - Downloads: 507
- [shunk031/wrime](https://huggingface.co/datasets/shunk031/wrime)
  - WRIME is a dataset for emotional intensity estimation, featuring both self-reported and reader-annotated intensity levels of social media posts from 50 crowd-sourced participants.
  - Downloads: 429
- [taishi-i/awesome-japanese-nlp-multilabel-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-multilabel-dataset)
  - This Japanese dataset provides multi-label annotations of NLP research fields for GitHub repositories, utilizing repository content like descriptions, READMEs, and images for training a multi-label classification model.
  - Downloads: 285
- [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja)
  - This repository provides a sharded, Japanese-language subset of the CC100 dataset in Parquet format.
  - Downloads: 285
- [hotchpotch/fineweb-2-edu-japanese-noise-detect-raw](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-noise-detect-raw)
  - This dataset provides RAW text data from `fineweb-2-edu-japanese` with Unicode normalization (NFKC) and noise inference using `fineweb-2-japanese-text-cleaner`, including `noise_spans` indicating noisy text segments (length ‚â•4) above a 0.7 threshold, licensed under ODC-By.
  - Downloads: 227
- [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)
  - This repository provides a Japanese named entity recognition dataset, version 2.0, created by Stockmark and based on Wikipedia content, licensed under CC-BY-SA 3.0, and detailed in the 2021 ANLP proceedings.
  - Downloads: 212
- [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101)
  - This repository provides a Japanese-language Wikipedia dataset from January 1st, 2023, formatted as Parquet files generated using the `datasets` library.
  - Downloads: 184
- [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law)
  - This dataset provides 8.75K comprehensive records of Japanese laws‚Äîincluding number, title, effective date, and full text‚Äîsourced from e-Gov, deduplicated to the latest versions as of August 1, 2023.
  - Downloads: 182
- [RyokoExtra/JapaneseGoblin](https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin)
  - JapaneseGoblin is a JSONL dataset derived from the Touhou Wiki, intended for unsupervised text generation and potentially text classification, primarily in English with some Japanese content.
  - Downloads: 161
- [tet550/jawiki_sentences](https://huggingface.co/datasets/tet550/jawiki_sentences)
  - Jawiki Sentences is a Japanese Wikipedia-based dataset of cleaned sentences, each labeled with its article and section title, released under CC BY-SA 4.0 and GFDL licenses.
  - Downloads: 149
- [morinoko-inari/ruby-rails-ja-en](https://huggingface.co/datasets/morinoko-inari/ruby-rails-ja-en)
  - This repository provides a work-in-progress Japanese-English dataset sourced from Ruby and Ruby on Rails documentation, including synthetically generated data, for use in machine learning projects.
  - Downloads: 143
- [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja)
  - This repository provides the Japanese subset of the wiki40b dataset, formatted as three parquet files generated via a provided Python script.
  - Downloads: 141
- [llm-jp/AnswerCarefully](https://huggingface.co/datasets/llm-jp/AnswerCarefully)
  - AnswerCarefully is a commercially-usable dataset of potentially harmful prompts in Japanese and other languages designed to improve LLM safety, with restrictions against misuse and redistribution but allowing derivative works under specified conditions.
  - Downloads: 139
- [systemk/washi](https://huggingface.co/datasets/systemk/washi)
  - This repository provides a focused Japanese text dataset, ‚ÄúWashi,‚Äù created by resampling approximately 5% of the CulturaX corpus to prioritize documents similar to XLSum and Aozora Bunko, aiming for improved language model performance.
  - Downloads: 137
- [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
  - Japanese-Heron-Bench is a 102-question benchmark dataset for evaluating Japanese Vision-Language Models, featuring 21 images categorized by conversation, detail, complexity, and seven subcategories like anime, culture, and landscape.
  - Downloads: 132
- [matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B](https://huggingface.co/datasets/matsuo-lab/JP-LLM-Corpus-PII-Filtered-10B)
  - This dataset provides approximately 10 billion Japanese text tokens filtered from CommonCrawl to remove sensitive personal information (PII) using rule-based and machine learning techniques, released under a CC license with the caveat that complete PII removal isn't guaranteed.
  - Downloads: 126
- [MomoyamaSawa/Voice-KusanagiNene](https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene)
  - This repository provides a labeled dataset of voice clips from Nene Hoshino (CV: Machico) from *Project Sekai Colorful Stage! feat. Hatsune Miku*, with plans for future expansion and standardization, and a linked QQ group for a full character dataset.
  - Downloads: 125
- [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset)
  - This dataset provides Japanese named entity recognition (NER) training data extracted from Wikipedia, licensed under CC-BY-SA 3.0 and developed by Stockmark Inc.
  - Downloads: 119
- [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset)
  - This dataset classifies GitHub repository descriptions as relevant (1) or not relevant (0) to Japanese natural language processing, using pre-2022 data for training and 2023 data for testing.
  - Downloads: 107
- [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus)
  - This dataset is the livedoor News Corpus, used in the book *Introduction to Large Language Models*, providing news articles from livedoor News under a CC BY-ND 2.1 JP license for Named Entity Recognition (NER) tasks.
  - Downloads: 100
- [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)
  - This dataset provides Japanese language data derived from the japanese-alpaca-lora project, requiring further documentation.
  - Downloads: 91
- [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset)
  - This dataset provides named entity recognition (NER) labels for Wikinews articles in Japanese, featuring 8 entity types and licensed under CC BY 2.5, specifically for use with the book *Introduction to Large Language Models*.
  - Downloads: 91
- [ikedachin/CC-news-2024-October-cleaned-cpt-set-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-cpt-set-250127)
  - This dataset contains cleaned Japanese news articles from September and October 2024, preprocessed with dates added and tokenized to ~1000 tokens for efficient continued pre-training with llm-jp/llm-jp-3-13b, targeting a 1024 output token limit.
  - Downloads: 90
- [kajuma/CC-news-2024-July-October-cleaned](https://huggingface.co/datasets/kajuma/CC-news-2024-July-October-cleaned)
  - This repository provides a cleaned Japanese news corpus of 612M tokens from Common Crawl (July-October 2024), extracted with Uzushio and filtered using pipeline_03a.conf, suitable for LLM training like llm-jp/llm-jp-13b-v1.0.
  - Downloads: 86
- [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1)
  - This repository provides the English/Japanese dataset used to train the shisa-7b-v1 language model, with further details available in that model's documentation.
  - Downloads: 76
- [litagin/ehehe-corpus](https://huggingface.co/datasets/litagin/ehehe-corpus)
  - This repository provides a dataset subject to a LICENSE agreement that users must acknowledge and review before use.
  - Downloads: 73
- [llm-jp/ac-self-inst](https://huggingface.co/datasets/llm-jp/ac-self-inst)
  - AnswerCarefully is a commercially-usable dataset of potentially harmful prompts‚Äîin Japanese and other languages‚Äîintended to improve LLM safety, with strict limitations against misuse and redistribution but allowing derivative works under specific conditions.
  - Downloads: 60
- [SakanaAI/ChouBun](https://huggingface.co/datasets/SakanaAI/ChouBun)
  - ChouBun is a Japanese language benchmark evaluating long-context LLM performance across extractive QA and abstractive summarization tasks using multi-source documents and GPT/Claude-generated question-answer pairs.
  - Downloads: 54
- [mohamed-khalil/AnimeSongsLyrics](https://huggingface.co/datasets/mohamed-khalil/AnimeSongsLyrics)
  - This dataset provides anime song lyrics in Parquet format for research and enthusiast use, offering a structured collection of lyrics data.
  - Downloads: 54
- [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords)
  - This repository provides a list of 100 common Japanese stopwords extracted from CC-100 and Wikipedia, specifically designed for use with the nagisa text analysis library.
  - Downloads: 54
- [recruit-jp/japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)
  - This dataset provides images for evaluating Japanese image classification models across four tasks, including 101 types of Japanese food, under a CC-BY-4.0 license.
  - Downloads: 52
- [onewanto/sample-dataset-wikipedia-financial-terms](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-financial-terms)
  - This dataset provides a working sample of records from the January 2023 Japanese Wikipedia (range3/wikipedia-ja-20230101) specifically focusing on articles categorized under "Investment."
  - Downloads: 51
- [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese)
  - This Japanese dataset contains three-line summaries and indexing information for thousands of mycological taxonomy papers from the Daikinrin website, maintained by Atsushi Nakajima.
  - Downloads: 47
- [onewanto/sample-dataset-wikipedia-nikkei225](https://huggingface.co/datasets/onewanto/sample-dataset-wikipedia-nikkei225)
  - This dataset is a work sample containing records of articles related to the "Nikkei 225" category, extracted from the Japanese portion of the range3/wikipedia-ja-20230101 Wikipedia dataset in Parquet format.
  - Downloads: 47
- [numad/yuho-text-2024](https://huggingface.co/datasets/numad/yuho-text-2024)
  - This dataset contains excerpts from 2024 Japanese regulatory filings (Êúâ‰æ°Ë®ºÂà∏Â†±ÂëäÊõ∏) sourced from EDINET, including company information, dates, and unique identifiers like EDINET codes and JCNs.
  - Downloads: 47
- [oshizo/HSClustering-ja](https://huggingface.co/datasets/oshizo/HSClustering-ja)
  - This dataset provides clustered data for training and evaluating embedding models, derived from Japanese customs pre-ruling responses (item classification) with text combining item names and cargo summaries, labeled by HS code section, and split into train/test sets while maintaining label proportions.
  - Downloads: 43
- [mohamed-khalil/AnimeQuotes](https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes)
  - This repository provides a dataset of inspiring anime quotes with character attribution, sourced from Anime Motivation, formatted as a list of dictionaries for analysis and enjoyment.
  - Downloads: 43
- [community-datasets/covid_tweets_japanese](https://huggingface.co/datasets/community-datasets/covid_tweets_japanese)
  - This dataset provides 53,640 annotated Japanese tweets (January-June 2020) related to COVID-19, designed for text classification tasks and requiring Twitter API access for original tweet retrieval.
  - Downloads: 42
- [JunSotohigashi/JapaneseWikipediaTypoDataset](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset)
  - This repository provides a Hugging Face-compatible version of the Kyoto University Japanese Wikipedia Input Error Dataset (v2) for error detection and correction, licensed under CC-BY-SA 3.0.
  - Downloads: 41
- [Miwa-Keita/zenz-v2.5-dataset](https://huggingface.co/datasets/Miwa-Keita/zenz-v2.5-dataset)
  - This dataset, zenz-v2.5, comprises 190M pairs of kana-kanji conversion data (‚Äúleft context-input-output‚Äù) for training conditional language models, including zenz-v2.5 models of varying sizes, and includes the AJIMEE-Bench evaluation benchmark.
  - Downloads: 40
- [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese)
  - This Japanese dataset compiles manually extracted diagnostic characters‚Äîshared or differing traits‚Äîfrom thousands of mycological taxonomy papers summarized on the Daikinrin website.
  - Downloads: 39
- [ikedachin/CC-news-2024-October-cleaned-sft-250127](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-250127)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ9Êúà„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 37
- [Nan-Do/OpenSubtitlesJapanese](https://huggingface.co/datasets/Nan-Do/OpenSubtitlesJapanese)
  - This repository provides a cleaned, UTF-8 encoded Japanese subtitle dataset derived from OpenSubtitles, containing over 7000 movies/shows with text, timing, and metadata in Parquet format, prepared for use with Open Assistant.
  - Downloads: 36
- [FrancophonIA/Jibiki_fr_ja](https://huggingface.co/datasets/FrancophonIA/Jibiki_fr_ja)
  - Jibiki.fr provides a collaboratively built, large-coverage French-Japanese dictionary and aligned bilingual corpus sourced from multiple dictionaries (Cesselin, Raguet-Martin, JMdict) and Wikipedia, currently containing over 154,000 entries.
  - Downloads: 36
- [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k)
  - This repository expands the Japanese Cosmopedia dataset to 100k entries, incorporating contributions from kunishou, and includes translated text generation prompts‚Äîavailable at the linked Hugging Face dataset.
  - Downloads: 33
- [ikedachin/CC-news-2024-October-cleaned-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-1204)
  - This dataset contains Japanese news articles from September and October 2024, cleaned and adjusted to approximately 1000 tokens‚Äîoptimized for efficient learning with the llm-jp/llm-jp-3-13b tokenizer, assuming a 1024 output token limit.
  - Downloads: 32
- [ikedachin/CC-news-2024-October-cleaned-sft-1204](https://huggingface.co/datasets/ikedachin/CC-news-2024-October-cleaned-sft-1204)
  - kajuma/CC-news-2024-July-October-cleaned„ÇíÂÖÉ„Å´„ÄÅ10Êúà„ÅÆ„Éã„É•„Éº„Çπ„ÅÆ„Åø„ÇíÊäú„ÅçÂá∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ
  - Downloads: 32
- [hpprc/honyaku](https://huggingface.co/datasets/hpprc/honyaku)
  - This repository provides a manually translated, passage- and sentence-level English-Japanese dataset of Wikipedia introductions, created with a permissive license for machine learning use and leveraging CALM3-22B-Chat & Qwen 2.5 32B for reference while avoiding restrictive translation tool licenses.
  - Downloads: 31
- [labofsahil/animelist-dataset](https://huggingface.co/datasets/labofsahil/animelist-dataset)
  - This repository provides a JSON dataset of anime metadata with cross-references to popular anime platforms like MAL, AniList, and Kitsu, based on the Manami Project's offline database.
  - Downloads: 29
- [seiya/jp-disease-finding-dataset](https://huggingface.co/datasets/seiya/jp-disease-finding-dataset)
  - This dataset offers approximately 7,000 Japanese medical journal articles (2003-2023) with linked disease names, related findings/symptoms, supporting text, and article metadata in JSON-Lines format.
  - Downloads: 27
- [ryota39/open_preference-v0.3](https://huggingface.co/datasets/ryota39/open_preference-v0.3)
  - This repository provides a public Japanese RLHF dataset reformatted as a classification task (chosen/rejected sentences) built using synthetically generated text from Phi-3-medium, offering a resource for reward model training.
  - Downloads: 25
- [Sunbread/SyosetuNames-3.5M](https://huggingface.co/datasets/Sunbread/SyosetuNames-3.5M)
  - SyosetuNames-3.5M is a corpus of 3.5 million cleaned Japanese light novel character names from syosetu.com, designed for culturally aware NLP applications like NER and name generation.
  - Downloads: 23
- [Hoshikuzu/JParaCrawl](https://huggingface.co/datasets/Hoshikuzu/JParaCrawl)
  - JParaCrawl is a large, publicly available English-Japanese parallel corpus built by web crawling and automatic sentence alignment, accessible via the `datasets` library.
  - Downloads: 23
- [Nurture-intelligence/ins_dataset](https://huggingface.co/datasets/Nurture-intelligence/ins_dataset)
  - This synthetic question-answering dataset, built from the sakura_japanese_dataset, was generated using Nurture-intelligence/Gemma-2-108B-DPO-v0.1 with Pro-type inference, and is subject to both the original dataset's license and Gemma Terms of Use.
  - Downloads: 22
- [sergicalsix/Japanese_NER_Data_Hub](https://huggingface.co/datasets/sergicalsix/Japanese_NER_Data_Hub)
  - J-NER is a Japanese named entity recognition dataset comprising 1,570 examples of 157 entity types‚Äîsourced from Wikipedia and designed for large language model training‚Äîwith five positive and negative examples per entity.
  - Downloads: 22
- [ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets](https://huggingface.co/datasets/ganchengguang/Text-Classification-and-Relation-Event-Extraction-Mix-datasets)
  - Giellm is a Japanese general information extraction dataset built from the Livedoor News corpus, designed for training large language models with mutual reinforcement, as detailed in the linked arXiv paper.
  - Downloads: 21
- [oshizo/JMDNClustering-ja](https://huggingface.co/datasets/oshizo/JMDNClustering-ja)
  - This repository provides a clustered dataset, sourced and processed from Japan's PMDA website, for training and evaluating embedding models, featuring text data (‚Äúgeneric name‚Äù + ‚Äúdefinition‚Äù) and corresponding ‚Äúclassification code‚Äù labels, split into train/test sets with preserved label proportions.
  - Downloads: 21
- [hpprc/en-ja-align](https://huggingface.co/datasets/hpprc/en-ja-align)
  - This repository provides scripts to download, parse, and preprocess the publicly available en-ja-alignÊó•Ëã±ÂØæË®≥Êñá dataset (Uchiyama et al., 2003) for English-Japanese parallel text, without redistributing the data itself.
  - Downloads: 17
- [numad/yuho-text-2014-2022](https://huggingface.co/datasets/numad/yuho-text-2014-2022)
  - This repository provides a dataset of excerpts from Japanese securities reports (2014-2022) filed via EDINET, including company details, document metadata, and reporting periods.
  - Downloads: 17
- [Coaso/test-dolly-15ja-for-stftrainer](https://huggingface.co/datasets/Coaso/test-dolly-15ja-for-stftrainer)
  - This repository provides a JSONL conversion of the dolly-15k-ja dataset, formatted for use with the SFTTrainer, licensed under CC BY SA 3.0.
  - Downloads: 17
- [kanhatakeyama/AutoWikiQA](https://huggingface.co/datasets/kanhatakeyama/AutoWikiQA)
  - This project automatically generates question-and-answer pairs from Japanese Wikipedia articles using a Mixtral 8x22b GGUF (5bit) model, leveraging the TSUBAME4.0 supercomputer, though filtering may be needed due to potential hallucinations.
  - Downloads: 16
- [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp)
  - JBLiMP provides a JSONL dataset of validated linguistic minimal pairs for Japanese, used for benchmarking natural language processing models, as detailed in Someya and Oseki (2023).
  - Downloads: 16
- [AlienKevin/ndlbib-furigana](https://huggingface.co/datasets/AlienKevin/ndlbib-furigana)
  - This repository provides a furigana dataset derived from bibliographic data of the National Diet Library, available for download as a zipped archive.
  - Downloads: 16
- [tellarin-ai/ntx_llm_inst_japanese](https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_japanese)
  - This repository provides the Japanese subset of the NTX dataset converted to the Aya instruction format and licensed under CC-BY-SA 4.0, referencing the full instruction-formatted dataset on Hugging Face.
  - Downloads: 15
- [tamdiep106/autotrain-data-tam_jp](https://huggingface.co/datasets/tamdiep106/autotrain-data-tam_jp)
  - This AutoTrain dataset, created for the ‚Äútam_jp‚Äù project, provides Japanese (ja) language data instances structured as context-based examples for automated training.
  - Downloads: 13
- [ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM](https://huggingface.co/datasets/ganchengguang/Sentence-Classification-and-NER-Mix-Datasets-SCNM)
  - This repository provides the dataset and code for the SLG framework, a sentence-to-label generation approach for multi-task Japanese sentence classification and named entity recognition, detailed in the linked research paper.
  - Downloads: 13
- [BASF-AI/PubChemWikiJAPC](https://huggingface.co/datasets/BASF-AI/PubChemWikiJAPC)
  - This dataset provides English-Japanese paragraph pairs with binary labels indicating whether they refer to the same or different entities, extending the existing PubChem & Wikipedia classification task to a multilingual setting.
  - Downloads: 12
- [DancingPrismPJ/Wikipedia-Horse-Dataset](https://huggingface.co/datasets/DancingPrismPJ/Wikipedia-Horse-Dataset)
  - This dataset provides 600 Japanese sentences extracted from Wikipedia articles about thoroughbred racehorses, annotated with nine named entity types‚Äîincluding a specific ‚Äúracehorse name‚Äù tag‚Äîfor named entity recognition tasks.
  - Downloads: 12
- [wolf4032/token-classification-japanese-search-local-cuisine](https://huggingface.co/datasets/wolf4032/token-classification-japanese-search-local-cuisine)
  - This repository provides a dataset of cooking-related questions paired with keywords categorized by AREA, TYPE, SEASON, and INGREDIENT, alongside code for language model fine-tuning and application development using this data.
  - Downloads: 11
- [YANS-official/senryu-marusen](https://huggingface.co/datasets/YANS-official/senryu-marusen)
  - This dataset contains crawled data from the ‚ÄúKawaryu Toshu Marusen‚Äù Japanese senryu (short poem) submission site, comprising 5346 submissions across 376 prompts, formatted for text-to-text tasks.
  - Downloads: 11
- [p1atdev/oiocha](https://huggingface.co/datasets/p1atdev/oiocha)
  - This dataset contains 221 award-winning haiku poems from the Itoen Shinhaiku Grand Prix, including author/judge comments, translations, language tags, and image URLs.
  - Downloads: 11
- [ryota39/Aya_ja](https://huggingface.co/datasets/ryota39/Aya_ja)
  - Aya_ja is a Japanese instruction-following dataset containing 6,259 manually annotated input-output pairs for use in tasks like instruction tuning and language modeling.
  - Downloads: 11
### Multimodality
- [litagin/reazon-speech-v2-denoised](https://huggingface.co/datasets/litagin/reazon-speech-v2-denoised)
  - Reazon Speech v2 DENOISED Reazon Speech v2„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíUVR„Çí‰ΩøÁî®„Åó„Å¶BGM„ÇÑ„Éé„Ç§„Ç∫Èô§Âéª„Åó„Åü„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éü„É©„Éº„Åß„Åô„ÄÇ
  - Downloads: 8,174
- [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset with detailed, community-sourced tags (avg. 30 per image) suitable for training image classification and multi-label tagging models.
  - Downloads: 4,016
- [Elite35P-Server/EliteVoiceProject](https://huggingface.co/datasets/Elite35P-Server/EliteVoiceProject)
  - This project creates a non-official dataset of voice data from Hololive VTuber Sakura Miko for use in applications like speech recognition, adhering to the hololive production secondary creation guidelines and utilizing git-lfs for audio files.
  - Downloads: 1,110
- [JMMMU/JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
  - JMMMU is a Japanese multimodal benchmark designed to rigorously evaluate Large Multimodal Model (LMM) performance, addressing cultural biases present in existing benchmarks through expert-created, culture-agnostic questions.
  - Downloads: 1,106
- [tanganke/kmnist](https://huggingface.co/datasets/tanganke/kmnist)
  - KMNIST is a dataset of 70,000 handwritten Japanese characters (digits and kanji) for image classification tasks.
  - Downloads: 1,021
- [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech)
  - ReazonSpeech is a large, freely available 35,000+ hour Japanese audio dataset in FLAC format for Automatic Speech Recognition (ASR) research, intended for use under Japanese copyright law Article 30-4.
  - Downloads: 927
- [joujiboi/japanese-anime-speech-v2](https://huggingface.co/datasets/joujiboi/japanese-anime-speech-v2)
  - Japanese Anime Speech Dataset V2 is a 292,637-clip audio-text dataset from visual novels intended to improve automatic speech recognition accuracy, and is distinct from its V1 counterpart.
  - Downloads: 876
- [nyanko7/yandere2023](https://huggingface.co/datasets/nyanko7/yandere2023)
  - Yandere2023 is a 1.2 million image MIT-licensed anime illustration dataset with diverse, high-quality sources and tagging, intended for research and development.
  - Downloads: 799
- [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech)
  - Japanese-anime-speech is an audio-text dataset of transcribed dialogue from visual novels, intended to improve automatic speech recognition accuracy‚Äîparticularly for anime and similar Japanese media‚Äîusing models like OpenAI's Whisper.
  - Downloads: 758
- [ThePioneer/japanese-photos](https://huggingface.co/datasets/ThePioneer/japanese-photos)
  - This dataset provides diverse, high-quality images of Japan‚Äìspanning landscapes, culture, and daily life‚Äìcaptured in the 2020s for AI training.
  - Downloads: 694
- [alfredplpl/anime-with-caption-cc0](https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0)
  - This repository provides a CC-0 licensed dataset of AI-generated anime images with Japanese captions (including translations) created for ethical machine learning, featuring image, original prompt, and dense captions generated by Phi-3 models.
  - Downloads: 470
- [trojblue/sakugabooru2025](https://huggingface.co/datasets/trojblue/sakugabooru2025)
  - Sakugabooru2025 provides a dataset of over 240,000 curated animation clips ‚Äì primarily Japanese anime ‚Äì sourced from Sakugabooru.com, addressing the need for video data in the growing field of AI animation.
  - Downloads: 407
- [YANS-official/senryu-test](https://huggingface.co/datasets/YANS-official/senryu-test)
  - This dataset contains 100 Japanese *senryu* (short poems) ‚Äì 70 image-to-text and 30 text-to-text ‚Äì crawled from photo *senryu* websites and created by YANS committee members, designed for evaluating generative models.
  - Downloads: 264
- [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
  - JA-VG-VQA-500 is a 500-sample Japanese Visual Genome VQA dataset used to evaluate EvoVLM-JP-v1-7B, available under a Creative Commons license through the `datasets` library.
  - Downloads: 260
- [Silviase/Japanese-Heron-Bench](https://huggingface.co/datasets/Silviase/Japanese-Heron-Bench)
  - This dataset provides a cleaned version of the Japanese-Heron-Bench image, context, and question set for evaluating vision-language models in Japanese.
  - Downloads: 260
- [Marianoleiras/voxpopuli_es-ja](https://huggingface.co/datasets/Marianoleiras/voxpopuli_es-ja)
  - Dataset Dataset Summary This dataset is designed for automatic speech recognition (ASR) and translation tasks, enabling the conversion of Spanish speech into Japanese text.
  - Downloads: 254
- [jpft/danbooru2023](https://huggingface.co/datasets/jpft/danbooru2023)
  - Danbooru2023 is a 5+ million image anime dataset, meticulously crowdsourced and tagged with detailed metadata (average 30 tags/image) for tasks like image classification and multi-label tagging.
  - Downloads: 225
- [kadirnar/japanese-voice-combined](https://huggingface.co/datasets/kadirnar/japanese-voice-combined)
  - This repository provides a large, combined Japanese voice dataset of over 86,000 samples‚Äîsourced from StoryTTS, genshin-voice, and japanese-anime-speech‚Äîfor speech recognition, text-to-speech, and machine learning applications.
  - Downloads: 193
- [ayousanz/voicevox-voice-corpus](https://huggingface.co/datasets/ayousanz/voicevox-voice-corpus)
  - This repository provides a large-scale Japanese speech dataset built with VOICEVOX, utilizing the ITA, Tsukuyomi-chan, and ROHAN corpora, comprising 445,793 .wav files totaling over 577 hours of audio.
  - Downloads: 178
- [Lami/Lux-Japanese-Speech-Corpus](https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus)
  - Lux Japanese Speech Corpus provides a dataset of 96kHz/16bit Japanese speech recordings‚Äîboth raw and cleaned‚Äîspoken by a virtual character, along with corresponding transcripts and metadata under a CC BY 4.0 license.
  - Downloads: 173
- [efwkjn/reazonspeech_mtl](https://huggingface.co/datasets/efwkjn/reazonspeech_mtl)
  - This repository provides Japanese Automatic Speech Recognition (ASR) transcriptions generated using OpenAI's Whisper model on the Reazon Speech dataset, excluding the audio files.
  - Downloads: 173
- [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus)
  - This dataset provides approximately 39 million Japanese characters of high-quality text extracted from 1,924 research papers (including NLP2024) and 360 journal articles licensed under CC-BY, intended for pre-training language models and RAG applications.
  - Downloads: 168
- [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja)
  - This dataset provides English and Japanese captions generated by BLIP for Pok√©mon images from the FastGAN dataset, used for training Pok√©mon text-to-image models.
  - Downloads: 133
- [Emu-Academic/pjsk-emu-dataset](https://huggingface.co/datasets/Emu-Academic/pjsk-emu-dataset)
  - This dataset contains 2735 WAV audio clips extracted from the game *Project Sekai*'s character Emu Otori, intended for research use with So-vits-svc 4.0, and licensed under CC-BY-NC 4.0 with copyright retained by SEGA and voice actors.
  - Downloads: 123
- [YANS-official/senryu-test-with-references](https://huggingface.co/datasets/YANS-official/senryu-test-with-references)
  - This dataset contains Japanese *senryu* (short poem) data crawled from two websites, featuring 70 image-to-text and 30 text-to-text prompts with two curated responses each, for tasks involving generating poems from given prompts.
  - Downloads: 108
- [mesolitica/Speech-Translation-Instructions](https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions)
  - This repository provides translated speech instructions‚Äîderived from the Common Voice dataset across 120 languages‚Äîinto English, Arabic, Japanese, Mandarin, and French, ideal for fine-tuning Speech LLMs.
  - Downloads: 90
- [chitsanfei/pjsk-emu-dataset](https://huggingface.co/datasets/chitsanfei/pjsk-emu-dataset)
  - This dataset provides 2735 WAV audio clips of Emu Otori from Project Sekai for research using So-vits-svc 4.0, licensed under CC-BY-NC 4.0 with SEGA and voice actor Hina Kino retaining full rights.
  - Downloads: 78
- [YANS-official/ogiri-debug](https://huggingface.co/datasets/YANS-official/ogiri-debug)
  - This dataset facilitates evaluating large language models on three Japanese *okashi* (humorous response) generation tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîusing provided image and text prompts.
  - Downloads: 76
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k-formatted)
  - This dataset enhances the Aratako synthetic Japanese roleplay collection (based on DeepSeek-V3-0324) with system messages and formatting, released under the MIT license.
  - Downloads: 72
- [hhim8826/japanese-anime-speech-v2-split](https://huggingface.co/datasets/hhim8826/japanese-anime-speech-v2-split)
  - This repository provides a split of the Japanese anime speech dataset from joujiboi/japanese-anime-speech-v2, likely for easier use in machine learning or speech processing tasks.
  - Downloads: 67
- [kanhatakeyama/SyntheticTextOpenMathInstruct](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextOpenMathInstruct)
  - This repository provides a Japanese text corpus generated by Phi-3 from randomly sampled data, utilizing Tokyo Tech's TSUBAME4.0 supercomputer for some computations and based on the OpenMathInstruct-1-1.8m-ja code.
  - Downloads: 62
- [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP)
  - This corpus provides 120 participant audio recordings of telephone conversations in Japanese collected in the United States, requiring citation per TalkBank rules.
  - Downloads: 53
- [hotchpotch/fineweb-2-japanese-noise-spans](https://huggingface.co/datasets/hotchpotch/fineweb-2-japanese-noise-spans)
  - This dataset provides 330k labeled Japanese web text spans identified as noise (navigation, ads, etc.) using an LLM (cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese) and formatted as JSON for noise detection tasks.
  - Downloads: 50
- [TLME/Umamusume-voice-transcription](https://huggingface.co/datasets/TLME/Umamusume-voice-transcription)
  - This repository provides a transcribed dataset of voice lines from the game *Umamusume Pretty Derby*, totaling 77 characters and over 10,000 seconds of audio across various characters like Tokai Teio and Vodka.
  - Downloads: 50
- [jaCappella/jaCappella](https://huggingface.co/datasets/jaCappella/jaCappella)
  - jaCappella is a Japanese a cappella vocal ensemble corpus featuring scores and isolated audio tracks of six-part arrangements of public domain children's songs.
  - Downloads: 49
- [Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k)
  - This dataset provides approximately 1000 Japanese roleplaying instructions created by applying the Magpie method to NVIDIA‚Äôs Nemotron-4-340B-Instruct, built with DeepInfra, and may contain low-quality records due to minimal post-filtering.
  - Downloads: 49
- [umiyuki/JDocQA_SingleImage](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage)
  - JDocQA_SingleImage is a Japanese question answering dataset derived from JDocQA‚Äôs test split, consisting of single-image/question pairs with associated text, IDs, answers, answer types, and original PDF filepaths‚Äîoptimized for size and practicality.
  - Downloads: 46
- [ayousanz/japanese-music-emotion](https://huggingface.co/datasets/ayousanz/japanese-music-emotion)
  - This repository provides a dataset of Japanese music analyzed for emotion using Music2Emotion, featuring JSONL files with video metadata and predicted moods, valence, and arousal scores.
  - Downloads: 42
- [YANS-official/senryu-shashin](https://huggingface.co/datasets/YANS-official/senryu-shashin)
  - This dataset contains crawled data ‚Äì including images and text ‚Äì from the ‚ÄúHome Mate Senryu Grand Prize‚Äù photo-based senryu (short poem) contest, comprising 435 prompts and 1767 responses, intended for use within the YANS hackathon.
  - Downloads: 40
- [deepghs/fgo_voices_jp](https://huggingface.co/datasets/deepghs/fgo_voices_jp)
  - This repository provides a 66.4-hour Japanese voice dataset of 30,800 recordings from *Fate/Grand Order* characters, filtered to include only voices from single voice actors, for ASR/ASV model training and evaluation.
  - Downloads: 39
- [ayousanz/reazon-speech-v2-all-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-speechMOS-analyze)
  - This repository presents speech quality analysis results‚Äîspecifically MOS scores and transcriptions‚Äîfor the reazon-research/reazonspeech-v2 dataset, saved as a JSON file and visualized with histograms, utilizing resources provided by AiHUB.
  - Downloads: 38
- [Fhrozen/CABankSakura](https://huggingface.co/datasets/Fhrozen/CABankSakura)
  - This repository provides the Japanese Sakura Corpus, a dataset of 31 participants‚Äô audio recordings from Japan, requiring citation and adherence to TalkBank usage rules (DOI: 10.21415/T5M90R).
  - Downloads: 35
- [kunishou/HelpSteer-35k-ja](https://huggingface.co/datasets/kunishou/HelpSteer-35k-ja)
  - This repository provides a Japanese translation of the HelpSteer dataset, intended for experimenting with and aligning Large Language Models using NVIDIA‚Äôs SteerLM technique and NeMo Aligner.
  - Downloads: 35
- [JapanDegitalMaterial/Places_in_Japan](https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan)
  - This dataset provides CC0-licensed images of places in Japan for training text-to-image or other machine learning models without copyright concerns.
  - Downloads: 33
- [hotchpotch/msmarco-ja-hard-negatives](https://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives)
  - This repository provides Japanese MS MARCO data with hard negatives mined through normalization, filtering, and selection of queries and collections, and includes SPLADE model training for comparison with mMARCO performance.
  - Downloads: 33
- [oshizo/japanese-text-image-retrieval-train](https://huggingface.co/datasets/oshizo/japanese-text-image-retrieval-train)
  - This dataset pairs images of PDF pages (created from JDocQA training data and resized to 896px, 700px, or 588px) with OCR text (processed using NDLOCR, potentially containing "„Äì" for failed reads) and question-answer pairs generated by Qwen/Qwen2.5-14B-Instruct to train image retrieval models.
  - Downloads: 32
- [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA)
  - Japanese LLaVA Instruct 150K is a 150K Japanese-translated version of the LLaVA visual instruction dataset, licensed under CC BY-NC-4.0 and adhering to OpenAI‚Äôs terms.
  - Downloads: 28
- [oshizo/LawClustering-ja](https://huggingface.co/datasets/oshizo/LawClustering-ja)
  - This repository provides a clustered dataset of 6,127 Japanese legal documents (from e-Gov XML data, specifically ‚ÄúHeisei‚Äù or ‚ÄúReiwa‚Äù eras) with text from law titles and provisions, labeled by legal classification, and split into train/test sets maintaining label proportions.
  - Downloads: 28
- [Kendamarron/japanese-photo-instruction](https://huggingface.co/datasets/Kendamarron/japanese-photo-instruction)
  - This synthetic dataset, built using images from ThePioneer/japanese-photos, was generated with Qwen/Qwen2-VL-7B-Instruct and Qwen/Qwen2.5-32B-Instruct-AWQ models.
  - Downloads: 27
- [ThePioneer/japanese-photos-2-with-vids](https://huggingface.co/datasets/ThePioneer/japanese-photos-2-with-vids)
  - This dataset provides high-quality images and videos of diverse Japanese landscapes, culture, and daily life, captured primarily in the 2020s, for AI training and research.
  - Downloads: 26
- [Calvin-Xu/Furigana-Aozora-Speech](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora-Speech)
  - This dataset provides a furigana-annotated speech corpus derived from Aozora Bunko and Sapie audio Daisy data, containing 3,361,443 cleaned entries with kanji.
  - Downloads: 26
- [JapanDegitalMaterial/Scenery_of_japan](https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan)
  - This dataset provides CC0 licensed images of Japanese scenery for training text-to-image models and other applications, serving as a template for new dataset creation.
  - Downloads: 24
- [YANS-official/ogiri-test](https://huggingface.co/datasets/YANS-official/ogiri-test)
  - This dataset from Bokete, a Japanese humor site, provides 56 images and 44 text prompts for three multimodal tasks‚Äîimage-to-text, text-to-text, and text-image-to-text‚Äîderived from the CLoT-Oogiri-Go dataset and detailed in a CVPR2024 project.
  - Downloads: 22
- [hotchpotch/jaqket_cc](https://huggingface.co/datasets/hotchpotch/jaqket_cc)
  - This dataset comprises quiz data from JAQKET, specifically those released under the CC-BY-SA-4.0 license, intended for use with the `datasets` library and originating from the AI-Oh competition.
  - Downloads: 22
- [hpprc/tanaka-corpus](https://huggingface.co/datasets/hpprc/tanaka-corpus)
  - This repository provides a Hugging Face Datasets version of the Tanaka Corpus, preprocessed into a dataset of Japanese-English sentence pairs with unique IDs.
  - Downloads: 21
- [ayousanz/common-voice-speechMOS-analyze](https://huggingface.co/datasets/ayousanz/common-voice-speechMOS-analyze)
  - This repository presents speech quality analysis results (MOS scores) for the Common Voice Corpus 17.0, stored as a JSON file with counts of audio files exceeding various SNR thresholds, and computed using resources provided by AiHUB.
  - Downloads: 20
- [yutakobayashi/diet-members-voice-embeddings](https://huggingface.co/datasets/yutakobayashi/diet-members-voice-embeddings)
  - This repository provides a dataset of voice embeddings for Japanese Diet (parliament) members, created using speechbrain/spkrec-ecapa-voxceleb, suitable for speaker separation and analysis tasks like parliamentary & speech analytics.
  - Downloads: 17
- [davidstap/kanji_definitions](https://huggingface.co/datasets/davidstap/kanji_definitions)
  - This dataset provides 6315 1024x1024 PNG images of Kanji characters, paired with their textual definitions, adapted from the KanjiVG project.
  - Downloads: 15
- [toshi456/LLaVA-JP-Instruct-108K](https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K)
  - LLaVA JP Instruct 108K is a 108,000-instruction dataset, formatted for LLaVA, created from Japanese Visual Genome VQA and docci_ja data under an Apache 2.0 license.
  - Downloads: 15
- [oshizo/ASRClustering-ja](https://huggingface.co/datasets/oshizo/ASRClustering-ja)
  - This dataset provides clustered text data‚Äîextracted from EDINET business descriptions and labeled with industry codes‚Äîfor training and evaluating embedding models, split into train/test sets with balanced label proportions.
  - Downloads: 13
- [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions)
  - This dataset filters high-quality records‚Äîtotaling 5,475 with Apache 2.0, CC-BY-SA-3.0, and MIT licenses‚Äîfrom oasst1-89k-ja, databricks-dolly-15k-ja, and hh-rlhf-49k-ja, specifically for JGLUE benchmark tasks (JcommonsenseQA, MARC-ja, JSQuAD).
  - Downloads: 12
- [BigleBomb/japanese-vet-terms](https://huggingface.co/datasets/BigleBomb/japanese-vet-terms)
  - This dataset provides Japanese audio and transcriptions of veterinary medicine terminology‚Äîcovering drugs, diseases, and symptoms‚Äîfor training speech recognition or natural language processing models.
  - Downloads: 12
- [Calvin-Xu/FLFL-Aozora-Speech-Train](https://huggingface.co/datasets/Calvin-Xu/FLFL-Aozora-Speech-Train)
  - This repository provides a cleaned and filtered dataset of 2,536,041 entries with furigana annotations derived from the Aozora Bunko and SAPIE audiobooks, improved from the original flawed Whisper transcriptions and used for training Calvin-Xu/FLFL.
  - Downloads: 11
- [toshi456/Rakuten-Alpaca-Data-32K](https://huggingface.co/datasets/toshi456/Rakuten-Alpaca-Data-32K)
  - This repository provides a 32,000-item Japanese instruction dataset, Rakuten-Alpaca-Data-32K, automatically generated using RakutenAI-7B-chat inspired by Stanford Alpaca, and built upon the seed data `seed_tasks_japanese.jsonl`‚Äîfiltering is recommended due to potential quality issues.
  - Downloads: 11
- [sin2piusc/jgca_v2_50k_2](https://huggingface.co/datasets/sin2piusc/jgca_v2_50k_2)
  - common voice, google fleurs, JSUTv1.1, JAS_v2 (joujiboi/japanese-anime-speech-v2)
  - Downloads: 11
### Multilinguality
- [defunct-datasets/amazon_reviews_multi](https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi)
  - This dataset provides multilingual Amazon product reviews (English, Japanese, German, French, Chinese, Spanish) from 2015-2019 for text classification, but is currently inaccessible due to data provider decisions.
  - Downloads: 1,476
- [lmg-anon/vntl-leaderboard](https://huggingface.co/datasets/lmg-anon/vntl-leaderboard)
  - This repository benchmarks Large Language Models (LLMs) on English translation quality of Japanese Visual Novels, providing a comparative leaderboard alongside established translation tools.
  - Downloads: 1,427
- [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval)
  - JHumanEval is a manually revised, Japanese translation of the HumanEval code generation benchmark, designed to evaluate Japanese LLM's ability to generate code from potentially imperfect documentation.
  - Downloads: 1,037
- [llm-jp/relaion2B-en-research-safe-japanese-translation](https://huggingface.co/datasets/llm-jp/relaion2B-en-research-safe-japanese-translation)
  - This repository provides a Japanese translation of the ReLAION-2B-en-research-safe English dataset, created using the Gemma-2-9b-it LLM and the vLLM inference library via the text2dataset tool.
  - Downloads: 1,003
- [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives)
  - This repository provides Japanese queries from the MMarco dataset, paired with hard negatives retrieved using both multilingual e5 and BM25 (Anserini) models for information retrieval research.
  - Downloads: 389
- [hpprc/kaken-translations-ja-en](https://huggingface.co/datasets/hpprc/kaken-translations-ja-en)
  - llm-jp-corpus-v3„ÅÆkaken„Çµ„Éñ„Çª„ÉÉ„Éà‰∏≠„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅQwen/Qwen2.5-32B-Instruct„ÇíÁî®„ÅÑ„Å¶Êó•Êú¨Ë™û„Åã„ÇâËã±Ë™û„Å´ÁøªË®≥„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 365
- [hotchpotch/ms_marco_japanese](https://huggingface.co/datasets/hotchpotch/ms_marco_japanese)
  - This repository provides a Japanese translation of the MS MARCO dataset, created using google/madlad400-3b-mt and structured like the original, though translation quality is limited and comparison with higher-quality datasets like mMARCO is recommended.
  - Downloads: 214
- [p1atdev/gsm8k-ja-slim](https://huggingface.co/datasets/p1atdev/gsm8k-ja-slim)
  - This repository provides a Japanese translation of the GSM8K reasoning dataset, along with extracted answers, utilizing a quantized language model and addressing potential data inconsistencies.
  - Downloads: 178
- [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli)
  - This repository provides the filtered training and development sets from the JSNLI v1.1 dataset, used in the book *Introduction to Large Language Models*, licensed under CC BY-SA 4.0.
  - Downloads: 176
- [ayousanz/OSCOR-2301-ja-cleaned](https://huggingface.co/datasets/ayousanz/OSCOR-2301-ja-cleaned)
  - This dataset provides a cleaned Japanese subset of the OSCAR-2301 corpus‚Äîcontaining 94M Japanese words and 181.2GB of data‚Äîwith specific, unsuccessfully cleaned metadata files excluded.
  - Downloads: 160
- [haiFrHust/VNJPTranslate](https://huggingface.co/datasets/haiFrHust/VNJPTranslate)
  - This repository provides a 3.3 million row Vietnamese-Japanese parallel corpus for machine translation and natural language processing research.
  - Downloads: 143
- [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction)
  - ichikara-instruction (Non Commercial) LLM„ÅÆ„Åü„ÇÅ„ÅÆÊó•Êú¨Ë™û„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø ÂÖ¨Èñã„Éö„Éº„Ç∏ ÂÖ¨Èñã„Éö„Éº„Ç∏„Çà„Çä„ÄÅ Êú¨„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÄÅË®ÄË™ûÂá¶ÁêÜÂ≠¶‰ºöÁ¨¨ÔºìÔºêÂõûÂπ¥Ê¨°Â§ß‰ºö„Å´„Åä„ÅÑ„Å¶Áô∫Ë°®„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
  - Downloads: 117
- [Nexdata/English-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/English-Japanese_Parallel_Corpus_Data)
  - This repository provides a free 850,000 sentence sample of a larger paid English-Japanese parallel corpus covering diverse topics and suitable for machine translation and text analysis.
  - Downloads: 98
- [Nexdata/Japanese_Pronunciation_Dictionary](https://huggingface.co/datasets/Nexdata/Japanese_Pronunciation_Dictionary)
  - This repository provides a sample of a 101,702-entry Japanese pronunciation dictionary created by linguists, intended for research and development of Japanese Automatic Speech Recognition (ASR) technology, with a link to the full paid dataset.
  - Downloads: 94
- [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k)
  - This dataset provides sentence-aligned Japanese web novel chapters and English fan translations, including metadata like series titles and alignment scores, for document translation research.
  - Downloads: 86
- [hpprc/kaken-trans-ja-en](https://huggingface.co/datasets/hpprc/kaken-trans-ja-en)
  - This dataset provides Japanese-to-English translations of the kaken subset from the llm-jp-corpus-v3 using Qwen/Qwen2.5-32B-Instruct, aiming to create an open, parallel English-Japanese corpus under a CC-BY 4.0 license.
  - Downloads: 84
- [llm-jp/Synthetic-JP-EN-Coding-Dataset](https://huggingface.co/datasets/llm-jp/Synthetic-JP-EN-Coding-Dataset)
  - LLM-jp‚Äôs Synthetic-JP-EN-Coding-Dataset is an instruction tuning dataset‚Äîa subset of Aratako‚Äôs 801k dataset‚Äîcreated through a Japanese collaborative project for Japanese-English coding tasks.
  - Downloads: 73
- [Nexdata/Chinese-Japanese_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Chinese-Japanese_Parallel_Corpus_Data)
  - This repository provides a sample of a 9.83 million sentence pair Chinese-Japanese parallel corpus in TXT format, covering diverse fields and suitable for machine translation and text analysis.
  - Downloads: 72
- [umiyuki/Ani-Bench-JP](https://huggingface.co/datasets/umiyuki/Ani-Bench-JP)
  - Ani-Bench-JP is a Japanese anime knowledge benchmark dataset comprising 100 quiz questions‚Äî20 per anime‚Äîfrom five popular series, designed to evaluate LLM understanding in Japanese.
  - Downloads: 69
- [llm-jp/oasst2-33k-ja](https://huggingface.co/datasets/llm-jp/oasst2-33k-ja)
  - This repository hosts oasst2-33k-ja, a Japanese instruction tuning dataset translated from an English oasst2 subset using DeepL, built upon kunishou/oasst2-135k-ja and developed by the LLM-jp collaboration.
  - Downloads: 68
- [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja)
  - This repository provides the Japanese language portion of the Guanaco dataset, similar to related datasets like alpaca-guanaco-japanese-gpt-1b.
  - Downloads: 59
- [Nexdata/Japanese-English_Parallel_Corpus_Data](https://huggingface.co/datasets/Nexdata/Japanese-English_Parallel_Corpus_Data)
  - This repository provides a 380,000-sentence sample of a larger, paid Japanese-English parallel corpus suitable for machine translation, text analysis, and NLU tasks, excluding sensitive content.
  - Downloads: 59
- [liboaccn/OPUS-MIT-5M](https://huggingface.co/datasets/liboaccn/OPUS-MIT-5M)
  - OPUS-MIT-5M is a balanced, 5-million sentence pair dataset from the OPUS corpus designed for robust multilingual image translation across 20 languages.
  - Downloads: 56
- [DataLabX/ScreenTalk_JA](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA)
  - ScreenTalk_JA is a 30-hour dataset of ~10,000 Japanese audio-Chinese text pairs from movies/TV shows, designed for speech translation and multilingual speech understanding research, released under CC BY 4.0.
  - Downloads: 56
- [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus)
  - This repository provides a quality-filtered, 1 million row subset of the JParaCrawl v3 English-Japanese parallel corpus, addressing issues with alignment and completeness in the original dataset.
  - Downloads: 55
- [izumi-lab/piqa-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/piqa-ja-mbartm2m)
  - This repository provides the Japanese translation of the PIQA dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed identically to the original PIQA.
  - Downloads: 53
- [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja)
  - This dataset provides corrections and translations for the Japanese portion of the multilingual LLAVA-Bench-in-the-wild benchmark, building upon the original liuhaotian/llava-bench-in-the-wild data.
  - Downloads: 53
- [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja)
  - hh-rlhf-12k-ja is a 12,000-entry Japanese human preference dataset, translated from hh-rlhf via DeepL, comprising random samples from its harmless, helpful, and rejection-sampled training splits.
  - Downloads: 51
- [FrancophonIA/XFUND](https://huggingface.co/datasets/FrancophonIA/XFUND)
  - XFUND is a multilingual form understanding benchmark dataset featuring human-labeled key-value pairs across seven languages, designed for evaluating form analysis capabilities.
  - Downloads: 50
- [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01)
  - This repository provides a deduplicated and randomized dataset of English-Japanese sentence pairs sourced from Tatoeba.org, optimized for machine translation or language learning.
  - Downloads: 49
- [NilanE/SmallParallelDocs-Ja_En-6k](https://huggingface.co/datasets/NilanE/SmallParallelDocs-Ja_En-6k)
  - This repository provides a Japanese-English parallel text dataset for translation, licensed under Apache 2.0 with potential source-specific restrictions, including metadata indicating fanfiction sources and missing data.
  - Downloads: 45
- [saillab/alpaca_japanese_taco](https://huggingface.co/datasets/saillab/alpaca_japanese_taco)
  - This repository provides the TaCo dataset for cross-lingual instruction tuning, formatted with instruction, input, and multi-lingual output pairs, as detailed in the linked research paper.
  - Downloads: 45
- [FreedomIntelligence/MMLU_Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)
  - This repository provides a Japanese translation of the MMLU dataset, created using GPT-3.5-turbo, for use in multilingual research like MultilingualSIFT.
  - Downloads: 41
- [Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Translation-Dataset-Magpie-Nemotron-4-20k)
  - This repository provides a 20,000-record Japanese-English translation dataset generated using the Magpie technique applied to Nvidia's Nemotron-4-340B-Instruct, alongside the code used for its creation, noting the dataset may contain low-quality records due to minimal post-filtering.
  - Downloads: 39
- [Mitsua/wikidata-parallel-descriptions-en-ja](https://huggingface.co/datasets/Mitsua/wikidata-parallel-descriptions-en-ja)
  - This repository provides a parallel English-Japanese corpus derived from Wikidata dumps, filtered for machine translation and formatted for use with Hugging Face transformers.
  - Downloads: 38
- [mpasila/ParallelFiction-Ja_En-100k-json](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-json)
  - This repository provides a JSON conversion of the NilanE/ParallelFiction-Ja_En-100k dataset‚Äîsentence-aligned Japanese/English web novel chapters‚Äîformatted for use in training translation models with text-generation-webui.
  - Downloads: 37
- [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k)
  - This dataset is a modified version of kunishou/hh-rlhf-49k-ja, excluding examples where `ng_translation` is equal to 1.
  - Downloads: 37
- [turing-motors/LLaVA-Pretrain-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Pretrain-JA)
  - Japanese LLaVA Pretrain is a Japanese-translated version of the original LLaVA dataset, enabling similar multi-modal research applications for the Japanese language, subject to the original dataset's licensing (CC-3M, BLIP).
  - Downloads: 37
- [joujiboi/bluemoon-fandom-1-1-rp-jp-translated](https://huggingface.co/datasets/joujiboi/bluemoon-fandom-1-1-rp-jp-translated)
  - This repository provides a Japanese translation of a cleaned ‚Äúbluemoon-fandom-1-1-rp‚Äù dataset, utilizing the command-r-08-2024 model via the openrouter API for faster, resource-efficient, and uncensored NSFW translation.
  - Downloads: 37
- [aixsatoshi/Longcontext-aozora-instruction](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-instruction)
  - This repository provides a long-text instruction dataset built from the Aozora Bunko corpus, primarily aimed at showcasing question-answering styles with long documents, and intentionally lacks answer filtering to preserve challenging examples for fine-tuning.
  - Downloads: 37
- [yubo0306/fed_ja](https://huggingface.co/datasets/yubo0306/fed_ja)
  - This repository provides a Japanese translation of the FED dataset using the Google Cloud Translate API v2, with a caveat that some dimensions may have inconsistencies due to machine translation and require careful consideration.
  - Downloads: 37
- [llm-jp/mbpp-ja](https://huggingface.co/datasets/llm-jp/mbpp-ja)
  - This repository offers a Japanese translation of the MBPP dataset, created using LLM-jp and DeepL, with contributions from Han, Otake, Ozaki, and Miyao.
  - Downloads: 35
- [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent)
  - CoTangent is a manually created, high-quality Japanese dataset of 100 Chain-of-Thought examples, available in both connected and separated CoT/output formats, with the connected format exhibiting more natural transitions.
  - Downloads: 34
- [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation)
  - This repository provides paired Japanese and Vietnamese sentences for machine translation and language learning purposes.
  - Downloads: 34
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k-50k)
  - This dataset provides 50,000 English sentences extracted from the larger 801k Synthetic Japanese-English coding dataset, referencing the original dataset‚Äôs documentation for details and usage notes.
  - Downloads: 30
- [tellarin-ai/llm-japanese-dataset-vanilla-aya-format](https://huggingface.co/datasets/tellarin-ai/llm-japanese-dataset-vanilla-aya-format)
  - This repository provides a 2.46M-row Japanese instruction-tuning dataset in Aya format, converted from v1.0.0 and released under CC-BY-SA 4.0.
  - Downloads: 26
- [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja)
  - LIMA-JA is a Japanese translation and minorly adjusted version of Meta's LIMA dataset, accessible via the `datasets` library for conversational AI research.
  - Downloads: 25
- [DataLabX/ScreenTalk_JA2ZH-XS](https://huggingface.co/datasets/DataLabX/ScreenTalk_JA2ZH-XS)
  - ScreenTalk_JA is a 30-hour Japanese speech and Simplified Chinese text dataset of ~10,000 samples designed for speech translation and multilingual speech understanding research, released under CC BY 4.0.
  - Downloads: 24
- [Hoshikuzu/Tanaka-corpus](https://huggingface.co/datasets/Hoshikuzu/Tanaka-corpus)
  - This repository provides the Tanaka-corpus, a publicly available Japanese-English parallel text dataset compiled by Professor Yasuhito Tanaka and used for machine translation research.
  - Downloads: 24
- [aixsatoshi/Longcontext-aozora-summary](https://huggingface.co/datasets/aixsatoshi/Longcontext-aozora-summary)
  - This dataset provides summaries for long texts sourced from the Aozora Bunko clean dataset, licensed under CC BY 4.0.
  - Downloads: 24
- [izumi-lab/sciq-ja-mbartm2m](https://huggingface.co/datasets/izumi-lab/sciq-ja-mbartm2m)
  - This repository provides the Japanese translation of the SciQ dataset, created using the facebook/mbart-large-50-many-to-many-mmt model and licensed under CC-BY-NC 3.0.
  - Downloads: 23
- [oshizo/japanese-wikipedia-paragraphs-embeddings](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs-embeddings)
  - This repository provides a Faiss index and SentenceTransformer model for semantic search within the Japanese Wikipedia paragraphs dataset, vectorized using the intfloat/multilingual-e5-base model.
  - Downloads: 22
- [saillab/alpaca-japanese-cleaned](https://huggingface.co/datasets/saillab/alpaca-japanese-cleaned)
  - This repository provides the TaCo dataset used for research on improving cross-lingual transfer and chain-of-thought reasoning in low-resource language LLMs, requiring citation of the associated ICLR 2024 paper.
  - Downloads: 21
- [mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context](https://huggingface.co/datasets/mpasila/ParallelFiction-Ja_En-100k-alpaca-4k-context)
  - This repository provides a chunked, Alpaca-formatted version of the ParallelFiction-Ja_En-100k dataset‚ÄîJapanese web novel chapters with English translations‚Äîoptimized for the augmxnt/shisa-base-7b-v1 model.
  - Downloads: 20
- [karakuri-ai/corrected-mt-bench-ja](https://huggingface.co/datasets/karakuri-ai/corrected-mt-bench-ja)
  - This repository provides a corrected Japanese translation of MT-Bench, utilizing and building upon Stability AI's Japanese MT-Bench dataset.
  - Downloads: 19
- [hpprc/alt-parallel-en-ja](https://huggingface.co/datasets/hpprc/alt-parallel-en-ja)
  - This dataset provides the Japanese-English parallel portion of the Asian Language Treebank (ALT) corpus, sourced from Hugging Face datasets, and cites the original 2016 ALT introduction paper.
  - Downloads: 18
- [kurogane/DSR1D-Llama-8B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-Llama-8B-aya-ja-1k-generated)
  - This repository provides 1000 generated responses from DeepSeek-R1-Distill-Llama-8B fine-tuned on aya-ja-evol-instruct-calm3-dpo, created with 8-bit quantization and intended for reference or pre-processing due to potential inaccuracies and formatting issues.
  - Downloads: 18
- [werty1248/OpenOrca-EnKoZhJa-18k](https://huggingface.co/datasets/werty1248/OpenOrca-EnKoZhJa-18k)
  - This repository provides a multilingual dataset of Korean, Chinese, and Japanese translations derived from OpenOrca, aligned by ID and prioritizing the most similar translations based on embedding similarity.
  - Downloads: 16
- [toshi456/ViQuAE-JA](https://huggingface.co/datasets/toshi456/ViQuAE-JA)
  - This repository provides a Japanese translation of the ViQuAE dataset, focusing on translating the original answers into Japanese while leaving the answers themselves unchanged.
  - Downloads: 15
- [Moleys/Filtered-Japanese-English-Parallel-Corpus](https://huggingface.co/datasets/Moleys/Filtered-Japanese-English-Parallel-Corpus)
  - This repository provides a Python function (`prompt`) for rigorously evaluating the accuracy and quality of Japanese-to-English translations based on strict criteria like completeness, grammar, and overall quality.
  - Downloads: 15
- [U23-lab/wiki40b_qa_ja](https://huggingface.co/datasets/U23-lab/wiki40b_qa_ja)
  - This repository provides a question-answering dataset generated from the Japanese wiki40b dataset.
  - Downloads: 15
- [seungwon929/Ja-miracl](https://huggingface.co/datasets/seungwon929/Ja-miracl)
  - Ja-miracl provides a Japanese information retrieval dataset, converted to the BeIR format for compatibility with the mteb benchmark.
  - Downloads: 13
- [ebisuke/liz-nojaloli-ja-ds](https://huggingface.co/datasets/ebisuke/liz-nojaloli-ja-ds)
  - This repository provides a manually created Japanese dataset (‚Äúliz-nojaloli-ja‚Äù) for preparing Reinforcement Learning from Human Feedback (RLHF) data, potentially referencing code from Qiita.
  - Downloads: 11
- [hpprc/TALPCo](https://huggingface.co/datasets/hpprc/TALPCo)
  - This dataset provides Japanese-English translation pairs from the TALPCo corpus in Hugging Face format, with whitespace stemming from word segmentation removed, and licensed under CC-BY 4.0.
  - Downloads: 11
- [ltvmoon/opusbook_ja_en](https://huggingface.co/datasets/ltvmoon/opusbook_ja_en)
  - This repository provides Japanese-English translation resources licensed under CC-BY 4.0.
  - Downloads: 11
### Semantic Text Processing
- [hotchpotch/wikipedia-passages-jawiki-embeddings](https://huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings)
  - This repository provides Japanese Wikipedia embeddings and a Faiss index for RAG applications, including a HuggingFace Space demo, evaluation datasets, and comparisons of various Japanese embeddings like OpenAI's `text-embedding-3-small` under CC-BY-SA-4.0 (except OpenAI files).
  - Downloads: 4,017
- [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB)
  - JMTEB is a Japanese text embedding benchmark featuring 24 datasets across 6 tasks for evaluating model performance on Japanese language understanding.
  - Downloads: 2,362
- [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE)
  - JGLUE is a Japanese NLU benchmark dataset, built from scratch by Yahoo Japan and Waseda University‚Äôs Kawahara Lab, designed to evaluate and advance general language understanding in Japanese.
  - Downloads: 1,726
- [Coldog2333/JMedBench](https://huggingface.co/datasets/Coldog2333/JMedBench)
  - JMedBench is a Japanese biomedical LLM benchmark dataset with a provided evaluation framework (med-eval) designed to assess performance and encourage contributions.
  - Downloads: 1,259
- [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)
  - This repository provides a Japanese instruction-following (chat) dataset for fine-tuning Large Language Models‚Äîlike LoRA adaptation of English LLMs‚Äîwith updates to remove data subject to licensing changes and address output issues in included resources like Alpaca, Wikipedia, and ALT.
  - Downloads: 377
- [llm-book/aio-passages-bpr-bert-base-japanese-v3](https://huggingface.co/datasets/llm-book/aio-passages-bpr-bert-base-japanese-v3)
  - This dataset provides passage embeddings generated using llm-book/bert-base-japanese-v3-bpr-passage-encoder for the ‚ÄúAI King‚Äù competition passages (llm-book/aio-passages), licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 306
- [tokyotech-llm/lmsys-chat-1m-synth](https://huggingface.co/datasets/tokyotech-llm/lmsys-chat-1m-synth)
  - LMSYS-Chat-1M-Synth is a Japanese/English synthetic conversation dataset, derived from LMSYS-Chat-1M and used for post-training Llama-3.1-Swallow and Gemma-2 models.
  - Downloads: 288
- [Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Preference-Dataset-Qwen2.5_72B-191k)
  - This dataset provides 190,854 synthetic Japanese preference labels, generated using five open-source models (including Tanuki and Qwen) and judged by Qwen/Qwen2.5-72B, designed to mitigate positional bias.
  - Downloads: 279
- [llm-jp/llm-jp-instructions](https://huggingface.co/datasets/llm-jp/llm-jp-instructions)
  - llm-jp-instructions is a manually created Japanese instruction dataset, available via the `datasets` library with train, development, and test splits for v1.0.
  - Downloads: 252
- [noname0202/merged-ja](https://huggingface.co/datasets/noname0202/merged-ja)
  - This repository merges and extracts lines of 256 characters or less from the neody/c4-ja-cleaned, neody/cc100-ja-cleaned, and neody/oscar-ja-cleaned Japanese datasets.
  - Downloads: 225
- [alfredplpl/simple-zundamon](https://huggingface.co/datasets/alfredplpl/simple-zundamon)
  - This dataset provides simple, author-created data for the "Zunda Mon" character, formatted for LLM-jp and ChatGPT, intended for character LLM development and testing with a license from („Åö„Éªœâ„Éª„Åç„Çá).
  - Downloads: 206
- [dahara1/FineWeb2-HQ-ja-20B](https://huggingface.co/datasets/dahara1/FineWeb2-HQ-ja-20B)
  - This repository provides a 200GB subset of the multilingual FineWeb2-HQ dataset, specifically containing Japanese text data split into 14 JSONL chunks for easier handling.
  - Downloads: 201
- [VOICEVOX/kanalizer-dataset](https://huggingface.co/datasets/VOICEVOX/kanalizer-dataset)
  - Kanalizer provides datasets for a library that predicts pronunciations from English words, with code available in VOICEVOX/kanalizer and pre-trained models in VOICEVOX/kanalizer-model.
  - Downloads: 174
- [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla)
  - This repository provides a Japanese chat dataset, derived from izumi-lab/llm-japanese-dataset, for fine-tuning large language models‚Äîspecifically for instruction-response tasks using methods like LoRA‚Äîand details are available in the linked research papers.
  - Downloads: 147
- [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences)
  - This repository provides the Japanese Wikipedia sentences dataset used in the book "Introduction to Large Language Models," sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 124
- [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs)
  - This repository provides a dataset of Wikipedia paragraphs used in the book *Introduction to Large Language Models*, sourced from singletongue/wikipedia-utils and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 104
- [hotchpotch/cc100-ja-documents](https://huggingface.co/datasets/hotchpotch/cc100-ja-documents)
  - This repository provides document-level combined versions of the cc100/cc100-ja Japanese text dataset, originally split by lines, while maintaining the original cc100 license.
  - Downloads: 102
- [cyberagent/YokaiEval](https://huggingface.co/datasets/cyberagent/YokaiEval)
  - YokaiEval is a Japanese dataset of 810 multiple-choice questions designed to evaluate Large Language Models‚Äô knowledge of Japanese folklore, specifically focusing on *yokai* (supernatural entities) across six knowledge categories.
  - Downloads: 82
- [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja)
  - This repository hosts databricks-dolly-15k-ja, a Japanese translation of the dolly-15k instruction tuning dataset created collaboratively by LLM-jp using DeepL.
  - Downloads: 82
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k-formatted)
  - This dataset provides 39.6k formatted Japanese roleplay conversations generated with gpt-4o-mini, including system messages, licensed under CC-BY-NC-SA 4.0, and prohibits use for competing with OpenAI services.
  - Downloads: 70
- [hpprc/jawiki-bullet-points](https://huggingface.co/datasets/hpprc/jawiki-bullet-points)
  - This dataset consists of bullet-point lists generated from Japanese Wikipedia text using rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b and DeepSeek-R1-Distill-Qwen-32B-Japanese models, licensed under CC-BY-SA 4.0.
  - Downloads: 69
- [hotchpotch/fineweb-2-edu-japanese-scores](https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese-scores)
  - This dataset provides educational scores (0-4) for nearly 310,000 Japanese texts from the fineweb-2 dataset, assessed using the Deepseek API and static embeddings for determining educational value.
  - Downloads: 67
- [DataPilot/Zero_SFT_Ja_v2_b3t4](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2_b3t4)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese synthetic dataset of high-quality prompts and AI outputs generated using Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 63
- [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered)
  - This repository provides a filtered and modified Japanese/Chinese parallel dataset from WikiMatrix v1, enhanced with semantic similarity filtering (LaBSE, threshold 0.6) and Traditional to Simplified Chinese conversion using zhconv.
  - Downloads: 63
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÁ¥Ñ19800‰ª∂„ÅÆÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„ÅÆÂØæË©±„ÇíÂèéÈå≤„Åó„ÅüÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 63
- [Aratako/Open-Platypus-Japanese-masked-formatted](https://huggingface.co/datasets/Aratako/Open-Platypus-Japanese-masked-formatted)
  - This dataset converts the Open-Platypus Japanese masked language modeling data into OpenAI message format for use in large language models.
  - Downloads: 60
- [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored)
  - This repository provides a Japanese translation of the Databricks Dolly 15k dataset, enhanced with BERTScore-calculated translation quality scores to identify and address issues like inconsistencies and errors in the machine translation.
  - Downloads: 53
- [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese)
  - This repository provides a continuously updated, synthetically generated Japanese question-answering dataset created with OpenAI's GPT-4, intended for fine-tuning non-English, particularly Japanese, open-source language models.
  - Downloads: 53
- [Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k](https://huggingface.co/datasets/Aratako/Self-Instruct-Qwen2.5-72B-Instruct-60k)
  - This repository provides a 60k synthetic Japanese instruction dataset‚Äîcreated using Magpie, Evol-Instruct, and Self-Instruct with Qwen2.5-72B‚Äîlicensed under Apache 2.0 but subject to the Qwen License for model training.
  - Downloads: 51
- [DataPilot/ichikara-instruction-003-sharegpt](https://huggingface.co/datasets/DataPilot/ichikara-instruction-003-sharegpt)
  - DataPilot converted the Japanese instruction-tuning dataset, kinokokoro/ichikara-instruction-003, into the widely-used ShareGPT format for improved conversational LLM training and provides it as JSON Lines data.
  - Downloads: 50
- [DataPilot/Zero_SFT_Ja_by_Mistral_Small](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_by_Mistral_Small)
  - DataPilot/Zero_SFT_Ja_by_Mistral_Small is a Japanese synthetic dataset of high-quality prompts and AI outputs generated using the Mistral Small 3.1 24B Instruct model, formatted as JSONL for supervised fine-tuning.
  - Downloads: 45
- [Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja](https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja)
  - This dataset is a Japanese role-playing learning set‚Äîa translation of Bluemoon_Top50MB, sorted and fixed‚Äîcreated using karakuri-lm-8x7b-chat-v0.1-awq and DeepInfra, with limitations regarding long dialogues and potential LLM repetition resulting in some record removal and inaccurate token counts.
  - Downloads: 44
- [Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k)
  - This repository provides a 69,000-record Japanese-English coding dialogue dataset generated using Magpie with models like Nemotron, Phi-3, Mixtral, and Calm3, alongside the code used for its creation, noting potential quality variations due to minimal post-filtering.
  - Downloads: 44
- [zetavg/ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed)
  - This repository provides a Markdown-converted and language-labeled version of the RyokoAI/ShareGPT52K dataset, utilizing tools for CJK whitespace, HTML-to-Markdown conversion, Chinese conversion, and language detection.
  - Downloads: 42
- [watashihakobashi/ogiri](https://huggingface.co/datasets/watashihakobashi/ogiri)
  - This dataset, created by the University of Tokyo's Matsuo & Iwasawa Lab's 2024 LLM course, comprises human-generated inputs and outputs from two language models (watashiha-gpt-6b & Watashiha-Llama-2-13B-Ogiri-sft) for supervised fine-tuning (SFT) research and education.
  - Downloads: 42
- [Atotti/VTuber-overview](https://huggingface.co/datasets/Atotti/VTuber-overview)
  - This dataset comprises detailed, natural language summaries of VTuber activities, characteristics, and collaborations‚Äîcollected using GPT-4o Search Preview with a cost of $27.04 for 36,276 tokens.
  - Downloads: 38
- [inu-ai/ggml-japanese-gpt2](https://huggingface.co/datasets/inu-ai/ggml-japanese-gpt2)
  - This repository provides a runnable ggml-Japanese-GPT2 model for Windows, requiring downloaded bin and SentencePiece model files and using a command-line interface for text generation, though the xsmall model currently has issues.
  - Downloads: 38
- [shi3z/Japanese_Wikipedia_Conversation](https://huggingface.co/datasets/shi3z/Japanese_Wikipedia_Conversation)
  - This dataset comprises conversational data generated by GPT-3.5-Turbo using the Japanese Wikipedia (izumi-lab/wikipedia-ja-20230720) as its source, and is **not** for commercial use.
  - Downloads: 30
- [lennart-finke/SimpleStories-JA](https://huggingface.co/datasets/lennart-finke/SimpleStories-JA)
  - SimpleStories is a diverse, annotated short story dataset generated by gpt-4o-mini (and expanding to other models), available in English and Japanese, and built as an improvement upon TinyStories for NLP tasks.
  - Downloads: 30
- [takosama/databricks-dolly-15k-ja-google-trans](https://huggingface.co/datasets/takosama/databricks-dolly-15k-ja-google-trans)
  - This repository provides a Japanese translation of Databricks‚Äô Dolly project, licensed under CC BY-SA 3.0, utilizing and adapting content from sources like Wikipedia under the same license.
  - Downloads: 28
- [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset)
  - This repository provides a clean, 5.1 million sentence Japanese dataset with context, suitable for training models in unsupervised semantic similarity and related tasks.
  - Downloads: 27
- [Gustav114514/work](https://huggingface.co/datasets/Gustav114514/work)
  - This repository provides a fine-tuned XLSR-53 model for Japanese speech recognition, trained on Common Voice, CSS10, and JSUT datasets, requiring 16kHz sampled input.
  - Downloads: 24
- [kunishou/HelpSteer2-20k-ja](https://huggingface.co/datasets/kunishou/HelpSteer2-20k-ja)
  - This repository provides a Japanese translation of NVIDIA's HelpSteer2 dataset, used for aligning and training reward models with SteerLM and Nemotron-4-430B-Reward.
  - Downloads: 21
- [augmxnt/shisa-pretrain-en-ja-v1](https://huggingface.co/datasets/augmxnt/shisa-pretrain-en-ja-v1)
  - Shisa-base-7b-v1 is pre-trained on this dataset of mixed Japanese/English tokens, sampled from MADLAD-400 using a 90/10 ratio via DSIR.
  - Downloads: 19
- [Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-example Japanese coding dialogue dataset generated using the Magpie technique applied to Nvidia's Nemotron-4-340B-Instruct, along with the code used for its creation, noting the dataset may contain low-quality records due to minimal post-filtering.
  - Downloads: 19
- [if001/elementray_m](https://huggingface.co/datasets/if001/elementray_m)
  - This dataset provides Japanese example sentences generated with calm3-22b, covering various grammatical patterns including polite forms, negation, desires, progressives, and requests, with cleaned instances of failed generations.
  - Downloads: 19
- [YukiTomita-CC/ELYZA-tasks-100_Human_solved](https://huggingface.co/datasets/YukiTomita-CC/ELYZA-tasks-100_Human_solved)
  - This repository provides human-generated responses to the ELYZA-tasks-100 benchmark for Japanese LLM evaluation, achieving an average GPT-4o score of 3.69 and a Claude 3.5 Sonnet score of 4.42.
  - Downloads: 19
- [Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Instruction-Selected-Evolved-26.5k)
  - This repository provides a 26.5k Japanese instruction dataset generated by applying Magpie and Evol-Instruct techniques‚Äîusing models like Tanuki and Qwen‚Äîto cluster and refine a larger 100k instruction set, licensed under Apache 2.0 with Qwen restrictions.
  - Downloads: 18
- [if001/elementray_l](https://huggingface.co/datasets/if001/elementray_l)
  - This dataset provides cleaned Japanese example sentences demonstrating various grammatical patterns including politeness, negation, desire, progress, and inference, generated using the calm3-22b language model.
  - Downloads: 17
- [p1atdev/fake-news-jp](https://huggingface.co/datasets/p1atdev/fake-news-jp)
  - This repository provides a Japanese fake news dataset converted for Hugging Face datasets, including text content, labels indicating truthfulness (real, partial/full GPT-2 generated), and character counts for real/fake portions.
  - Downloads: 16
- [kenkensz9/nareba1691](https://huggingface.co/datasets/kenkensz9/nareba1691)
  - This repository provides a model fine-tuned on both the kenkensz9/1242tw2 dataset and a curated collection of 330 personality-driven tweets, designed to generate tweets and assign them scores ranging from 10 (excellent) to 8 (so-so).
  - Downloads: 16
- [Rio-Rf/oscar_2023_filtered_and_ai_text_filtered](https://huggingface.co/datasets/Rio-Rf/oscar_2023_filtered_and_ai_text_filtered)
  - This dataset combines human-written (OSCAR) and LLM-generated (GPT-3.5 Turbo) Japanese text for evaluating the performance of LLMs in detecting machine-generated text.
  - Downloads: 14
- [kunishou/oasst2-chat-68k-ja](https://huggingface.co/datasets/kunishou/oasst2-chat-68k-ja)
  - This dataset provides a ShareGPT-formatted, Japanese language conversion of the OpenAssistant/oasst2 135k dataset, optimized for multi-turn conversation fine-tuning with potentially high computational resource requirements.
  - Downloads: 13
- [pokutuna/tasks-ime-and-kakko-jp](https://huggingface.co/datasets/pokutuna/tasks-ime-and-kakko-jp)
  - This repository provides manually created variations of tasks‚Äîpresenting conversion candidates like IME and correcting bracket matching‚Äîdeveloped to address weaknesses in a model created by @pokutuna for a 2024 University of Tokyo Matsuo Lab Deep Learning competition.
  - Downloads: 13
- [Chasottco/Japanese-patent-evaluation-dataset-01](https://huggingface.co/datasets/Chasottco/Japanese-patent-evaluation-dataset-01)
  - This repository provides an experimental dataset of 50 queries and evaluations‚Äîgenerated with ChatGPT-4o focusing on five perspectives including patent attorney references (excluding direct recommendations)‚Äîalong with manually created answers sourced from open patent databases.
  - Downloads: 13
- [zenless-lab/jamp](https://huggingface.co/datasets/zenless-lab/jamp)
  - Jamp is a controlled Japanese temporal inference dataset, with training, test, and template data, designed to benchmark the generalization capacity of language models regarding time-related reasoning.
  - Downloads: 12
- [R1b3y/NE4Mitsua](https://huggingface.co/datasets/R1b3y/NE4Mitsua)
  - NE4Mitsua is a negative embedding for Mitsua Diffusion One designed to enhance realism, increase image complexity, and simplify anime-style illustration generation within the Stable Diffusion web UI.
  - Downloads: 12
- [fufufukakaka/pokemon_battle_team_dataset_regulation_f](https://huggingface.co/datasets/fufufukakaka/pokemon_battle_team_dataset_regulation_f)
  - This dataset records Pok√©mon VGC Regulation F battle team selections collected from YouTube streams, including data from the author (trainer_id 13), and was presented at a remote Pok√©mon research community event in May 2024.
  - Downloads: 12
- [kajuma/llm-jp-corpus-v3-ja](https://huggingface.co/datasets/kajuma/llm-jp-corpus-v3-ja)
  - LLM-jp Corpus v3 (excluding Wikipedia due to its CC-BY-SA license) provides a mirrored Japanese language dataset for large language model training and research.
  - Downloads: 12
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted)
  - Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k-formatted Ê¶ÇË¶Å gpt-4o-mini„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™û„É≠„Éº„É´„Éó„É¨„Ç§„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-19.8k„Å´system message„ÇíËøΩÂä†„Åó„Å¶Êï¥ÂΩ¢„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
- [kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50](https://huggingface.co/datasets/kurogane/Magpie_llama-3-youko-8b_prompt_extract_example50)
  - This repository explores prompt extraction using the Magpie method and the rinna/llama-3-youko-8b language model, based on prompts from a research paper, despite the model not being instruction-tuned.
  - Downloads: 11
### Natural Language Interfaces
- [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions)
  - Rakuda provides 40 Japanese-language questions‚Äîcovering history, society, government, and geography‚Äîto benchmark and rank the performance of Japanese large language models.
  - Downloads: 741
- [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA)
  - JQaRA is a Japanese question answering dataset designed to evaluate the effectiveness of Retrieval-Augmented Generation (RAG) for improving the accuracy of Large Language Models (LLMs) by enabling them to retrieve external knowledge.
  - Downloads: 537
- [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET)
  - JAQKET is a Japanese open-domain question answering dataset, built from Wikipedia articles, designed to promote QA/machine reading research with multiple-choice questions (v1.0 & v2.0).
  - Downloads: 492
- [nu-dialogue/real-persona-chat](https://huggingface.co/datasets/nu-dialogue/real-persona-chat)
  - RealPersonaChat is a Japanese dialogue corpus of approximately 14,000 conversations, featuring speaker personas and personality traits, intended for research with ethical considerations regarding privacy and misuse.
  - Downloads: 487
- [Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-SFW-DeepSeek-V3-0324-20k)
  - This dataset provides 20,000 synthetic Japanese roleplaying conversations (10-20 turns each) with detailed metadata like genre, tags, and character/setting information, formatted for use with language models via OpenAI messages.
  - Downloads: 378
- [line-corporation/JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
  - JIC-VQA is a challenging Japanese visual question answering benchmark, built on a Japanese image classification dataset of 101 food types, featuring multiple-choice questions designed to test vision-language models.
  - Downloads: 334
- [llm-book/llm-jp-eval](https://huggingface.co/datasets/llm-book/llm-jp-eval)
  - This repository provides the Japanese question-answering dataset ‚Äúja-vicuna-qa-benchmark‚Äù used for evaluating large language models with llm-jp-eval, licensed under Apache 2.0.
  - Downloads: 241
- [YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset](https://huggingface.co/datasets/YukiTomita-CC/AKU-d_ms-0.5B-v0.1_dataset)
  - This repository provides the 1.56B token text dataset‚Äî comprising Japanese dialogue and movie recommendation data under CC-BY licenses‚Äîused for pre-training the AKU-d_ms-0.5B-chat-v0.1 model.
  - Downloads: 213
- [EQUES/YakugakuQA](https://huggingface.co/datasets/EQUES/YakugakuQA)
  - YakugakuQA is a Japanese question answering dataset of over 4K pharmacist license exam Q&A pairs (2012-2024) with accompanying commentaries, now including image data, released under a CC-BY-4.0 license.
  - Downloads: 199
- [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD)
  - JaQuAD is a 39,696 question-answer pair dataset for Japanese machine reading comprehension, built from Wikipedia and designed as a Japanese analogue to SQuAD, achieving strong results when fine-tuning BERT-Japanese.
  - Downloads: 176
- [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR)
  - JaCWIR is a small-scale Japanese information retrieval evaluation dataset comprising 5,000 questions and ~500,000 web page titles/descriptions sourced from Hatena Bookmark, designed to assess systems answering natural language queries with diverse web content.
  - Downloads: 161
- [sbintuitions/JEMHopQA](https://huggingface.co/datasets/sbintuitions/JEMHopQA)
  - JEMHopQA is a Japanese multi-hop question answering dataset with explainable derivation steps, featuring compositional and comparison questions based on linked Wikipedia articles, licensed under CC BY-SA 4.0.
  - Downloads: 151
- [ryo0634/bsd_ja_en](https://huggingface.co/datasets/ryo0634/bsd_ja_en)
  - The Business Scene Dialogue (BSD) dataset is a Japanese-English parallel corpus of written business conversations constructed across diverse scenarios through monolingual creation and translation.
  - Downloads: 149
- [shi3z/rachel](https://huggingface.co/datasets/shi3z/rachel)
  - This repository provides a growing, manually created dataset designed for training Japanese chatbots.
  - Downloads: 126
- [Silviase/augeobench](https://huggingface.co/datasets/Silviase/augeobench)
  - AugeoBench is a Japanese multimodal QA dataset featuring geometry problems with contextual descriptions, questions, images, and exact/textual answers for evaluating AI reasoning.
  - Downloads: 117
- [sbintuitions/aio-extended-answers](https://huggingface.co/datasets/sbintuitions/aio-extended-answers)
  - AIÁéã (AIO) is a Japanese quiz dataset‚Äîspecifically version 2.0's validation set‚Äîcontaining questions with manually verified answers, identified by unique IDs, competition names, timestamps, and sections.
  - Downloads: 102
- [Aratako/Japanese-RP-Bench-testdata-SFW](https://huggingface.co/datasets/Aratako/Japanese-RP-Bench-testdata-SFW)
  - Japanese-RP-Bench-testdata-SFW is a synthetic evaluation dataset for benchmarking Japanese language model role-playing abilities, featuring diverse settings, characters, and dialogue tones, generated using Claude 3.5 Sonnet outputs with a permissive license.
  - Downloads: 74
- [Atom007/mc4-japanese-data](https://huggingface.co/datasets/Atom007/mc4-japanese-data)
  - mc4 is a massively multilingual machine translation dataset containing 100 languages sourced from Common Crawl, designed for training and evaluating translation models.
  - Downloads: 66
- [Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-Qwen2.5-72B-Answered)
  - This dataset consists of high-quality, 96k-context instructions from Aratako/Magpie-Tanuki-8B, re-generated with answers from Qwen2.5-72B-Instruct, and is primarily licensed under Apache 2.0, but subject to the Qwen License for model training.
  - Downloads: 64
- [Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k](https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k)
  - This dataset contains approximately 1,000 synthetic Japanese roleplay dialogues, each with 10 turns, generated using Nvidia's Nemotron-4-340B-Instruct and Magpie, potentially containing low-quality records and a tendency to prematurely end longer conversations.
  - Downloads: 61
- [U23-lab/everyday_conversations_ja](https://huggingface.co/datasets/U23-lab/everyday_conversations_ja)
  - This repository provides a Japanese translation of the everyday-conversations-llama3.1-2k dataset, formatted as topic-based user-assistant dialogue pairs translated with DeepL, under an Apache 2.0 license.
  - Downloads: 58
- [Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone](https://huggingface.co/datasets/Nexdata/Japanese_Conversational_Speech_by_Mobile_Phone)
  - This repository provides a sample of the Nexdata Japanese Conversational Speech dataset, featuring roughly 1000 speakers engaged in natural, manually-transcribed face-to-face conversations on diverse topics.
  - Downloads: 50
- [Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k](https://huggingface.co/datasets/Aratako/Synthetic-Japanese-Roleplay-gpt-4o-mini-39.6k)
  - This dataset contains approximately 39,600 synthetic Japanese roleplay conversations generated with gpt-4o-mini, featuring detailed settings (genre, tags, world, characters, tone) and formatted for easy model training via system messages.
  - Downloads: 49
- [sode-k/txt_suicidality](https://huggingface.co/datasets/sode-k/txt_suicidality)
  - txt: Wrime-v1„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÈÉ®ÂàÜ„ÅÆ‰∏ÄÈÉ®„Å®„ÄÅOpenAI„Å´ÁîüÊàê„Åï„Åõ„ÅüÊñáÁ´†„Çí„Éô„Éº„Çπ„Å´„ÄÅtohoku-nlp/bert-base-japanese-whole-word-masking „Åß„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüÊñáÁ´†„ÇíÊñáËÑà„ÅåÊàê„ÇäÁ´ã„Å§ÂΩ¢„ÅßÂêàÊàê„Åó„ÄÅÊñ∞„Åü„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„ÇÇ„ÅÆ„ÄÇ
  - Downloads: 49
- [kanhatakeyama/AutoMultiTurnByCalm3-22B](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByCalm3-22B)
  - This repository provides a multi-turn Q&A dataset automatically generated using Calm3-22b from open data sources (oasst2, databricks-dolly, minnade, cyberagent/chatbot-arena) under various licenses (Apache 2.0, CC-BY-SA-3.0, CC0, CC-BY-4.0).
  - Downloads: 48
- [shi3z/OpenOrcaJapanese](https://huggingface.co/datasets/shi3z/OpenOrcaJapanese)
  - This repository provides a Japanese translation of the Open-Orca dataset, currently about 20% complete, and available for commercial use.
  - Downloads: 47
- [kanhatakeyama/AutoMultiTurnByMixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/AutoMultiTurnByMixtral8x22b)
  - This repository provides a multi-turn Q&A dataset automatically generated using Mixtral-8x22B with data sourced from oasst2, databricks-dolly, minnade, and cyberagent/chatbot-arena, adhering to respective licenses (Apache 2.0, CC-BY-SA-3.0, CC0, CC-BY-4.0).
  - Downloads: 44
- [p1atdev/ja-stackoverflow](https://huggingface.co/datasets/p1atdev/ja-stackoverflow)
  - This repository provides a Japanese QA dataset derived from Stack Overflow, featuring processed question-answer pairs with markdown formatting, code block handling, and base64 image replacement, available in both default and simplified subsets.
  - Downloads: 42
- [umiyuki/JDocQA_SingleImage_200](https://huggingface.co/datasets/umiyuki/JDocQA_SingleImage_200)
  - JDocQA_SingleImage_200 is a Japanese (ja-JP) question answering dataset derived from shunk031/JDocQA, containing 200 single-image questions with text, IDs, answers, and answer types, optimized for reduced size and faster processing.
  - Downloads: 41
- [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja)
  - This repository provides a Japanese dialogue summarization dataset translated from dialogsum and CSDS, offering resources for training and evaluating summarization models in Japanese.
  - Downloads: 39
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja_cleaned)
  - This dataset is a manually cleaned, Japanese translation of the WikiHow Know-How Question Answering dataset.
  - Downloads: 38
- [DeL-TaiseiOzaki/Tengentoppa-sft-v1.0](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v1.0)
  - This repository provides Tengentoppa, a large Japanese instruction-following dataset created by merging 16 diverse sources in JSON format for supervised fine-tuning of language models.
  - Downloads: 36
- [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k)
  - This repository provides a commercially usable, 80,000+ multi-turn conversation dataset generated from Japanese Wikipedia using Llama2Pro8B, automatically screened but potentially containing unusual dialogue.
  - Downloads: 35
- [llm-book/ja-vicuna-qa-benchmark](https://huggingface.co/datasets/llm-book/ja-vicuna-qa-benchmark)
  - This repository provides the Kyoto University's Japanese Vicuna QA Benchmark, an 80-question dataset across 10 categories for evaluating Japanese LLM responses without reference answers, licensed under Apache 2.0.
  - Downloads: 33
- [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO)
  - This repository provides Japanese chatbot conversations generated using the Swallow-MX-8x7b model, based on single-turn prompts translated from the CC-BY 4.0 licensed Chatbot Arena Conversations JA dataset and utilizing a Facebook translation model.
  - Downloads: 31
- [shi3z/ja_conv_wikipedia_orion14B_100K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_100K)
  - This repository provides a commercially-usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of its Orion-14B Series Community License and acknowledging computing resources from ABCI.
  - Downloads: 30
- [Kendamarron/pret-a-porter-instruction-v0.1](https://huggingface.co/datasets/Kendamarron/pret-a-porter-instruction-v0.1)
  - This dataset comprises human-checked and corrected instructions for open-source LLMs, with outputs generated using Swallow-MX, though accuracy isn't guaranteed, and was created during the LOCAL AI HACKATHON #000.
  - Downloads: 30
- [OmniAICreator/Japanese-Roleplay-Dialogues](https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues)
  - This repository provides a Japanese role-playing chat dialogue corpus, both in original and filtered forms (removing single-poster and short-post threads), for research and natural language processing.
  - Downloads: 28
- [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats)
  - This dataset contains Japanese conversations extracted from public-domain books in Aozora Bunko using a heuristic approach to identify utterances within quotation marks, with code available for reproduction.
  - Downloads: 28
- [DataPilot/databricks-dolly-15k-Nyan-ja](https://huggingface.co/datasets/DataPilot/databricks-dolly-15k-Nyan-ja)
  - This dataset is a Japanese translation of the ‚Äúdatabricks-dolly-15k‚Äù dataset, further modified with ‚Äúnya!‚Äù appended to each sentence using ArrowPro-7B-KUJIRA, intended for stylistic alteration rather than performance improvement.
  - Downloads: 26
- [p1atdev/japanese-stackexchange](https://huggingface.co/datasets/p1atdev/japanese-stackexchange)
  - This dataset provides question-answer pairs derived from Japanese Stack Exchange data, processed with markdown formatting, image handling, and includes IDs for questions, answers, accepted answers, and popular answers, with a simplified subset also available.
  - Downloads: 26
- [llm-jp/japanese-photos-conversation](https://huggingface.co/datasets/llm-jp/japanese-photos-conversation)
  - This dataset provides 11,808 multi-turn conversational instructions generated by GPT-4o about images of Japan, sourced from Hugging Face and filtered during creation.
  - Downloads: 24
- [Aihometr/anime-your-name](https://huggingface.co/datasets/Aihometr/anime-your-name)
  - This dataset provides AI-generated subtitles (with potential Turkish and Japanese errors) specifically for chatbot training, and is not intended for translation purposes.
  - Downloads: 23
- [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin)
  - This repository provides a filtered version of the Aozora Bunko clean dataset, specifically including texts written in ‚Äúshinshinjitana‚Äù (modern orthography).
  - Downloads: 23
- [shi3z/ja_conv_wikipedia_llama2pro8b_3k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_3k)
  - This repository provides a 3,000-conversation dataset generated from Japanese Wikipedia using llama2Pro8B, commercially usable under its license, though with potentially unvetted dialogue.
  - Downloads: 22
- [tombailey/oasst1-ja](https://huggingface.co/datasets/tombailey/oasst1-ja)
  - This repository provides a Japanese language subset of the OpenAssistant Conversations Dataset, formatted as paired human-assistant messages‚Äîpotentially lacking full conversational context‚Äîfor use in dialogue modeling.
  - Downloads: 22
- [nu-dialogue/jmultiwoz](https://huggingface.co/datasets/nu-dialogue/jmultiwoz)
  - JMultiWOZ is a 4,246-dialogue Japanese multi-domain dataset, collected via Wizard-of-Oz, for task-oriented dialogue research‚Äîspecifically dialogue state tracking‚Äîcovering six domains like restaurant and hotel.
  - Downloads: 19
- [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped)
  - This dataset removes prompts matching those from chatbot-arena-ja-calm2-7b-chat, providing a filtered dataset for chatbot training or evaluation.
  - Downloads: 19
- [Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k](https://huggingface.co/datasets/Aratako/Synthetic-JP-Conversations-Magpie-Nemotron-4-10k)
  - This repository provides a 10,000-record Japanese instruction tuning dataset generated by applying the Magpie method to Nvidia's Nemotron-4-340B-Instruct, along with the dataset creation code, noting the absence of post-generation filtering and potential for low-quality records.
  - Downloads: 18
- [shi3z/ja_conv_wikipedia_llama2pro8b_20k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_20k)
  - This dataset provides 60,000 commercially-usable, multi-turn conversations generated from Japanese Wikipedia using Llama2Pro8B, with automated screening but potential for unusual dialogue.
  - Downloads: 16
- [if001/elementray_small](https://huggingface.co/datasets/if001/elementray_small)
  - This dataset provides simple Japanese sentences demonstrating various grammatical patterns‚Äîincluding polite forms, negation, desire, progress, requests, permission, obligation, and speculation‚Äîgenerated using the `if001/elementray_m calm3-22b` model and then cleaned.
  - Downloads: 15
- [shi3z/ja_conv_wikipedia_llama2pro8b_10k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_10k)
  - This repository provides 3,000 commercially-usable, multi-turn conversations generated from Japanese Wikipedia data using llama2Pro8B, acknowledging potential for unrefined dialogue due to automated screening.
  - Downloads: 14
- [GENIAC-Team-Ozaki/WikiHowNFQA-ja](https://huggingface.co/datasets/GENIAC-Team-Ozaki/WikiHowNFQA-ja)
  - This dataset is a Japanese translation of the WikiHowNFQA dataset, providing question-answer pairs based on WikiHow articles.
  - Downloads: 13
- [shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K](https://huggingface.co/datasets/shi3z/Qarasu_Wikipedia_multiturn_human_gpt_10K)
  - This repository provides a Japanese multi-turn conversation dataset, generated with Qarasu14B from Wikipedia data for non-commercial use and compatible with Axolotl, leveraging the Tsuginosuke AI Super Computer.
  - Downloads: 13
- [shi3z/ja_conv_wikipedia_orion14B_10K](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_orion14B_10K)
  - This repository provides a commercially-usable, multi-turn Japanese conversation dataset generated with Orion14B-Chat, requiring careful review of the Orion-14B Models Community License and acknowledging computing resources from ABCI.
  - Downloads: 13
### Text Generation
- [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU)
  - JMMLU is a Japanese language benchmark, based on and extending the MMLU dataset, designed to evaluate large language model performance with both translated and culturally-specific Japanese questions.
  - Downloads: 684
- [DataPilot/Zero_SFT_Ja_v2](https://huggingface.co/datasets/DataPilot/Zero_SFT_Ja_v2)
  - DataPilot/Zero_SFT_Ja_v2_b3t4 is a Japanese dataset of synthetically generated, high-quality prompts and AI outputs created with Mistral Small 3.1 24B Instruct, formatted as JSONL for supervised fine-tuning.
  - Downloads: 271
- [neulab/odex](https://huggingface.co/datasets/neulab/odex)
  - ODEX is a multilingual NL-to-code benchmark with 1,707 human-written test cases across English, Spanish, Japanese, and Russian, designed for evaluating open-domain execution-based code generation.
  - Downloads: 256
- [Aratako/Synthetic-JP-EN-Coding-Dataset-801k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-801k)
  - This repository provides a synthetic, 801k-item code instruction-following dataset (Synthetic-JP-EN-Coding-Dataset) with both Japanese (173k) and English (627k) examples, generated and expanded from the original Magpie dataset using Evol-Instruct with model metadata included.
  - Downloads: 206
- [YANS-official/ogiri-bokete](https://huggingface.co/datasets/YANS-official/ogiri-bokete)
  - This dataset from YANS-official contains Japanese humorous responses (‚Äúokashi‚Äù) sourced from the Bokete website, featuring text-to-text, image-to-text, and text-image-to-text tasks with a total of 600 prompts and 2355 responses.
  - Downloads: 195
- [allganize/RAG-Evaluation-Dataset-JA](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-JA)
  - Allganize RAG Leaderboard evaluates the performance of Japanese Retrieval-Augmented Generation (RAG) solutions across five industries, providing datasets and automated evaluation for Parser, Retrieval, and Generation components ‚Äì a currently unique benchmark.
  - Downloads: 194
- [Aratako/iterative-dpo-data-for-ORPO-iter3](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-ORPO-iter3)
  - This repository provides a Japanese preference dataset for Reinforcement Learning from Human Feedback (RLHF), created by scoring and pairing responses from Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter2 and Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, licensed under META LLAMA 3.1, Gemma Terms of Use, and Qwen LICENSE AG.
  - Downloads: 155
- [Aratako/Synthetic-JP-EN-Coding-Dataset-567k](https://huggingface.co/datasets/Aratako/Synthetic-JP-EN-Coding-Dataset-567k)
  - Synthetic-JP-EN-Coding-Dataset-567k Magpie„Å´„Çà„Å£„Å¶‰ΩúÊàê„Åó„Åü„Ç≥„Éº„ÉâSFT„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„ÇãAratako/Synthetic-JP-EN-Coding-Dataset-Magpie-69k„ÇíÂÖÉ„Å´„ÄÅEvol-Instruct„ÅÆ„Çà„ÅÜ„Å™ÊâãÊ≥ï„ÇíÁî®„ÅÑ„Å¶Ë§áÊï∞„ÅÆinstruction„Å®resonse„ÇíÁîüÊàê„ÅóÊã°Âºµ„Åó„Å¶‰ΩúÊàê„Åó„Åü„ÄÅÊó•Ëã±Ê∑∑Âêà567077‰ª∂„ÅÆ„Ç≥„Éº„ÉâSFTÁî®ÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 153
- [NekoFi/whisper_toku](https://huggingface.co/datasets/NekoFi/whisper_toku)
  - Dataset Description
  - Downloads: 145
- [kanhatakeyama/japanese-corpus-categorized](https://huggingface.co/datasets/kanhatakeyama/japanese-corpus-categorized)
  - This repository provides a clustered Japanese text corpus of approximately 10,000 items, created by cleaning web corpora like mc4-ja and using unsupervised learning, intended for information analysis purposes with some files in Parquet format‚Äîdownload via Git LFS.
  - Downloads: 115
- [noname0202/oscar-cleaned-256](https://huggingface.co/datasets/noname0202/oscar-cleaned-256)
  - This repository provides a dataset of Japanese text snippets extracted from neody/oscar-ja-cleaned, limited to 256 characters or less.
  - Downloads: 114
- [Aratako/magpie-sft-v1.0-dpo-judged](https://huggingface.co/datasets/Aratako/magpie-sft-v1.0-dpo-judged)
  - This Japanese preference dataset, built upon llm-jp/magpie-sft-v1.0, uses Aratako/Llama-Gemma-2-27b-SFT-trial1 to regenerate responses, judged by google/gemma-2-27b-it against Qwen/Qwen2.5-32B-Instruct outputs, and is subject to META LLAMA 3.1, Gemma Terms of Use, and Qwen licensing (with attribution requirements for models trained on it).
  - Downloads: 111
- [SNOW-NLP/snow_simplified_japanese_corpus](https://huggingface.co/datasets/SNOW-NLP/snow_simplified_japanese_corpus)
  - SNOW T15 & T23 is a 50,000-sentence simplified Japanese corpus with original, simplified, and English translations, suitable for text simplification and Japanese-English translation using a 2,000-word vocabulary.
  - Downloads: 109
- [hpprc/r1-distill-qwen-pseudo-qa](https://huggingface.co/datasets/hpprc/r1-distill-qwen-pseudo-qa)
  - This dataset contains question-answer pairs automatically generated from Japanese Wikipedia using DeepSeek-R1-Distill-Qwen-32B-Japanese, and is released under the CC-BY-SA 4.0 license.
  - Downloads: 67
- [Aratako/iterative-dpo-data-for-SimPO-iter2](https://huggingface.co/datasets/Aratako/iterative-dpo-data-for-SimPO-iter2)
  - This repository provides a Japanese preference dataset for SimPO training, created by scoring and ranking responses generated from Aratako/Llama-Gemma-2-27b-CPO_SimPO-iter1 using Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8, and is subject to META LLAMA 3.1, Gemma, and Qwen licenses.
  - Downloads: 64
- [DataPilot/Generated-dataset-by-deepseek-v2.5](https://huggingface.co/datasets/DataPilot/Generated-dataset-by-deepseek-v2.5)
  - This dataset combines null-instruct-ja and DeepSeek-v2.5 (q4 quantization) generated using ollama and 7x A5000 GPUs, licensed under the DeepSeek license.
  - Downloads: 59
- [SousiOmine/TagInstruct-JP](https://huggingface.co/datasets/SousiOmine/TagInstruct-JP)
  - This repository provides a prototype dataset, generated by Deepseek-V3-0324, designed to improve adherence to constrained system prompts for language models, with code and data released under the MIT license.
  - Downloads: 55
- [hpprc/llmjp-kaken](https://huggingface.co/datasets/hpprc/llmjp-kaken)
  - This dataset converts a subset of the LLM-JP-Corpus-v3 into Hugging Face format, adding article titles sourced from original URLs where available, all licensed under CC-BY 4.0.
  - Downloads: 53
- [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct)
  - Amenokaku-Code-Instruct is a 5.2K instruction dataset for code-related tasks‚Äîincluding code generation, behavior checks, and bug fixing‚Äîbuilt from commercially licensed and permissioned programming learning content, primarily in Japanese (translated from English).
  - Downloads: 50
- [Nurture-intelligence/thinking_dataset_v1](https://huggingface.co/datasets/Nurture-intelligence/thinking_dataset_v1)
  - This dataset provides question data generated using Qwen/Qwen2.5-32B-Instruct via Ollama, intended for building reasoning models, and is licensed under Apache 2.0 only for the questions, as answers were generated by another model.
  - Downloads: 50
- [Ego/jpflan](https://huggingface.co/datasets/Ego/jpflan)
  - This repository provides a templated dataset of ~40 Japanese downstream tasks, formatted for instruction tuning LLMs with up to 20,000 high-quality, non-machine-translated samples‚Äîsplit between 0-shot and few-shot (up to 3) examples.
  - Downloads: 50
- [Kendamarron/jimba-wiki-instruction-calm3](https://huggingface.co/datasets/Kendamarron/jimba-wiki-instruction-calm3)
  - This repository provides a synthetic instruction dataset, `jimba-wiki-instruction-calm3`, created by generating instructions based on Japanese Wikipedia text using the CALM3-22B-Chat model, aiming for reduced hallucination but with potential unfiltered outputs.
  - Downloads: 45
- [YANS-official/ogiri-keitai](https://huggingface.co/datasets/YANS-official/ogiri-keitai)
  - This repository provides a dataset of all questions and answers from the Japanese TV show *Keitaioogiri*, scraped from a blog archive and structured into a table with details like question ID, episode, type, question text, and a list of responses.
  - Downloads: 42
- [YANS-official/ogiri-test-with-references](https://huggingface.co/datasets/YANS-official/ogiri-test-with-references)
  - This dataset from Bokete, a Japanese comedy site, provides text, image, and text-image data for three tasks‚Äîtext-to-text, image-to-text, and text-image-to-text‚Äîtotaling 56 images/prompts and 112 responses, originating from the CLoT-Oogiri-Go dataset used in CVPR2024.
  - Downloads: 39
- [DataPilot/in-foxhound-ja](https://huggingface.co/datasets/DataPilot/in-foxhound-ja)
  - This dataset provides Japanese instruction data, translated using KUJIRA, primarily focused on investment, Berkshire Hathaway, and Warren Buffett, originally from glaive-ai‚Äôs in-foxhound.
  - Downloads: 39
- [DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k)
  - ÂêàÊàêÊó•Êú¨Ë™ûÊåáÁ§∫„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàQwen2.5-32B-instructÔºâ
  - Downloads: 37
- [alfredplpl/commoncatalog-cc-by-ext](https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext)
  - This repository extends the CommonCatalog CC-BY dataset with English and Japanese captions generated by Phi-3 models, enabling commercial use under the CC-BY license and efficient streaming access via a CSV file and Hugging Face Datasets.
  - Downloads: 36
- [YANS-official/senryu-debug](https://huggingface.co/datasets/YANS-official/senryu-debug)
  - This dataset facilitates evaluation of large language models on *okashi* (humorous response) generation through two tasks ‚Äì text-to-text and image-to-text ‚Äì with corresponding prompts and IDs.
  - Downloads: 34
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-CC)
  - This repository provides automatically generated Q&A data sourced from Common Crawl, utilizing the Mixtral-8x22B-Instruct-v0.1-GGUF model, with randomized text excerpts to reduce reliance on original content, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 33
- [waddledee/three_line_summarization_for_japanese_news_articles](https://huggingface.co/datasets/waddledee/three_line_summarization_for_japanese_news_articles)
  - This dataset provides 3,907 rows of 3-line summaries from Livedoor News, formatted with prompts for Llama v2, and recommends adding "[R_START]" and "[R_END]" as special tokens for training.
  - Downloads: 33
- [blastai/Open_o1_sft_Pro_translated_jp](https://huggingface.co/datasets/blastai/Open_o1_sft_Pro_translated_jp)
  - This dataset is a Japanese translation of the Open_o1_sft_Pro dataset, generated using Qwen2.5-14B-Instruct, following a specific conversation template and inheriting the original dataset's license.
  - Downloads: 29
- [hpprc/llmjp-warp-html](https://huggingface.co/datasets/hpprc/llmjp-warp-html)
  - This dataset converts level 2 filtered data from the llm-jp-corpus-v3 warp_html into Hugging Face format, adding article titles sourced from original URLs, all licensed under CC-BY 4.0.
  - Downloads: 28
- [iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Elzya-13B)
  - This repository provides a Japanese dataset generated by ELYZA-japanese-Llama-2 for evaluating AI text detection and self-instruct methods, sourced from GPT-4-Self-Instruct-Japanese with licensing details mirroring the base model.
  - Downloads: 26
- [speed/english_quotes_ja](https://huggingface.co/datasets/speed/english_quotes_ja)
  - This dataset provides Japanese translations of English quotes from the Abirate/english_quotes dataset, generated using the llm-jp/llm-jp-3.7b-instruct model and licensed under CC BY 4.0.
  - Downloads: 22
- [iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B](https://huggingface.co/datasets/iam-ajaymeena/Self-Instruct-Japanese-Qwen1.5-14B)
  - This repository provides a Japanese dataset generated by Qwen/Qwen1.5-14B for evaluating AI text detection and supporting self-instruct methods, based on instructions from the GPT-4-Self-Instruct-Japanese dataset.
  - Downloads: 22
- [creative-graphic-design/CAMERA](https://huggingface.co/datasets/creative-graphic-design/CAMERA)
  - CAMERA is a Japanese ad text generation dataset (BCP-47 ja-JP) designed to advance research in multimodal ad text generation models.
  - Downloads: 21
- [swdq/ethics](https://huggingface.co/datasets/swdq/ethics)
  - This Japanese dataset provides 10K-100K question-answer pairs labeled with "evil" and "justice" ‚Äì generated using anthracite-org/magnum-v4-12b ‚Äì for both classification and generation tasks, licensed under Apache-2.0.
  - Downloads: 19
- [alfredplpl/genai-terminology-en-ja](https://huggingface.co/datasets/alfredplpl/genai-terminology-en-ja)
  - This repository provides a Japanese-English glossary of generative AI terminology, intended to improve translation quality with models like GPT-4, though accuracy isn't guaranteed.
  - Downloads: 17
- [toshi456/NLVR-JA](https://huggingface.co/datasets/toshi456/NLVR-JA)
  - This repository provides a Japanese translation of the NLVR (Natural Language for Visual Reasoning) dataset, originally developed by lil-lab for visual reasoning tasks.
  - Downloads: 15
- [ryota39/open_preference_v0.2](https://huggingface.co/datasets/ryota39/open_preference_v0.2)
  - This repository provides a Japanese RLHF dataset, reformatted from open_preference_v0.1 as a classification task (chosen/rejected labels), acknowledging potential quality issues due to synthetic and translated text.
  - Downloads: 15
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA-other](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA-other)
  - This repository provides automatically generated Japanese Q&A data using MaziyarPanahi's Mixtral-8x22B-Instruct-v0.1-GGUF, sourced from team-created data and Common Crawl (with its terms of use applying), and designed with low textual reliance on original sources, though cleaning is recommended due to potential unnatural phrasing.
  - Downloads: 14
- [kai271/TinyStories-Japanese](https://huggingface.co/datasets/kai271/TinyStories-Japanese)
  - This repository provides a dataset of approximately 3000 Japanese children‚Äôs stories, synthetically generated with GPT-4o-mini using simplified vocabulary, as detailed in the linked research paper.
  - Downloads: 14
- [ayousanz/reazon-speech-v2-all-WAND-SNR-analyze](https://huggingface.co/datasets/ayousanz/reazon-speech-v2-all-WAND-SNR-analyze)
  - This repository presents a JSON dataset (reazonspeech-all-wada-snr.json) containing SNR values and transcriptions from analysis of the reazon-research/reazonspeech-v2[all] dataset, with 1,208,360 entries exceeding an SNR of 100, and analysis performed with resources from AiHUB.
  - Downloads: 13
- [kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated](https://huggingface.co/datasets/kurogane/DSR1D-qwen-2.5-32B-aya-ja-1k-generated)
  - This repository provides 1k Japanese responses generated using DeepSeek-R1-Distill-Qwen-32B on the aya-ja-evol-instruct-calm3-dpo-masked dataset, exhibiting potential issues with `<think>` token generation and limited practical usability.
  - Downloads: 13
### Syntactic Text Processing
- [hotchpotch/sentence_transformer_japanese](https://huggingface.co/datasets/hotchpotch/sentence_transformer_japanese)
  - This repository provides Japanese datasets restructured for easy SentenceTransformers training, particularly contrastive learning, filtered with reranking scores from multiple Hugging Face datasets like LLM-JP, MSMARCO-JA, and hpprc/emb.
  - Downloads: 529
- [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs)
  - This repository provides a modified parsing and chunking method for Wikipedia data, built upon a fork of singletongue/wikipedia-utils and utilizing data crawled between December 5-8, 2023.
  - Downloads: 92
- [Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)
  - This dataset annotates the Aratako/Magpie-Tanuki-8B-97k dataset‚Äîcreated by applying the Magpie method to Tanuki-8B‚Äîwith difficulty, quality, and category labels using cyberagent/calm3-22b-chat, based on specific prompting instructions for each annotation type.
  - Downloads: 86
- [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki)
  - This dataset provides clean, structurally-preserved text extracted from the January 1, 2024 Japanese Wikipedia dump, offering article text and paragraph data suitable for various NLP tasks, alongside pre-processing scripts.
  - Downloads: 76
- [Sakalti/Multilingal-sakalt-data](https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data)
  - This repository provides a multilingual dataset released under the MIT license.
  - Downloads: 73
- [p1atdev/danbooru-ja-tag-pair-20241015](https://huggingface.co/datasets/p1atdev/danbooru-ja-tag-pair-20241015)
  - This repository provides a dataset of approximately 150,000 Danbooru tag pairs‚Äîlinking English tags to corresponding Japanese tags‚Äîcreated using wiki data, fasttext filtering, and the calm3-22b-chat LLM to improve accuracy and ensure each pair includes at least one Japanese translation.
  - Downloads: 71
- [deepghs/erairaws_infos](https://huggingface.co/datasets/deepghs/erairaws_infos)
  - This repository integrates data from erai-raws and MyAnimeList for over 2056 anime, providing IDs and resources like RSS feeds, publication dates, and links to platforms such as AniDB, Kitsu, and LiveChart.
  - Downloads: 71
- [numad/yuho-text-2023](https://huggingface.co/datasets/numad/yuho-text-2023)
  - ÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆurlÂàó„ÅåÂá∫ÂÖ∏„Å®„Å™„Çä„Åæ„Åô„ÄÇ
  - Downloads: 69
- [HachiML/alpaca_jp_python](https://huggingface.co/datasets/HachiML/alpaca_jp_python)
  - Alpaca-jp-python is a Japanese synthetic dataset, curated by HachiML under the Apache 2.0 license, generated using the Stanford Alpaca methodology and Mistral AI's Mixtral-8x22B-Instruct-v0.1, and accessible via the `datasets` library.
  - Downloads: 68
- [hatakeyama-llm-team/CommonCrawlPDFJa](https://huggingface.co/datasets/hatakeyama-llm-team/CommonCrawlPDFJa)
  - This repository provides Japanese text data extracted from PDF files sourced from CommonCrawl, along with associated code.
  - Downloads: 64
- [Aratako/Magpie-Tanuki-8B-97k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-97k)
  - Magpie-Tanuki-8B-97k is a 97,269-record Japanese dialogue dataset created by applying the Magpie method to the weblab-GENIAC/Tanuki-8B-dpo-v1.0 model, potentially including lower-quality records due to lack of post-filtering.
  - Downloads: 48
- [ThePioneer/Artificial-super-girlfriend-for-fine-tuning](https://huggingface.co/datasets/ThePioneer/Artificial-super-girlfriend-for-fine-tuning)
  - This dataset provides approximately 2800 high-quality (beauty score 87+) images of a generated virtual girlfriend (versions 2.1 & 2.6) designed to mitigate portrait rights issues, with over 1000 images scoring 90+ for beauty.
  - Downloads: 46
- [team-hatakeyama-phase2/LLMChat](https://huggingface.co/datasets/team-hatakeyama-phase2/LLMChat)
  - This dataset contains 2139 human-evaluated question-answer pairs and ratings from LLMChat, a system used to benchmark 13 LLMs‚Äîincluding Tanuki and Llama-3‚Äîby comparing their responses in a side-by-side, human preference assessment between August 19-25, 2024.
  - Downloads: 37
- [kurogane/metamath_ja_950_reka3flash](https://huggingface.co/datasets/kurogane/metamath_ja_950_reka3flash)
  - This repository provides a cleaned version of the first 1000 MetaMathQA problems, translated to Japanese using RekaAI/reka-flash-3 and filtered to maintain formatting, resulting in a dataset of approximately 950 problems requiring further output validation.
  - Downloads: 33
- [p1atdev/novecomi-novel-metadata](https://huggingface.co/datasets/p1atdev/novecomi-novel-metadata)
  - This project scrapes metadata (excluding full text) from the Dengeki Bunko novecomi website.
  - Downloads: 29
- [Calvin-Xu/Furigana-Aozora](https://huggingface.co/datasets/Calvin-Xu/Furigana-Aozora)
  - This dataset provides furigana annotations derived from Aozora Bunko and SAPIE braille data, with 307 validation-corrected mismatches identified in the original text.
  - Downloads: 28
- [Sakalti/hachiwari](https://huggingface.co/datasets/Sakalti/hachiwari)
  - This repository likely hosts content related to the popular Japanese character "Chiikawa" („Å°„ÅÑ„Åã„Çè), potentially including assets or a related project inspired by the "hachiwari" creator.
  - Downloads: 27
- [Calvin-Xu/Furigana-NDLBIB](https://huggingface.co/datasets/Calvin-Xu/Furigana-NDLBIB)
  - This dataset provides furigana annotations derived from the National Diet Library's bibliographic data, with validation performed to remove 5064 instances of mismatches.
  - Downloads: 26
- [den2nova/den2niji](https://huggingface.co/datasets/den2nova/den2niji)
  - This repository provides a 280-image dataset of female illustrations generated with Niji Journey v5, intended for LoRA model training and merging transparency, including some copyrighted characters, with accompanying tags‚Äîusage is permitted with restrictions against illegal or harmful activity and copyright infringement.
  - Downloads: 24
- [LiuliFox/stickers](https://huggingface.co/datasets/LiuliFox/stickers)
  - „Çã„Çä„ÅÆ„Çπ„ÉÜ„ÉÉ„Ç´„Éº is a collection of fun stickers featuring the character Ruri.
  - Downloads: 23
- [hpprc/quiz-works](https://huggingface.co/datasets/hpprc/quiz-works)
  - This dataset contains freely reusable quiz questions sourced from Quiz Works as of August 4-5, 2024, suitable for RAG and document retrieval systems, with acknowledgement to Quiz Works.
  - Downloads: 23
- [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database)
  - This repository provides a dataset of fungal traits‚Äîmorphological and ecological characteristics extracted from fungal descriptions using natural language processing‚Äîcurrently for casual use only, as academic purposes are discouraged due to potential inaccuracies in the automatically extracted data.
  - Downloads: 21
- [tzmtwtr/tw-posts-japanese-v2](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese-v2)
  - This repository provides a refined dataset of 50,000 Delite posts by data creator t_w, corrected and restructured for embedding learning, with usage limited to training and prohibiting redistribution under Japanese law.
  - Downloads: 21
- [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA)
  - This repository provides automatically generated Q&A data created using Mixtral-8x22B, derived from various openly licensed (CC-BY-SA-3.0, Apache-2.0) sources and designed with low textual similarity to the originals, though cleaning is recommended.
  - Downloads: 21
- [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja)
  - This repository provides a Japanese question-answering dataset of approximately 1,300 manually created pairs focused on Databricks, sourced from official blogs, FAQs, and Qitta posts by Databricks employees.
  - Downloads: 20
- [tzmtwtr/tw-posts-japanese](https://huggingface.co/datasets/tzmtwtr/tw-posts-japanese)
  - This repository provides a processed dataset of 50,000 Delite posts by user t_w, optimized for embedding learning, with usage permitted for training but redistribution prohibited under Japanese law.
  - Downloads: 20
- [cc-clean/CC-MAIN-2019-51](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-51)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA (Information-technology Promotion Agency).
  - Downloads: 20
- [Kendamarron/multiturn-qwen2.5-32b](https://huggingface.co/datasets/Kendamarron/multiturn-qwen2.5-32b)
  - This repository provides a 3-turn multi-turn instruction dataset generated with Qwen/Qwen2.5-32B-Instruct-AWQ, containing mixed English and Chinese records requiring potential filtering, and based on instructions from Aratako/Magpie-Tanuki-8B-annotated-96k.
  - Downloads: 19
- [tomo1222/Japanese-QA111dataset](https://huggingface.co/datasets/tomo1222/Japanese-QA111dataset)
  - Manually created data is provided within this repository.
  - Downloads: 18
- [hpprc/quiz-no-mori](https://huggingface.co/datasets/hpprc/quiz-no-mori)
  - This dataset contains freely reusable quiz questions sourced from Quiz Forest as of August 5, 2024, suitable for RAG, document search, and other applications, with a permissive license allowing commercial use and redistribution‚Äîrespectful usage of Quiz Forest is requested.
  - Downloads: 18
- [hpprc/jsec](https://huggingface.co/datasets/hpprc/jsec)
  - JSEC„Éõ„Éº„É†„Éö„Éº„Ç∏ likely provides resources and information related to the Japan Software Engineering Certification (JSEC) exam and its associated fields.
  - Downloads: 15
- [aki-0421/commoncatalog-cc-by-ja-300k](https://huggingface.co/datasets/aki-0421/commoncatalog-cc-by-ja-300k)
  - This repository provides the first 300k entries of a Japanese image-caption dataset, extending alfredplpl/commoncatalog-cc-by-ja with images resized to within 512px, licensed under CC-BY.
  - Downloads: 15
- [HachiML/Evol-Alpaca-gen3-500](https://huggingface.co/datasets/HachiML/Evol-Alpaca-gen3-500)
  - Evol-Alpaca-gen3-500 is a Japanese synthetic dataset, created using the Evol-Instruction method and mistralai's Mixtral-8x22B-Instruct-v0.1, based on Stanford Alpaca‚Äôs seed tasks and licensed under Apache 2.0.
  - Downloads: 14
- [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja)
  - ApolloCorpus-ja is a 525k instruction-tuning dataset comprised of automatically translated Japanese versions of the open-source ApolloCorpus, a multilingual medical dataset, with a note of potential translation errors for careful LLM use in healthcare.
  - Downloads: 14
- [DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-qwen2.5-32B-10K-ja)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, formatted as JSON, and licensed under Apache-2.0, designed for training and evaluating large language models.
  - Downloads: 13
- [hama-jp/magpie-qwen-turbo-27k](https://huggingface.co/datasets/hama-jp/magpie-qwen-turbo-27k)
  - This repository provides a 26,728-subset of high-quality, annotated Japanese instruction data‚Äîfiltered for quantity and regenerated with Qwen-2.5-turbo‚Äîintended for fine-tuning small language models for general chat (excluding coding) focusing on information seeking, reasoning, planning, and editing tasks.
  - Downloads: 13
- [hotchpotch/jaqket_v1_qa_wikija_context](https://huggingface.co/datasets/hotchpotch/jaqket_v1_qa_wikija_context)
  - ‚ö†
  - Downloads: 13
- [turing-motors/LLaVA-v1.5-Instruct-620K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-v1.5-Instruct-620K-JA)
  - Japanese LLaVA v1.5 Instruct 620K is a 620K Japanese translation of the LLaVA v1.5 Visual Instruct dataset, intended for visual instruction tuning in Japanese language models under a CC BY-NC-4.0 license.
  - Downloads: 12
- [infinity-blackhole/nhentai](https://huggingface.co/datasets/infinity-blackhole/nhentai)
  - This repository provides a CBZ-format manga dataset from Nhentai, including adult content and metadata, for research in image analysis (segmentation, panel detection) and Japanese text recognition.
  - Downloads: 11
- [lissette/Nanami-Chiaki-audio](https://huggingface.co/datasets/lissette/Nanami-Chiaki-audio)
  - This repository provides speech data for Chiaki Nanami from the *Danganronpa* series.
  - Downloads: 11
### Responsible NLP
- [cc-clean/CC-MAIN-2019-39](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-39)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes with copyright restrictions beyond that.
  - Downloads: 1,987
- [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010)
  - This repository provides the Japanese Web Corpus 2010, licensed for research purposes only, with automatically added punctuation via morphological analysis, including conversion scripts and tools.
  - Downloads: 1,098
- [DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-llm-jp-3-13b-20k)
  - This repository provides a 20,000-sample Japanese instruction-following dataset automatically generated using the LLM-JP 3.13B Instruct model, featuring instructions, optional Chain-of-Thought reasoning, and self-refined responses in JSON format.
  - Downloads: 920
- [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k)
  - This dataset comprises manually extracted FAQs from Japanese government websites, licensed under CC-BY-4.0, intended for instruction tuning of large language models and serving as a linked resource.
  - Downloads: 342
- [hpprc/paraphrase-qa](https://huggingface.co/datasets/hpprc/paraphrase-qa)
  - This dataset contains query-answer pairs generated by an LLM, based on paraphrases of Japanese Wikipedia text, and is released under a CC-BY-SA 4.0 license.
  - Downloads: 184
- [JunSotohigashi/JapaneseWikipediaTypoDataset_kanji](https://huggingface.co/datasets/JunSotohigashi/JapaneseWikipediaTypoDataset_kanji)
  - This HuggingFace dataset provides filtered Japanese Wikipedia typo data‚Äîspecifically kanji conversion errors‚Äîsplit into pre- and post-error text segments for typo correction tasks.
  - Downloads: 140
- [cc-clean/CC-MAIN-2019-30](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-30)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to IPA for resource support.
  - Downloads: 98
- [kanhatakeyama/SyntheticText](https://huggingface.co/datasets/kanhatakeyama/SyntheticText)
  - This repository provides text regenerated with Phi-3 from large (tens of GB) datasets‚ÄîWikibooks, Wikipedia, Cosmopedia, and legal case data‚Äîpotentially requiring Git LFS for full access, with some computations performed on the TSUBAME4.0 supercomputer.
  - Downloads: 73
- [cc-clean/CC-MAIN-2019-35](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-35)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, offered for research purposes with copyright restrictions requiring permission for non-research use, and acknowledges support from IPA (Information-technology Promotion Agency).
  - Downloads: 70
- [SousiOmine/Japanese-Pythonic-FunctionCall](https://huggingface.co/datasets/SousiOmine/Japanese-Pythonic-FunctionCall)
  - This repository provides a low-quality dataset of 1002 examples for enabling Python function calling in chat LLMs, generated with Qwen2.5 and Phi-4, and containing potential issues with missing/Chinese tools and repetitive/low-quality responses.
  - Downloads: 58
- [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst2 dataset, automatically translated with DeepL and formatted for fine-tuning large language models, with code provided to convert it into an instruction-output format.
  - Downloads: 58
- [kanhatakeyama/SyntheticTextWikiTranslate](https://huggingface.co/datasets/kanhatakeyama/SyntheticTextWikiTranslate)
  - This repository provides a large, automatically translated Japanese-English corpus generated by re-generating random Japanese text from Wikibooks, Wikipedia, and code using Phi-3, potentially requiring Git LFS for full access due to its size (tens of GB).
  - Downloads: 53
- [weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-evol-instruct-calm3-dpo-masked)
  - This repository provides publicly released models and datasets under a license agreement prohibiting enjoyment of expressed ideas/emotions, disclaiming quality guarantees, and requiring legal compliance from users and any third parties accessing the content.
  - Downloads: 46
- [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja)
  - This dataset provides a ShareGPT-formatted, multi-turn Japanese conversation dataset derived from OpenAssistant/oasst1-89k-ja, suitable for fine-tuning large language models (requiring significant computational resources).
  - Downloads: 37
- [RJZ/ConceptNetSyntheticPhi3Text_ja](https://huggingface.co/datasets/RJZ/ConceptNetSyntheticPhi3Text_ja)
  - This dataset contains Japanese sentences generated by Phi-3 based on ConceptNet 5.7 triples, aiming to express relationships between subjects, relations, and objects.
  - Downloads: 36
- [Aruno/guanaco_jp](https://huggingface.co/datasets/Aruno/guanaco_jp)
  - This repository provides Japanese prompts from the GuanacoDataset, identified and extracted using language detection.
  - Downloads: 36
- [kanhatakeyama/multiturn-conv-from-aozora-bunko](https://huggingface.co/datasets/kanhatakeyama/multiturn-conv-from-aozora-bunko)
  - This repository provides automatically generated multi-turn dialogue data created with Calm3-22B-chat, based on randomly selected text from the Aozora Bunko library, using a simplified version of "I Am a Cat" as a test case.
  - Downloads: 29
- [weblab-GENIAC/aya-ja-nemotron-dpo-masked](https://huggingface.co/datasets/weblab-GENIAC/aya-ja-nemotron-dpo-masked)
  - This repository provides openly published models and datasets under a license agreement requiring users to respect intellectual property, acknowledge no warranty, and ensure compliance with all applicable laws when using and sharing the content.
  - Downloads: 25
- [ikeno-ada/Japanese-English_translation_of_contents_HScodes](https://huggingface.co/datasets/ikeno-ada/Japanese-English_translation_of_contents_HScodes)
  - This repository provides Japanese postal service data (as of May 9, 2024) for translating item descriptions in international mail (English, Chinese) and includes HS codes.
  - Downloads: 22
- [weblab-GENIAC/Open-Platypus-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/Open-Platypus-Japanese-masked)
  - This repository provides publicly available models and datasets under a license requiring users to respect intellectual property, accept no warranty of quality or legality, and ensure compliance with all applicable laws.
  - Downloads: 20
- [weblab-GENIAC/jwinogrande](https://huggingface.co/datasets/weblab-GENIAC/jwinogrande)
  - This repository provides publicly released models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 19
- [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja)
  - This repository provides a deduplicated and preprocessed version of the mmarco query-passage dataset, with IDs referencing indices in the `collection` subset for direct data access, and adhering to the original dataset's license.
  - Downloads: 18
- [MakiAi/Tokama_Club_QA](https://huggingface.co/datasets/MakiAi/Tokama_Club_QA)
  - This dataset provides question-answer pairs about characters from the *Touhou Project*'s Tokama Club, formatted as a CSV for use in training chatbots, question answering systems, and machine learning models.
  - Downloads: 18
- [masajek/openassistant-guanaco-ja](https://huggingface.co/datasets/masajek/openassistant-guanaco-ja)
  - This repository provides a Japanese-only subset of the Open Assistant dataset, sourced from the timdettmers/openassistant-guanaco base dataset.
  - Downloads: 17
- [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja)
  - This repository provides a deduplicated and pre-processed Japanese question-passage dataset (mqa) with cleaned text and NFKC normalization, accessible via indexed IDs in the `collection` data.
  - Downloads: 15
- [weblab-GENIAC/OpenBookQA-Japanese-masked](https://huggingface.co/datasets/weblab-GENIAC/OpenBookQA-Japanese-masked)
  - This repository provides publicly released models and datasets under a license agreement prohibiting commercial use, disclaiming warranty, and requiring adherence to legal compliance by users and any third parties.
  - Downloads: 15
- [weblab-GENIAC/jbbh](https://huggingface.co/datasets/weblab-GENIAC/jbbh)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use, disclaiming warranties, and requiring adherence to legal compliance by users and any recipients.
  - Downloads: 15
- [weblab-GENIAC/jarc](https://huggingface.co/datasets/weblab-GENIAC/jarc)
  - This repository provides publicly released models and datasets under a license agreement prohibiting commercial use, disclaiming warranty, and requiring adherence to legal compliance by both users and any third parties it's shared with.
  - Downloads: 15
- [kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b](https://huggingface.co/datasets/kanhatakeyama/CreativeCommons-RAG-QA-Mixtral8x22b)
  - This repository provides automatically generated Japanese question-answer pairs for Retrieval-Augmented Generation (RAG) using data from Wikibooks, Wikipedia, and case law, intended for pre-training rather than direct instruction-tuning, with some computations performed on the TSUBAME4.0 supercomputer.
  - Downloads: 14
- [kenkensz9/kenkensz9_1242tw2](https://huggingface.co/datasets/kenkensz9/kenkensz9_1242tw2)
  - This dataset comprises 1243 carefully selected tweets (May 2022 - May 2024) known for expressing nuanced ideas or unique perspectives, intended for fine-tuning language models‚Äîparticularly for character development and tweet generation with fixed system/input prompts.
  - Downloads: 14
- [hpprc/ja-en-r1-distill-qwen](https://huggingface.co/datasets/hpprc/ja-en-r1-distill-qwen)
  - This dataset contains Japanese translations of English Wikipedia text generated using DeepSeek-R1-Distill-Qwen-32B, including input/output pairs and few-shot examples, released under a CC-BY-SA 4.0 license.
  - Downloads: 13
- [DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k](https://huggingface.co/datasets/DeL-TaiseiOzaki/magpie-reasonig-ja-qwen2.5-72b-16k)
  - This dataset comprises approximately 16,000 automatically generated Japanese instruction-following samples‚Äîincluding instructions, reasoning steps, initial & refined responses‚Äîcreated using the Qwen2.5 72B Instruct model for LLM training and evaluation.
  - Downloads: 12
- [aipracticecafe/wataoshi-dialogues-rp](https://huggingface.co/datasets/aipracticecafe/wataoshi-dialogues-rp)
  - This dataset contains cleaned lines from the anime ‚ÄúMy Next Life as a Villainess,‚Äù featuring dialogue primarily from the character Lay and responses from Claire, with no ownership or responsibility assumed by the creator.
  - Downloads: 11
- [weblab-GENIAC/jhellaswag](https://huggingface.co/datasets/weblab-GENIAC/jhellaswag)
  - This repository provides publicly available models and datasets under a license agreement prohibiting commercial use and disclaiming any warranty or liability from the publisher.
  - Downloads: 11
- [cc-clean/CC-MAIN-2019-49](https://huggingface.co/datasets/cc-clean/CC-MAIN-2019-49)
  - This repository provides a Japanese-only dataset extracted from CommonCrawler using cc-downloader-rs, intended for research purposes and requiring copyright permissions for other uses, with gratitude to the IPA (Information-technology Promotion Agency).
  - Downloads: 11
### Reasoning
- [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli)
  - JSNLI is a Japanese natural language inference (NLI) dataset, a translation of the SNLI benchmark, provided in TSV format with morphologically analyzed text using JUMAN++.
  - Downloads: 156
- [Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-easy-math-instruction-88k-qwen2.5-bakeneko-32b-instruct)
  - This dataset consists of synthetic instruction-following data generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct, filtered to include only records where responses from two distinct system prompts (logical/math assistant vs. Python programmer) matched.
  - Downloads: 143
- [elyza/JaMARD](https://huggingface.co/datasets/elyza/JaMARD)
  - JaMARD is a high-quality, assured-correctness Japanese mathematical dataset generated by translating English PRM800K & GSM8K problems and providing chain-of-thought reasoning.
  - Downloads: 130
- [nlp-waseda/JCQ](https://huggingface.co/datasets/nlp-waseda/JCQ)
  - JCQ is a Japanese NLP dataset of 700 creativity questions‚Äîbased on the Torrance Test of Creative Thinking‚Äîdesigned to evaluate creative thinking through tasks like unusual uses, consequence prediction, and hypothetical scenarios.
  - Downloads: 108
- [sbintuitions/JSQuAD](https://huggingface.co/datasets/sbintuitions/JSQuAD)
  - JSQuAD is a Japanese reading comprehension dataset based on SQuAD 1.1, utilizing a 2021 Wikipedia dump and licensed under CC BY-SA 4.0, aimed at ensuring reproducible evaluation scores and providing a modified version of SB Intuitions.
  - Downloads: 86
- [jaeyong2/ja-reasoning](https://huggingface.co/datasets/jaeyong2/ja-reasoning)
  - This repository provides a 100k question-answering dataset in Japanese, generated using Gemini Pro 2.5, and licensed under the Open Data Commons Attribution license.
  - Downloads: 83
- [tohoku-nlp/abc-multiple-choice](https://huggingface.co/datasets/tohoku-nlp/abc-multiple-choice)
  - This dataset, abc-multiple-choice, is a Japanese multiple-choice question answering dataset built from questions used in the ‚Äúabc‚Äù quiz competition, intended for research purposes only and detailed in the NLP2024/JLR2024 paper.
  - Downloads: 82
- [Manual-Dataset-Creation-Project/Malum-230](https://huggingface.co/datasets/Manual-Dataset-Creation-Project/Malum-230)
  - Malum-230 is a human-crafted Japanese dataset of multi-turn conversations and passages for training and evaluating logical reasoning capabilities in language models like Qwen2.5-7B.
  - Downloads: 82
- [Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered](https://huggingface.co/datasets/Aratako/magpie-reasoning-llama-nemotron-70b-100k-filtered)
  - This dataset filters the magpie-reasoning-llama-nemotron-70b-100k dataset to exclude examples lacking refined answers, then converts the remaining data into OpenAI message format.
  - Downloads: 81
- [Inoichan/OpenO1-SFT-JA](https://huggingface.co/datasets/Inoichan/OpenO1-SFT-JA)
  - OpenO1-SFT (Japanese Translation) is a 77,312-sample dataset of Japanese-translated Chain of Thought reasoning examples, created using google/gemma-2-27b-it for fine-tuning language models.
  - Downloads: 63
- [DeL-TaiseiOzaki/reasoning-finetuning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/reasoning-finetuning-ja)
  - Êó•Êú¨Ë™ûÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà Ê¶ÇË¶Å „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅSkunkworksAI/reasoning-0.01 „Å´Âê´„Åæ„Çå„Çã„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„Éá„Éº„Çø„ÇíÂü∫„Å´„ÄÅQwen/Qwen2.5-32B-Instruct „É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶‰ΩúÊàê„Åó„ÅüÊó•Êú¨Ë™ûÁâà„ÅÆÊåáÁ§∫„ÉªÊé®Ë´ñ„ÉªÂõûÁ≠î„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 59
- [Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct](https://huggingface.co/datasets/Kendamarron/magpie-japanese-math-instruction-17k-qwen2.5-bakeneko-32b-instruct)
  - This repository provides a 17k synthetic instruction dataset for Japanese mathematical problems, generated with Magpie using rinna/qwen2.5-bakeneko-32b-instruct and filtered for consistent answers across two system prompts‚Äîone for a logical assistant and another for a Python-coding programmer.
  - Downloads: 58
- [hotchpotch/japanese-qa-reasoning-100k](https://huggingface.co/datasets/hotchpotch/japanese-qa-reasoning-100k)
  - This dataset contains Japanese question-answer pairs and reasoning traces generated by DeepSeek-R1 from the fineweb2-edu-japanese dataset, licensed under ODC-By.
  - Downloads: 55
- [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja)
  - This repository provides a commercially-usable, 1.8 million instruction-tuning dataset‚Äîa Japanese translation of OpenMathInstruct-1‚Äîcontaining question-solution pairs for mathematics, generated with Mixtral-8x7B and validated against GSM8K and MATH benchmarks under a permissive NVIDIA license.
  - Downloads: 51
- [Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted](https://huggingface.co/datasets/Aratako/magpie-qwen2.5-32b-reasoning-100k-formatted)
  - magpie-qwen2.5-32b-reasoning-100k-formatted DeL-TaiseiOzaki/magpie-qwen2.5-32b-reasoning-100k„ÇíOpenAI messagesÂΩ¢Âºè„Å´Â§âÊèõ„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 51
- [Inoichan/NuminaMath-Enhanced-CoT-JA-50K](https://huggingface.co/datasets/Inoichan/NuminaMath-Enhanced-CoT-JA-50K)
  - This repository offers a 50k-subset of the NuminaMath CoT dataset, enhanced to promote multistep, reflective reasoning in Japanese language models by encouraging repeated self-evaluation of solution steps.
  - Downloads: 50
- [sbintuitions/JCommonsenseQA](https://huggingface.co/datasets/sbintuitions/JCommonsenseQA)
  - JCommonsenseQA is a Japanese multiple-choice question answering dataset, based on CommonsenseQA and ConceptNet, designed to evaluate commonsense reasoning abilities and built using crowdsourcing, licensed under CC BY-SA 4.0.
  - Downloads: 48
- [DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-reasoning-ja)
  - This repository provides a high-quality, 1800-example Japanese instruction-following, reasoning, and answer dataset generated using the Qwen/Qwen2.5-32B-Instruct model, based on SkunkworksAI/reasoning-0.01.
  - Downloads: 46
- [Inoichan/KUM-Bench](https://huggingface.co/datasets/Inoichan/KUM-Bench)
  - KUM-Bench is a Japanese reasoning benchmark, utilizing challenging 2023 Kyoto University math entrance exam questions to evaluate the problem-solving skills of Large Language Models.
  - Downloads: 43
- [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset)
  - Sakura_dataset is a small, high-quality Japanese dataset licensed for commercial use, featuring commonsense and mathematical reasoning problems sourced from existing and custom-created datasets, with example code for use with models like rinna/japanese-gpt-neox-3.6b.
  - Downloads: 42
- [DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k](https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-qwen2.5-32b-reasoning-100k)
  - This repository provides a 125,000-sample Japanese instruction-following dataset automatically generated using the Qwen2.5-32B-instruct model, featuring diverse, multi-persona instructions and Chain-of-Thought reasoning, in JSONL format under the Apache-2.0 license.
  - Downloads: 24
- [aixsatoshi/Chat-with-cosmopedia](https://huggingface.co/datasets/aixsatoshi/Chat-with-cosmopedia)
  - This repository provides a high-density, multi-turn Japanese conversation dataset, synthetically created from the Japanese-localized COSMOPEDIA, focusing on reasoning, knowledge, and interactive dialogue.
  - Downloads: 22
- [zenless-lab/jnli](https://huggingface.co/datasets/zenless-lab/jnli)
  - JGLUE-JNLI is a Japanese Natural Language Inference (NLI) dataset for evaluating relationships‚Äîentailment, contradiction, or neutral‚Äîbetween premise and hypothesis sentences.
  - Downloads: 21
- [Inoichan/NuminaMath-CoT-JA-100K](https://huggingface.co/datasets/Inoichan/NuminaMath-CoT-JA-100K)
  - This repository provides a 100k-sample Japanese translation of the NuminaMath CoT dataset, featuring math problems and their Chain of Thought solutions, translated using google/gemma-2-27b-it.
  - Downloads: 20
- [zenless-lab/jsem](https://huggingface.co/datasets/zenless-lab/jsem)
  - JSeM is a Japanese semantic test suite, based on FraCaS, designed for evaluating textual entailment by assessing native speaker judgments of implication, neutrality, or contradiction between premise-hypothesis pairs.
  - Downloads: 19
- [Kendamarron/jimba-instruction-simplify-200](https://huggingface.co/datasets/Kendamarron/jimba-instruction-simplify-200)
  - This dataset consists of 200 simplified instruction-following examples derived from the Kendamarron/jimba-instuction-1k-beta dataset, created to replicate the "Wizard LM" In-depth evolving process as a result of collaboration between a Discord server and Metadata Lab at a local AI hackathon.
  - Downloads: 14
- [sionic-ai/LogicJa](https://huggingface.co/datasets/sionic-ai/LogicJa)
  - LogicJa is a 210-question, multi-turn Japanese language benchmark evaluating reasoning across seven domains‚Äîmath, writing, coding, understanding, grammar, culture, and general logic‚Äîwith 30 questions per category.
  - Downloads: 12
- [misdelivery/OpenMathInstruct-ja-phi-3-medium-test](https://huggingface.co/datasets/misdelivery/OpenMathInstruct-ja-phi-3-medium-test)
  - kunishou/OpenMathInstruct-1-1.8m-ja „ÅÆquestion_ja„Çí„ÇÇ„Å®„Å´phi-3-medium„Å´„Çà„Çä„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„ÇíÁî®„ÅÑ„Å™„ÅÑÂΩ¢Âºè„ÅßÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ
  - Downloads: 12
### Responsible & Trustworthy NLP
- [tokyotech-llm/swallow-magpie-ultra-v0.1](https://huggingface.co/datasets/tokyotech-llm/swallow-magpie-ultra-v0.1)
  - This repository releases a 42k Japanese-English parallel dataset‚Äîa subset of Swallow-Magpie-Ultra-v0.1‚Äîintended for instruction tuning of the tokyotech-llm/Llama-3.1-Swallow models.
  - Downloads: 106
- [Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data](https://huggingface.co/datasets/Nexdata/10341_Hours_Unsupervised_Spontaneous_Japanese_Speech_Data)
  - This dataset provides a 10,341-hour sample of unsupervised Japanese speech spanning 28 domains, offering quality-tested data for improving AI performance while prioritizing user privacy and legal compliance.
  - Downloads: 75
- [tarudesu/gendec-dataset](https://huggingface.co/datasets/tarudesu/gendec-dataset)
  - Gendec is a machine learning framework and dataset for gender detection from Japanese names, detailed in a research paper accepted to ISDA'23, intended for research purposes only.
  - Downloads: 71
- [Aratako/LLMChat-Judge-Results](https://huggingface.co/datasets/Aratako/LLMChat-Judge-Results)
  - This repository contains pairwise comparison data evaluating responses from two LLMChat models using various other models, created to verify the consistency between human and automated (open LLM) evaluations, and licensed under the terms of team-hatakeyama-phase2/LLMChat.
  - Downloads: 61
- [ibm/AttaQ-JA](https://huggingface.co/datasets/ibm/AttaQ-JA)
  - AttaQ-JA Dataset Card AttaQ red teaming dataset was designed to evaluate Large Language Models (LLMs) by assessing their tendency to generate harmful or undesirable responses, which consists of 1402 carefully crafted adversarial questions.
  - Downloads: 52
- [longisland3/NMLE](https://huggingface.co/datasets/longisland3/NMLE)
  - This repository provides a dataset of past Japanese medical licensing exam questions (NMLE, 110th-117th exams) for model evaluation, RAG, and exam overviews, licensed under CC-BY-NC-ND 4.0 for non-commercial use.
  - Downloads: 45
- [ibm-research/AttaQ-JA](https://huggingface.co/datasets/ibm-research/AttaQ-JA)
  - AttaQ-JA is a 1402-question Japanese red-teaming dataset designed to evaluate LLM vulnerability to generating harmful responses through adversarial prompting, containing potentially offensive content.
  - Downloads: 37
- [p1atdev/LLM-jp-Toxicity-Dataset](https://huggingface.co/datasets/p1atdev/LLM-jp-Toxicity-Dataset)
  - LLM-jp Toxicity Dataset provides a Japanese language dataset for evaluating and mitigating toxicity in large language models, linked to further details at [https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset](https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset).
  - Downloads: 25
- [hpprc/janli](https://huggingface.co/datasets/hpprc/janli)
  - JaNLI is a Japanese adversarial Natural Language Inference (NLI) dataset, modeled after HANS, designed to test model understanding of Japanese linguistics and identify vulnerabilities.
  - Downloads: 21
- [naist-nlp/multils-japanese](https://huggingface.co/datasets/naist-nlp/multils-japanese)
  - This repository provides access to code for generating a private, non-public dataset designed to prevent unintentional inclusion in LLM training data.
  - Downloads: 15
- [MilosNaniwa/WarChestDojo](https://huggingface.co/datasets/MilosNaniwa/WarChestDojo)
  - This repository provides a dataset detailing game states with unit characteristics ‚Äì including ID, class, team, location, and visibility ‚Äì identified by unique game, state, and snapshot IDs, and timestamps.
  - Downloads: 14
### Multilinguality and Text Generation
- [turing-motors/Cauldron-JA](https://huggingface.co/datasets/turing-motors/Cauldron-JA)
  - Cauldron-JA is a Japanese vision-language dataset created by translating the Idefics2 fine-tuning dataset, The Cauldron, excluding OCR, coding, and graph-related data to preserve data consistency.
  - Downloads: 1,187
- [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja)
  - This repository provides a Japanese translation of the databricks-dolly-15k dataset, licensed under CC-BY-SA-3.0, for use in multilingual AI/NLP applications.
  - Downloads: 669
- [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)
  - This repository provides a Japanese translation of the OpenAssistant/oasst1 dataset, flagging failed translations ("ng_translation": 1) where English and Japanese texts are identical, with recent manual corrections to code-related translation errors.
  - Downloads: 116
- [sappho192/Tatoeba-Challenge-jpn-kor](https://huggingface.co/datasets/sappho192/Tatoeba-Challenge-jpn-kor)
  - This repository provides a Japanese-Korean paired text dataset from Helsinki-NLP/Tatoeba-Challenge for training machine translation models, excluding commercial use.
  - Downloads: 63
- [EQUES/japanese_ultrachat_6.6k](https://huggingface.co/datasets/EQUES/japanese_ultrachat_6.6k)
  - Japanese Ultrachat 6.6k is a 7,532-example Japanese dataset, machine-translated from a subset of ultrachat_200k using DeepSeek-R1-Distill-Qwen-32B-Japanese, with IDs corresponding to the original data.
  - Downloads: 47
- [Hoshikuzu/Japanese-Law-Translation](https://huggingface.co/datasets/Hoshikuzu/Japanese-Law-Translation)
  - This repository provides a 260k sentence Japanese-English parallel corpus of legal texts sourced from the Japanese Law Translation database, accessible via the `datasets` library.
  - Downloads: 39
- [Ego/jpflan-raw](https://huggingface.co/datasets/Ego/jpflan-raw)
  - This repository provides a collection of ~40 natively-Japanese, open-source datasets for downstream tasks and instruction finetuning of Large Language Models (LLMs).
  - Downloads: 39
- [YYama0/CT-RATE-JPN](https://huggingface.co/datasets/YYama0/CT-RATE-JPN)
  - CT-RATE-JPN is a deduplicated, Japanese-translated version of the CT-RATE dataset, offering chest CT volumes and radiology reports to support Japanese medical AI development.
  - Downloads: 37
- [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation)
  - This dataset provides 69K Japanese translations of the ‚Äúdatabricks-dolly-15k‚Äù dataset, licensed under CC BY SA 3.0, for ja-en translation tasks.
  - Downloads: 31
- [mohamed-khalil/KaidanNihonbunka](https://huggingface.co/datasets/mohamed-khalil/KaidanNihonbunka)
  - Kaidan Nihonbunka is a dataset of Japanese ‚Äúkaidan‚Äù (ghost stories) linked to the Hyakumonogatari ritual, offering a resource for studying Japanese folklore and supernatural tales.
  - Downloads: 28
### Information Retrieval
- [hpprc/reranker-scores](https://huggingface.co/datasets/hpprc/reranker-scores)
  - This dataset provides reranking scores from five multilingual/Japanese rerankers (BGE, GTE, RURI, RURIv3, ja-CE) applied to existing Japanese search/QA datasets containing positive and negative example documents for each query.
  - Downloads: 1,597
- [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa)
  - This repository provides AutoWikiQA, the largest freely available Japanese question-answering dataset generated using Swallow-MX from Wikipedia text, notable for its diverse question-answer formats and suitability for QA model training and RAG development.
  - Downloads: 283
- [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever)
  - This dataset provides QA pairs for training document retrieval models, sourced from cl-tohoku/quiz-datasets and the ‚ÄúAI King‚Äù competition, with varied licensing including CC BY-SA for some quiz questions and Wikipedia content.
  - Downloads: 80
- [ayousanz/vtuber-youtube-list-dataset](https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset)
  - This dataset provides JSONL-formatted metadata for YouTube channels, labeled as either VTuber (1) or non-VTuber (0), to facilitate text classification model training and evaluation, primarily in Japanese with potential mixed languages, and licensed under MIT.
  - Downloads: 68
- [llm-book/aio-passages](https://huggingface.co/datasets/llm-book/aio-passages)
  - This repository provides the passage dataset used in the ‚ÄúAI King‚Äù competition featured in the book *Introduction to Large Language Models*, sourced from cl-tohoku/quiz-datasets and licensed under CC BY-SA 3.0 and GFDL.
  - Downloads: 50
- [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja)
  - This dataset provides Japanese question-answering pairs with human-retrieved Wikipedia articles, detailing the data collection process and structure for research purposes.
  - Downloads: 46
- [CausalLM/Retrieval-SFT-Chat](https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat)
  - This repository provides 1 million synthetic, long-context, multi-turn chat entries‚Äîderived from rewritten web text using large language models‚Äîfor continued pre-training and research on data/internet culture.
  - Downloads: 45
- [kunishou/cosmopedia-100k-ja-preview](https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-preview)
  - This dataset provides automatically translated Japanese versions of records 20k-100k from the cosmopedia-100k dataset, excluding those with translation errors, and will be merged with and then removed from a 0-20k translation effort.
  - Downloads: 19
- [llm-book/jawiki-20220404-c400](https://huggingface.co/datasets/llm-book/jawiki-20220404-c400)
  - Jawiki-20220404-c400 is a dataset of short (‚â§400 characters) passages from Japanese Wikipedia, used as a baseline for the AIÁéã question answering competition.
  - Downloads: 14
### Sentiment Analysis
- [KakologArchives/KakologArchives](https://huggingface.co/datasets/KakologArchives/KakologArchives)
  - This dataset archives 11 years of historical comments from the now-defunct "Niconico Realtime Commentary" service, preserving a valuable record of user interactions before its 2020 discontinuation and subsequent data loss.
  - Downloads: 4,280,614
- [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)
  - ELYZA-tasks-100 is a 100-example Japanese dataset for evaluating instruction-tuned models, featuring complex tasks‚Äîincluding summarization, reasoning, and creative generation‚Äîwith annotated evaluation perspectives to ensure consistent assessment.
  - Downloads: 3,046
- [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment)
  - This dataset provides a binary positive/negative sentiment classification version of the WRIME Japanese sentiment analysis dataset, intended for use with the "Large Language Model Introduction" book's sample code.
  - Downloads: 541
- [bandad/sayoko-tts-corpus](https://huggingface.co/datasets/bandad/sayoko-tts-corpus)
  - This repository provides the Sayoko TTS corpus, an 81-year-old female Japanese voice dataset with both noisy (wav_noise) and denoised (wav) audio files, alongside phoneme labels for speech synthesis tasks, downloadable from Google Drive or Hugging Face Hub.
  - Downloads: 157
- [Kendamarron/jimba-instuction-1k-beta](https://huggingface.co/datasets/Kendamarron/jimba-instuction-1k-beta)
  - This repository provides a manually checked and corrected Japanese instruction dataset generated from the outputs of the calm2-7b-chat model, detailed in the linked Zenn article.
  - Downloads: 119
- [WarriorMama777/databricks-dolly-15k-ja_cool](https://huggingface.co/datasets/WarriorMama777/databricks-dolly-15k-ja_cool)
  - This dataset adapts the databricks-dolly-15k instruction-following dataset to mimic the emotionless speech patterns of the character Yuki Nagato from "The Melancholy of Haruhi Suzumiya" by modifying Japanese politeness levels.
  - Downloads: 13
### Linguistics & Cognitive NLP
- [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja)
  - This repository provides FuguMT-translated and manually refined English sentences originally from the `chosen` dataset within the `helpful-base` portion of the anthropics/hh-rlhf project.
  - Downloads: 30
- [asahi-research/newsq_test](https://huggingface.co/datasets/asahi-research/newsq_test)
  - NewsQ is a freely available Japanese question-answering benchmark for current affairs, distributed via Hugging Face under a terms of use agreement requiring user registration and data submission for access and usage monitoring.
  - Downloads: 21
